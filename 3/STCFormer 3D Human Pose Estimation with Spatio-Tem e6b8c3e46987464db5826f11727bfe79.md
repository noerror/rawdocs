# STCFormer: 3D Human Pose Estimation with Spatio-Temporal Criss-cross Attention

*이 논문에서는 동영상에서 3D 인체 포즈 추정을 위한 혁신적인 모델인 시공간 교차 트랜스포머(Spatio-Temporal Criss-cross Transformer)를 소개합니다. STC포머는 두 가지 경로 접근 방식을 사용하여 공간과 시간적 상관관계를 별도로 탐색합니다. 인체의 동적 체인 구조를 통합하여 새로운 위치 임베딩 기능을 생성합니다. STC포머 모델은 3D 인체 포즈 추정을 위한 두 가지 벤치마크 데이터 세트에서 수행한 실험에서 최첨단 기법을 능가하는 우수한 성능과 일반화 능력을 입증했습니다.*

[https://github.com/zhenhuat/STCFormer](https://github.com/zhenhuat/STCFormer)

[https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2023_paper.pdf](https://openaccess.thecvf.com/content/CVPR2023/papers/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2023_paper.pdf)

### 1. Introduction

3D 사람 포즈 추정은 인간과 로봇의 상호작용, 가상 현실, 동작 예측 등 다양한 응용 분야에서 중요한 연구 분야입니다. 기존 방식은 먼저 2D 키포인트를 감지한 다음 3D로 변환하지만, 이 방식은 깊이 데이터 부족과 모호성으로 인한 문제가 있습니다.

그리드 컨볼루션, 그래프 컨볼루션, 다층 퍼셉트론과 같은 다양한 기술을 사용하여 더 나은 포즈 추정을 위해 비디오 시퀀스에서 시간적 단서를 사용하는 몇 가지 개선 사항이 제안되었습니다. 자연어 처리(NLP)와 컴퓨터 비전(CV) 모두에서 강력한 도구인 트랜스포머 구조는 3D 사람 포즈 추정을 위한 시공간적 상관관계를 모델링하는 데 가능성을 보여주었습니다.

하지만 트랜스포머 모델은 프레임 수에 따라 계산이 4배로 증가하기 때문에 특히 전체 비디오 시퀀스에 적용할 경우 계산 비용이 많이 들 수 있습니다. 따라서 현재 트랜스포머 구조는 대부분 각 프레임에 대한 공간 정보를 먼저 처리한 다음 시간적 트랜스포머를 사용하여 시퀀스를 컴파일하도록 설계되어 있습니다. 그러나 이러한 접근 방식은 여러 프레임에 걸친 관절 간의 관계를 완전히 탐색하지 못합니다.

이 논문에서는 공간 및 시간 정보를 동시에 처리하는 시공간 교차 주의(STC)라는 새로운 메커니즘을 소개합니다. STC는 공동 특징을 두 부분으로 나누고 멀티헤드 자기 주의(MSA)를 적용하여 공간 또는 시간 축을 따라 맥락을 캡슐화합니다. 그런 다음 두 경로에서 수집한 정보를 결합하여 계산 비용을 줄이면서 공간 및 시간 축을 효과적으로 교차합니다.

![(a) 전체 비디오에서 모든 관절에 대해 공간-시간적 주의를 사용하여, (b) 공간과 시간 컨텍스트를 각각 포착하는 두 단계로 프레임워크를 분리하고, (c) 우리의 공간-시간적 십자형 주의 (STC)인 두 경로 블록이 공간과 시간 정보를 병렬로 모델링하는 3D 인간 자세 추정을 위한 공간-시간적 상관관계 모델링. 수용 영역의 시각화에서 각 주의 전략이 커버하는 관절들은 빨간색 노드로 표시됩니다.](STCFormer%203D%20Human%20Pose%20Estimation%20with%20Spatio-Tem%20e6b8c3e46987464db5826f11727bfe79/Untitled.png)

(a) 전체 비디오에서 모든 관절에 대해 공간-시간적 주의를 사용하여, (b) 공간과 시간 컨텍스트를 각각 포착하는 두 단계로 프레임워크를 분리하고, (c) 우리의 공간-시간적 십자형 주의 (STC)인 두 경로 블록이 공간과 시간 정보를 병렬로 모델링하는 3D 인간 자세 추정을 위한 공간-시간적 상관관계 모델링. 수용 영역의 시각화에서 각 주의 전략이 커버하는 관절들은 빨간색 노드로 표시됩니다.

저자들은 3D 인체 포즈 추정을 위한 새로운 아키텍처인 STCFormer를 소개합니다. 이 아키텍처는 여러 개의 STC 블록을 쌓아 구축됩니다. 또한 인체의 정적 및 동적 부분을 더 잘 이해하기 위해 구조 강화 위치 임베딩(SPE)이라는 새로운 유형의 위치 임베딩을 포함합니다.

실험 결과, 제안된 STCFormer 아키텍처는 더 적은 수의 파라미터를 사용함에도 불구하고 Human3.6M 및 MPI-INF-3DHP 데이터 세트에서 기존의 최첨단 기법을 능가하는 성능을 보였습니다.

### 2. Related Work

단안 3D 인체 자세 추정이란 이미지나 2D 좌표와 같은 단일 2D 관점에서 3D 공간에서 인체 관절 위치를 파악하는 작업입니다. 초기 방법에서는 그래픽 또는 제한적인 접근 방식을 사용하여 인체 골격과 공간 관계를 이해했습니다. 딥러닝이 등장하면서 3D 포즈 추정을 위한 다양한 신경망이 개발되었는데, 이는 1단계와 2단계로 나뉩니다.

1단계 방법은 입력 이미지에서 3D 포즈를 직접 계산하므로 많은 양의 이미지 포즈 데이터와 상당한 연산 자원이 필요합니다. 2단계 방법은 먼저 2D 포즈 감지기를 사용하여 2D 관절 좌표를 추정하고, 완전 연결 네트워크, 그리드 컨볼루션 네트워크, 순환 신경망 또는 그래프 컨볼루션 네트워크와 같은 기술을 사용하여 이러한 2D 좌표를 3D 공간으로 변환합니다. 2단계 방법은 1단계 방법보다 덜 까다롭지만 깊이 정보가 부족하기 때문에 여전히 깊이 모호성이라는 문제가 있습니다.

이를 극복하기 위해 최근의 기술은 비디오 시퀀스에서 인접 프레임의 시간적 컨텍스트를 사용하여 3D 좌표 추정을 개선합니다. 시간적 완전 컨볼루션 네트워크(TCN), 시퀀스에서 중요한 프레임/포인트를 식별하는 주의 메커니즘, 시공간 그래프 컨볼루션 네트워크가 사용되는 방법 중 하나입니다.

트랜스포머 기반 방법도 시공간적 상관관계를 모델링하는 데 사용됩니다. 주목할 만한 방법으로는 포즈포머, MH포머, 스트라이디드포머, 크로스포머, PATA, 믹스스테 등이 있으며, 모두 공간 및 시간 트랜스포머 블록을 다양한 방식으로 사용하여 공간 및 시간 상관관계를 개별적으로 또는 반복적으로 모델링합니다.

이 논문에서는 3D 인체 포즈 추정을 위한 새로운 트랜스포머 기반 방법인 STCFormer를 소개합니다. 공간 정보 모델링과 시간 정보 모델링을 분리하는 기존 방법과 달리 STCFormer는 병렬 경로에서 두 가지를 동시에 모델링합니다. 그런 다음 이러한 경로는 다층 퍼셉트론(MLP)을 사용하여 결합됩니다. 또한 STC포머는 인체의 국소 구조를 탐색하기 위해 특별히 설계된 새로운 위치 임베딩 기능을 통합합니다.

### 3. Spatio-Temporal Criss-cross Transformer

시공간적 십자형 트랜스포머(STCFormer)는 이 제안의 기초가 되는 트랜스포머 아키텍처에 대한 간략한 검토로 시작됩니다. 트랜스포머는 다용도 표현 학습 아키텍처로, 주로 멀티헤드 자기 주의 모듈(MSA)과 피드 포워드 네트워크(FFN)의 두 가지 모듈로 구성됩니다. MSA는 토큰 간 선호도 행렬을 계산하고 여러 토큰에 걸쳐 정보를 전파합니다. 이는 공식적으로 방정식 (1)로 나타낼 수 있습니다. FFN은 기본적으로 두 개의 선형 레이어와 그 사이에 GELU 활성화 함수가 있는 비선형 매핑인 다중 레이어 퍼셉트론(MLP)을 포함합니다. MLP의 출력은 방정식 (2)에 의해 주어집니다.

![우리가 제안한 공간-시간적 십자형 변환기 (STCFormer)의 개요. (a) 그것은 주로 L 순차적 STC 블록으로 구성됩니다. 각 블록은 공간-시간적 십자형 주의를 통해 토큰 간의 컨텍스트를 집계하고, 다중 계층 퍼셉트론 (MLP)을 통해 각 토큰을 비선형으로 매핑합니다. (b) 우리의 STC 블록과 구조 강화 위치 임베딩 (SPE)의 구조.](STCFormer%203D%20Human%20Pose%20Estimation%20with%20Spatio-Tem%20e6b8c3e46987464db5826f11727bfe79/Untitled%201.png)

우리가 제안한 공간-시간적 십자형 변환기 (STCFormer)의 개요. (a) 그것은 주로 L 순차적 STC 블록으로 구성됩니다. 각 블록은 공간-시간적 십자형 주의를 통해 토큰 간의 컨텍스트를 집계하고, 다중 계층 퍼셉트론 (MLP)을 통해 각 토큰을 비선형으로 매핑합니다. (b) 우리의 STC 블록과 구조 강화 위치 임베딩 (SPE)의 구조.

이를 통해 각 트랜스포머 블록은 단축 연결을 통해 MSA와 MLP를 순서대로 활용하여 구성됩니다. 이는 방정식 (3)으로 표현됩니다. 한 블록의 출력은 마지막 블록까지 다음 블록의 입력으로 사용됩니다.

그림 2(여기에는 표시되지 않음)에 설명된 제안된 STC포머는 조인트 기반 임베딩, 스택형 STC 블록, 회귀 헤드의 세 단계로 구성됩니다. 조인트 기반 임베딩은 각 조인트의 입력 2D 좌표를 특징 공간에 투영합니다. STC 블록은 시공간적 컨텍스트를 집계하여 각 관절의 표현을 업데이트합니다. 최종 회귀 헤드는 학습된 특징을 사용하여 3D 좌표를 추정합니다.

관절 기반 임베딩에서는 먼저 2D 포즈 시퀀스가 관절 기반 임베딩 레이어에 의해 고차원 임베딩으로 투영됩니다. 이 레이어는 각 2D 좌표에 독립적으로 FC 레이어를 적용한 다음 GELU를 활성화합니다. 이전 트랜스포머와 달리 공간 차원을 유지하므로 시공간적 교차 주의로 계산 비용을 절감할 수 있습니다.

STC 블록은 표준 트랜스포머 블록에서 파생된 것으로, 원래의 MSA 레이어를 시공간적 교차 주의로 대체합니다. 또한 새로운 위치 임베딩 기능인 구조 강화 위치 임베딩(SPE)을 통합하여 로컬 구조에 대한 설명 능력을 향상시킵니다.

아키텍처의 마지막 부분은 3D 포즈 좌표를 추정하기 위해 STC 블록 위에 설정된 선형 회귀 헤드입니다. 이 아키텍처는 추정된 3D 좌표와 실측 3D 좌표 간의 평균 제곱 오차(MSE)를 최소화하여 최적화됩니다. 이 최적화 프로세스는 방정식 (4)로 표현됩니다.

3.3 시공간 교차 주의

시공간 교차 주의(STC)는 전체 시공간 주의의 4제곱 계산 비용을 회피하기 위해 조인트 간의 시공간 의존성을 보다 효율적으로 포착합니다. 저자들은 채널을 병렬 그룹으로 나누고 각 그룹에 서로 다른 피처 컨텍스트화 작업을 적용하는 그룹 컨텍스트화에서 영감을 얻은 전략을 활용합니다. 이 프로세스는 공간적 맥락과 시간적 맥락 모두에 대해 발생하며, 이러한 연산은 별도의 자기 주의 모듈에서 계산됩니다.

![(a) 다른 관절의 운동 궤적의 계수 행렬. (b) 신체 관절이 다섯 부분, 즉 g∗로 나뉩니다. 관련성이 높은/낮은 부분은 각각 밝은/어두운 파란색으로 색칠됩니다. 움직임 데이터는 Human3.6M의 훈련 세트에서 행동하는 배우 S6에 의해 생성됩니다.](STCFormer%203D%20Human%20Pose%20Estimation%20with%20Spatio-Tem%20e6b8c3e46987464db5826f11727bfe79/Untitled%202.png)

(a) 다른 관절의 운동 궤적의 계수 행렬. (b) 신체 관절이 다섯 부분, 즉 g∗로 나뉩니다. 관련성이 높은/낮은 부분은 각각 밝은/어두운 파란색으로 색칠됩니다. 움직임 데이터는 Human3.6M의 훈련 세트에서 행동하는 배우 S6에 의해 생성됩니다.

매트릭스 X로 표시되는 입력 임베딩은 먼저 쿼리(Q), 키(K), 값(V)에 매핑됩니다. 그런 다음 채널 차원을 따라 시간 그룹과 공간 그룹의 두 그룹으로 나뉩니다. 시간적 상관관계는 여러 프레임에 걸쳐 조인트 간의 관계를 포착하는 축별 다중 헤드 자기 주의(MSAT)를 통해 계산됩니다. 단일 프레임에서 조인트 간의 연결을 나타내는 공간적 상관관계는 축별 MSA 구성요소(MSAS)를 사용하여 유사하게 계산됩니다.

그런 다음 두 주의 레이어의 출력을 채널 차원을 따라 연결하여 공간 및 시간 축의 십자형과 유사한 수용 필드를 형성합니다. 여러 STC 블록을 쌓으면 시스템은 완전한 시공간적 주의에 근접할 수 있습니다.

3.4 구조적으로 향상된 위치 임베딩

위치 임베딩은 트랜스포머 모델에서 매우 중요합니다. STC포머의 경우, 저자들은 신체 관절의 국소 구조를 활용하는 구조 강화 위치 임베딩(SPE)을 제안합니다. 관절은 인체의 동적 체인 구조에 따라 다섯 가지 범주로 분류됩니다. 각 그룹에 위치 임베딩이 할당되어 같은 부위의 관절이 동일한 임베딩 벡터를 갖도록 합니다.

그러나 신체의 일부 부위는 상대적인 움직임을 보이므로 관절의 궤적이 밀접하게 관련되어 있지 않습니다. 이러한 경우 이러한 관절에 동일한 임베딩 벡터를 할당하면 모션 패턴을 간과할 수 있습니다. 이 문제를 해결하기 위해 인접한 조인트에 시공간 컨볼루션 연산을 적용하여 로컬 구조를 캡처합니다.

SPE 함수는 모든 조인트에서 동시에 사용되므로 정적/동적 판단을 결정할 필요가 없습니다. 이 이중 기능 접근 방식을 통해 다양한 움직임 패턴을 가진 부품을 처리할 수 있습니다. 이 SPE 함수를 STC에 주입하면 결과 방정식은 STC 방정식에 SPE 항을 추가합니다.

저자들은 파이토치 플랫폼에서 몇 줄의 파이썬 코드를 사용하여 SPE가 포함된 STC 블록을 쉽게 구현할 수 있다고 주장합니다. 여기에는 표준 트랜스포머에서 사전 정의된 MSA 및 MLP 함수와 SPE를 위한 기본 임베딩 레이어 및 Conv2d 레이어 구성이 포함됩니다.

### 4. Experiments

STCFormer 아키텍처는 두 가지 대규모 데이터 세트에서 광범위하게 평가되었습니다: Human3.6M과 MPI-INF-3DHP입니다. 이 섹션에서 도출된 핵심 사항은 다음과 같습니다:

Human3.6M: 이 데이터 세트는 실내 3D 인체 포즈 추정용으로 널리 사용됩니다. 11명의 피사체가 15개의 일반적인 동작을 수행하며 총 360만 개의 비디오 프레임이 포함되어 있습니다. 프로토콜 1(P1)과 프로토콜 2(P2)의 두 가지 프로토콜에서 평균 관절 위치 오류(MPJPE)가 평가 지표로 사용됩니다.

MPI-INF-3DHP: 이 대규모 데이터 세트는 그린 스크린, 비그린 스크린, 실외의 세 가지 장면으로 구성됩니다. 훈련용 8가지 활동과 평가용 7가지 활동을 수행하는 8명의 배우를 기록합니다. 사용된 평가 지표는 MPJPE(P1), 150mm의 정확한 키포인트 비율(PCK), 곡선 아래 면적(AUC) 결과입니다.

구현 측면에서 모델은 PyTorch 툴킷으로 구현되었으며 GTX 2080Ti GPU가 장착된 서버에서 실행되었습니다. 훈련을 위해 각 미니 배치는 128개의 시퀀스로 구성되었으며, 네트워크 파라미터는 기본 학습률 0.001의 아담 옵티마이저로 20개의 에포크에 대해 최적화하고 각 에포크마다 0.96씩 감쇠시켰습니다.

STCFormer 모델은 Human3.6M 데이터 세트에서 여러 최신 기법과 비교한 결과, 15개 범주 중 10개 범주에서 보고된 최고의 성능을 달성하며 놀라운 성능을 보였습니다. 지상 실측 2D 포즈를 입력으로 사용할 때, 후처리를 거친 STCFormer는 21.3mm의 최고 P1 오류를 기록했습니다. 전반적으로 이 결과는 다양한 유형의 입력에 대한 STCFormer의 영향과 효과, 그리고 가장 낮은 평균 오차와 다양한 오차 범위에 걸쳐 더 나은 분포를 얻을 수 있는 능력을 입증합니다.

이 섹션에서는 MPI-INF-3DHP 데이터 세트에서 STCFormer 모델의 성능을 평가했습니다. 주요 내용은 다음과 같습니다:

MPI-INF-3DHP에서의 성능: STCFormer는 MPI-INF-3DHP 데이터 세트에서 최신 모델을 능가하는 성능을 보였으며, PCK 98.7%, AUC 83.9%, P1 오류 23.1mm로 보고된 최고의 성능을 달성했습니다. 특히, 이 복잡한 데이터 세트에서 훨씬 더 큰 P1 오차 감소로 MixSTE 모델을 능가하여 STC포머의 견고성과 일반화 기능을 입증했습니다.

절제 연구: 모델의 성능을 더 잘 이해하기 위해 CPN으로 추정된 2D 포즈를 입력으로 사용하여 Human3.6M 데이터 세트에 대한 제거 연구를 수행했습니다. 그 결과, 입력 프레임 수를 늘리면 성능이 향상되는 것을 관찰할 수 있었습니다. 또한 모델 성능에 대한 각 디자인 구성 요소의 기여도를 평가했습니다. 공간적 주의 경로와 시간적 주의 경로 모두 오차를 크게 줄였으며, 이 두 축을 따라 상관관계를 모델링하는 것이 중요하다는 것을 강조했습니다. 다양한 위치 임베딩 전략도 오류 감소에 기여했습니다.

![STCFormer의 공간 및 시간 주의 모듈에서의 주의 지도 시각화. x축과 y축은 각각 쿼리와 예측된 출력에 해당합니다.](STCFormer%203D%20Human%20Pose%20Estimation%20with%20Spatio-Tem%20e6b8c3e46987464db5826f11727bfe79/Untitled%203.png)

STCFormer의 공간 및 시간 주의 모듈에서의 주의 지도 시각화. x축과 y축은 각각 쿼리와 예측된 출력에 해당합니다.

정성적 분석: 주의 시각화를 통해 모델이 다양한 동작의 비디오에서 관절 간의 서로 다른 패턴을 학습하고, 인간 동작의 연속성으로 인해 인접한 프레임 간에 강한 상관관계를 보이는 것을 관찰했습니다. 3D 인간 포즈 추정 시각화에서는 복잡한 포즈 관절이 있는 까다로운 동작을 포함하여 다양한 동작에 걸쳐 3D 포즈를 재구성하는 데 있어 STCFormer의 성능이 더 우수하다는 것을 보여주었습니다.

![StridedFormer [22], MHFormer [23], P-STMO [39] 그리고 우리의 STCFormer에 의한 3D 자세 추정의 예. 회색 골격은 실제 3D 자세입니다. 파란색, 오렌지색, 그리고 녹색 골격은 각각 추정된 인간 몸의 왼쪽 부분, 오른쪽 부분, 그리고 몸통을 나타냅니다.](STCFormer%203D%20Human%20Pose%20Estimation%20with%20Spatio-Tem%20e6b8c3e46987464db5826f11727bfe79/Untitled%204.png)

StridedFormer [22], MHFormer [23], P-STMO [39] 그리고 우리의 STCFormer에 의한 3D 자세 추정의 예. 회색 골격은 실제 3D 자세입니다. 파란색, 오렌지색, 그리고 녹색 골격은 각각 추정된 인간 몸의 왼쪽 부분, 오른쪽 부분, 그리고 몸통을 나타냅니다.

이 분석은 3D 인체 포즈 추정 작업의 다양한 과제를 처리하는 데 있어 STCFormer 모델이 효과적이라는 점을 강조합니다.

### 5. Conclusion

결론적으로 이 논문에서는 동영상에서 3D 인체 포즈 추정을 위한 시공간 교차 트랜스포머(Spatio-Temporal Criss-cross Transformer)를 소개했습니다. STC포머 모델은 두 가지 경로 접근 방식으로 공간 및 시간적 상관관계를 탐색하고 인체의 동적 연쇄 구조를 활용하여 새로운 위치 임베딩 함수로 로컬 컨텍스트를 모델링합니다.

STC포머는 채널 차원을 따라 관절 특징을 두 그룹으로 나누고 각 그룹에 대한 공간적, 시간적 상호 작용을 모델링하는 여러 STC 블록을 사용합니다. 이렇게 하면 공간 및 시간 축을 가로지르는 STC 블록의 수용 필드가 생성됩니다.

두 개의 벤치마크 데이터 세트에 대한 실험을 통해 STC포머의 효과와 일반화 능력이 입증되었으며, 3D 인체 포즈 추정에 유망한 방법임을 보여주었습니다. 다른 최신 기법을 능가하는 성능을 보여줌으로써 공간적, 시간적 상관관계를 포착하는 혁신적인 접근 방식과 인체 구조를 모델에 성공적으로 통합한 것을 검증했습니다.

- 입력과 출력
    
    이 연구에서 제안된 Spatio-Temporal Criss-cross Transformer (STCFormer)는 비디오에서 3D 인간 자세 추정을 위해 공간적 상관관계와 시간적 상관관계를 탐색합니다.
    
    입력:
    STCFormer의 입력은 2D 포즈 데이터입니다. 연구에서는 MPI-INF-3DHP와 Human3.6M(P-STMO, VideoPose3D) 과 같은 데이터 세트에서 추출된 비디오 프레임에서 이 2D 포즈 데이터를 사용했습니다. 이 모델은 일련의 프레임(예: 9, 27 또는 81 프레임)을 사용하여 시간에 따른 공간적 관계와 시간적 상관관계를 모델링합니다.
    
    출력:
    STCFormer의 출력은 비디오 내의 인간의 3D 자세 추정치입니다. 이는 골격 관절의 3D 좌표로 표현되며, 시간에 따른 인간 동작의 공간적 및 시간적 표현을 포착하려고 시도합니다. 이 출력은 주로 행동 인식, 비디오 분석, 컴퓨터 비전 등의 애플리케이션에서 사용됩니다.