# CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation with Transformers

[https://github.com/huaaaliu/RGBX_Semantic_Segmentation](https://github.com/huaaaliu/RGBX_Semantic_Segmentation)

[https://arxiv.org/pdf/2203.04838.pdf](https://arxiv.org/pdf/2203.04838.pdf)

### I. INTRODUCTION

이 문서는 자율주행차(AV)의 장면 이해, 특히 이미지를 의미론적으로 의미 있는 영역으로 변환하는 이미지 시맨틱 분할을 사용하여 자율주행차가 정보에 입각한 의사 결정을 내리는 데 도움을 주는 방법에 대해 설명합니다. 하지만 현재 모델에는 몇 가지 한계가 있습니다. 예를 들어, 비슷한 색상이나 질감을 가진 물체를 구별하는 데 어려움을 겪을 수 있습니다.

이러한 문제를 극복하기 위해 적색, 녹색, 청색(RGB) 이미지를 깊이, 열, 편광, 이벤트 정보, LiDAR 데이터와 같은 다른 유형의 센서 데이터와 융합하는 범용 모델이 제안되었습니다. RGB-X 시맨틱 세분화라고 하는 이 새로운 접근 방식은 장면 이해를 향상시킵니다.

현재 멀티 모달 시맨틱 분할 방법은 일반적으로 두 가지 범주로 나뉩니다. 첫 번째 유형은 단일 네트워크를 사용하여 RGB와 다른 모달리티에서 특징을 추출하고, 두 번째 유형은 두 개의 개별 네트워크를 사용하여 두 모달리티에서 특징을 추출한 다음 이를 융합합니다.

이 문서에서는 RGB-X 시맨틱 세분화를 향상시키기 위해 새로운 범용 모델인 CMX를 제안합니다. 이 모델은 특징 상호 작용 및 융합을 위해 두 개의 모듈이 있는 두 개의 스트림 아키텍처(RGB 및 X-모달)를 사용합니다. 크로스 모달 특징 정류 모듈(CM-FRM)은 바이 모달 특징을 보정하여 두 스트림이 상호 보완적인 정보 단서에 집중하고 서로 다른 모달의 불확실성과 노이즈 측정의 영향을 완화할 수 있도록 지원합니다. 기능 융합 모듈(FFM)은 교차 주의 메커니즘과 혼합 채널 임베딩을 활용하여 수정된 기능을 의미 예측을 위한 단일 기능으로 병합하여 모달리티 조합 간에 더 나은 상호 작용을 가능하게 합니다.

제안된 CMX 모델은 5개의 서로 다른 다중 모달 시맨틱 세분화 작업에 대해 테스트되었으며, 긍정적인 결과를 도출하고 특수 아키텍처보다 우수한 성능을 보였습니다. 또한 연구진은 모델을 추가로 테스트하기 위해 새로운 RGB-이벤트 시맨틱 세분화 벤치마크를 설정했습니다.

요약하자면, 연구진은 RGB 이미지 데이터와 다양한 센서 모달리티를 결합하여 AV가 주변 환경을 더 잘 이해할 수 있는 새로운 프레임워크를 모색하고 있습니다. CMX로 알려진 이 시스템은 초기 테스트에서 유망한 결과를 보여주었습니다.

### 2. RELATED WORK

섹션 2, '관련 연구'에서는 이 주제와 관련된 선행 연구를 두 가지 주요 클러스터로 분류하여 자세히 살펴봅니다: 트랜스포머 기반 시맨틱 세그멘테이션과 멀티모달 시맨틱 세그멘테이션입니다.

A. 트랜스포머 기반 시맨틱 세분화: 이 글에서는 엔드 투 엔드 픽셀 분류 방식으로 고밀도 시맨틱 세그먼테이션을 처리하는 완전 컨볼루션 네트워크(FCN)의 진화에 대해 간략하게 설명합니다. 그 후 멀티스케일 특징 표현을 수집하기 위해 FCN 구조를 보강한 여러 모델이 등장했습니다. 최근에는 초기 레이어에서 글로벌 컨텍스트를 캡처하는 비전 트랜스포머를 백본으로 사용하는 모델도 여러 개 등장했습니다. 그러나 이러한 모델은 RGB 이미지만 고려하는 경우가 많아 깊이, 열화상, 편광, 이벤트 및 LiDAR 데이터와 같은 추가 데이터가 유용한 실제 조건에서는 제한적일 수 있습니다.

B. 다중 모드 시맨틱 세분화: 이 텍스트에서는 장면에 대한 이해를 돕기 위해 까다로운 실제 조건에서 멀티 모달 센싱을 사용하는 것이 바람직한지에 대해 설명합니다. 이전 작업은 대부분 RGB-심도 또는 RGB-열과 같은 단일 감지 모드 조합 시나리오에 중점을 두었습니다. 멀티 모달 시맨틱 세분화를 위한 일반적인 전략은 상호 보완적인 정보를 레이어 디자인에 통합하거나 두 개의 병렬 모달 스트림을 연결하는 융합 체계를 개발하는 것이었습니다. 그러나 이러한 접근 방식은 특정 모달리티 조합에 지나치게 한정되어 있어 일반화 가능성이 제한되는 경우가 많습니다. 이 백서의 저자는 포괄적인 교차 모달 상호 작용에 중점을 두고 다양한 멀티 모달 조합을 수용할 수 있는 보다 유연하고 일반화 가능한 접근 방식을 목표로 합니다.

### 3 PROPOSED FRAMEWORK: CMX

A. 프레임워크 개요: CMX는 두 개의 병렬 백본을 사용하여 RGB 및 X-모달 입력에서 특징을 추출합니다. 이 프레임워크는 두 모달의 특징을 정정하는 크로스 모달 특징 정정 모듈(CM-FRM)을 도입합니다. 또한 2단계 피처 퓨전 모듈(FFM)은 정류된 피처를 단일 피처 맵으로 융합한 다음 최종 시맨틱 맵으로 변환하도록 설계되었습니다.

B. 교차 모달 특징 정류: CM-FRM 모듈은 서로 다른 모달리티에 포함된 노이즈 정보를 처리하도록 설계되었습니다. 이 모듈은 두 가지 모달리티에서 나오는 특징을 채널별 및 공간별 두 가지 차원으로 보정합니다. 채널별 특징 보정은 전 세계적으로 작동하는 반면, 공간별 특징 보정은 로컬 정보를 보정합니다. 두 가지 모두 보다 정밀한 멀티 모달 특징 추출과 상호작용을 가능하게 합니다.

C. 특징 융합: FFM은 정보 상호 작용을 향상시키고 두 가지 모달리티의 특징을 단일 특징 맵으로 병합하기 위해 구축되었습니다. 이 작업은 두 단계로 이루어집니다. 첫 번째 단계에서는 교차 주의 메커니즘을 사용하여 대칭적인 이중 경로 구조를 통해 두 가지 모달 특징이 정보를 교환합니다. 두 번째 단계에서는 채널 임베딩 프로세스를 통해 이러한 피처를 단일 피처 맵으로 결합합니다. FFM의 설계에는 여러 수준의 포괄적인 상호 작용이 포함되어 있으며, 이는 감지 데이터 조합을 일반화하고 RGB-X 시맨틱 세분화를 향상시키는 데 중요합니다.

5가지 조합을 중심으로 멀티 모드 데이터 표현에 대해 자세히 설명합니다: RGB-심도, RGB-열, RGB-편광, RGB-이벤트 및 RGB-LiDAR입니다. 다음은 간략한 개요입니다:

RGB-심도: 깊이 이미지는 범위, 위치 및 윤곽 정보를 제공하므로 RGB와 결합하면 다양한 공간 위치에서 물체를 더 잘 구분하는 데 도움이 됩니다. 원시 깊이 이미지는 HHA 이미지로 인코딩됩니다(수평 차이, 지상 높이, 중력 방향에 따른 각도 캡처).

RGB-열화상: 빛이 충분하지 않은 상황에서는 물체와 배경을 구분할 수 없습니다. 열화상 이미지는 물체의 적외선 특성을 포착하여 이러한 상황에서 유용하게 사용할 수 있습니다. 단일 채널 열화상 이미지는 백본 입력과 일치하도록 3번 복사됩니다.

RGB 편광: 편광 카메라는 RGB 이미지에서 주변 환경과 혼동되는 경우가 많은 반사율이 높은 물체에 대한 추가 정보를 제공합니다. 편광 센서는 광학 편광 정보를 기록하며, 빛의 편광 패턴을 특성화하는 데 사용되는 선형 편광도(DoLP)와 선형 편광 각도(AoLP)를 사용합니다. 그런 다음 세분화를 위해 단색 또는 삼색 편광 큐로 계산됩니다.

RGB-이벤트: 이벤트 데이터는 다이나믹 레인지가 높고 시간적 해상도가 높으며 모션 블러의 영향을 받지 않으므로 동적 환경에서 유용합니다. 원시 이벤트는 사전 처리되어 RGB 이미지와 동일한 공간 차원을 가진 텐서로 변환됩니다. 이 연구에서는 다양한 이벤트 시간 빈 설정을 살펴봅니다.

RGB-LiDAR: LiDAR는 신뢰할 수 있고 정확한 공간 깊이 정보를 제공합니다. LiDAR 데이터는 RGB 이미지의 표현과 일치하는 범위 뷰 이미지와 유사한 형식으로 변환됩니다. LiDAR 3D 포인트를 2D 이미지 좌표에 투영하는 작업은 KITTI-360 데이터 세트에 따라 수행됩니다.

이러한 다양한 조합은 시맨틱 분할 성능 향상에 기여하는 더 넓은 범위의 특징을 캡처하는 것을 목표로 합니다.

### 4 EXPERIMENT DATASETS AND SETUPS

텍스트는 테스트에 사용된 실험 데이터 세트와 시맨틱 세그멘테이션 작업에서 제안한 CMX 모델의 실험 설정에 대한 개요를 제공하고 있습니다. 요약은 다음과 같습니다:

NYU Depth V2, SUN-RGBD, Stanford2D3D, ScanNetV2, Cityscapes 데이터 세트를 포함한 5개의 RGB-Depth 시맨틱 세그멘테이션 데이터 세트를 사용했습니다. 기타 모달리티 데이터세트에는 RGB-열 세분화를 위한 RGB-T MFNet, RGB-편광 세분화를 위한 RGB-P ZJU, RGB-이벤트 세분화를 위한 RGB-E EventScape, RGB-LiDAR 세분화를 위한 RGB-L KITTI-360이 포함됩니다. 이러한 데이터 세트는 실내, 실외, 낮, 밤, 다양한 기상 조건을 포함한 여러 시나리오에 걸쳐 다양한 이미지 해상도와 클래스 범주로 구성되어 있습니다. 이러한 데이터 세트는 다양한 상황과 데이터 양식의 조합에 걸쳐 CMX 모델을 검증하는 데 도움이 됩니다.

구현 세부 사항에서는 무작위 뒤집기 및 스케일링을 통해 데이터 증강을 적용하고, ImageNet에서 사전 학습된 Mix Transformer 인코더를 모델의 백본으로 사용합니다. 최적화 전략은 가중치 감쇠를 사용하는 AdamW이며, 학습 속도는 폴리 학습 속도 스케줄을 따릅니다. 손실 함수는 교차 엔트로피입니다. 모델의 성능을 측정하기 위한 주요 평가 지표는 모든 시맨틱 클래스에 걸쳐 평균화된 평균 교집합(mIoU)입니다. 보다 구체적인 설정은 부록에 자세히 설명되어 있습니다.

### 5 EXPERIMENT RESULTS AND ANALYSES

이 섹션에서는 RGB-X 시맨틱 분할을 위해 제안한 CMX(크로스 모달 퓨전) 모델의 실험 결과를 소개합니다.

A. RGB-뎁스 데이터 세트에서의 결과:
CMX는 NYU Depth V2, Stanford2D3D, SUN-RGBD, ScanNetV2, Cityscapes 등 여러 RGB-뎁스 데이터 세트에서 테스트되었습니다. CMX는 이러한 모든 데이터 세트에서 최고의 점수를 받았습니다. 이 결과는 다양한 실내 및 실외 시나리오에서 CMX의 효과와 학습 능력을 입증합니다.

B. RGB-열 데이터 세트에 대한 결과:
CMX의 일반화 가능성은 RGB-Thermal MFNet 데이터 세트에서 추가로 테스트되었습니다. 이 모델은 다른 최신 모델에 비해 우수한 성능을 보여 주며, RGB-열 방식 내부의 상호 보완적인 정보를 효과적으로 활용할 수 있는 능력을 강조합니다.

C. RGB-편광 데이터 세트에 대한 결과:
ZJU-RGB-P 데이터세트에서 CMX의 성능을 테스트한 결과, 이 모델은 다른 RGB-편광 융합 방식보다 우수한 성능을 보였습니다. 분석 결과, 삼원색 표현이 단색 표현보다 일관되게 더 나은 성능을 보였으며, 선형 편광 각도(AoLP) 및 선형 편광 정도(DoLP) 표현을 사용할 때의 이점도 나타났습니다.

이 외에도 일련의 제거 연구, 효율성 및 정성적 분석과 함께 RGB-LiDAR 데이터 세트에 대한 CMX의 결과가 발표될 예정입니다. 실험 결과는 다양한 모달리티 조합에 걸쳐 의미적 분할 작업에서 제안된 CMX 모델의 효과와 다용도성을 입증합니다.

이 요약은 멀티 모달 데이터의 의미론적 분할을 위한 새로운 모델인 CMX 모델을 제안한 연구 논문의 실험 결과를 설명한 것으로 보입니다. 이 새로운 모델은 다양한 유형의 센서 데이터를 결합하여 의미적 분할 작업의 결과를 개선합니다.

RGB 이벤트 데이터 세트의 결과

RGB-이벤트 데이터 세트에서 CMX 모델을 다른 모델과 비교한 결과, 다른 방법보다 훨씬 높은 64.28%의 mIoU(평균 교집합 대비 합집합)를 달성했습니다. 이 모델은 특히 차량, 보행자, 신호등과 같은 전경 물체의 식별을 개선하는 데 효과적이었습니다.

이 연구에서는 CMX 모델에 다른 백본을 사용할 때의 효과도 조사했습니다. 여기에는 CNN 기반 모델과 트랜스포머 기반 모델이 포함되었습니다. CMX 모델은 RGB 전용 모델보다 일관되게 우수한 성능을 보여 다양한 백본과의 호환성 및 효율성을 보여주었습니다.

RGB-LiDAR 데이터 세트의 결과

CMX 모델은 RGB-LiDAR 데이터 세트에서도 우수한 성능을 발휘하여 64.31%의 최첨단 mIoU를 달성했습니다. 이는 이전 모델에 비해 크게 개선된 것으로 모달 융합에서 대칭 듀얼 스트림 아키텍처를 사용하는 것의 가치를 보여줍니다.

절제 연구

아키텍처의 여러 부분이 세분화에 어떤 영향을 미치는지 알아보기 위해 절제 연구를 수행했습니다. CM-FRM과 FFM이라는 두 가지 모듈이 RGB-X 시맨틱 세그먼테이션에 중요한 것으로 밝혀졌습니다. 이 모듈의 변형을 테스트한 결과 전체 버전이 더 효과적인 것으로 나타났습니다.

효율성 분석

계산 효율성 측면에서 CMX 모델은 계산 복잡도가 비슷하거나 낮은 이전 모델보다 훨씬 더 높은 mIoU를 달성하며 우수한 성능을 보였습니다. 이는 CMX 모델이 정확성과 효율성의 균형이 잘 잡혀 있음을 나타냅니다.

정성적 분석

세분화 결과의 시각화는 CMX 모델이 다양한 시나리오에서 효과적이라는 것을 보여주었습니다. 특히 조도가 낮거나 움직이는 물체와 같은 까다로운 조건에서 물체를 식별하는 데 성공적이었습니다. 이는 CMX 모델이 다목적이며 강력한 시맨틱 장면 이해에 효과적이라는 주장을 뒷받침합니다.

### 6 CONCLUSION

이 연구는 RGB-X 시맨틱 분할을 성공적으로 탐색하고 자율주행차를 위한 다목적 비전 트랜스포머 기반 크로스 모달 융합 아키텍처인 CMX(Cross-Modal eXchange)를 제안했습니다. 이 모델은 멀티 모달 픽셀 단위의 시맨틱 장면 이해를 향상시켜 다양한 센서 데이터 조합을 가능하게 합니다.

이 연구에서는 상호 작용을 촉진하고 정확한 RGB-X 시맨틱 분할을 달성하기 위해 크로스 모달 기능 정류 모듈(CM-FRM)과 기능 융합 모듈(FFM)을 도입합니다. CM-FRM은 채널 및 공간별 정정을 통해 포괄적인 특징 보정을 수행합니다. FFM은 크로스 어텐션과 혼합 채널 임베딩의 조합을 통해 글로벌 정보 교환을 향상시킵니다.

또한 이 연구는 고밀도-희소 데이터 융합에 대한 CMX의 적응성을 평가하기 위해 RGB-이벤트 시맨틱 세분화 벤치마크를 설정하여 편광 및 이벤트 데이터의 효과적인 표현에 대한 인사이트를 제공합니다. 결과적으로 CMX 모델은 RGB-D, RGB-열, RGB-편광, RGB-이벤트, RGB-LiDAR 조합을 포함한 9가지 벤치마크에서 최첨단 결과를 달성했습니다.

향후 연구팀은 CMX를 확장하여 세 가지 이상의 센서 데이터를 융합하여 보다 복잡한 크로스 모달 융합을 촉진하는 것을 목표로 하고 있습니다. 또한 다운스트림 시맨틱 세분화 작업에서 서로 다른 모달리티 간의 상호 보완적인 시너지 효과를 학습하기 위해 멀티 모달 사전 학습을 탐색할 계획입니다.

### APPENDIX A

부록 A에는 실험 구현에 관한 자세한 정보가 제공됩니다. PyTorch를 사용하여 NYU Depth V2, SUN-RGBD, Stanford2D3D, ScanNetV2, Cityscapes, RGB-T MFNet, RGB-P ZJU, RGB-E EventScape, RGB-L KITTI-360을 포함한 다양한 데이터 세트에서 모델을 실행했습니다. GPU 유형, 에포크 수, 이미지 해상도, 배치 크기, 백본 유형 등 이러한 구현에 사용된 구성은 각 데이터 세트의 특정 특성 및 요구 사항에 따라 다릅니다.

부록 B에서는 도시 경관 데이터 세트의 세분화 결과부터 시작하여 보다 정성적인 분석이 제공됩니다. 노이즈가 있는 깊이 측정에서도 모드 간 상호 보완적인 특징을 보정하고 융합하는 CMX 모델의 능력이 강조되어 있습니다. 그러나 그림자가 있고 조명이 약한 지역에서는 여전히 문제가 남아 있습니다. 다양한 감지 방식에 걸친 몇 가지 실패 사례를 제시하여 모델이 만족스러운 결과를 제공하는 데 어려움을 겪었던 시나리오를 강조합니다. 또한 CM-FRM의 입력 및 수정된 특징의 시각화를 통해 특징 정정을 살펴봅니다. 교차 모달 보정 후 두 스트림 모두에서 특징이 향상되었음을 알 수 있습니다.

마지막으로 부록 C에서는 독일 연구 센터 헬름홀츠 협회, 바덴 뷔르템베르크 과학 연구 예술부, "KIT 미래 분야" 프로젝트를 통한 우수 대학, 항저우 SurImage 기술 회사, HAICORE@KIT 파티션의 헬름홀츠 협회 이니셔티브 및 네트워킹 펀드 등 이 작업을 지원한 다양한 기관에 감사를 표합니다. 이 작업은 부분적으로 MWK와 연방 교육연구부가 자금을 지원하는 호레카 슈퍼컴퓨터에서 수행되었습니다.