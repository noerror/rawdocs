# PIFuHD: Multi-Level Pixel-Aligned Implicit Function for
High-Resolution 3D Human Digitization

[https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt?usp=sharing#scrollTo=1TfPAtL4CyZw](https://colab.research.google.com/drive/11z58bl3meSzo6kFqkahMa35G5jmh2Wgt?usp=sharing#scrollTo=1TfPAtL4CyZw)

최근 심층 신경망을 이용한 3D 인체 형상 추정 기술이 발전하면서 실제 응용 분야에서 잠재력이 입증되었습니다. 그러나 이러한 방식은 대규모 컨텍스트와 높은 해상도가 모두 필요하기 때문에 매우 상세한 재구성에 실패하는 경우가 많습니다. 현재 하드웨어의 메모리 제한으로 인해 이전 방법에서는 저해상도 이미지를 사용해야 하므로 3D 추정치의 정확도가 떨어집니다. 이러한 한계를 극복하기 위해 연구진은 다단계의 엔드투엔드 학습 가능한 아키텍처를 제안합니다. 거친 레벨은 전체적인 추론을 위해 저해상도로 전체 이미지를 처리하고, 미세 레벨은 고해상도 이미지를 사용하여 세부적인 지오메트리에 집중합니다. 이 접근 방식은 단일 이미지 인체 형상 재구성을 위해 1k 해상도 입력 이미지를 완전히 활용함으로써 기존의 최첨단 기술보다 훨씬 뛰어난 성능을 발휘합니다.

![Untitled](PIFuHD%20Multi-Level%20Pixel-Aligned%20Implicit%20Function%202234fce8d48e409586c99d1194043df3/Untitled.png)

### 1. Introduction

의료 영상과 가상 현실을 비롯한 다양한 애플리케이션에서 고충실도 인체 디지털화는 매우 중요합니다. 멀티뷰 시스템을 사용하면 정확한 재구성이 가능하지만 비용이 많이 들고 쉽게 접근할 수 없습니다. 딥러닝 모델은 단일 이미지에서 재구성된 이미지를 얻을 수 있는 가능성을 보여주었지만, 아직 전문 캡처 시스템보다 성능이 낮습니다.

이 연구는 단일 이미지에서 옷을 입은 사람의 고충실도 3D 재구성을 달성하여 손가락, 얼굴 특징, 옷 주름과 같은 디테일을 복구하는 것을 목표로 합니다. 기존 접근 방식은 메모리 제한으로 인해 고해상도 이미지를 충분히 활용하지 못합니다. 크게 두 가지 접근 방식이 있는데, 거친 방식과 고충실도 모델 기반 방식이 있습니다. 두 가지 방법 모두 세부적인 재구성이 가능하지만 입력 이미지에 존재하는 실제 디테일을 재현하지 못하는 경우가 많습니다.

저자는 옷을 입은 사람의 3D 지오메트리를 고해상도 1k 이미지로 추론하여 후처리 없이 원본 입력 이미지의 디테일을 유지하는 엔드투엔드 멀티레벨 프레임워크를 소개합니다. 이 방법은 고해상도 입력에서 학습한 이미지 특징과 학습된 전체론적 임베딩을 원활하게 융합할 수 있는 픽셀 정렬 암시 함수(PIFu) 표현을 기반으로 합니다.

관찰되지 않은 뒷면을 재구성하기 위해 이 시스템은 이미지 간 변환 네트워크를 사용하여 뒷면 법선을 생성함으로써 재구성의 지각 품질을 크게 개선하고 보이는 부분과 가려진 부분 사이의 일관된 디테일 수준을 보장합니다.

- 이 작업의 주요 기여는 다음과 같습니다:
- 1k 이미지 해상도의 고해상도 3D 옷을 입은 사람 재구성을 위한 암시적 표면 학습을 위한 엔드투엔드 훈련 가능한 거친-세밀한 프레임워크.
- 관찰되지 않은 영역의 불확실성을 효과적으로 처리하여 높은 디테일의 완벽한 재구성을 가능하게 하는 방법.

### 2. Related Work

단일 뷰 3D 인체 재구성은 카메라 광선을 따라 깊이 모호성이 발생하기 때문에 제대로 된 포지셔닝이 어려운 문제입니다. 파라메트릭 3D 모델은 이 문제를 극복하기 위해 자주 사용되지만, 모델 구축에 사용되는 단일 템플릿 메시와 데이터로 인해 표현력이 제한됩니다. 이러한 형상 표현을 사용하면 큰 변형과 토폴로지 변화를 처리하기가 어렵습니다.

연구자들은 단일 뷰에서 '자유형' 3D 인체 지오메트리를 직접 회귀하는 비파라메트릭 방법을 제안했습니다. 이러한 접근 방식은 입력 및 출력 표현에 따라 다릅니다. 일부는 해상도와 디테일이 증가하면서 이산화된 체적 표현을 생성하는 DeepHuman과 같은 체적 표현을 사용합니다. 그러나 이산 복셀 표현에 필요한 큐빅 메모리 요구 사항은 달성 가능한 해상도를 제한합니다.

다른 방법으로는 파라메트릭 모델 공간 위에 자유형 변형을 적용하거나 대상 인물의 깊이 맵을 예측하는 방법이 있습니다. 최근에 도입된 픽셀 정렬 암시적 함수(PIFu)는 특정 3D 위치에 대한 점유를 결정하는 함수를 회귀시켜 출력 볼륨의 이산화된 표현이 필요하지 않습니다. 글로벌 피처 벡터를 사용하는 암시적 표면 표현과 달리 PIFu는 완전 컨볼루션 이미지 피처를 사용하여 입력 이미지에 존재하는 로컬 디테일을 유지합니다. 이 접근 방식은 전체 출력 볼륨을 동시에 메모리에 저장할 필요 없이 고충실도 3D 지오메트리를 재구성하는 데 탁월합니다.

최근 몇 가지 접근 방식은 텍스처 맵 표현을 사용하여 기하학적 또는 색상 디테일을 추정하여 고품질 3D 텍스처 또는 지오메트리를 재구성하는 데 중점을 둡니다. 예를 들어 Tex2Shape 접근 방식은 래핑되지 않은 UV 공간에서 변위를 회귀하여 고품질 3D 지오메트리를 재구성하는 것을 목표로 합니다. 하지만 이 접근 방식은 템플릿 메시의 토폴로지와 선택한 UV 파라미터화에 의해 제한되므로 다른 토폴로지와 눈에 보이는 심 아티팩트 문제가 발생할 수 있습니다.

최근의 방법은 신경망 모델을 활용하여 중간 텍스처 또는 뎁스 표현을 예측한 다음 최종 3D 지오메트리 출력을 재구성하는 데 사용합니다. 유니티의 작업은 고품질 또는 고해상도 합성 인체 이미지를 생성하는 접근 방식과도 관련이 있습니다. 최근의 일부 방법은 고품질 합성 얼굴을 생성하여 기존 GAN 기반 접근 방식의 한계를 극복하는 데 중점을 둡니다. 시맨틱 분할 작업에서도 비슷한 절충점을 추구합니다.

### 3. Method

제시된 방법은 512x512 해상도 입력 이미지를 가져와 저해상도 특징 임베딩(128x128)을 얻는 최근 도입된 픽셀 정렬 암시 함수(PIFu) 프레임워크에 기반합니다. 더 높은 해상도의 출력을 얻기 위해 이 프레임워크 위에 픽셀 정렬 예측 모듈이 추가로 쌓입니다. 미세 모듈은 고해상도 이미지(1024x1024)를 입력으로 받아 고해상도 이미지 특징(512x512)으로 인코딩합니다.

두 번째 모듈은 첫 번째 모듈의 고해상도 특징 임베딩과 3D 임베딩을 가져와 점유 확률 필드를 예측합니다. 재구성의 품질과 충실도를 더욱 향상시키기 위해 앞면과 뒷면의 노멀 맵을 이미지 공간에서 예측하여 네트워크에 추가 입력으로 공급합니다. 이 방법에 대한 개요는 원본 논문의 그림 2를 참조하세요.

### 3.1. Pixel-Aligned Implicit Function

이전 작업에서 소개한 PIFu는 제안된 방법의 거친 수준을 형성합니다. PIFu의 목표는 조밀한 3D 볼륨의 점유를 추정하여 3D 공간의 한 점이 인체 내부에 있는지 여부를 결정하는 것입니다. 대상 3D 공간을 이산화하는 이전 접근 방식과 달리 PIFu는 연속 카메라 공간에서 주어진 3D 위치에 대한 이진 점유 값을 예측하는 함수를 모델링합니다. 이 접근 방식은 메모리 효율적이며 대상 3D 볼륨에 대한 이산화가 필요하지 않으므로 고충실도 3D 지오메트리를 얻는 데 매우 중요합니다.

대규모 데이터 세트는 엔드투엔드 방식으로 함수를 훈련하는 데 사용됩니다. 추론하는 동안 3D 공간을 균일하게 샘플링하여 점유를 추론하고 행진 큐브를 사용하여 최종 등각 표면을 추출합니다. 그러나 기존 그래픽 하드웨어의 메모리 제한으로 인해 PIFu의 입력 크기와 이미지 특징 해상도는 제한적입니다. 일관된 깊이 추론을 위해 네트워크는 이미지 전체를 수용 필드로 덮도록 설계되어야 합니다.

이러한 제한으로 인해 이 방법은 더 높은 해상도의 이미지를 입력으로 가져와 특징 임베딩에 해상도를 유지할 수 없습니다. PIFu의 연속 표현은 임의의 해상도로 3D 지오메트리를 표현할 수 있지만, 실제로는 피처 해상도에 의해 표현력이 제한됩니다. 따라서 장거리 전체론적 추론에서 비롯되는 견고성과 피처 임베딩 해상도를 높여 표현력 간의 균형을 효과적으로 맞출 수 있는 방법이 필요합니다.

### 3.2. Multi-Level Pixel-Aligned Implicit Function

고충실도 3D 인체 디지털화를 위한 다단계 접근 방식은 1024x1024 해상도 이미지를 입력으로 사용합니다. 이는 두 가지 레벨의 PIFu 모듈로 구성됩니다:

거친 레벨은 PIFu[35]와 유사하게 다운샘플링된 512x512 이미지를 입력으로 받아 128x128 해상도의 백본 이미지 특징을 생성하여 전역 기하학적 정보를 통합하는 데 중점을 둡니다.
미세 레벨은 원본 1024x1024 해상도 이미지를 입력으로 받아 512x512 해상도의 백본 이미지 특징을 생성하여 더 미묘한 디테일을 추가하는 데 중점을 둡니다([35]의 구현보다 4배 높은 해상도).
미세 레벨 모듈은 절대 깊이 값 대신 거친 레벨에서 추출한 3D 임베딩 특징을 사용합니다. 거친 레벨 모듈은 PIFu와 유사하게 정의되지만 예측된 앞면 및 뒷면 노멀 맵도 사용합니다.

미세 레벨은 고해상도 입력에서 이미지 특징을 인코딩하며 저해상도 특징 추출기와 유사한 구조를 갖습니다. 주요 차이점은 미세 레벨의 수용 필드가 이미지 전체를 커버하지 않는다는 점입니다. 그러나 완전한 컨볼루션 아키텍처 덕분에 랜덤 슬라이딩 윈도우로 네트워크를 훈련하고 원본 이미지 해상도(즉, 1024x1024)에서 추론할 수 있습니다.

미세 레벨은 첫 번째 픽셀 정렬 MLP의 특징을 3D 임베딩으로 가져오기 때문에, 네트워크 설계가 증가된 이미지 해상도와 네트워크 용량을 적절히 활용할 수 있다면 글로벌 재구성 품질이 저하되지 않고 개선될 것입니다. 또한 미세 네트워크는 정규화(즉, 전 세계적으로 일관된 3D 깊이를 생성하는 작업)를 처리할 필요가 없으며 전체 이미지를 볼 필요가 없으므로 이미지 크롭으로 훈련할 수 있습니다. 이는 메모리 제약의 제한 없이 고해상도 이미지 입력을 가능하게 하는 데 매우 중요합니다.

### 3.3. Front-to-Back Inferen

사람의 뒷모습은 이미지에서 직접 관찰할 수 없기 때문에 정확한 형상을 예측하는 것은 쉽지 않은 문제입니다. 따라서 뒷모습은 전적으로 MLP 예측 네트워크에 의해 추론되어야 합니다. 이 문제의 모호하고 복합적인 특성으로 인해 3D 재구성은 매끄럽지 않고 특징이 없는 경향이 있습니다. 이는 부분적으로는 점유 손실이 불확실성 하에서 평균 재구성을 선호하고 최종 MLP 레이어가 학습해야 하는 예측 함수가 복잡하기 때문입니다.

이 문제를 해결하기 위해 연구자들은 이 추론 문제의 일부를 특징 추출 단계로 옮기면 네트워크가 더 선명한 재구성된 지오메트리를 생성하는 데 도움이 될 수 있다는 사실을 발견했습니다. 이미지 공간에서 3D 지오메트리의 프록시로 노멀 맵을 예측하고 이러한 노멀 맵을 픽셀 정렬 예측자에 피처로 제공하면, 3D 재구성은 이러한 맵에 따라 특정 3D 지오메트리를 추론하여 MLP가 더 쉽게 디테일을 생성할 수 있도록 안내합니다.

이미지 공간의 후면 및 정면 노멀은 RGB 색상에서 노멀 맵으로 매핑하는 pix2pixHD 네트워크를 사용하여 예측됩니다. 최근의 접근 방식과 유사하게 이 방법은 옷을 입은 사람과 같이 충분히 제한된 문제 영역에서 보이지 않는 뒷면에 대해 그럴듯한 출력을 생성합니다. 이렇게 예측된 노멀 맵을 특징 추출 단계에 통합하면 네트워크는 입력 이미지에서 직접 관찰되지 않은 영역에 대해서도 보다 상세하고 정확한 3D 재구성을 생성할 수 있습니다.

### 3.4. Loss Functions and Surface Sampling

모델에 사용되는 손실 함수는 최종 모델에서 복구되는 세부 정보에 상당한 영향을 미칠 수 있습니다. 연구진은 이전 작업에서와 같이 평균 L1 또는 L2 손실을 사용하는 대신 샘플링된 지점 집합에서 확장된 이진 교차 엔트로피(BCE) 손실을 사용합니다. 확장 BCE 손실은 다음과 같이 정의됩니다:

Lo =ΣX∈Sλf∗(X) 로그 f{L,H}(X)+(1 - λ) (1 - f∗(X)) log (1 - f{L,H}(X)),

여기서 S는 손실이 평가되는 샘플 집합을 나타내고, λ는 S에서 표면 외부의 점 비율, f∗(-)는 해당 위치의 지상 실측 점유도, f{L,H}(-)는 Sect의 각 픽셀 정렬 암시적 함수입니다. 3.2.

이전 작업에서와 마찬가지로 균일한 볼륨 샘플과 균일하게 샘플링된 표면 점 주위에 가우시안 섭동을 사용하여 표면 주변의 중요도 샘플링을 혼합하여 점을 샘플링합니다. 이 샘플링 방식은 표면으로부터의 거리의 역수에 비례하여 포인트를 샘플링하는 방식에 비해 더 선명한 결과를 생성합니다.

실제로 표면에 가우스 볼이 혼합되어 있는 경우 곡률이 높은 영역(가우스 볼 반경의 역수까지) 근처에서 샘플링 밀도가 더 높습니다. 곡률은 표면 지오메트리의 2차 미분이기 때문에 곡률에 기반한 중요도 샘플링은 재구성된 3D 모델의 디테일과 충실도를 크게 향상시킵니다. 이 접근 방식을 통해 네트워크는 최종 3D 인체 디지털화에서 복잡한 디테일을 더 잘 캡처하고 재현할 수 있습니다.

### 4. Experimental Results

연구진은 500개의 고해상도 사진 측량 스캔으로 구성된 RenderPeople 데이터 세트를 450개의 피사체로 구성된 훈련 세트와 50개의 피사체로 구성된 테스트 세트로 나누어 사용했습니다. 메시에는 HDRI Haven의 163차 구형 고조파를 사용하여 미리 계산된 래디언스 전송이 적용되어 렌더링됩니다. 각 피사체는 고도를 0°로 고정하여 요 축의 모든 각도에서 렌더링됩니다. 이전 작업과 달리 COCO 데이터 세트를 사용하여 임의의 배경 이미지를 증강하므로 사전 프로세스로 세그먼테이션이 필요하지 않습니다.

구현 세부 사항:

1. 저해상도 및 고해상도 수준의 이미지 인코더는 각각 4개와 1개의 스택이 있는 스택형 모래시계 네트워크를 사용합니다.
2. 피처 크기는 거친 레벨의 경우 128x128x256, 미세 레벨의 경우 512x512x16입니다.
3. 거친 PIFu 모듈은 512x512로 크기가 조정된 입력 이미지와 배치 크기 8로 사전 훈련됩니다.
4. 미세 PIFu는 배치 크기 8과 512x512 크기의 무작위 창 크롭으로 훈련됩니다.
5. 10에포크마다 0.1씩 가중치가 감소하고 표면 주위에 균일 샘플링과 중요도 샘플링이 혼합된 8000개의 샘플링된 포인트가 있는 RMSProp을 사용합니다.

표면 법선 추론은 4개의 다운샘플링 레이어와 9개의 잔여 블록으로 구성된 네트워크 아키텍처를 사용합니다. 이 네트워크는 목적 함수를 사용하여 앞면과 뒷면 법선을 개별적으로 예측하는 두 개의 네트워크를 훈련합니다:

LN = LVGG + λl1Ll1,

여기서 LVGG는 Johnson 등이 제안한 지각 손실이고, Ll1은 예측과 기준값 법선 사이의 l1 거리입니다. 실험에서 상대 가중치 λl1은 5.0으로 설정되었습니다. 이들은 수렴할 때까지 학습률이 2.0 × 10^-4인 아담 옵티마이저를 사용합니다.

### 4.1. Evaluations

제거 연구는 고충실도 재구성에 기여하는 요소를 평가하기 위해 여러 가지 대안으로 다단계 픽셀 정렬 암시적 함수를 평가합니다:

1. 3D 임베딩의 중요성: 연구진은 학습된 3D 임베딩 대신 절대 깊이 값에 따라 미세 레벨 모듈을 조절하면 학습 및 테스트 정확도가 크게 저하된다는 사실을 발견했습니다. 따라서 고충실도 3D 재구성을 위해서는 전체적인 추론과 고해상도 이미지 특징의 조합이 필수적입니다.
2. 디자인 선택의 평가: 연구팀은 세분화된 이미지 인코더만을 사용한 픽셀 정렬 암시적 함수, 전역 특징을 사용한 컨디셔닝, 입력 크기를 조정한 단일 PIFu, 엔드투엔드 및 대체 훈련을 모두 사용한 제안된 다중 레벨 PIFu 등 다양한 변형을 구현했습니다. 그 결과 피처 임베딩의 공간 해상도가 높으면 로컬 디테일이 크게 향상되지만 로컬 피처만 사용하면 과적합이 발생하고 일반화가 어려워지는 것으로 나타났습니다. 글로벌 컨텍스트를 추가하면 도움이 되지만 글로벌 특징에 정확한 공간 정보가 부족하면 견고성이 저하됩니다. 또는 거친 모듈과 미세 모듈을 함께 훈련하면 엔드투엔드 훈련보다 정확도가 더 높아집니다.
3. 뒷면 정상 추론의 중요성: 입력 이미지만 가져오는 PIFu는 모호함으로 인해 누락된 영역에 대한 재구성이 흐릿합니다. 이미지 간 변환 네트워크를 사용하여 직접 가이드를 제공하면 앞면과 뒷면 모두에서 재구성 정확도가 크게 향상되고 주름이 더 사실적으로 표현됩니다.

결론적으로, 이 절제 연구는 고충실도 재구성을 달성하기 위해 전체적인 추론, 고해상도 이미지 특징, 뒷면 노멀 추론이 중요하다는 것을 보여줍니다.

### 4.2. Comparisons

저자는 이 방법을 공개적으로 사용 가능한 피플 스냅샷 데이터 세트의 최첨단 3D 인간 재구성 방법과 정성적으로 비교합니다. 비교 대상 방법에는 멀티스케일 복셀(DeepHuman), 픽셀 정렬 암시 함수(PIFu), 변위와 표면 법선을 사용한 텍스처 매핑이 포함된 인간 파라메트릭 모델(Tex2shape) 등 다양한 형상 표현이 있습니다.

Tex2shape와 DeepHuman은 거칠고 미세한 전략을 채택하지만, 기본 셰이프의 표현력이 제한적이기 때문에 세분화 효과는 미미합니다. DeepHuman의 복셀 표현은 공간 해상도를 제한하고, Tex2shape의 템플릿 기반 접근 방식은 다양한 토폴로지와 큰 변형을 처리하는 데 어려움을 겪습니다. Tex2shape는 주름과 같은 일부 고유한 모양을 유지하지만, 기성 인간 고밀도 대응 맵을 사용하여 이미지 공간에서 텍스처 매개변수화로 매핑이 불완전하기 때문에 결과 모양이 충실도가 떨어집니다.

반면, 제안된 방법은 기본 모양과 정제된 모양 모두에 대해 표현적인 모양 표현을 완전히 활용하고 픽셀 수준에서 3D 지오메트리를 직접 예측하여 입력 이미지에 존재하는 모든 디테일을 유지합니다. 이는 다른 최신 접근 방식과 비교하여 세밀한 디테일과 충실도를 캡처하는 측면에서 제안된 방법이 우수하다는 것을 보여줍니다. 보다 정성적인 결과는 그림 7에서 확인할 수 있습니다.

### 5. Discussion and Future Work

저자는 전체적인 정보와 국소적인 디테일을 공동으로 추론하여 추가적인 후처리나 부가 정보 없이 단일 이미지에서 옷을 입은 사람의 고해상도 3D 재구성을 달성하는 멀티레벨 프레임워크를 제시합니다. 다단계 픽셀 정렬 암시적 함수는 스케일 피라미드를 통해 글로벌 컨텍스트를 암시적 3D 임베딩으로 점진적으로 전파하여 이전 접근 방식에 한계가 있는 명시적 지오메트리에 대한 성급한 결정을 피함으로써 이를 달성합니다.

이 실험은 정확하고 정밀한 재구성을 위해 3D 인식 컨텍스트를 통합하는 것이 중요하다는 것을 보여줍니다. 또한 이미지 영역의 모호성을 우회하면 가려진 영역에서 3D 재구성 디테일의 일관성이 크게 향상된다는 사실도 보여줍니다.

다단계 접근 방식은 3D 임베딩을 추출하는 이전 단계의 성공에 의존하기 때문에 기준 모델의 견고성을 개선하면 전반적인 재구성 정확도에 직접적인 도움이 될 것으로 예상됩니다. 향후 작업에는 시맨틱 세분화, 포즈, 파라메트릭 3D 얼굴 모델과 같은 사람별 선행 모델을 통합하고 암시적 표면에 대한 2D 감독을 추가하여 자연 상태의 입력을 추가로 지원하는 작업이 포함될 수 있습니다.