# InstructPix2Pix: Learning to Follow Image Editing Instructions

[https://arxiv.org/pdf/2211.09800.pdf](https://arxiv.org/pdf/2211.09800.pdf)

1. Introduction

사람이 작성한 명령어를 기반으로 이미지를 편집하는 방법인 InstructPix2Pix를 소개합니다. 언어 모델(GPT-3)과 텍스트-이미지 모델(Stable Diffusion)에 대한 지식을 결합하여 이미지 편집 예제 데이터 세트를 생성하여 모델을 학습시킵니다. InstructPix2Pix는 추가 예제 이미지나 미세 조정 없이도 몇 초 만에 이미지를 빠르게 편집할 수 있습니다. 합성 데이터로 학습되었지만 실제 이미지와 사용자가 작성한 지침으로 일반화할 수 있습니다. 이 모델을 통해 쉽고 직관적인 이미지 편집이 가능하며, 오브젝트 교체, 이미지 스타일 변경, 설정 수정 등 다양한 작업을 수행할 수 있습니다.

2

이 백서에서는 사전 학습된 대규모 모델을 결합하여 사람이 작성한 지침에 따라 이미지 편집과 같은 멀티모달 작업을 해결하는 방법에 대해 설명합니다. 이 논문에서는 GPT-3 및 안정 확산 모델을 사용하여 쌍을 이루는 멀티모달 훈련 데이터를 생성합니다. 최근 확산 모델의 발전으로 다양한 모달리티를 위한 최첨단 이미지 합성 및 생성 모델이 가능해졌습니다. InstructPix2Pix는 기존의 텍스트 기반 이미지 편집 방법과 달리 지침에 따라 편집할 수 있어 더욱 표현력이 풍부하고 정확하며 직관적입니다. 이 접근 방식은 대규모 언어 모델에 사람의 지시를 따르도록 가르치는 최근 연구에서 영감을 얻었습니다. 마지막으로, 제너레이티브 모델을 다운스트림 작업의 학습 데이터 소스로 사용하는 것은 저렴하고 풍부한 데이터를 제공할 수 있다는 잠재력 때문에 관심을 받고 있습니다.

3

저자들은 지침 기반 이미지 편집을 지도 학습 문제로 취급합니다. 먼저 언어 모델과 텍스트-이미지 모델이라는 두 가지 대규모 사전 학습 모델을 사용하여 편집 전후의 텍스트 편집 지침과 이미지의 쌍을 이루는 학습 데이터 세트를 생성합니다. 텍스트 영역에서는 GPT-3을 미세 조정하여 편집 지침과 편집 후 결과 텍스트 캡션을 생성합니다. 그런 다음 사전 학습된 텍스트-이미지 모델을 사용하여 한 쌍의 캡션을 한 쌍의 이미지로 변환합니다. 이 단계의 과제는 이미지 일관성을 보장하는 것으로, 프롬프트 투 프롬프트 방식을 사용하여 이를 해결합니다. 두 이미지 간의 유사성을 제어하여 텍스트 기반 지침에 부합하는 다양하고 고품질의 이미지 쌍을 생성합니다.

![그림 2. 우리의 방법은 이미지 편집 데이터 세트를 생성하고 해당 데이터 세트에 대한 확산 모델을 훈련하는 두 부분으로 구성됩니다. (a) 우리는 먼저 미세 조정된 GPT-3를 사용하여 지침과 편집된 캡션을 생성합니다. (b) 그런 다음 StableDiffusion [52]을 Promptto-Prompt [17]와 함께 사용하여 캡션 쌍에서 이미지 쌍을 생성합니다. 이 절차를 사용하여 450,000개 이상의 훈련 예제로 구성된 데이터 세트(c)를 생성합니다. 예제. (d) 마지막으로, 생성된 데이터에 대해 InstructPix2Pix 확산 모델을 학습시켜 명령어에서 이미지를 편집합니다. 추론 시점에서 모델이 일반화되어 사람이 작성한 지침에서 실제 이미지를 편집합니다.](InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%202593da58729548cba4706f9f0344b119/Untitled.png)

그림 2. 우리의 방법은 이미지 편집 데이터 세트를 생성하고 해당 데이터 세트에 대한 확산 모델을 훈련하는 두 부분으로 구성됩니다. (a) 우리는 먼저 미세 조정된 GPT-3를 사용하여 지침과 편집된 캡션을 생성합니다. (b) 그런 다음 StableDiffusion [52]을 Promptto-Prompt [17]와 함께 사용하여 캡션 쌍에서 이미지 쌍을 생성합니다. 이 절차를 사용하여 450,000개 이상의 훈련 예제로 구성된 데이터 세트(c)를 생성합니다. 예제. (d) 마지막으로, 생성된 데이터에 대해 InstructPix2Pix 확산 모델을 학습시켜 명령어에서 이미지를 편집합니다. 추론 시점에서 모델이 일반화되어 사람이 작성한 지침에서 실제 이미지를 편집합니다.

저자들은 지침에 따라 이미지를 편집하기 위해 InstructPix2Pix라는 조건부 확산 모델을 학습시킵니다. 이 모델은 대규모 텍스트-이미지 잠복 확산 모델인 Stable Diffusion을 기반으로 합니다. 확산 모델은 일련의 노이즈 제거 자동 인코더를 통해 데이터 샘플을 생성하며, 잠복 확산 모델은 사전 학습된 변형 자동 인코더의 잠복 공간에서 작업하여 효율성과 품질을 향상시킵니다.

사전 학습된 안정적 확산 체크포인트에서 InstructPix2Pix 모델을 미세 조정하여 이미지 번역 작업에서 더 나은 성능을 발휘할 수 있도록 지원합니다. 이 모델은 텍스트 편집 지침을 입력으로 사용하며 이미지 컨디셔닝을 위해 추가 입력 채널이 추가됩니다. 이 모델은 분류기 없는 가이드를 사용하여 생성된 이미지의 시각적 품질과 입력 지침에 대한 대응을 개선하는 데 도움이 됩니다.

점수 네트워크에는 입력 이미지와 텍스트 지침이라는 두 가지 컨디셔닝 입력이 있습니다. 생성된 샘플이 입력 이미지 및 편집 명령어와 얼마나 강하게 일치하는지 제어하기 위해 두 컨디셔닝 입력 모두에 분류기 없는 안내가 적용됩니다. 저자들은 논문에서 이 두 가지 매개변수가 생성된 샘플에 미치는 영향을 보여줍니다.

4
저자들은 다양한 실제 사진과 아트웍에 대한 명령어 기반 이미지 편집 모델인 InstructPix2Pix의 결과를 제시합니다. 이 모델은 물체 교체, 계절 및 날씨 변경, 재질 속성 수정 등 여러 가지 까다로운 편집 작업을 성공적으로 수행합니다. 연구팀은 이미지 일관성과 편집 품질을 측정하는 두 가지 메트릭을 사용하여 이 모델을 최근 작업물인 SDEdit 및 Text2Live와 정성적으로 비교하고 SDEdit와 정량적으로 비교합니다.

SDEdit에 비해 이미지 편집에서 동일성을 유지하고 개별 개체를 분리하는 데 있어 InstructPix2Pix가 더 나은 성능을 보였습니다. Text2Live는 특정 유형의 편집에 대해 설득력 있는 결과를 생성할 수 있지만 처리할 수 있는 편집 범주에 제한이 있습니다. 정량적 비교에 따르면 SDEdit에 비해 동일한 방향 유사도 값에 대한 이미지 일관성이 더 높은 것으로 나타났습니다.

![Untitled](InstructPix2Pix%20Learning%20to%20Follow%20Image%20Editing%20I%202593da58729548cba4706f9f0344b119/Untitled%201.png)

저자들은 데이터 세트 크기와 필터링 접근 방식에 대한 실험도 수행했습니다. 데이터 세트 크기를 줄이면 더 큰 이미지 편집을 수행하는 데 영향을 미치고, CLIP 필터링을 제거하면 전반적인 이미지 일관성이 감소한다는 사실을 발견했습니다. 분류기 없는 안내 척도를 조정하면 일관성과 편집 강도의 균형을 맞추는 데 도움이 되며, 특정 범위에서 최상의 결과를 얻을 수 있습니다.