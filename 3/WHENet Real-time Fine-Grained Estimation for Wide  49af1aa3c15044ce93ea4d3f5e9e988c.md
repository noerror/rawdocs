# WHENet: Real-time Fine-Grained Estimation for Wide Range Head Pose

[https://arxiv.org/pdf/2005.10353.pdf](https://arxiv.org/pdf/2005.10353.pdf)

1

헤드 포즈 추정(HPE)은 이미지 또는 비디오를 기반으로 사람의 고개가 돌아가는 방향을 결정하는 프로세스를 말합니다. 이 기술은 가상 현실, 운전자 지원, 모션 캡처, 시선 추정 등 다양한 분야에서 활용되고 있습니다.

시선추정은 상호 시선 및 운전자와 보행자 간의 상호 작용과 같은 다양한 사회적 상호 작용에서 중요한 역할을 합니다. 대화 중에 시각적 단서를 제공하고, 화자/청자 역할을 전환할 시점을 파악하는 데 도움을 주며, 동의 여부를 표시할 수도 있습니다. 기계가 인간과 자연스럽게 상호 작용하려면 머리의 포즈를 이해하고 이에 반응하는 것이 중요합니다.

![효율적인 네트워크[40] 백본을 분류 및 회귀 손실과 결합하여 [33]을 개선한 WHENet 풀-레인지 머리 부분 자세 추정 네트워크(가운데)입니다. 새로운 래핑 손실이 큰 요 좌표에서 네트워크를 안정화합니다. 이 네트워크를 [18]에서 재활용한 앞면 뷰를 사용하여 훈련시키면 WHENet는 모바일 친화적 모델로 앞면 뷰뿐만 아니라 무거운 가림막, 패션 액세서리, 불리한 조명이 있는 머리 부분의 자세를 예측할 수 있습니다. 이럼에도 불구하고 WHENet은 앞면이나 프로필 뷰에 제한된 기존 최첨단 방법보다 정확하거나 더 정확합니다. 일부 이미지는 [51]&[41]에서 가져왔습니다.](WHENet%20Real-time%20Fine-Grained%20Estimation%20for%20Wide%20%2049af1aa3c15044ce93ea4d3f5e9e988c/Untitled.png)

효율적인 네트워크[40] 백본을 분류 및 회귀 손실과 결합하여 [33]을 개선한 WHENet 풀-레인지 머리 부분 자세 추정 네트워크(가운데)입니다. 새로운 래핑 손실이 큰 요 좌표에서 네트워크를 안정화합니다. 이 네트워크를 [18]에서 재활용한 앞면 뷰를 사용하여 훈련시키면 WHENet는 모바일 친화적 모델로 앞면 뷰뿐만 아니라 무거운 가림막, 패션 액세서리, 불리한 조명이 있는 머리 부분의 자세를 예측할 수 있습니다. 이럼에도 불구하고 WHENet은 앞면이나 프로필 뷰에 제한된 기존 최첨단 방법보다 정확하거나 더 정확합니다. 일부 이미지는 [51]&[41]에서 가져왔습니다.

많은 HPE 방식은 애플리케이션이 많고 얼굴 특징이 풍부하기 때문에 전면 및 프로필 뷰에 중점을 두지만, 모든 범위의 고개 회전을 처리할 수 있는 시스템이 필요합니다. 이는 운전자 지원, 모션 캡처, 광고 및 소매업에 특히 유용합니다.

하지만 기존 방식은 대규모 네트워크 시스템인 경우가 많아 모바일 및 임베디드 플랫폼, 특히 여러 하위 시스템이 동시에 작동해야 하는 자율 주행 분야에는 적합하지 않을 수 있습니다.

이러한 문제를 해결하기 위해 와이드 헤드포지션 추정 네트워크(WHENet)가 개발되었습니다. 이 네트워크는 모바일 친화적인 구조를 사용하여 모든 범위의 헤드 턴(요)을 커버할 수 있도록 HPE를 확장합니다. WHENet 개발팀은 다음과 같은 중요한 공헌을 했습니다:

풀레인지 HPE에서 전방 시야의 요 정확도를 크게 개선하는 랩핑 손실 생성.
전체 범위 HPE에서 데이터를 학습하고 검증하는 데 적합한 CMU Panoptic 데이터 세트에 대한 자동화된 라벨링 프로세스 개발.
네트워크를 전체 범위의 요에 맞게 조정하여 전체 범위 HPE에서 최고 성능을 달성하고 다른 작업을 위해 훈련되었음에도 불구하고 좁은 범위 HPE에서 최고 성능의 1.8% 이내에 도달했습니다.
희망넷을 간단히 수정하면 원래 네트워크에 비해 29%, AFLW2000 및 BIWI 데이터 세트의 RGB 이미지에서 좁은 범위 HPE의 경우 현재 최고 FSANet에 비해 5~13% 개선될 수 있음을 보여줍니다.

2

사람의 고개를 돌리는 방향을 결정하는 작업인 헤드 포즈 추정(HPE)은 25년 이상 연구되어 온 주제입니다. 이 작업에 대한 다양한 접근 방식이 개발되어 왔으며, 크게 다음과 같이 분류할 수 있습니다:

고전적인 방법: 여기에는 입력 이미지와 미리 레이블이 지정된 템플릿 세트를 비교하여 머리 자세를 결정하는 템플릿 매칭이 포함됩니다. 그러나 이 방법은 유사한 신원과 포즈를 구분하는 데 어려움을 겪는 경우가 많습니다. 또 다른 고전적인 방법은 가능한 각 포즈에 대해 별도의 검출기를 훈련시키는 캐스케이드 검출기입니다. 이 방법의 문제점은 예측 해상도와 여러 감지기가 활성화되는 시점을 결정하는 것입니다.

기하학적 및 변형 가능한 모델: 이 방법은 입력 이미지의 특징을 활용하여 머리 포즈를 결정합니다. 이러한 방법의 복잡성은 특징을 감지하는 데 있으며, 이는 그 자체로 잘 연구된 문제입니다.

회귀 및 분류 방법: 이 방법은 레이블이 지정된 훈련 데이터를 기반으로 포즈를 예측합니다. 회귀 방법은 수학적 모델을 사용하여 포즈를 직접 예측하는 반면, 분류 방법은 불연속적인 포즈 집합에서 포즈를 예측합니다. 분류 방법의 예측 해상도는 일반적으로 더 낮습니다.

다중 작업 방법: 이 방법은 HPE와 다른 얼굴 분석 작업을 결합합니다. 연구에 따르면 관련 작업을 동시에 훈련하면 작업을 개별적으로 훈련하는 것보다 더 나은 성능을 얻을 수 있다고 합니다.

전체 범위 방법: 대부분의 HPE 데이터 세트는 정면과 프로파일 뷰에 초점을 맞추기 때문에 이 방법은 덜 일반적입니다. 전체 범위 방법은 포즈를 세분화된 빈 또는 클래스로 분류하여 요를 결정합니다. 그러나 제안된 방법과 달리 이 방법에서는 피치 및 롤이 예측되지 않습니다.

이러한 방법에는 BIWI, AFLW2000, 300W-LP 등 다양한 데이터 세트가 사용됩니다. 이러한 데이터 세트의 대부분은 정면과 옆모습만 다루며 머리의 전체 포즈는 다루지 않습니다.

와이드 헤드포즈 추정 네트워크(WHENet) 개발에 중요한 리소스는 반구 전체를 커버하는 여러 대의 보정된 카메라에서 피사체를 캡처하는 CMU 파놉틱 데이터세트입니다. 이 데이터셋은 3D로 얼굴 랜드마크를 제공하여 팀이 정면에 가까운 뷰에서 머리 포즈를 추정하고 이 포즈를 사용하여 비정면 뷰포인트에 라벨을 지정할 수 있도록 합니다. 이를 통해 이 방법은 전방 뷰로 훈련할 수 있으며, 카메라 상대 포즈의 전체 범위를 포괄합니다.

3

와이드 헤드포지션 추정 네트워크(WHENet)는 이전 연구의 다중 손실 프레임워크를 기반으로 설계되었습니다. 이 이전 프레임워크는 컨볼루션 백본과 소프트맥스 함수와 교차 엔트로피 손실을 사용하여 헤드의 피치, 요, 롤을 특정 카테고리로 분류하는 별도의 완전 연결된 네트워크를 결합했습니다. 또한 정밀도를 위해 평균 제곱 오차(MSE) 회귀 손실이 적용되었습니다.

![그림 2: 머리 부분의 자세(a)는 피치(빨간색 축), 요(녹색 축) 및 롤(파란색 축) 각도에 의해 파라미터화됩니다. 우리가 제안한 래핑 손실 함수(b-왼쪽)는 자세가 유사하지만 MSE가 극단적인 손실 값을 생성하는 실제 값에서 180° 이상의 예측을 과도하게 처벌하는 것을 피합니다. 이것은 주제가 카메라를 향해 있는 경우(b-오른쪽) 예측을 개선하는데, 여기서 MSE로 훈련된 네트워크는 요 좌표에 대해 180°에 가까운 오류를 가집니다. x(빨간색) 축은 주제의 왼쪽 귀와 정렬되어야 합니다. (c)에서는 CMU Panoptic 데이터셋 [18]에서 유일륨 각도를 추출하기 위해 정면 뷰를 제공하는 가상 카메라 외부 설정과 진짜 외부 설정을 계산합니다. 이를 통해 우리는 전체 범위의 HPE 작업을 위해 수만 개의 전방 뷰 이미지를 자동으로 라벨링할 수 있습니다. 우리는 이것을 처음으로 하는 것으로 믿고 있습니다.](WHENet%20Real-time%20Fine-Grained%20Estimation%20for%20Wide%20%2049af1aa3c15044ce93ea4d3f5e9e988c/Untitled%201.png)

그림 2: 머리 부분의 자세(a)는 피치(빨간색 축), 요(녹색 축) 및 롤(파란색 축) 각도에 의해 파라미터화됩니다. 우리가 제안한 래핑 손실 함수(b-왼쪽)는 자세가 유사하지만 MSE가 극단적인 손실 값을 생성하는 실제 값에서 180° 이상의 예측을 과도하게 처벌하는 것을 피합니다. 이것은 주제가 카메라를 향해 있는 경우(b-오른쪽) 예측을 개선하는데, 여기서 MSE로 훈련된 네트워크는 요 좌표에 대해 180°에 가까운 오류를 가집니다. x(빨간색) 축은 주제의 왼쪽 귀와 정렬되어야 합니다. (c)에서는 CMU Panoptic 데이터셋 [18]에서 유일륨 각도를 추출하기 위해 정면 뷰를 제공하는 가상 카메라 외부 설정과 진짜 외부 설정을 계산합니다. 이를 통해 우리는 전체 범위의 HPE 작업을 위해 수만 개의 전방 뷰 이미지를 자동으로 라벨링할 수 있습니다. 우리는 이것을 처음으로 하는 것으로 믿고 있습니다.

WHENet은 이 프레임워크를 채택하되 전체 범위의 헤드 포즈 추정에 적용하기 위해 수정했습니다. 요는 120개의 빈으로 나뉘며, 각 빈은 3도에 걸쳐 전체 범위의 요 값을 포괄합니다. 피치와 롤도 마찬가지로 66개의 구간으로 나뉩니다. 분류 손실(Lcls)과 회귀 손실(Lreg)이라는 두 가지 유형의 손실이 계산되며, 최종 손실은 이러한 손실의 가중치 조합을 통해 얻어집니다.

이전 모델과 크게 달라진 점은 회귀에 MSE 손실 대신 '래핑 손실'을 도입한 것입니다. 래핑된 손실은 각 예측된 요를 해당 데이터 세트 주석과 정렬하는 데 필요한 최소 회전 각도를 계산합니다. 이 변경은 요가 150도를 초과할 때 MSE 손실에서 관찰되는 불규칙한 동작을 처리하는 데 매우 중요합니다.

네트워크의 백본은 WHENet이 이전 작업과 차별화되는 또 다른 영역입니다. 이전 연구에서는 AlexNet 및 ResNet50과 같은 대규모 네트워크를 사용했지만, WHENet은 더 가볍고 효율적인 모델인 EfficientNet-B0을 사용합니다. 따라서 리소스가 제한되어 있고 여러 하위 시스템이 동시에 작동해야 하는 모바일 또는 임베디드 애플리케이션에 이상적입니다.

예비 테스트에서 WHENet은 저전력 임베디드 플랫폼에 성공적으로 포팅되어 초당 60프레임에 가까운 추론 속도를 달성하며 실시간 애플리케이션에 대한 잠재력을 입증했습니다.

4

이 연구에서는 300W-LP, AFLW2000, BIWI 등 머리 자세 추정(HPE)을 위한 몇 가지 주요 데이터 세트를 활용합니다. 처음 두 데이터 세트는 3D 고밀도 얼굴 정렬을 사용하여 3D 모델을 2D 이미지에 맞춰 정확한 헤드 포즈 실측 데이터를 제공합니다. BIWI 데이터 세트는 피험자가 정면 위치에서 가능한 모든 각도로 촬영한 비디오 시퀀스로 구성됩니다. 이전 연구와 마찬가지로, 연구팀은 테스트에는 AFLW2000과 BIWI를, 훈련에는 300W-LP를 사용합니다.

그러나 이러한 데이터 세트 중 어느 것도 절대 요가 100도보다 큰 예시를 제공하지 않습니다. 이 문제를 해결하기 위해 300W-LP와 약 30대의 HD 카메라에서 비디오를 캡처한 CMU 팬옵틱 데이터셋의 데이터를 결합한 새로운 데이터셋을 생성했습니다. 이 데이터세트에는 3D 얼굴 랜드마크와 보정된 카메라 외생 및 내생이 포함되어 있어 해당 카메라 상대 헤드 포즈 오일러 각을 계산할 수 있습니다.

팬옵틱 데이터 세트는 배경 변화가 제한되어 있어 네트워크가 다양한 배경에서 피사체를 구별하는 방법을 학습하는 데 도움이 되지 않기 때문에 독립형 훈련에는 적합하지 않다는 것을 알게 되었습니다. 따라서 -99도에서 99도의 요 범위를 커버하는 300W-LP와 결합된 반면, Panoptic 데이터 세트는 대부분 이 범위를 벗어난 데이터를 제공합니다.

작업의 상당 부분은 자동 라벨링 및 노이즈 감소를 위해 참조 3D 얼굴 랜드마크 세트와 가상 카메라를 사용하여 Panoptic 데이터세트에서 카메라 상대 헤드 포즈 오일러 각도를 계산하는 신뢰할 수 있는 절차를 개발하는 것이었습니다. 또한 피사체 머리 주위의 구형 점으로 이루어진 '헬멧'을 기반으로 한 새로운 이미지 자르기 방법을 도입했습니다.

와이드 헤드포지션 추정 네트워크(WHENet)의 훈련은 두 단계로 이루어집니다. 먼저 300W-LP 데이터 세트와 학습률이 1e-5인 ADAM 옵티마이저를 사용하여 요, 피치, 롤에 대한 예측 범위가 -99~99도인 좁은 범위 모델(WHENet-V)을 훈련합니다. 그 후, 동일한 옵티마이저와 학습 속도를 사용하여 120개의 요 빈이 있는 풀레인지 WHENet을 훈련하고, WHENet-V의 초기 가중치와 300W-LP 및 Panoptic 데이터 세트의 훈련 데이터를 사용합니다.

2단계 접근 방식은 CMU 데이터의 배경 변화가 제한적이기 때문에 전체 범위 네트워크가 더 잘 수렴하고 더 유용한 기능을 학습하는 데 도움이 됩니다. 견고성을 향상시키기 위해 훈련 중에 이미지를 최대 15배까지 무작위로 다운샘플링합니다. 데이터 세트는 거의 동일한 양으로 사용되며, 300W-LP는 좁은 범위의 샘플을 제공하고 Panoptic 데이터 세트는 주로 넓은 범위를 처리합니다.

5

연구의 광범위한 결과 섹션에서 요의 래핑을 처리하기 위해 절대 래핑 오차(AWE)와 평균 래핑 오차(MAWE)를 도입합니다. 피치와 롤을 예측하지 않고 비공개 데이터 세트에서 테스트한 결과, 와이드 헤드포지션 추정 네트워크(WHENet)가 다른 방법보다 성능이 뛰어나다는 것을 알게 됩니다. 또한, 전체 범위 WHENet은 좁은 범위 작업을 위해 특별히 훈련되지 않았음에도 불구하고 BIWI 및 AFLW2000에서 낮은 평균 오류를 달성하고 FSANet에 근소한 차이로 1등을 놓치지 않았습니다.

또한 WHENet은 전체 범위 입력을 처리할 때 인상적인 성능을 발휘하여 정확도는 거의 떨어지지 않으면서도 더 복잡한 FSANet보다 우수한 성능을 보였습니다. 이는 전방 시야를 크게 개선하는 WHENet에 사용된 래핑 손실 함수 덕분입니다.

좁은 범위의 결과 측면에서 WHENet-V는 3DDFA와 같은 CNN 기반 방법뿐만 아니라 Dlib, KEPLER, FAN과 같은 다른 랜드마크 기반 방법과 희망넷 및 FSANet과 같은 랜드마크가 없는 방법보다 성능이 뛰어납니다. 특히 WHENet-V는 BIWI에서 13.1%, AFLW2000에서 4.7% 더 나은 성능을 보여 두 데이터 세트의 성능을 총 8.4% 향상시켰습니다.

또한 랩드 손실을 도입하면 평균 제곱 오차(MSE) 손실에 비해 극단적인 포즈에 대한 오류를 50% 이상 줄일 수 있으며, 다양한 요 각도에 걸쳐 보다 일관된 결과를 얻을 수 있다는 연구 결과가 있습니다. 또한 고정된 템플릿을 사용하는 대신 CMU 데이터세트에 주석을 다는 데 사용되는 템플릿 키포인트를 각 피사체에 맞게 조정하면 WHENet의 성능을 개선할 수 있다는 점에 주목하여 이 아이디어는 향후 연구를 위해 남겨두었습니다.

메타변수 α와 β에 대한 추가 제거 연구와 해상도의 효과, 비디오 및 깊이 기반 방법과의 상대적 비교는 부록에서 확인할 수 있습니다. 전반적으로 이 결과는 넓은 범위와 좁은 범위의 헤드 포즈 추정 작업 모두에서 WHENet과 WHENet-V의 유망한 성능을 보여줍니다.