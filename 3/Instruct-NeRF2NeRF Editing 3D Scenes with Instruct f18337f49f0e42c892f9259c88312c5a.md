# Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions

[https://arxiv.org/pdf/2303.12789.pdf](https://arxiv.org/pdf/2303.12789.pdf)

![Untitled](Instruct-NeRF2NeRF%20Editing%203D%20Scenes%20with%20Instruct%20f18337f49f0e42c892f9259c88312c5a/Untitled.png)

1. Introduction

신경망 3D 재구성 기술을 통해 실제 이미지에서 디지털 3D 장면을 더 쉽게 만들 수 있게 되었습니다. 그러나 이러한 3D 장면을 위한 편집 도구는 아직 개발이 미흡합니다. Instruct-NeRF2NeRF는 사용자가 텍스트 지침만으로 3D NeRF 장면을 편집할 수 있는 새로운 방법입니다. 이를 통해 일반 사용자가 3D 편집에 더 쉽게 접근하고 직관적으로 사용할 수 있습니다.

이를 위해 이 방법은 2D 확산 모델, 특히 InstructPix2Pix 모델의 모양 및 모양 정보를 사용합니다. 이 모델을 NeRF 장면의 개별 이미지에 직접 적용하면 일관되지 않은 결과가 나올 수 있습니다. 이 문제를 해결하기 위해 반복적 데이터 세트 업데이트(Iterative DU) 방법은 NeRF 입력 이미지를 편집하고 편집된 이미지로 3D 표현을 업데이트하는 방식을 번갈아 사용합니다.

이 접근 방식은 다양한 NeRF 장면에서 평가되고 다른 방법과 비교되어 사람, 사물 및 대규모 장면에서 광범위한 편집을 수행할 수 있는 능력을 입증했습니다.

2. Related Work

NeRF(신경 방사 필드)는 보정된 사진에서 사실적인 3D 장면을 만드는 데 널리 사용되는 방법입니다. NeRF를 편집하는 것은 기본 표현 방식 때문에 까다롭습니다. 물리학 기반 유도 바이어스, 예술적 스타일화 등 다양한 접근 방식으로 NeRF를 편집할 수 있습니다. 그러나 이러한 방법에는 국소화된 편집을 통합할 수 없거나 참조 이미지가 필요하다는 등의 한계가 있습니다.

![Untitled](Instruct-NeRF2NeRF%20Editing%203D%20Scenes%20with%20Instruct%20f18337f49f0e42c892f9259c88312c5a/Untitled%201.png)

이 연구는 2D 이미지 조건부 확산 모델을 활용하여 직관적인 언어 기반 편집 지침을 사용하여 3D 장면을 편집하는 새로운 접근 방식을 제안합니다. 이를 통해 마스크나 기타 복잡한 툴 없이도 오브젝트 또는 전체 씬의 모양이나 지오메트리를 편집할 수 있는 보다 유연하고 전체적인 접근 방식이 가능합니다.

GPT 및 ChatGPT와 같은 대규모 언어 모델은 복잡한 작업을 지정하기 위한 '프로그래밍 언어'로서 자연어의 잠재력을 입증했습니다. 이 작업은 3D로 인스트럭션 편집을 시연하는 최초의 사례로, 전문 도구나 지식이 없는 초보자도 쉽게 사용할 수 있습니다. 사용자는 자연어 명령어를 사용하여 3D 편집에 대한 풍부한 경험 없이도 고품질의 결과물을 얻을 수 있습니다.

3. Method

이 방법은 "그를 알버트 아인슈타인으로 바꿔줘"와 같은 자연어 편집 명령과 함께 재구성된 NeRF 장면, 캡처된 이미지, 카메라 포즈, 카메라 보정을 입력으로 받습니다. NeRF 피사체의 편집된 버전과 입력 이미지의 편집된 버전을 출력합니다. 이 방법은 확산 모델을 사용하여 캡처된 시점의 이미지 콘텐츠를 반복적으로 업데이트하고 표준 NeRF 학습을 통해 이러한 편집 내용을 3D로 통합합니다.

NeRF는 체적 3D 장면을 재구성하고 렌더링하기 위한 표현입니다. 캡처한 이미지 모음과 해당 카메라 파라미터를 사용하여 3D 장면을 최적화합니다. 인스트럭트픽스2픽스는 RGB 이미지, 텍스트 인스트럭션, 노이즈 입력 이미지를 사용하여 입력 이미지의 편집 버전을 생성하는 이미지 편집을 위한 확산 기반 방식입니다.

제안된 방법은 확산 모델을 사용하여 훈련 데이터 세트 이미지를 반복적으로 업데이트하고 이러한 업데이트된 이미지에 대해 NeRF를 훈련하여 전역적으로 일관된 3D 표현으로 통합하는 교대 업데이트 방식을 통해 작동합니다. 이 프로세스를 통해 원본 씬의 구조와 동일성을 유지하면서 확산 전구를 3D 씬에 점진적으로 통합할 수 있습니다.

이 방법의 핵심 구성 요소는 반복적 데이터 세트 업데이트로, NeRF에서 이미지를 렌더링하고 확산 모델로 업데이트한 후 이를 사용하여 NeRF 재구성을 감독하는 과정을 번갈아 가며 수행합니다. 이 프로세스는 초기 반복에서 일관되지 않은 편집을 초래할 수 있지만 결국에는 전역적으로 일관된 편집 장면으로 수렴됩니다.

4

연구진은 3D 장면 편집 방식을 평가하기 위해 다양한 실제 장면을 대상으로 실험을 진행했습니다. 연구진은 스마트폰과 미러리스 카메라로 촬영한 다양한 복잡성의 360도 장면을 사용했으며, 콜맵 또는 폴리캠 앱을 통해 포즈를 추출했습니다. 실험에는 정성적 평가와 다른 방법과의 비교가 포함되었습니다. 실험 결과, 이 접근 방식은 새로운 오브젝트를 추가하고 재질을 수정하는 등 다양한 편집 작업을 효과적으로 수행할 수 있는 것으로 나타났습니다. 또한 시간, 계절, 조건의 변화에 적응하면서 대규모 장면에도 잘 작동했습니다. 이 방법에는 InstructPix2Pix의 문제점을 계승하고 아티팩트에 직면하는 등 몇 가지 한계가 있지만, 3D 씬 편집 영역에서 유망한 결과를 보여주었습니다.

5

연구진은 일반 사용자가 3D 씬 편집에 더 쉽게 접근할 수 있도록 하는 중요한 단계인 Instruct-NeRF2NeRF를 도입했습니다. 이 방법을 사용하면 자연스러운 텍스트 지침을 사용하여 직관적으로 NeRF 씬을 편집할 수 있으며, 3D 일관성을 유지하면서 편집할 수 있습니다. 연구팀은 캡처한 다양한 NeRF 씬에 대한 접근 방식의 효과를 입증하여 사람, 사물, 대규모 환경에 대한 다양한 편집을 수행할 수 있음을 입증했습니다.

논문의 추가 구현 세부 사항에서 저자는 편집된 NeRF를 최적화하는 방법이 확산 모델을 지침으로 사용한다고 설명합니다. 편집된 NeRF는 최적화 과정에서 편집의 유형과 강도가 달라지며, 결과는 다양한 반복 횟수를 거친 후에 표시됩니다. 저자들은 InstructPix2Pix의 공개 구현을 사용하며, 그들의 방법은 그 한계를 상속한다는 점에 유의합니다. 저자들은 편집된 NeRF 장면이 원래 재구성된 NeRF에 비해 약간 더 흐릿한 텍스처를 포함하는 경우가 많으며, 그 원인으로 InstructPix2Pix에서 사용하는 자동 인코더가 있을 수 있다고 논의합니다. 정량적 평가에서는 두 가지 지표를 보고합니다: 클립 방향 점수 및 클립 방향 일관성 점수입니다. CLIP 방향성 점수는 텍스트 캡션의 변화가 이미지의 변화와 얼마나 일치하는지를 측정하고, CLIP 일관성 점수는 렌더링된 새로운 카메라 경로에서 인접한 각 프레임 쌍의 CLIP 임베딩의 코사인 유사성을 측정합니다.

- 인스트럭트픽스2픽스
    
    인스트럭트픽스2픽스는 AI를 사용하여 텍스트 지시에 따라 이미지를 수정하는 이미지 편집 방식입니다. 노이즈가 있는 이미지로 시작하여 텍스트에 설명된 대로 원하는 편집 결과와 일치하도록 이미지를 점진적으로 변형합니다. 이 방법은 이미지의 인코딩된 버전에서 작동한 다음 편집 후 다시 눈에 보이는 이미지로 디코딩합니다. 이 접근 방식을 사용하면 자연어 지침에 따라 이미지를 더 쉽게 편집할 수 있습니다.