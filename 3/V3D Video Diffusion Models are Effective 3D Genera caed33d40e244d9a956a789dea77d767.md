# V3D: Video Diffusion Models are Effective 3D Generators

[https://arxiv.org/abs/2403.06738](https://arxiv.org/abs/2403.06738)

- Mar 2024

![V3D에 의해 생성된 섬세한 3D 자산. 우리의 접근 방식은 3분 이내에 고화질의 3D 객체를 생성할 수 있습니다.](V3D%20Video%20Diffusion%20Models%20are%20Effective%203D%20Genera%20caed33d40e244d9a956a789dea77d767/Untitled.png)

V3D에 의해 생성된 섬세한 3D 자산. 우리의 접근 방식은 3분 이내에 고화질의 3D 객체를 생성할 수 있습니다.

### 1 Introduction

이 논문의 서론에서는 확산 기반 이미지 생성 기술의 발전과 함께 자동 3D 생성 분야에서 최근 이루어진 주목할 만한 진보에 대해 설명합니다. 특히, 텍스트에서 이미지로의 확산 모델을 활용하여 직접적인 전제 조건을 추출하는 Score Distillation Sampling과 같은 접근 방법이 3D 생성 분야에서 큰 성공을 거두었다고 언급합니다. 하지만, 이런 최적화 기반 방법들은 처리 속도가 느리고, 특정 출력에 치우치는 모드 붕괴나 양면성 문제(Janus problem)와 같은 문제점을 가지고 있습니다.

이에 대한 대안으로, 다양한 방식들이 제안되어 이 문제들을 해결하려고 시도하였습니다. 그 중 하나는 이미지를 직접 3D 표현으로 변환하는 것이고, 다른 하나는 일관된 다중 시점 이미지를 먼저 생성한 후 이를 이용해 3D 모델을 복원하는 것입니다. 이러한 방식들은 3D 데이터의 크기와 모델의 용량이 증가함에 따라 생성 시간을 크게 단축시켰지만, 여전히 훈련 과정에는 한계가 있습니다.

최근에는 비디오 확산 모델이 복잡한 장면과 동적인 요소를 시공간적 일관성을 가지고 생성할 수 있는 능력으로 인해 큰 관심을 받고 있습니다. 이러한 모델은 다양한 관점에서 3D 객체를 관찰하는 비디오를 생성할 수 있어, 3D 세계를 인식하는 데 큰 가능성을 제시합니다. 그러나 다중 시점에서의 일관성을 유지하는 것은 여전히 큰 도전 과제입니다.

이러한 배경을 바탕으로, 이 논문에서는 V3D라는 새로운 3D 생성 접근 방법을 제안합니다. V3D는 비디오 확산 모델을 미세 조정하여 다양한 관점에서의 이미지를 일관되게 생성한 후, 이를 기반으로 3D 모델을 복원함으로써 고품질의 3D 생성을 가능하게 합니다. 이 접근법은 객체 중심 생성과 장면 수준 생성 모두에 적용될 수 있으며, 고품질의 3D 자산을 빠르게 재구성할 수 있는 새로운 가능성을 열어줍니다.

### 2 Related Work

이 분야의 초기 연구들은 주로 CLIP 같은 도구를 활용해 개별 장면에 대한 최적화 방법에 초점을 맞추었습니다. DreamFusion은 이 분야의 선구적 작업으로, 더 강력한 확산 전제 조건을 도입하여 3D 자산에서 렌더링된 이미지와 확산 전제 조건 사이의 차이를 최소화하는 방식을 채택했습니다. 이후의 연구들은 이 패러다임을 품질, 최적화 속도, 양면성 문제 해결 등 다양한 측면에서 개선하고 확장해왔습니다.

또 다른 연구 방향은 최적화가 아닌 방법을 탐색하는 것으로, 일관된 다중 시점 이미지를 먼저 생성한 다음 이를 이용해 해당 3D 모델을 복원하는 접근 방식 등이 포함됩니다. SyncDreamer와 Wonder3D는 명시적인 복셀과 3D 컨볼루션을 결합하거나, 이미지 확산 모델을 미세 조정하여 생성된 시점들과 일치하는 정규 맵을 사용하는 등의 방법으로 일관된 다중 시점 생성의 일관성을 강화했습니다.

더불어, 일부 연구는 다중 시점 이미지를 직접 3D 표현으로 매핑하는 방식을 탐구하였습니다. 이러한 접근법은 더 메모리 효율적인 3D 가우시안 스플래팅을 사용하여 상대적으로 높은 해상도의 감독을 지원하는 등, 단일 또는 희소한 시점에서 직접 3D를 예측하는 대형 트랜스포머를 채택하였습니다.

새로운 시점 합성에 관한 연구는 NeRF와 3D 가우시안 스플래팅이 충분한 수의 입력으로 인상적인 성능을 보여주었지만, 희소한 시점에서는 추가적인 전제 조건이 필요함을 보여줍니다. 초기 연구는 일반화 가능한 장면 재구성을 위한 회귀 기반 또는 GAN 기반 방법에 중점을 두었지만, 고화질 데이터의 부족과 모델 용량의 한계로 인해 생성된 새로운 시점 이미지들은 종종 흐릿하고 일반화 능력이 부족했습니다. 이후의 연구들은 확산 전제 조건을 통합하여 장면 수준의 새로운 시점 합성을 개선하는 방법을 제안했습니다.

이러한 배경을 바탕으로, 이 논문은 V3D라는 새로운 접근 방식을 제안하며, 이는 비디오 확산 모델을 활용하여 일관된 다중 시점 프레임을 생성한 후 이를 바탕으로 기본 3D 콘텐츠를 재구성하는 프레임워크입니다. 이는 객체 및 장면 생성 모두에 일반적으로 적용 가능하며, 비디오 확산 모델의 기하학적 일관성 능력을 크게 향상시키는 방식입니다.

### 3 Approach

이 접근법의 핵심 아이디어는 다중 시점 합성을 비디오 생성으로 개념화하여, 대규모 사전 훈련된 비디오 확산 모델의 구조와 강력한 전제 조건을 활용하는 것입니다. 이 방법은 객체 중심의 이미지-투-3D 및 장면 수준의 새로운 시점 합성에 대해 구체적으로 논의합니다.

![제안된 V3D의 개요](V3D%20Video%20Diffusion%20Models%20are%20Effective%203D%20Genera%20caed33d40e244d9a956a789dea77d767/Untitled%201.png)

제안된 V3D의 개요

**객체 중심 3D 생성**

V3D는 기본 비디오 확산 모델을 3D 객체의 360° 오빗 비디오로 미세 조정하여, 단일 이미지에서 밀도 높은 다중 시점 이미지를 예측합니다. 이는 대규모 사전 훈련된 비디오 확산 모델이 제공하는 3D 세계에 대한 광범위한 이해를 활용하여 3D 데이터의 부족 문제를 해결하고, 비디오 확산 모델의 내재적인 네트워크 구조를 효과적으로 사용하여 충분한 수의 다중 시점 이미지를 생성합니다.

**조건 설정**

이 접근법은 전면 시점의 고수준 의미 정보를 확산 U-Net에 크로스 어텐션을 통해 주입하고, 입력의 채널 차원을 따라 그것의 잠재 표현을 연결함으로써 구현됩니다. 이는 비디오의 카메라 포즈와 복원에 사용되는 카메라 포즈 간의 불일치가 최종 결과에 중대한 손상을 일으킬 수 있음을 고려하여, 고도 각도를 지정하지 않고 무작위로 객체를 회전시키는 것과 같은 수정을 포함합니다.

**데이터 및 훈련**

객체 중심 이미지-투-3D를 위해, Objaverse 데이터셋에서 고품질의 3D 삼각형 메쉬로 구성된 하위 집합에서 모델을 미세 조정합니다. 훈련 과정은 EDM 프레임워크를 따르며, 분류자 없는 가이드를 가능하게 하기 위해 전면 시점의 잠재 표현과 CLIP 임베딩을 무작위로 0으로 설정합니다.

**3D 재구성 및 메쉬 추출**

비디오 확산 모델로 생성된 밀도 높은 시점을 바탕으로 하는 다음 단계는 이러한 시점들로부터 기본 3D 객체를 재구성하는 것입니다. 여기서는 이미지 수준의 지각 손실을 사용하여 다중 입력 이미지 간의 일관성을 해결하고, 3D 가우시안 스플래팅을 사용하여 고효율적인 3D 재구성을 지원합니다.

**초기화 및 메쉬 추출**

3D 가우시안 스플래팅의 효과적인 초기화를 위해, 공간 조각화를 제안하며, 이를 통해 필요하지 않은 최적화를 제거하고 초기 가우시안의 위치를 대략적으로 결정합니다. 실제 세계 응용을 위해, 먼저 SDF를 사용하여 표면을 추출한 다음, 생성된 다중 시점 이미지로 외관을 세밀하게 조정하는 새로운 메쉬 추출 방법을 제안합니다.

**장면 수준 새로운 시점 합성**

V3D는 또한 장면 수준의 새로운 시점 합성을 위해 비디오 확산을 확장합니다. 이는 주어진 카메라 경로를 따라 이미지를 생성하는 것을 포함하며, 이를 위해 PixelNeRF 인코더를 비디오 확산 모델에 통합하여 카메라 포즈를 정밀하게 제어하고 여러 입력 이미지에 쉽게 적응할 수 있습니다.

**데이터 및 훈련**

장면 수준 합성을 위해서는 MVImgNet에서 실세계 비디오 클립으로 모델을 미세 조정합니다. 이 과정에서는 PixelNeRF 인코더의 매개변수를 규제하는 간소화된 확산 손실을 사용하며, 입력 시점의 조건을 무작위로 선택하여 모델을 훈련합니다.

![제안된 V3D와 최신 3DGS 기반 이미지-투-3D 방법(LGM [85] 및 TriplaneGaussian [117]) 및 최적화 기반 방법(Magic123 [69] 및 ImageDream [95]) 사이의 정성적 비교. 해당 360° 비디오는 보충 비디오에서 시연됩니다.](V3D%20Video%20Diffusion%20Models%20are%20Effective%203D%20Genera%20caed33d40e244d9a956a789dea77d767/Untitled%202.png)

제안된 V3D와 최신 3DGS 기반 이미지-투-3D 방법(LGM [85] 및 TriplaneGaussian [117]) 및 최적화 기반 방법(Magic123 [69] 및 ImageDream [95]) 사이의 정성적 비교. 해당 360° 비디오는 보충 비디오에서 시연됩니다.

이 챕터는 V3D 방법론이 어떻게 객체 중심 및 장면 수준 3D 생성 문제에 접근하는지, 그리고 이를 구현하기 위해 사용되는 기술적 세부 사항을 상세히 설명합니다. V3D는 기존의 비디오 확산 모델을 활용하여 고도의 일관성과 품질을 가진 3D 콘텐츠를 생성할 수 있는 새로운 방법을 제시합니다.

### 4 Experiments

이 챕터는 V3D가 객체 중심 3D 생성과 장면 수준의 새로운 시점 합성에서 어떻게 성능을 발휘하는지에 대한 정량적 및 정성적 비교를 통해 이전 연구와의 차별점을 보여줍니다.

**객체 중심 3D 생성**

- **정성적 비교**: V3D는 3DGS 기반 방법과 SDS 기반 방법에 비해 뛰어난 품질의 결과를 보여줍니다. 특히, V3D는 더 선명하고 세밀한 텍스처를 가진 3D 객체를 생성하며, 다른 방법들에 비해 전면 시점의 정렬과 실제감이 뛰어납니다. 이러한 결과는 V3D가 기존 방법들에 비해 상당한 개선을 이루었음을 나타냅니다.

![제안된 V3D와 최신 다중 시점 생성 방법, SyncDreamer [50] 및 Wonder3D [52] 사이의 정성적 비교. 해당 360° 비디오는 보충 비디오에서 보여집니다.](V3D%20Video%20Diffusion%20Models%20are%20Effective%203D%20Genera%20caed33d40e244d9a956a789dea77d767/Untitled%203.png)

제안된 V3D와 최신 다중 시점 생성 방법, SyncDreamer [50] 및 Wonder3D [52] 사이의 정성적 비교. 해당 360° 비디오는 보충 비디오에서 보여집니다.

- **정량적 비교**: 사용자 연구를 통해 V3D와 다른 방법들을 비교합니다. 평가자들은 V3D가 조건 이미지와의 정렬과 생성된 객체의 실제감 측면에서 가장 설득력 있는 모델로 평가했습니다. 이는 V3D가 다른 경쟁 모델들을 큰 차이로 능가함을 시사합니다.

**장면 수준 새로운 시점 합성**

![CO3D 데이터셋의 소화전 하위 집합에서 SparseFusion과의 장면 수준 새로운 시점 합성에 대한 정성적 비교. COLMAP을 사용하여 희소 포인트 클라우드를 재구성하고, 실제 이미지로 추출된 포인트 클라우드와의 포인트 수 및 Chamfer 거리를 보고합니다.](V3D%20Video%20Diffusion%20Models%20are%20Effective%203D%20Genera%20caed33d40e244d9a956a789dea77d767/Untitled%204.png)

CO3D 데이터셋의 소화전 하위 집합에서 SparseFusion과의 장면 수준 새로운 시점 합성에 대한 정성적 비교. COLMAP을 사용하여 희소 포인트 클라우드를 재구성하고, 실제 이미지로 추출된 포인트 클라우드와의 포인트 수 및 Chamfer 거리를 보고합니다.

- V3D는 CO3D 데이터셋의 10가지 범주에서 장면 수준 새로운 시점 합성의 성능을 검증합니다. 이 실험에서 V3D는 이미지 메트릭 측면에서 이전의 최신 기법들을 명확한 차이로 능가하는 성능을 보여줍니다. 이는 V3D가 사전 훈련된 비디오 확산 모델을 활용하여 장면 수준의 새로운 시점 합성에서 효과적인 성능을 발휘할 수 있음을 입증합니다.
- **정성적 비교**: CO3D 데이터셋의 특정 하위 집합에 대한 V3D와 SparseFusion의 결과를 비교합니다. V3D로 생성된 이미지에서 복원된 포인트 클라우드는 더 많은 포인트를 포함하고 실제 이미지로부터 복원된 포인트 클라우드와 더 가까운 Chamfer 거리를 보여줍니다. 이는 V3D가 재구성 품질과 다중 시점 일관성 측면에서 유의미한 이점을 제공함을 나타냅니다.

![Untitled](V3D%20Video%20Diffusion%20Models%20are%20Effective%203D%20Genera%20caed33d40e244d9a956a789dea77d767/Untitled%205.png)

Ablation 연구. 미세 조정 단계의 수, 대규모 사전 훈련, 그리고 강력한 노이즈 분포가 약속된 결과를 달성하기 위해 중요하다는 것을 보여줍니다.

**기타 실험**

- **Ablation Study**: 대규모 비디오 사전 훈련의 중요성, 미세 조정 단계의 수, 사전 훈련된 모델의 잠재력, 그리고 카메라 조건 설정의 영향에 대한 추가 실험을 통해 V3D의 다양한 구성 요소가 최종 성능에 미치는 영향을 분석합니다. 이 실험들은 V3D의 성공적인 구현을 위해 특정 요소들이 얼마나 중요한지를 보여줍니다.

이 챕터는 V3D가 제안하는 새로운 접근 방법이 기존 방법들과 비교하여 우수한 성능을 보이는 여러 증거를 제공합니다. 특히, 사용자 연구와 비교 실험을 통해 V3D의 우수성을 강조하며, 다양한 설정에서의 실험을 통해 그 유효성을 입증합니다. 이를 통해 V3D가 고품질의 3D 생성에 있어 강력하고 유용한 도구임을 확인할 수 있습니다.

### 5 Limitations and Conclusion

**한계**

이 연구에서 제시한 V3D 접근법은 여러 면에서 혁신적이지만, 몇 가지 한계점을 가지고 있습니다. 복잡한 객체나 장면에서 일관성 있는 다중 시점 생성이나 정확한 기하학적 구조를 재현하는 데 있어 불완전한 결과를 낼 수 있습니다. 또한, 다중 시점 사이의 일관성 유지나 불합리한 기하학적 형태의 생성과 같은 문제들은 여전히 도전적인 과제로 남아 있습니다. 이러한 한계는 향후 연구에서 개선될 필요가 있습니다.

**결론**

이 논문은 비디오 확산 모델을 활용하여 고품질의 3D 생성을 가능하게 하는 V3D라는 새로운 접근법을 제안합니다. 객체 중심 및 장면 수준 생성 모두에서 V3D는 기존 방법들과 비교하여 뛰어난 성능을 보였습니다. V3D는 사전 훈련된 비디오 확산 모델의 강력한 3D 인식 능력을 바탕으로, 일관된 다중 시점 이미지를 생성하고 이를 통해 정밀한 3D 모델을 재구성합니다. 실험 결과는 V3D가 기존의 최신 기술들을 능가하는 성능을 달성했음을 보여줍니다.

또한, V3D는 비디오 확산 모델을 3D 생성 작업에 효과적으로 적용하는 새로운 방법론을 제시함으로써, 이 분야에서의 진전을 촉진할 수 있습니다. 이 접근법은 향후 고품질의 3D 콘텐츠 생성을 위한 효율적이고 강력한 도구로서 활용될 잠재력을 가지고 있습니다. 연구자들은 V3D의 한계를 극복하고, 이 기술을 다양한 3D 생성 및 컴퓨터 비전 작업에 적용하기 위해 추가적인 연구를 진행할 것으로 기대됩니다.