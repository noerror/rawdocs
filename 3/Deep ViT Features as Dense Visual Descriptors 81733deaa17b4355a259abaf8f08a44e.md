# Deep ViT Features as Dense Visual Descriptors

[https://dino-vit-features.github.io/](https://dino-vit-features.github.io/)

[https://github.com/ShirAmir/dino-vit-features](https://github.com/ShirAmir/dino-vit-features)

[https://arxiv.org/abs/2112.05814](https://arxiv.org/abs/2112.05814)

- Dec 2021

### 1 Introduction

이 글에서는 컨볼루션 신경망(CNN)의 대안으로 시각 작업에서 비전 트랜스포머(ViT)의 잠재력과 효과에 대해 설명합니다. 이러한 ViT는 폐색, 적대적 공격, 텍스처 편향에 대해 더 탄력적인 것으로 입증되었습니다.

![우리는 깊은 ViT 특징에 대한 새로운 관찰을 기반으로, 기본적인 시각 작업(e.g. 부분 공통 세분화 및 의미론적 대응)을 해결하기 위한 경량의 제로샷 방법을 고안했습니다. 우리의 방법은 이미지가 서로 다른 클래스(e.g. 여우와 표범)에 속하는 등의 어려운 설정에서도 적용 가능합니다.](Deep%20ViT%20Features%20as%20Dense%20Visual%20Descriptors%2081733deaa17b4355a259abaf8f08a44e/Untitled.png)

우리는 깊은 ViT 특징에 대한 새로운 관찰을 기반으로, 기본적인 시각 작업(e.g. 부분 공통 세분화 및 의미론적 대응)을 해결하기 위한 경량의 제로샷 방법을 고안했습니다. 우리의 방법은 이미지가 서로 다른 클래스(e.g. 여우와 표범)에 속하는 등의 어려운 설정에서도 적용 가능합니다.

사전 학습된 신경망의 활성화 계층에서 추출된 특징인 딥 피처는 전통적으로 시각 작업에 사용되어 왔습니다. 그러나 이러한 특징은 대부분 CNN 기반 모델의 맥락에서 검토되어 왔습니다. 이 연구는 사전 학습된 두 가지 ViT 모델에 초점을 맞춰 ViT의 딥 피처를 사용할 때 얻을 수 있는 이점을 살펴보는 것을 목표로 합니다.

첫 번째 모델은 이미지 분류를 위해 훈련된 지도형 ViT이고, 두 번째 모델은 자가 증류 접근법을 사용하여 훈련된 자가 지도형 ViT(DINO-ViT)입니다. 연구진은 이러한 모델의 자기 주의 모듈을 자세히 살펴보고 여러 계층에 걸쳐 다양한 측면을 분석합니다.

연구 결과, DINO-ViT 기능은 높은 공간 해상도와 의미론적 객체 부분으로 높은 수준의 정보를 캡처하는 강력한 기능을 가지고 있는 것으로 밝혀졌습니다. 또한, DINO-ViT 기능은 서로 다르지만 관련된 객체 클래스 간에 의미 정보를 공유하는 것으로 보입니다. 또한 연구진은 위치 정보가 레이어를 거치면서 점차 감소하므로 중간 레이어가 위치와 의미를 모두 인코딩한다는 점에 주목했습니다.

연구진은 이러한 DINO-ViT의 특징을 공동 분할, 부품 공동 분할, 의미점 대응 등 다양한 비전 작업에 적용합니다. 또한 입력 이미지의 수나 영역에 제한이 없는 경우와 같이 까다로운 조건에서 부품 공동 분할을 처리하기 위해 연구를 확장하고 있습니다. 또한 이 부품 공동 분할을 비디오에 어떻게 적용할 수 있는지도 보여줍니다.

이 연구는 DINO-ViT 기능이 이미 충분히 강력하여 각 작업에 맞게 특별히 설계된 최첨단 모델과 비교하여 경쟁력 있는 결과를 얻을 수 있다는 점을 강조합니다. 연구진은 ViT 특징이 주목도를 넘어서는 국지적인 의미 정보를 제공하며, 이러한 새로운 관찰을 통해 가벼운 제로샷 방법론으로 공동 세분화 및 의미 대응을 처리할 수 있다고 결론지었습니다. 이 연구는 극한 환경에서 부품 공동 세분화를 입증한 최초의 연구입니다.

### 2 Related Work

이 글에서는 시각 작업 분야에서 기존의 컨볼루션 신경망(CNN)을 대체할 수 있는 비전 트랜스포머(ViT)의 잠재력에 대해 살펴봅니다. CNN은 물체 감지, 분할, 이미지 생성 등 다양한 시각 작업의 기초를 형성하는 기능으로 광범위하게 사용되어 왔습니다. 그러나 CNN은 텍스처에 대한 강한 편향성, 위치 정보 부족, 장거리 종속성을 캡처할 수 있는 제한된 용량과 같은 내재적 한계를 가지고 있습니다.

최근에는 덜 제한적인 아키텍처를 갖춘 비전 트랜스포머(ViT)가 CNN의 강력한 대안으로 부상하고 있습니다. 이 기술은 오클루전, 적대적 공격, 텍스처 편향에 대해 더 나은 견고성을 보여줍니다. 자체 증류 방식을 사용하여 레이블 없이 훈련된 ViT 모델인 DINO-ViT라는 특정 모델도 언급됩니다. 이 모델의 기능은 이미지 검색과 객체 분할에 효과적인 것으로 입증되었습니다.

여기서는 여러 계층(관심 계층의 키, 값, 쿼리)에 걸쳐 비전 트랜스포머의 기능을 더 깊이 파고드는 데 중점을 둡니다. 목표는 이러한 기능의 새로운 속성을 발견하고 이러한 발견을 근본적인 비전 작업을 해결하는 데 적용하는 것입니다.

세 가지 작업이 강조 표시됩니다:

공동 세분화: 이는 세트의 모든 이미지에 공통된 객체를 공동으로 분할하는 것을 목표로 합니다. 저자들은 특히 클래스 간 시나리오에서 기존의 CNN 기반 방법보다 성능이 뛰어난 비지도 접근 방식을 제안합니다.

부품 공동 세그멘테이션: 이 작업은 유사한 이미지 집합에서 공통된 물체 부분을 찾는 것입니다. 저자는 이 작업을 위해 사전 학습된 자율 지도 ViT를 사용하여 CNN 기반 방법과 경쟁적으로 수행하며 더 까다로운 시나리오로 적용 범위를 확장합니다.

의미적 대응: 이 작업에는 두 이미지 사이에서 의미적으로 일치하는 점을 찾는 것이 포함됩니다. 저자는 ViT 기능이 이전의 비지도 방식보다 포즈와 스케일의 차이에 더 강하면서 지도 방식과 경쟁할 수 있음을 보여줍니다.

본질적으로 이 연구는 비전 트랜스포머에 대한 이해를 높이고 실제 시각 작업에 적용하여 CNN 기반 모델과 비교하여 경쟁력 있는 성능을 보여주는 것을 목표로 합니다.

### 3 ViT Features as Local Patch Descriptors

저자들은 이미지의 로컬 패치 설명자로서의 비전 트랜스포머(ViT) 기능에 대한 탐구에 대해 설명합니다. 전체 이미지를 한 번에 처리하는 CNN과 달리 ViT는 이미지를 겹치지 않는 작은 패치로 나누고 각 패치를 개별적으로 처리하여 공간 정보를 보존하고 의미 및 위치 세부 사항을 더 잘 표현할 수 있습니다.

![ViT 아키텍처 (왼쪽). 이미지는 n개의 비중첩 패치로 분할되고 [CLS] 토큰을 얻습니다. 이러한 패치들은 임베딩되어, 위치 임베딩이 추가되고, 변형계층을 통과합니다. 각 패치는 각 계층에서 특징의 세트와 직접 연결되어 있습니다: 키, 질의, 값, 그리고 토큰; 각각이 패치 설명자로 사용될 수 있습니다. 깊은 특징 시각화를 위한 PCA (오른쪽): 지도 및 자율 학습 (a) ViTs 및 (b) CNN-ResNet 모델에 적용됩니다. 우리는 AFHQ [8]의 18개 이미지를 각 모델에 공급하여, 주어진 계층에서 특징을 추출하고, 이들에 대해 PCA를 수행합니다. 각 모델에 대해, 예시 이미지 (그림 7의 달마시안 개 왼쪽)에 대한 각 계층에서 PCA 구성요소를 시각화합니다: 첫 번째 구성요소는 상단에 표시되고, 두 번째부터 네 번째 구성요소는 아래에 RGB 이미지로 표시됩니다. ResNet PCA는 시각화를 위해 업샘플링됩니다.](Deep%20ViT%20Features%20as%20Dense%20Visual%20Descriptors%2081733deaa17b4355a259abaf8f08a44e/Untitled%201.png)

ViT 아키텍처 (왼쪽). 이미지는 n개의 비중첩 패치로 분할되고 [CLS] 토큰을 얻습니다. 이러한 패치들은 임베딩되어, 위치 임베딩이 추가되고, 변형계층을 통과합니다. 각 패치는 각 계층에서 특징의 세트와 직접 연결되어 있습니다: 키, 질의, 값, 그리고 토큰; 각각이 패치 설명자로 사용될 수 있습니다. 깊은 특징 시각화를 위한 PCA (오른쪽): 지도 및 자율 학습 (a) ViTs 및 (b) CNN-ResNet 모델에 적용됩니다. 우리는 AFHQ [8]의 18개 이미지를 각 모델에 공급하여, 주어진 계층에서 특징을 추출하고, 이들에 대해 PCA를 수행합니다. 각 모델에 대해, 예시 이미지 (그림 7의 달마시안 개 왼쪽)에 대한 각 계층에서 PCA 구성요소를 시각화합니다: 첫 번째 구성요소는 상단에 표시되고, 두 번째부터 네 번째 구성요소는 아래에 RGB 이미지로 표시됩니다. ResNet PCA는 시각화를 위해 업샘플링됩니다.

이 연구는 ViT의 토큰(쿼리, 키, 값, 토큰)의 핵심 측면에 초점을 맞추는데, 이는 다른 측면에 비해 배경의 혼란에 덜 민감하게 반응하여 약간 더 나은 표현을 제공하기 때문입니다.

저자는 사전 학습된 두 가지 ViT 모델, 즉 지도 학습(이미지 분류를 위해 ImageNet 레이블로 학습)과 자가 지도 학습(자체 증류 방식을 사용하여 학습된 DINO-ViT)을 살펴봅니다. 이 모델에서 키의 주요 구성 요소를 시각화하여 ViT 모델이 모든 계층에서 공간 해상도를 유지하여 더 깊은 계층의 의미 정보와 공간 해상도를 교환하는 CNN-ResNet 모델에 비해 세분화된 의미 정보와 더 높은 공간 해상도를 제공한다는 것을 관찰했습니다.

초기 계층의 저수준 요소에서 심층 계층의 고수준 개념에 이르는 표현 계층 구조를 가진 CNN과 달리, ViT의 표현 계층 구조는 다릅니다. 얕은 계층의 피처에는 주로 위치 정보가 포함되지만, 더 깊은 계층에서는 더 많은 의미적 피처를 위해 위치 정보가 줄어듭니다. 그러나 중간 ViT 특징에는 위치 정보와 의미 정보를 모두 포함합니다.

![. t-SNE 시각화. 우리는 PASCAL-Parts [6]의 5가지 동물 카테고리에서 10개의 이미지를 가져옵니다. (a)는 대표 이미지와 실제 부분 세그먼트를 보여줍니다. 각 이미지에 대해 DINO-ViT와 지도 ViT에서 ViT 특징을 추출합니다. 각 모델에 대해, 모든 특징은 t-SNE [41]를 사용하여 2D로 공동으로 투영됩니다. 각 2D 포인트는 그것의 실제 부분에 따라 색칠되고, 그 모양은 클래스를 나타냅니다. (b)에서는 DINO-ViT 특징이 다른 객체 카테고리를 가로질러 주로 부분별로 구성되어 있습니다, 반면에 (c)에서 지도 ViT 특징은 객체 부분에 상관없이 주로 클래스별로 그룹화됩니다.](Deep%20ViT%20Features%20as%20Dense%20Visual%20Descriptors%2081733deaa17b4355a259abaf8f08a44e/Untitled%202.png)

. t-SNE 시각화. 우리는 PASCAL-Parts [6]의 5가지 동물 카테고리에서 10개의 이미지를 가져옵니다. (a)는 대표 이미지와 실제 부분 세그먼트를 보여줍니다. 각 이미지에 대해 DINO-ViT와 지도 ViT에서 ViT 특징을 추출합니다. 각 모델에 대해, 모든 특징은 t-SNE [41]를 사용하여 2D로 공동으로 투영됩니다. 각 2D 포인트는 그것의 실제 부분에 따라 색칠되고, 그 모양은 클래스를 나타냅니다. (b)에서는 DINO-ViT 특징이 다른 객체 카테고리를 가로질러 주로 부분별로 구성되어 있습니다, 반면에 (c)에서 지도 ViT 특징은 객체 부분에 상관없이 주로 클래스별로 그룹화됩니다.

연구진은 또한 감독형과 자가 감독형 ViT 모델 간의 차이점에 주목했습니다. 자체 감독된 DINO-ViT 키는 여러 클래스에 걸쳐 신체 부위의 의미적 유사성을 표시하는 반면, 감독된 ViT는 신체 부위와 관계없이 각 클래스 내에서 유사성을 표시합니다. 이는 감독된 ViT 공간 특징이 글로벌 클래스 정보를 강조하는 반면, DINO-ViT 특징에는 시맨틱 객체 부분과 유사한 로컬 시맨틱 정보가 있다는 것을 보여줍니다.

![ViT의 면면: 우리는 원본 이미지 (a)의 자홍색 포인트와 연관된 특징과 대상 이미지 (b)의 모든 특징 간의 유사성을 계산합니다. 우리는 중간 특징 (상단 행)과 마지막 계층의 특징 (하단 행)에 대해 이를 수행합니다. (c-f)는 DINO-ViT의 다른 면을 특징으로 사용할 때 발생하는 유사성 맵입니다: 토큰, 질의, 값, 그리고 키. 빨간색은 높은 유사성을 나타냅니다. 각 면에 대해, 대상 이미지에서 가장 가까운 점은 면 이름 근처에 지정된 고유한 색상으로 표시됩니다. 키들 (f)은 다른 면들에 비해 더 깔끔한 유사성 맵을 가지고 있습니다.](Deep%20ViT%20Features%20as%20Dense%20Visual%20Descriptors%2081733deaa17b4355a259abaf8f08a44e/Untitled%203.png)

ViT의 면면: 우리는 원본 이미지 (a)의 자홍색 포인트와 연관된 특징과 대상 이미지 (b)의 모든 특징 간의 유사성을 계산합니다. 우리는 중간 특징 (상단 행)과 마지막 계층의 특징 (하단 행)에 대해 이를 수행합니다. (c-f)는 DINO-ViT의 다른 면을 특징으로 사용할 때 발생하는 유사성 맵입니다: 토큰, 질의, 값, 그리고 키. 빨간색은 높은 유사성을 나타냅니다. 각 면에 대해, 대상 이미지에서 가장 가까운 점은 면 이름 근처에 지정된 고유한 색상으로 표시됩니다. 키들 (f)은 다른 면들에 비해 더 깔끔한 유사성 맵을 가지고 있습니다.

저자는 ViT 표현의 다양한 측면과 이미지 관련 작업에서 모델의 성능에 어떻게 기여하는지를 더 탐구하고 이해하는 것을 목표로 합니다.

### 4 Deep ViT Features Applied to Vision Tasks

저자는 추가 교육이나 미세 조정 없이도 여러 시각 작업에 효과적인 로컬 패치 설명자로 자체 감독 비전 트랜스포머 모델(DINO-ViT)을 사용하는 방법을 시연합니다. 공동 세분화, 부품 공동 세분화, 포인트 대응 및 해상도 향상을 위한 애플리케이션을 제안합니다.

![공통 세분화 및 부분 공통 세분화 파이프라인. 입력 이미지 (a)는 DINO-ViT에 별도로 공급되어 (b) 공간 밀도 설명자와 (c) 관심 맵 (ViT의 자체 주의 맵에서)을 얻습니다. 추출된 모든 설명자들은 함께 클러스터링됩니다 (d). 각 클러스터는 관심 맵 기반의 투표 과정을 통해 전경 또는 배경으로 지정됩니다. 전경 세그먼트는 공통 세분화 결과 (e)를 형성합니다. 이 과정은 전경 특징 (f)만을 사용하여 반복되어 공통 부분 (h)을 생성합니다.](Deep%20ViT%20Features%20as%20Dense%20Visual%20Descriptors%2081733deaa17b4355a259abaf8f08a44e/Untitled%204.png)

공통 세분화 및 부분 공통 세분화 파이프라인. 입력 이미지 (a)는 DINO-ViT에 별도로 공급되어 (b) 공간 밀도 설명자와 (c) 관심 맵 (ViT의 자체 주의 맵에서)을 얻습니다. 추출된 모든 설명자들은 함께 클러스터링됩니다 (d). 각 클러스터는 관심 맵 기반의 투표 과정을 통해 전경 또는 배경으로 지정됩니다. 전경 세그먼트는 공통 세분화 결과 (e)를 형성합니다. 이 과정은 전경 특징 (f)만을 사용하여 반복되어 공통 부분 (h)을 생성합니다.

공동 세분화의 경우 클러스터링과 투표를 포함하는 2단계 접근 방식을 제시하고, 그 다음 GrabCut을 사용하여 이진 공동 세분화 마스크를 구체화합니다. 설명자는 모든 이미지에서 의미론적 공통 세그먼트로 클러스터링된 다음 투표 절차를 거쳐 대부분의 이미지에서 눈에 띄고 공통적인 클러스터를 선택합니다.

부분 공동 세그멘테이션은 전경 설명자에 대해서만 클러스터링 단계를 반복하여 수행되며, 그 결과 이미지 전반에서 공통된 의미적 부분의 설명자가 함께 그룹화됩니다. 부품 마스크는 다중 레이블 CRF를 사용하여 더욱 세분화됩니다.

포인트 대응의 경우, 저자는 위치 바이어스와 비닝을 조합하여 사용합니다. 이전 레이어의 특징은 이미지 내 위치에 더 민감하기 때문에 사용되며, 위치와 의미 정보 간의 균형을 제공합니다.

마지막으로 해상도를 높이기 위해 저자는 테스트 시점에 겹치는 패치를 추출하고 그에 따라 위치 인코딩을 보간하도록 ViT를 수정하여 추가 학습 없이도 더 세밀한 공간 해상도를 얻을 수 있도록 합니다.

이 모든 애플리케이션은 다양한 시각 작업에서 DINO-ViT 표현의 효과를 보여주기 위해 고안되었습니다.

### 5 Results

저자들은 여러 작업에서 DINO-ViT 기반 방법의 결과와 평가를 제시합니다:

부품 공동 세분화:

이 방법은 별도의 교육 없이도 다양한 모양과 클래스, 독특한 피사체와 같은 까다로운 시나리오를 성공적으로 처리하여 이미지 쌍의 부품을 일관되게 분할합니다. 이 접근 방식은 비디오 공동 세분화에도 확장되어 프레임 전체에 걸쳐 시간적으로 일관된 부분을 제공합니다. 또한 다양한 동물의 얼굴과 새 종에 적용하여 비지도 방식에 비해 일관된 결과와 우수한 성능을 보여주었습니다.

![이미지 쌍의 부분 공통 세분화: 우리의 방법은 최소한 두 개의 입력 이미지로 주어진 공통 객체 부분을 의미론적으로 공통 세분화합니다. SM에서 더 많은 예시를 볼 수 있습니다.](Deep%20ViT%20Features%20as%20Dense%20Visual%20Descriptors%2081733deaa17b4355a259abaf8f08a44e/Untitled%205.png)

이미지 쌍의 부분 공통 세분화: 우리의 방법은 최소한 두 개의 입력 이미지로 주어진 공통 객체 부분을 의미론적으로 공통 세분화합니다. SM에서 더 많은 예시를 볼 수 있습니다.

![AFHQ에서의 부분 공통 세분화: 우리는 다양한 동물 얼굴의 1.5K 이미지를 포함한 AFHQ [8]의 테스트 세트에 우리의 방법을 적용합니다. 더 많은 결과는 SM에서 확인할 수 있습니다.](Deep%20ViT%20Features%20as%20Dense%20Visual%20Descriptors%2081733deaa17b4355a259abaf8f08a44e/Untitled%206.png)

AFHQ에서의 부분 공통 세분화: 우리는 다양한 동물 얼굴의 1.5K 이미지를 포함한 AFHQ [8]의 테스트 세트에 우리의 방법을 적용합니다. 더 많은 결과는 SM에서 확인할 수 있습니다.

![CUB에서의 부분 공통 세분화 비교: 우리는 CUB [55]에서 임의로 선택된 이미지에 대한 결과를 보여줍니다. 우리의 결과는 지도된 SCOPS [24]보다 부분 간에 더 의미론적으로 일관성이 있으며, 지도된 Choudhury 등 [9]에 경쟁력이 있습니다.](Deep%20ViT%20Features%20as%20Dense%20Visual%20Descriptors%2081733deaa17b4355a259abaf8f08a44e/Untitled%207.png)

CUB에서의 부분 공통 세분화 비교: 우리는 CUB [55]에서 임의로 선택된 이미지에 대한 결과를 보여줍니다. 우리의 결과는 지도된 SCOPS [24]보다 부분 간에 더 의미론적으로 일관성이 있으며, 지도된 Choudhury 등 [9]에 경쟁력이 있습니다.

공동 세분화:

저자들은 새로운 데이터 세트인 'PASCAL 공동 세분화'를 포함해 다양한 크기의 여러 공동 세분화 데이터 세트에서 접근 방식을 테스트했습니다. 이 방법은 비지도 방법을 능가하고 지도 방법의 성능과 일치하며, 심지어 클래스 간 시나리오에서 다른 모든 방법을 능가합니다. 이들은 제거 연구를 수행하여 다양한 기준선과 비교하고 이미지 간의 공통성에 초점을 맞추는 방법의 장점을 보여줍니다.

![클래스 간 공통 세분화를 위한 PASCAL-CO: 각 세트는 관련 클래스의 이미지를 포함합니다. 우리의 방법은 서로 다른 클래스의 모든 공통 객체 영역을 포착합니다, 이는 지도된 방법들 [31,56]과는 반대입니다. 관심 기준선 [3]의 결과는 잡음이 많습니다.](Deep%20ViT%20Features%20as%20Dense%20Visual%20Descriptors%2081733deaa17b4355a259abaf8f08a44e/Untitled%208.png)

클래스 간 공통 세분화를 위한 PASCAL-CO: 각 세트는 관련 클래스의 이미지를 포함합니다. 우리의 방법은 서로 다른 클래스의 모든 공통 객체 영역을 포착합니다, 이는 지도된 방법들 [31,56]과는 반대입니다. 관심 기준선 [3]의 결과는 잡음이 많습니다.

![NBB [1]와의 대응 비교: 동일 클래스 (상단 행)와 다른 클래스 (하단 행) 시나리오에서. 우리의 방법은 외형, 포즈, 그리고 크기 변화에 더욱 견고합니다. 전체 크기의 결과는 SM에서 이용할 수 있습니다.](Deep%20ViT%20Features%20as%20Dense%20Visual%20Descriptors%2081733deaa17b4355a259abaf8f08a44e/Untitled%209.png)

NBB [1]와의 대응 비교: 동일 클래스 (상단 행)와 다른 클래스 (하단 행) 시나리오에서. 우리의 방법은 외형, 포즈, 그리고 크기 변화에 더욱 견고합니다. 전체 크기의 결과는 SM에서 이용할 수 있습니다.

포인트 대응:

정성적으로 이 방법은 모양, 포즈, 스케일 변화에 대한 견고성 측면에서 VGG 기반 방법보다 우수한 성능을 보였습니다. 정량적으로는 Spair71k 데이터 세트에서 VGG 기반 방법의 성능을 능가하고 감독된 CATs 방법과의 성능 격차를 좁혔습니다. 제거 연구는 다른 패싯보다 키를 사용하는 선택과 모델의 초기 레이어에서 위치 민감도의 값을 검증합니다.

### 6 Conclusion

저자들은 다양한 감독 하에 비전 트랜스포머(ViT)가 학습한 기능에 대한 새로운 경험적 관찰을 수행했습니다. 그리고 이러한 인사이트를 여러 실제 비전 작업에 사용했습니다. 이러한 특징에 대해 간단한 제로 샷 방법론만을 사용하여 최첨단 감독 방법과 경쟁할 수 있는 결과를 얻을 수 있었습니다. 또한 다양한 클래스와 사용 가능한 훈련 세트가 부족한 도메인에서 부품 공동 세분화라는 새로운 기능을 도입했습니다. 저자들은 딥 ViT 기능이 딥 CNN 기능의 대안이 될 수 있다고 주장합니다. 저자들은 이 프로젝트에 기여한 동료들과 자금 지원에 감사를 표합니다.

- 요약
    
    소개 및 동기: 이 논문은 다양한 유형의 감독 하에 훈련된 비전 트랜스포머(ViT)가 학습한 내부 기능을 여러 시각 작업에 어떻게 활용할 수 있는지에 대한 탐색을 제시합니다. 저자는 이러한 기능을 추가적인 미세 조정이나 훈련 없이 제로 샷 방식으로 사용하여 공동 세분화 및 점 대응과 같은 작업을 수행하고자 합니다.
    
    방법론: 저자들은 DINO-ViT 기능을 로컬 패치 설명자로 사용하는 몇 가지 시각적 작업에 대한 접근 방식을 소개합니다.
    
    공동 세분화: 이미지 전반에서 공통 세그먼트를 찾기 위해 클러스터링과 투표를 포함하는 2단계 공동 세그멘테이션 접근 방식을 제시합니다. 이 프로세스에는 K-평균 클러스터링을 사용하여 디스크립터를 그룹화한 다음 투표 절차를 사용하여 대부분의 이미지에 공통적인 클러스터를 선택하는 과정이 포함됩니다. 그런 다음 이러한 클러스터는 '전경' 영역을 형성합니다.
    
    파트 공동 세분화: 클러스터링 단계를 반복하여 전경 오브젝트를 공통된 부분으로 추가적으로 공동 세그먼트화합니다. 다중 레이블 CRF를 사용하여 부품 마스크를 세분화합니다. 이 방법론은 다양한 객체 범주와 다양한 크기의 이미지 세트에 적용할 수 있습니다.
    
    포인트 대응: 각 디스크립터에 위치 정보와 컨텍스트를 통합하여 일치하는 점의 모호성을 줄이기 위해 노력합니다. '베스트 버디 페어(BBP)'라는 개념을 채택하여 이미지 간의 신뢰할 수 있는 일치를 감지합니다.
    
    결과: 저자들은 다양한 작업에 대한 방법론의 결과를 발표하고 토론합니다:
    
    부품 공동 세분화: 저자들은 제로샷 방법이 다양한 외형, 다양한 클래스, 훈련하기 어려운 도메인 등 다양한 과제에서 의미적으로 일관된 파트 세그먼트를 제공할 수 있음을 보여줍니다. 또한 비디오에 부품 공동 세그먼테이션을 적용하는 방법도 보여줍니다.
    
    공동 세그먼테이션: 이 논문에서는 여러 클래스 내 및 클래스 간 데이터 세트에 대한 공동 세분화 접근 방식을 평가하고 그 결과를 최첨단 지도 및 비지도 방법과 비교합니다. 이 방법은 지도 방식과 비교했을 때에도 경쟁력 있는 결과를 보여줍니다.
    
    포인트 대응: 저자들은 이 방법을 테스트하고 그 결과가 외모, 포즈, 스케일의 변화에 더 견고하다는 것을 보여줍니다. 또한 정량적 평가에서 다른 방법과 비교했을 때 우수한 성능을 보였습니다.
    
    결론: 이 논문은 저자들의 관찰과 방법이 시각 작업에서 딥 CNN 기능의 대안으로서 딥 ViT 기능의 잠재력을 보여준다는 주장으로 마무리됩니다. 저자는 가벼운 제로 샷 방법론으로 달성한 경쟁력 있는 결과와 클래스 및 까다로운 도메인에 걸친 부분 공동 세분화에서의 성공을 강조합니다.
    
- CNN vs ViT
    
    컨볼루션 신경망(CNN)과 비전 트랜스포머(ViT)는 모두 이미지 처리 작업에 사용되는 머신 러닝 모델이지만, 작동 원리는 매우 다릅니다:
    
    - 컨볼루션 신경망(CNN): 이 모델은 이미지와 같은 격자형 데이터를 처리하기 위해 특별히 설계되었습니다. 컨볼루션 레이어를 사용하여 이미지를 스캔하고 가장자리와 모양과 같은 로컬 특징을 감지합니다. 이는 이미지 위로 슬라이드하여 특징 맵을 출력하는 일련의 필터를 통해 이루어집니다. 또한 CNN은 풀링 레이어를 활용하여 표현의 공간 크기를 점진적으로 줄여 네트워크에서 매개변수와 계산의 양을 줄입니다. 이 프로세스는 모델이 계층적 표현을 학습하는 데 도움이 되므로 네트워크가 더 깊어질수록 감지하는 특징이 점점 더 복잡해집니다.
    - 비전 트랜스포머(ViT): 비전 트랜스포머는 기존 트랜스포머 모델이 문장을 단어의 연속으로 취급하는 것과 유사하게 이미지를 일련의 패치로 취급합니다. 그런 다음 이러한 패치는 1D 시퀀스에 선형적으로 임베드되고 트랜스포머에 의해 처리됩니다. ViT는 트랜스포머 모델의 자기 주의 메커니즘을 사용하여 이미지의 여러 부분 간의 관계를 포착합니다. CNN과 달리 레이어에 로컬 연결성을 명시적으로 적용하지 않고 대신 모델이 다양한 관계의 중요성을 학습할 수 있도록 합니다.
    
    본질적으로 CNN은 컨볼루션 레이어를 통해 로컬 특징에 더 중점을 두는 반면, ViT는 트랜스포머의 자체 주의 메커니즘을 통해 글로벌 관계에 중점을 둡니다.