# TokenFlow: Consistent Diffusion Features for Consistent Video Editing

*이 연구에서는 이미지 확산 모델을 이용한 새로운 텍스트 기반 비디오 편집 프레임워크를 제시하고 있습니다. 이는 확산 특성 공간에서 비디오의 내부 표현을 연구하고, 일관된 비디오 편집이 확산 특성 표현의 일관성을 통해 이루어질 수 있음을 보여줍니다. 이 방법은 기존의 베이스라인들을 능가하며, 시간적 일관성에서 중요한 향상을 보여줍니다.*

*그러나, 제한적인 부분도 있습니다. 이 방법은 원래 비디오의 움직임을 보존하는 데 초점이 맞춰져 있으므로, 구조적 변화를 필요로 하는 편집을 처리할 수 없습니다. 또한, 이 방법은 이미지 편집 기법을 기반으로 하여 원래 프레임의 구조를 보존하도록 설계되었습니다. 이미지 편집 기법이 구조를 보존하는 데 실패하면, 이 방법은 편집된 프레임에서 의미 없는 대응을 강제함으로써 시각적인 아티팩트를 초래할 수 있습니다. 마지막으로, LDM 디코더는 고주파 깜박임을 일으킵니다.*

*이 연구는 확산 모델의 공간에서 자연 비디오의 내부 표현(예: 시간적 중복성)에 대한 새로운 인사이트를 제공하며, 비디오 합성을 향상시키는 방법을 보여줍니다. 이는 이미지 모델을 비디오 작업에 활용하거나, 텍스트-비디오 모델을 설계하는 데 대한 미래 연구를 촉진할 수 있다고 믿습니다.*

[https://github.com/omerbt/TokenFlow](https://github.com/omerbt/TokenFlow)

[https://arxiv.org/abs/2307.10373](https://arxiv.org/abs/2307.10373)

- Jul 2023

### 1. Introduction

이 논문에서는 해상도, 길이, 복잡한 비디오 동역학에 관한 도전과제에 직면해 있는 새로운 분야인 텍스트-이미지 모델을 사용한 비디오 편집의 개념을 살펴봅니다. 연구자들은 공간 레이아웃과 원본 모션을 그대로 유지하면서 텍스트 프롬프트에 반응하는 고품질 비디오를 생성하는 것을 목표로 합니다.

![TokenFlow는 실제 비디오에 대해 일관된 고품질의 의미론적 수정을 가능하게 합니다. 주어진 입력 비디오(맨 윗줄)를 우리의 방법을 이용하여 목표 텍스트 프롬프트(중간 및 하단 행)에 따라 수정하면서, 원래 장면의 의미론적 레이아웃과 움직임을 유지합니다.](TokenFlow%20Consistent%20Diffusion%20Features%20for%20Consis%20ce531d0ecfa841a7bb4e830bc1517e9e/Untitled.png)

TokenFlow는 실제 비디오에 대해 일관된 고품질의 의미론적 수정을 가능하게 합니다. 주어진 입력 비디오(맨 윗줄)를 우리의 방법을 이용하여 목표 텍스트 프롬프트(중간 및 하단 행)에 따라 수정하면서, 원래 장면의 의미론적 레이아웃과 움직임을 유지합니다.

이 논문은 비디오 편집에 이미지 확산 모델을 사용할 때 직면하는 문제, 특히 비디오 프레임 전체에서 일관성을 유지하기가 어렵다는 점을 지적합니다. 기존 솔루션은 여러 프레임을 포함하도록 셀프 어텐션 모듈을 확장했지만, 이러한 접근 방식은 모션을 암시적으로만 보존하기 때문에 전문가가 복잡한 비디오 편집 방법을 사용해야 합니다.

이러한 문제를 극복하기 위해 이 논문에서는 원본 프레임 간 비디오 대응을 편집된 비디오에 적용하는 프레임워크를 제시합니다. 연구진은 자연스러운 비디오에는 유사한 모양이나 공유된 시각적 요소와 같이 프레임 간에 중복된 정보가 포함되어 있는 경우가 많다는 사실을 관찰했습니다. 연구진은 확산 모델에서 비디오의 내부 표현이 이와 동일한 특성을 보여주며, 프레임의 중복성과 시간적 일관성, RGB 및 확산 특징 공간의 밀접한 상관관계가 있음을 발견했습니다.

연구팀은 편집된 비디오의 특징이 프레임 간에 일관성을 유지하도록 함으로써 원본 비디오의 특징과 동일한 프레임 간 대응 및 중복성을 유지할 수 있다는 사실을 발견했습니다. 이들은 모델에서 쉽게 사용할 수 있는 원본 프레임 간 피처 대응을 사용하여 원본 비디오 역학을 기반으로 편집된 확산 피처를 전파했습니다.

토큰플로우라는 이름의 이 프레임워크는 추가 교육이나 미세 조정 없이 기존의 확산 기반 이미지 편집 방법과 함께 작동합니다. 이 새로운 접근 방식을 통해 복잡한 모션이 포함된 다양한 동영상에 대해 최첨단 편집 결과를 얻을 수 있었습니다. 이 논문은 또한 비디오 전반에서 확산 특징의 특성을 연구하는 실증적 분석을 제공합니다.

### 2. Related Work

이 논문에서는 생성적 적대 신경망(GAN)과 확산 모델을 활용하여 인상적인 텍스트 기반 이미지 생성 기능을 구현한 텍스트 기반 이미지 및 비디오 합성 분야의 선행 연구를 중점적으로 다룹니다. 특히 확산 모델은 최첨단 텍스트-이미지 생성기로 각광받고 있으며 2D 아키텍처를 시간적 차원으로 확장하여 텍스트-비디오 생성에 적용되었습니다. 그러나 현재의 비디오 확산 모델은 계산 및 메모리 집약적이며 이미지 모델에 비해 클립이 짧거나 시각적 품질이 떨어지는 경향이 있습니다. 저자는 추가 학습이나 미세 조정 없이 사전 학습된 이미지 확산 모델을 자신의 작업과 같은 비디오 합성 작업에 활용하는 최근 트렌드에 주목합니다.

일관된 비디오 스타일라이제이션과 관련하여 기존 방법에서는 프레임 단위로 이미지 편집 기술을 적용한 다음 시간적 불일치를 해결하기 위해 후처리를 수행했습니다. 이러한 방법은 빈번하게 발생하는 시간적 깜박임을 줄일 수 있지만, 콘텐츠의 상당한 변화에는 어려움을 겪습니다. 비디오를 2D 아틀라스 세트로 분해하는 것과 같은 다른 방법은 최소한의 노력으로 시간적 일관성을 달성할 수 있지만, 긴 훈련 시간과 표현 기능의 제한으로 인해 적용이 제한적입니다.

또한 이 논문에서는 비디오의 작은 패치가 여러 프레임에 걸쳐 광범위하게 반복되는 기존 방법과 키프레임을 편집하고 편집 내용을 비디오 전체에 전파하여 일관성을 단순화할 수 있는 방법을 언급하고 있습니다. 그러나 이러한 전파 방법은 조명이 바뀌거나 동적이 복잡한 동영상에서는 어려움을 겪으며, 포스트 프로세싱으로만 작동할 수 있습니다.

또한 저자는 최근 확산 기능 조작을 사용하여 다양한 편집 및 생성 작업에 텍스트-대-이미지 확산 모델을 적용하는 방법에 대해서도 설명합니다. 단일 프레임 이상에서 작동하도록 자기주의 모듈을 확장하면 전 세계적으로 공통된 외관을 가진 프레임을 생성하는 것으로 나타났으며, 이를 통해 전 세계적으로 일관된 비디오 편집을 달성하는 데 사용되었습니다. 그럼에도 불구하고 셀프 어텐션 모듈을 부풀리는 것만으로는 세분화된 시간적 일관성을 제공하지 못합니다. 바로 이 부분에서 토큰플로우를 사용하여 모델의 기능이 시간적으로 일관성을 갖도록 장려하는 작업이 시작됩니다.

### 3. Preliminaries

이 섹션에서는 점진적인 노이즈 제거 프로세스를 통해 데이터 분포를 근사화하는 생성 모델의 일종인 확산 확률론적 모델(DPM)을 소개합니다. 가우스 분포에서 추출한 노이즈가 있는 이미지로 시작하여 목표 분포에서 추출한 깨끗한 이미지가 될 때까지 DPM은 이미지의 노이즈를 서서히 제거합니다. DPM은 텍스트 컨디셔닝과 같은 추가 신호를 통합하여 조건부 분포를 학습할 수 있습니다.

결정론적 노이즈 제거 역모델(DDIM) 알고리즘은 Song 등이 제시한 알고리즘입니다. 이 알고리즘을 역으로 적용하면 깨끗한 이미지를 생성하는 데 사용되는 중간 노이즈 이미지를 검색할 수 있습니다.

이 논문에서는 대표적인 텍스트-이미지 확산 모델인 안정적 확산(SD)에 대해서도 소개합니다. SD는 잠상 이미지 공간에서 작동하며, 사전 학습된 인코더가 RGB 이미지를 이 공간에 매핑하고 디코더가 이러한 잠상을 고해상도 이미지로 다시 변환합니다. SD는 잔여, 자체 주의, 교차 주의 블록을 포함한 U-Net 아키텍처를 사용합니다. 잔여 블록은 이전 레이어의 활성화에 컨볼루션을 적용하고 크로스 어텐션은 텍스트 프롬프트에 따라 기능을 조정합니다.

SD의 자체 어텐션 블록은 기능을 쿼리(Q), 키(K) 및 값(V)으로 투영합니다. 이 블록의 출력은 A - V로 주어지며, 여기서 A = Attention(Q; K)입니다. Attention 연산은 이차원 투영 Q와 K 사이의 친밀도를 계산합니다. 이 연산은 소프트맥스 함수를 사용하여 Q와 K의 곱을 정규화합니다.

### 4. Method

이 백서의 '방법' 섹션에서는 원본 동영상의 모션과 시맨틱 레이아웃을 유지하면서 텍스트 프롬프트를 기반으로 편집된 동영상을 생성하는 프레임워크를 제시합니다. 이 프로세스는 사전 학습되고 고정된 텍스트-이미지 확산 모델을 활용하지만, 각 프레임에 독립적으로 이미지 편집 방법을 적용하는 대신(프레임 간 콘텐츠 불일치를 초래함), 편집 프로세스 중에 내부 확산 기능 간의 일관성을 적용하는 방법을 제안합니다.

![시간에 따른 확산 특징. 왼쪽: 주어진 입력 비디오(맨 윗줄)에 대해, 우리는 각 프레임에 대해 DDIM 역변환을 적용하고 ϵθ에서 가장 높은 해상도 디코더 레이어에서 특징을 추출합니다. 우리는 모든 프레임에서 추출된 특징(즉, 자기 주의 모듈에서의 출력 토큰)에 PCA를 적용하고 첫 번째 세 가지 구성 요소를 시각화합니다(두 번째 줄). 우리는 또한 RGB와 특징에 대한 x-t 슬라이스(원래 프레임에서 빨간색으로 표시됨)를 시각화합니다(하단 줄). 특징 표현은 시간에 걸쳐 일관되며 – 해당 영역은 비디오 전체에서 유사한 특징으로 인코딩됩니다. 중간: 각 프레임에 이미지 편집 방법([51])을 적용하여 얻은 수정된 비디오의 프레임과 특징 시각화; RGB의 불일치 패턴은 특징 공간에서도 분명합니다(예: 개의 몸에서). 오른쪽: 우리의 방법은 수정된 비디오가 원본 비디오와 같은 수준의 특징 일관성을 전달하도록 강제하며, 이는 RGB 공간에서 일관성 있고 고품질의 편집을 의미합니다.](TokenFlow%20Consistent%20Diffusion%20Features%20for%20Consis%20ce531d0ecfa841a7bb4e830bc1517e9e/Untitled%201.png)

시간에 따른 확산 특징. 왼쪽: 주어진 입력 비디오(맨 윗줄)에 대해, 우리는 각 프레임에 대해 DDIM 역변환을 적용하고 ϵθ에서 가장 높은 해상도 디코더 레이어에서 특징을 추출합니다. 우리는 모든 프레임에서 추출된 특징(즉, 자기 주의 모듈에서의 출력 토큰)에 PCA를 적용하고 첫 번째 세 가지 구성 요소를 시각화합니다(두 번째 줄). 우리는 또한 RGB와 특징에 대한 x-t 슬라이스(원래 프레임에서 빨간색으로 표시됨)를 시각화합니다(하단 줄). 특징 표현은 시간에 걸쳐 일관되며 – 해당 영역은 비디오 전체에서 유사한 특징으로 인코딩됩니다. 중간: 각 프레임에 이미지 편집 방법([51])을 적용하여 얻은 수정된 비디오의 프레임과 특징 시각화; RGB의 불일치 패턴은 특징 공간에서도 분명합니다(예: 개의 몸에서). 오른쪽: 우리의 방법은 수정된 비디오가 원본 비디오와 같은 수준의 특징 일관성을 전달하도록 강제하며, 이는 RGB 공간에서 일관성 있고 고품질의 편집을 의미합니다.

![세밀한 특징 상응. 출처 프레임에서 추출한 특징(즉,자기 주의 모듈에서의 출력 토큰)은 가까운 프레임을 재구성하는 데 사용됩니다. 이는: (a) 대상에서 각 특징을 출처에서 가장 가까운 특징으로 교환하는 것, 모든 레이어와 모든 생성 시간 단계에서, 그리고 (b) RGB 공간에서의 단순한 왜곡을 통해 이루어집니다, 최고 해상도 디코더 레이어에서 추출한 출처와 대상 특징 사이에서 계산된 최근접 이웃 필드(c)를 사용하여. 대상은 신뢰할 수 있게 재구성되며, 이는 특징 간의 높은 수준의 공간적 세밀도와 공유된 콘텐츠를 보여줍니다.](TokenFlow%20Consistent%20Diffusion%20Features%20for%20Consis%20ce531d0ecfa841a7bb4e830bc1517e9e/Untitled%202.png)

세밀한 특징 상응. 출처 프레임에서 추출한 특징(즉,자기 주의 모듈에서의 출력 토큰)은 가까운 프레임을 재구성하는 데 사용됩니다. 이는: (a) 대상에서 각 특징을 출처에서 가장 가까운 특징으로 교환하는 것, 모든 레이어와 모든 생성 시간 단계에서, 그리고 (b) RGB 공간에서의 단순한 왜곡을 통해 이루어집니다, 최고 해상도 디코더 레이어에서 추출한 출처와 대상 특징 사이에서 계산된 최근접 이웃 필드(c)를 사용하여. 대상은 신뢰할 수 있게 재구성되며, 이는 특징 간의 높은 수준의 공간적 세밀도와 공유된 콘텐츠를 보여줍니다.

토큰플로우라는 이름의 제안된 프레임워크는 크게 두 단계로 작동합니다:

키프레임 샘플링과 공동 편집: 각 생성 단계에서 프레임워크는 일련의 키프레임을 선택하고 텍스트 프롬프트에 따라 공동으로 편집합니다. 선택된 키프레임은 수정된 셀프 어텐션 블록에서 동시에 처리되어 전역적으로 공유되는 모양을 유도합니다. 각 프레임에 대한 이 블록의 출력은 확장된 주의 연산을 통해 계산되어 나중에 전체 비디오에 전파될 토큰 세트(Tbase)로 이어집니다.

토큰 플로우를 통한 전파 편집: 이전 단계의 T베이스가 주어지면 원본 비디오에서 추출한 토큰 대응을 기반으로 이러한 기능이 비디오 전체에 전파됩니다. 이 과정에는 각 원본 프레임의 토큰과 인접한 두 키프레임의 토큰의 최인접 이웃(NN)을 계산하는 작업이 포함됩니다. 그런 다음 각 공간 위치와 프레임에 대해 Tbase에서 토큰의 선형 조합을 계산하여 편집된 키프레임의 토큰을 비디오의 나머지 부분으로 전파합니다.

![TokenFlow 파이프라인. 위: 주어진 입력 비디오 I를 받아, 우리는 각 프레임을 DDIM 역변환하고, 각 시간 단계와 레이어에서의 토큰(즉, 자기 주의 모듈에서의 출력 특징)을 추출하고, 가장 가까운 이웃(NN) 검색을 사용하여 프레임 간 특징 상응을 계산합니다. 하단: 수정된 비디오는 다음과 같이 생성됩니다: 각 노이즈 제거 단계 t에서, (I) 우리는 노이즈가 있는 비디오 Jt에서 키 프레임을 샘플링하고 확장된 주의 블록을 사용하여 이들을 공동으로 수정합니다; 결과적으로 편집된 토큰의 집합은 Tbase입니다. (II) 우리는 원래 비디오 특징의 미리 계산된 상응에 따라 편집된 토큰을 비디오 전체에 전파합니다. Jt를 노이즈 제거하기 위해, 우리는 각 프레임을 네트워크에 공급하고, 생성된 토큰을 전파 단계(II)에서 얻은 토큰으로 교체합니다.](TokenFlow%20Consistent%20Diffusion%20Features%20for%20Consis%20ce531d0ecfa841a7bb4e830bc1517e9e/Untitled%203.png)

TokenFlow 파이프라인. 위: 주어진 입력 비디오 I를 받아, 우리는 각 프레임을 DDIM 역변환하고, 각 시간 단계와 레이어에서의 토큰(즉, 자기 주의 모듈에서의 출력 특징)을 추출하고, 가장 가까운 이웃(NN) 검색을 사용하여 프레임 간 특징 상응을 계산합니다. 하단: 수정된 비디오는 다음과 같이 생성됩니다: 각 노이즈 제거 단계 t에서, (I) 우리는 노이즈가 있는 비디오 Jt에서 키 프레임을 샘플링하고 확장된 주의 블록을 사용하여 이들을 공동으로 수정합니다; 결과적으로 편집된 토큰의 집합은 Tbase입니다. (II) 우리는 원래 비디오 특징의 미리 계산된 상응에 따라 편집된 토큰을 비디오 전체에 전파합니다. Jt를 노이즈 제거하기 위해, 우리는 각 프레임을 네트워크에 공급하고, 생성된 토큰을 전파 단계(II)에서 얻은 토큰으로 교체합니다.

이 알고리즘은 입력 비디오의 DDIM 반전으로 시작하여 일련의 노이즈 잠상을 추출합니다. 그 후, 각 생성 단계마다 키프레임 편집과 토큰 흐름 전파를 번갈아 가며 수행합니다. 네트워크의 각 계층에는 셀프 어텐션 블록의 입력과 출력 사이에 잔여 연결이 포함되어 있으므로 각 계층에서 TokenFlow를 수행해야 합니다.

### 5. Results

비디오 편집 기술의 결과는 움직이는 다양한 피사체가 등장하는 DAVIS 비디오 및 인터넷 비디오에서 평가됩니다. 비디오의 공간 해상도는 384x672 또는 512x512픽셀이며 길이는 40~200프레임입니다. 다양한 편집 결과를 얻기 위해 각 동영상에 다양한 텍스트 프롬프트가 사용됩니다.

![우리 방법의 샘플 결과입니다. 더 많은 예제와 전체 비디오 결과에 대해서는 우리의 웹페이지와 SM을 참조하십시오.](TokenFlow%20Consistent%20Diffusion%20Features%20for%20Consis%20ce531d0ecfa841a7bb4e830bc1517e9e/Untitled%204.png)

우리 방법의 샘플 결과입니다. 더 많은 예제와 전체 비디오 결과에 대해서는 우리의 웹페이지와 SM을 참조하십시오.

이 작업의 저자는 프레임 편집 방법으로 PnP-Diffusion을 사용하며, 이 방법이 각 프레임의 구조를 항상 완벽하게 보존하지는 못하지만, 여러 프레임이 비디오의 각 프레임 생성에 기여함으로써 견고성을 향상시킨다고 강조합니다. 또한 이 프레임워크는 이미지 구조를 정확하게 보존하는 모든 확산 기반 이미지 편집 기술과 결합할 수 있습니다.

베이스라인에 비해 저자의 방법은 편집 프롬프트에 따라 시간적으로 일관된 편집을 제공하므로 성능이 뛰어납니다. 특히 동영상 전체에서 피사체의 정체성과 배경의 일관성을 유지할 수 있습니다.

비교에 사용되는 기준선에는 Text2Video-Zero, Tune-a-Video, Gen-1 및 Text2LIVE가 포함됩니다. 또한 프레임별 확산 기반 이미지 편집 기준선인 PnP-확산과 단일 키프레임에 PnP-확산을 적용하여 별도의 기술을 사용하여 전체 비디오에 편집을 전파하는 방법도 고려합니다.

![비교. 우리는 우리의 방법을 Tune-A-Video [53], PnP-Diffusion [51]가 프레임 별로 적용된 경우, Gen-1 [10], 그리고 Text2Video-Zero[20]와 비교합니다. 전체 비디오 비교에 대해서는 우리의 웹페이지를 참조하십시오.](TokenFlow%20Consistent%20Diffusion%20Features%20for%20Consis%20ce531d0ecfa841a7bb4e830bc1517e9e/Untitled%205.png)

비교. 우리는 우리의 방법을 Tune-A-Video [53], PnP-Diffusion [51]가 프레임 별로 적용된 경우, Gen-1 [10], 그리고 Text2Video-Zero[20]와 비교합니다. 전체 비디오 비교에 대해서는 우리의 웹페이지를 참조하십시오.

정성적 평가 결과, 제작자의 방법이 결과 동영상의 편집 준수와 시간적 일관성을 모두 유지하는 데 있어 저명한 기준선보다 우수한 것으로 나타났습니다. 예를 들어, Tune-A-Video는 모션을 캡처하는 데 어려움을 겪어 긴 동영상의 경우 무의미한 편집이 발생하고, 각 프레임에 독립적으로 PnP를 적용하면 정교한 편집에도 불구하고 시간적 불일치가 발생합니다. 1세대는 또한 시간적 불일치와 낮은 프레임 품질로 어려움을 겪었습니다. Text2Video-Zero의 편집은 심한 지터링이 발생합니다.

저자의 방식은 Text2LIVE와 별도의 방법을 사용하여 단일 키프레임에서 편집을 전파하는 기법과도 비교됩니다. Text2LIVE는 시각적 품질이 제한적이며 동영상의 레이어 표현에 의존하기 때문에 학습하는 데 시간이 많이 걸리고 단순한 움직임이 있는 동영상에 제한됩니다. 전파 방식은 편집된 키프레임에 가깝지 않은 프레임에서 전파 아티팩트를 생성합니다.

![추가적인 질적 비교. RGB 전파([17])는 광학 흐름과 같은 저수준 신호에만 접근할 수 있으므로, 콘텐츠 공개나 복잡한 동력학을 가진 비디오에서 시각적 아티팩트를 만듭니다. Text2LIVE([1])는 CLIP 기반으로 있으며 확산 모델의 생성 사전을 사용하지 않으므로 시각적 품질 면에서 더 제한적입니다.](TokenFlow%20Consistent%20Diffusion%20Features%20for%20Consis%20ce531d0ecfa841a7bb4e830bc1517e9e/Untitled%206.png)

추가적인 질적 비교. RGB 전파([17])는 광학 흐름과 같은 저수준 신호에만 접근할 수 있으므로, 콘텐츠 공개나 복잡한 동력학을 가진 비디오에서 시각적 아티팩트를 만듭니다. Text2LIVE([1])는 CLIP 기반으로 있으며 확산 모델의 생성 사전을 사용하지 않으므로 시각적 품질 면에서 더 제한적입니다.

작성자 방법의 정량적 평가에는 편집 충실도와 시간적 일관성을 측정하는 것이 포함됩니다. 편집 충실도는 편집된 각 프레임의 클립 삽입과 대상 텍스트 프롬프트 간의 평균 유사도를 계산하여 결정됩니다. 시간적 일관성은 이전 연구에 따라 원본 영상의 광학적 흐름을 계산하고 이에 따라 편집된 프레임을 워핑한 후 워핑 오차를 측정하는 방식으로 측정합니다.

기준선과 비교했을 때, 저자의 방법이 가장 높은 CLIP 점수를 획득하여 편집된 비디오와 입력 안내 프롬프트가 잘 맞았음을 나타냅니다. 또한 이 방법은 워프 오차가 가장 낮아 시간적으로 일관된 결과를 얻을 수 있습니다.

또한 저자는 원본 비디오를 편집하지 않고 LDM 자동 인코더에 통과시키는 기준선인 'LDM 정찰'을 고려합니다. 이 기준선은 편집이 없기 때문에 클립 유사성이 높지는 않지만, LDM 자동 인코더의 불완전한 재구성으로 인해 워프 오류가 0이 되지도 않습니다.

제거 연구에서 저자들은 프레임워크의 주요 디자인 선택에 초점을 맞췄습니다. 먼저 시간적 일관성을 강제하는 토큰플로우를 제거했을 때 어떤 영향이 있는지 조사합니다. 토큰플로우를 확장된 주의로 대체하는 것은 계산적으로 까다롭고 긴 동영상에서는 잘 확장되지 않는 것으로 나타났습니다. 또한 각 생성 단계에서 키프레임 선택을 무작위화할 때의 효과도 연구했습니다.

절제 연구 결과에 따르면 토큰플로우가 시간적 일관성에 크게 기여하는 것으로 나타났습니다. 여러 프레임으로 셀프 어텐션을 확장하는 것만으로는 세분화된 시간적 일관성을 달성하기에는 충분하지 않습니다. 또한 키프레임을 고정하면 동영상이 짧은 클립으로 인위적으로 분할되어 워프 에러가 높아지는 것처럼 시간적 일관성에 부정적인 영향을 미칩니다. 이 효과는 보충 자료의 자르기 동영상에서 시각적으로 확인할 수 있습니다.

![한계. 우리의 방법은 원래 비디오의 특징 상응에 따라 비디오를 수정하므로, 그것은 구조 변화를 필요로 하는 수정을 처리할 수 없습니다.](TokenFlow%20Consistent%20Diffusion%20Features%20for%20Consis%20ce531d0ecfa841a7bb4e830bc1517e9e/Untitled%207.png)

한계. 우리의 방법은 원래 비디오의 특징 상응에 따라 비디오를 수정하므로, 그것은 구조 변화를 필요로 하는 수정을 처리할 수 없습니다.

### 6. Discussion

저자들은 이미지 확산 모델을 사용하는 텍스트 중심 비디오 편집을 위한 새로운 프레임워크 개발에 대해 설명합니다. 이 방법은 확산 특징 공간 내에서 비디오의 내부 표현을 조사합니다. 이를 통해 생성 과정에서 일관된 확산 특징 표현을 유지함으로써 일관된 비디오 편집이 가능하다는 것을 보여줍니다.

이 접근 방식은 기존 방법과 비교했을 때 특히 시간적 일관성을 개선하는 데 있어 기준선을 뛰어넘는 성능을 보입니다. 하지만 한계가 있습니다. 특히 원본 동영상의 움직임을 보존하도록 설계되어 구조적 변경이 필요한 편집을 관리할 수 없습니다. 이 방법은 원본 프레임의 구조를 유지하기 위해 확산 기반 이미지 편집 기법에 의존합니다. 그러나 이 이미지 편집 기법이 구조를 유지하지 못하면 편집된 프레임에서 의미 없는 대응을 적용하여 시각적 아티팩트가 발생할 수 있습니다. 또한 LDM 디코더는 일부 고주파 깜박임을 유발할 수 있지만, 개선된 디코더를 프레임워크에 통합하거나 포스트 프로세스 디플리커링을 사용하면 이 문제를 해결할 수 있습니다.

전반적으로 저자들의 연구는 확산 모델의 영역 내에서 자연스러운 비디오의 내부 표현을 조명하여 향상된 비디오 합성을 위해 이러한 모델을 활용할 수 있는 잠재력을 보여줍니다. 이는 특히 비디오 작업에 이미지 모델을 사용하고 텍스트에서 비디오로 변환하는 모델을 설계하는 등 향후 연구에 새로운 가능성을 열어줍니다.