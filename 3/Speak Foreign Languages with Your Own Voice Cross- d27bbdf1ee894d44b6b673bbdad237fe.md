# Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling

[https://arxiv.org/pdf/2303.03926.pdf](https://arxiv.org/pdf/2303.03926.pdf)

![단일 언어 화자를 위해 다른 언어로 개인화된 음성을 합성할 수 있는 VALL-E X의 전체 프레임워크. 소스 및 대상 텍스트에서 파생된 음소 시퀀스와 오디오 코덱 모델에서 파생된 소스 음향 토큰을 프롬프트로 삼아 VALL-E X는 대상 언어로 음향 토큰을 생성한 다음 대상 음성 파형으로 압축을 해제할 수 있습니다. 강력한 인컨텍스트 학습 기능 덕분에 VALL-E X는 학습을 위해 동일한 화자의 다국어 음성 데이터가 필요하지 않으며, 다국어 텍스트 음성 합성 및 음성 음성 번역과 같은 다양한 제로 샷 다국어 음성 생성 작업을 수행할 수 있습니다.](Speak%20Foreign%20Languages%20with%20Your%20Own%20Voice%20Cross-%20d27bbdf1ee894d44b6b673bbdad237fe/Untitled.png)

단일 언어 화자를 위해 다른 언어로 개인화된 음성을 합성할 수 있는 VALL-E X의 전체 프레임워크. 소스 및 대상 텍스트에서 파생된 음소 시퀀스와 오디오 코덱 모델에서 파생된 소스 음향 토큰을 프롬프트로 삼아 VALL-E X는 대상 언어로 음향 토큰을 생성한 다음 대상 음성 파형으로 압축을 해제할 수 있습니다. 강력한 인컨텍스트 학습 기능 덕분에 VALL-E X는 학습을 위해 동일한 화자의 다국어 음성 데이터가 필요하지 않으며, 다국어 텍스트 음성 합성 및 음성 음성 번역과 같은 다양한 제로 샷 다국어 음성 생성 작업을 수행할 수 있습니다.

### 1 Introduction

이 문서에서는 화자의 음성을 한 언어에서 다른 언어로 옮기는 것을 목표로 하는 언어 간 음성 합성의 과제에 대해 설명합니다. 기존 모델은 보이지 않는 소스 화자로부터 대상 음성을 합성하는 제로 샷 시나리오로 효과적으로 확장하지 못하며 화자 유사성이 낮은 경우가 많습니다. 이러한 문제를 해결하기 위해 저자는 강력한 컨텍스트 내 학습 기능을 활용하여 고품질의 제로 샷 교차 언어 음성 합성을 달성하는 교차 언어 신경 코덱 언어 모델인 VALL-E X라는 새로운 접근 방식을 제안합니다. 이 모델은 대규모 다국어 음성 데이터로 학습되며 화자의 목소리, 감정, 음성 배경을 포함한 음성 특성을 소스 언어에서 대상 언어로 전송할 수 있습니다. 저자들은 제로 샷 다국어 텍스트 음성 합성(XTTS)과 제로 샷 음성 음성 번역(S2ST)이라는 두 가지 종류의 다국어 음성 생성 작업에 대한 실험을 수행하여 화자 유사성, 음성 품질, 번역 품질, 음성 자연스러움 및 인간 평가 측면에서 VALL-E X가 강력한 기준선을 뛰어넘는 성능을 보여줬습니다.

### 2 Related Work

신경망과 WaveNet, HiFi-GAN, Diffwave와 같은 다양한 네트워크 프레임워크를 사용한 음성 및 오디오 합성의 최근 발전에 대해 설명합니다. 특히 텍스트에서 음성이나 소리를 합성하기 위한 텍스트 음성 변환(TTS) 또는 텍스트 소리 변환 접근 방식에 중점을 둡니다. 이산 오디오 표현 학습도 오디오 합성에 대한 새로운 접근 방식으로 떠오르고 있습니다. 관련 작업으로는 텍스트 설명을 입력으로 하는 자동 회귀 오디오 생성 모델을 사용하는 AudioGen과 AudioLM이 있습니다. 단일 언어 TTS 합성을 위해 신경 코덱 언어 모델을 사용하는 또 다른 관련 작업인 VALL-E도 있습니다. 그러나 이 글에서는 대상 언어의 합성 음성에서 원어 화자의 음성을 유지하는 것을 목표로 하는 다국어 음성 합성에 중점을 둡니다.

기존의 단일 언어 TTS보다 더 까다로운 다국어 텍스트 음성 변환(TTS) 합성에 대한 이전 작업에 대해 설명합니다. 이전 접근 방식에는 공유 음소 입력 표현 사용, 공유 음소 세트 도입, 화자 유사성 향상을 위한 다중 작업 학습 등이 있습니다. 그러나 이러한 접근 방식은 여전히 화자 유사성이 낮고 제로 샷 기능이 부족하다는 단점이 있습니다. 반면, 이 글에서 제안하는 프레임워크는 대규모 다국어 다중 화자 자동 음성 인식(ASR) 데이터를 활용하고 강력한 문맥 내 학습 능력을 갖춘 신경 코덱 언어 모델을 활용하여 이러한 문제를 해결합니다.

한 언어에서 다른 언어로 음성을 번역하는 것을 목표로 하는 음성 대 음성 번역(S2ST)에 대해서도 설명합니다. 이전 연구와 애플리케이션은 계단식 S2ST 시스템에 중점을 두었지만, 최근 연구에서는 엔드투엔드 S2ST 모델을 탐구했습니다. 그러나 동일한 화자로부터 이중 언어 음성 데이터를 수집하는 것이 어렵기 때문에 생성된 음성에서 소스 사운드 특성(화자, 감정, 음성 배경)을 유지하는 것이 중요한 과제입니다. 번역기나 번역기 2와 같은 기존 접근 방식은 화자 임베딩 또는 다중 화자 TTS 시스템에서 생성된 의사 이중 언어 음성 데이터에 의해 조건화된 목표 음성을 합성하는 데 한계가 있습니다. 이러한 문제를 해결하기 위해 이 글에서 제안하는 접근 방식은 언어 간 신경 코덱 언어 모델에 번역 모듈을 장착하여 S2ST 작업에서 사운드 특성을 유지하는 제로 샷 기능을 입증하는 것입니다.

### 3 Cross-Lingual Codec Language Model