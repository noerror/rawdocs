# Zero-1-to-3: Zero-shot One Image to 3D Object

[https://arxiv.org/pdf/2303.11328.pdf](https://arxiv.org/pdf/2303.11328.pdf)

[https://github.com/cvlab-columbia/zero123](https://github.com/cvlab-columbia/zero123)

### 1.Introduction

단 하나의 RGB 이미지로 물체의 카메라 시점을 변경할 수 있는 0-1-3 방식을 소개합니다. 이 접근 방식은 대규모 확산 모델이 자연 이미지에서 학습하는 기하학적 지식을 활용합니다. 합성 데이터 세트로 모델을 학습시킴으로써 상대적인 카메라 시점을 제어하여 지정된 카메라 변환을 통해 새로운 이미지를 생성할 수 있습니다. 이 모델은 합성 데이터세트로 훈련되었지만 인상파 그림을 비롯한 실제 이미지와 배포되지 않은 데이터세트에도 효과적으로 일반화할 수 있습니다.

2D 이미지로만 훈련된 대규모 확산 모델도 시각 세계에 대한 풍부한 3D 정보를 학습할 수 있음을 입증한 것이 주요 성과입니다. 이를 통해 새로운 시각 합성과 단일 RGB 이미지에서 물체의 제로샷 3D 재구성을 위한 최첨단 결과를 달성했습니다. 실험 결과, 인터넷 규모의 사전 학습을 활용하여 기존의 단일 뷰 3D 재구성 및 새로운 뷰 합성 모델보다 훨씬 뛰어난 성능을 보였습니다.

### 2. Related Work

최근 제너레이티브 이미지 아키텍처와 대규모 이미지-텍스트 데이터 세트의 발전으로 충실도가 높고 다양한 장면과 객체를 합성할 수 있게 되었습니다. 확산 모델은 확장 가능한 이미지 생성기를 학습하는 데 특히 효과적입니다. 하지만 이를 3D로 변환하려면 광범위한 주석이 달린 3D 데이터가 필요합니다. 신경 방사 필드(NeRF)는 고충실도 장면을 인코딩하기 위한 강력한 표현으로 부상했습니다. 연구는 확산 모델과 함께 시점 조건부 이미지 간 변환을 사용하여 새로운 뷰 합성에 대한 색다른 접근 방식을 취합니다. 또한 이를 3D 증류와 결합하여 단일 이미지에서 3D 형상을 재구성할 수 있습니다.

단일 뷰에서 3D 오브젝트를 재구성하는 것은 어렵고 강력한 선행 작업이 필요합니다. 이전 방법에서는 메시, 복셀 또는 포인트 클라우드로 표현되는 3D 프리미티브를 사용했지만 이러한 모델은 다양한 3D 데이터의 제약을 받으며 일반화 기능이 떨어집니다. 로컬 컨디셔닝 모델은 장면 재구성을 위해 로컬 이미지 특징을 직접 사용하며 도메인 간 일반화 성능이 더 뛰어나지만 일반적으로 근접 뷰 재구성에 국한됩니다. 이 연구에서는 사전 학습된 안정적 확산 모델에서 직접 풍부한 기하학적 정보를 추출할 수 있으므로 추가 깊이 정보가 필요하지 않다는 것을 보여줍니다.

### 3. Method

우리의 목표는 단 하나의 RGB 이미지만을 사용하여 다른 카메라 시점의 물체 이미지를 합성하는 모델을 만드는 것입니다. 이 작업을 수행하기 위해 안정적 확산과 같은 대규모 확산 모델을 사용합니다. 그러나 이러한 모델은 시점 간의 대응을 명시적으로 인코딩하지 않으며 시점 편향이 있을 수 있습니다.

카메라 외연을 제어하도록 모델을 학습시키기 위해 페어링된 이미지와 상대적인 카메라 외연의 데이터 세트를 사용하여 사전 학습된 확산 모델을 미세 조정합니다. 이 미세 조정을 통해 모델은 시점을 제어하여 사실적인 이미지를 생성하는 기능을 유지하면서 카메라 시점을 제어하는 일반적인 메커니즘을 학습할 수 있습니다. 이렇게 하면 모델에 제로 샷 기능이 설정되어 미세 조정 세트에 없는 오브젝트 클래스에 대한 새로운 뷰를 합성할 수 있습니다.

3D 재구성을 위해 하이브리드 컨디셔닝 메커니즘을 채택했습니다. 입력 이미지의 클립 임베딩과 상대적인 카메라 외연 및 크로스 어텐션을 결합하여 노이즈 제거 U-Net을 컨디셔닝합니다. 이를 통해 입력 이미지의 높은 수준의 시맨틱 정보를 제공하는 동시에 입력 이미지 자체는 노이즈 제거되는 이미지와 채널 연결되어 오브젝트의 정체성과 디테일을 유지합니다.

새로운 뷰를 합성하는 것 외에도 물체의 모양과 기하학적 구조를 캡처하는 완전한 3D 재구성이 필요한 경우가 많습니다.  스코어 제이코비안 체인(SJC) 프레임워크를 사용하여 텍스트-이미지 확산 모델의 전제 조건으로 3D 표현을 최적화합니다. 재구성의 충실도를 높이기 위해 분류기 없는 안내 값을 평소보다 높게 설정합니다. 무작위로 시점을 샘플링하고, 볼류메트릭 렌더링을 수행하며, 가우시안 노이즈로 결과 이미지를 교란하고, 입력 이미지, 포즈 CLIP 임베딩 및 타임스텝에 따라 조정된 U-Net을 사용하여 노이즈를 제거합니다. 또한 MSE 손실로 입력 뷰를 최적화하고 깊이 평활화 및 근거리 뷰 일관성 손실을 적용하여 NeRF 표현을 정규화합니다.

- Score Jacobian Chaining
    
    점수 자코비안 연쇄는 이미지 생성 모델에 사용되는 기법입니다. 여기에는 수학을 사용하여 여러 변환을 연결하여 모델이 보다 사실적인 이미지를 생성하도록 돕는 것이 포함됩니다. 이 기술은 물체의 새로운 보기 생성 및 3D 재구성과 같은 작업에서 모델의 성능을 향상시킵니다.
    

미세 조정을 위해 10만 명 이상의 아티스트가 제작한 800만 개 이상의 3D 모델이 포함된 Objaverse 데이터세트를 사용합니다. 이 데이터 세트는 풍부한 지오메트리, 세밀한 디테일, 머티리얼 속성을 갖춘 다양한 고품질 3D 모델을 제공합니다. 각 오브젝트에 대해 12개의 카메라 외형 매트릭스를 무작위로 샘플링하고 레이트레이싱 엔진으로 12개의 뷰를 렌더링합니다. 훈련 시 각 오브젝트에 대해 두 개의 뷰를 샘플링하여 이미지 쌍을 구성하고, 두 개의 외생 매트릭스에서 파생된 해당 상대 시점 변환을 사용할 수 있습니다.

### 4. Experiments

오브제버스 데이터 세트 외부의 데이터 세트를 사용하여 제로샷 신규 뷰 합성 및 3D 재구성에 대한 모델의 성능을 평가합니다. 다양한 복잡도 수준의 합성 물체와 장면에 대한 최신 방법과 모델을 정량적으로 비교하고 다양한 실제 이미지에 대한 정성적 결과를 보고합니다.

새로운 시각 합성: 이 과제에서는 단일 뷰 RGB 이미지에서 물체의 깊이, 질감, 모양을 암시적으로 학습하는 모델이 필요합니다. 이 접근 방식은 대규모 확산 모델에서 학습한 의미론적 및 기하학적 선행 모델을 효과적으로 활용합니다.

![Untitled](Zero-1-to-3%20Zero-shot%20One%20Image%20to%203D%20Object%20667018cd738f4c0b8c26c5f12c2cdb9f/Untitled.png)

3D 재구성: 확률론적 3D 재구성 프레임워크(예: SJC 또는 DreamFusion)를 적용하여 가장 가능성이 높은 3D 표현을 생성합니다. 뷰 조건부 확산 모델은 확산 모델에서 학습한 풍부한 2D 외관을 3D 지오메트리로 전달할 수 있는 경로를 제공합니다.

새로운 뷰 합성을 위해 DietNeRF, 이미지 베리에이션, SJC-I와 같은 제로샷, 단일 뷰 RGB 이미지 방법과 비교합니다. 3D 재구성의 경우 멀티뷰 압축 코딩, Point-E, SJC-I를 베이스라인으로 사용합니다.

Google 스캔 오브젝트와 RTMV 데이터 세트에서 두 가지 작업을 평가합니다. 새로운 뷰 합성의 경우, 이미지 유사성을 측정하기 위해 PSNR, SSIM, LPIPS 및 FID 메트릭을 사용합니다. 3D 재구성의 경우 모따기 거리와 체적 IoU를 측정합니다.

이 방법은 GSO 및 RTMV 데이터 세트 모두에서 실측 데이터와 거의 일치하는 매우 사실적인 이미지를 생성합니다. Point-E는 인상적인 제로샷 일반화 가능성을 보여주지만, 생성된 포인트 클라우드의 크기가 작기 때문에 적용 가능성에 한계가 있습니다. 우리의 모델은 지오메트리와 텍스처가 까다로운 오브젝트에도 잘 일반화되어 오브젝트 유형, 아이덴티티, 디테일을 유지하면서 충실도가 높은 뷰포인트를 합성합니다. 확산 모델은 단일 이미지에서 새로운 뷰를 합성할 때 근본적인 불확실성을 포착하는 데 특히 적합합니다.

우리의 방법은 실측 데이터와 일치하는 고충실도 3D 메시를 재구성합니다. MCC는 물체 뒤쪽의 지오메트리를 정확하게 추론하지 못하는 경우가 많지만, SJC-I는 의미 있는 지오메트리를 재구성하는 데 어려움을 겪습니다. Point-E는 제로 샷 일반화 능력이 뛰어나지만 불균일한 스파스 포인트 클라우드를 생성하여 재구성된 표면에 구멍이 생길 수 있습니다. 우리의 방법은 뷰 컨디셔닝 확산 모델의 다중 뷰 선행 요소를 활용하고 이를 NeRF 스타일 표현과 결합하여 이전 작업보다 모따기 거리와 체적 IoU를 개선합니다.

이 모델은 오브젝트의 정체성을 유지하면서 Dall-E-2와 같은 텍스트-이미지 모델에서 생성된 이미지의 새로운 뷰를 생성할 수 있습니다. 이는 많은 텍스트-3D 생성 애플리케이션에서 유용할 수 있습니다.

### 5. Discussion

이번 연구에서는 제로 샷, 단일 이미지 신규 뷰 합성 및 3D 재구성을 위한 새로운 접근 방식인 Zero1-to-3을 소개했습니다. 이 방법은 인터넷 규모의 데이터에 대해 사전 학습되고 풍부한 의미적 및 기하학적 선행 정보를 캡처하는 안정적 확산 모델을 활용합니다. 합성 데이터에서 모델을 미세 조정하면 카메라 시점을 제어하고 여러 벤치마크에서 최첨단 결과를 얻을 수 있습니다.

오브젝트에서 장면으로: 우리의 접근 방식은 배경이 평범한 단일 오브젝트에 대해 학습됩니다. 여러 오브젝트가 있는 장면에는 잘 일반화되지만, 복잡한 배경에서는 품질이 저하되어 중요한 과제를 안고 있습니다.

![Untitled](Zero-1-to-3%20Zero-shot%20One%20Image%20to%203D%20Object%20667018cd738f4c0b8c26c5f12c2cdb9f/Untitled%201.png)

장면에서 동영상으로: 단일 뷰에서 동적 장면의 기하학적 구조를 추론하면 새로운 연구 방향이 열릴 수 있습니다. 확산 기반 비디오 생성 접근 방식을 3D로 확장하면 이러한 기회를 탐색하는 데 도움이 될 수 있습니다.

그래픽 파이프라인과 안정적인 확산의 결합:  프레임워크는 스테이블 디퓨전으로부터 오브젝트에 대한 3D 지식을 추출합니다. 향후 작업에서는 유사한 메커니즘을 사용하여 장면 재조명과 같은 기존 그래픽 작업을 수행하는 방법을 모색할 수 있습니다.

- 제로샷 신규 뷰 합성 및 3D 재구성은 특정 물체의 예시를 본 적이 없는 상태에서(제로샷) 단일 입력 이미지에서 물체의 새로운 이미지 또는 3D 모델을 생성하는 컴퓨터 비전 작업입니다.
    
    제로 샷 신규 뷰 합성: 물체의 단일 이미지가 주어지면 마치 물체가 회전하거나 카메라가 물체 주위를 이동한 것처럼 다양한 각도 또는 관점에서 동일한 물체의 새로운 이미지를 생성하는 것입니다.
    
    3D 재구성: 물체의 단일 이미지가 주어지면 물체의 모양, 질감, 형상을 캡처하여 물체의 전체 3D 모델을 만드는 것이 목표입니다. 이를 통해 3D 공간에서 물체를 추가로 조작하거나 분석할 수 있습니다.