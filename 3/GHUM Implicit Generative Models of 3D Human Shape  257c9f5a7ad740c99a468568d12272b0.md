# GHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose

[https://arxiv.org/abs/2108.10842](https://arxiv.org/abs/2108.10842)

[https://github.com/google-research/google-research/tree/master/ghum](https://github.com/google-research/google-research/tree/master/ghum)

- 24 Aug 2021

### 1. Introduction

이 연구 논문에서는 엔드투엔드 학습 파이프라인을 사용하여 통계적 인체 형태 및 포즈 모델을 구축하는 방법에 대해 설명합니다. 이는 몰입형 사진, 증강/가상 현실, 3D 공간 추론과 같은 분야에서 사람의 동작과 표정을 정확하게 모델링하는 것이 중요하기 때문에 매우 중요합니다.

![GHS3D 데이터에 대한 GHUM 및 GHUML의 정확성 표현, 좌측에는 두 모델의 히트맵이 있다. 렌더링은 피사체의 다양한 몸통 포즈에 대한 등록(회색, 첫 번째 행), 그리고 GHUM 및 GHUML 재구성을 두 번째와 세 번째 행에서 각각 보여준다. 두 모델 모두에게서 좋은 디테일 캡처 수준을 확인할 수 있으며, GHUM의 정확성이 더 높다.](GHUM%20Implicit%20Generative%20Models%20of%203D%20Human%20Shape%20%20257c9f5a7ad740c99a468568d12272b0/Untitled.png)

GHS3D 데이터에 대한 GHUM 및 GHUML의 정확성 표현, 좌측에는 두 모델의 히트맵이 있다. 렌더링은 피사체의 다양한 몸통 포즈에 대한 등록(회색, 첫 번째 행), 그리고 GHUM 및 GHUML 재구성을 두 번째와 세 번째 행에서 각각 보여준다. 두 모델 모두에게서 좋은 디테일 캡처 수준을 확인할 수 있으며, GHUM의 정확성이 더 높다.

이러한 모델을 구축하려는 이전 시도는 대부분 얼굴, 손, 몸 등 특정 부위에만 집중하여 개별적으로 이루어졌습니다. 심지어 아담, 프랭크, SMPL-X와 같은 전신 모델도 대규모 데이터 저장소를 사용하여 엔드투엔드 전신 모델을 학습하는 대신 이미 학습된 구성 요소를 결합하여 구축했습니다.

이와는 대조적으로 이 연구에서는 모든 고해상도 데이터를 동시에 사용하여 포괄적인 엔드투엔드 모델을 훈련할 것을 제안합니다. 이 모델은 포즈, 모양, 표정, 손 조작 수준에서 전신 디테일을 캡처하도록 설계될 것입니다. 저자들은 60,000개 이상의 사실적인 동적 인체 스캔으로 구성된 새로운 3D 데이터 세트인 GHS3D와 4,000개 이상의 Caesar 전신 스캔을 활용할 예정입니다.

중간 해상도 모델인 GHUM과 특별히 설계된 저해상도 모델인 GHUML의 두 가지 모델이 소개됩니다. 이 두 모델의 성능은 다양한 선형 및 비선형 모델에서 등록 및 제한된 3D 표면 피팅에 대해 비교됩니다.

이 논문에서는 신체 부위 및 전신에 대한 3D 관절 표면 모델과 포즈 및 모양 추정 방법에 대한 기존 연구를 언급합니다. 이 논문은 모든 고해상도 데이터를 동시에 사용하고 모든 파라미터를 엔드 투 엔드(end-to-end)로 정제하는 것을 강조하면서 기존 연구와 방법론을 대조합니다.

이 모델은 이미지에서 포즈와 모양을 복구하는 등 다양한 분야에 응용할 수 있으며, 3D 휴먼 센싱의 경량 모바일 애플리케이션을 구현할 수도 있습니다. 또한 디테일 수준과 런타임 제약 조건에 따라 모델 간에 동적으로 전환할 수 있습니다.

### 2 Overview

이 연구에서는 일관된 토폴로지와 조정 가능한 변수인 α를 사용하여 체형과 관절을 나타내는 통계적 인체 모델인 X를 개발합니다. 먼저 인체 스캔 세트(비정형 포인트 클라우드)를 사용하여 신체 템플릿을 포인트 클라우드에 등록하고 동일한 토폴로지를 가진 새로운 메시(X*)를 생성합니다. 이렇게 등록된 메쉬는 모델 파라미터(α)가 입력에 맞게 조정되는 엔드투엔드 훈련 네트워크에서 사용됩니다.

![우리의 엔드-투-엔드 통계적 3D 조인트 인간 형태 모델 구축의 개요. 우리는 다양한 자세와 부드러운 조직 변형을 노출하는 고해상도 3D 바디 스캔을 포함한 'A' 포즈와 임의의 포즈로 시작한다. 또한, 우리는 상세한 표정과 다른 제스처, 물체를 잡는 손의 클로즈업 스캔을 수집한다. 몸체 랜드마크는 여러 가상의 관점에서 사실적인 3D 복원을 렌더링하고, 그림에서 그들을 감지하고 삼각화함으로써 자동으로 식별된다. 예술가가 설계한 전체 몸체 관절 메시는 가능한 한 정합성이 있는 표면 이전[41]에 따라 스펄스 랜드마크 상응성과 밀도있는 최근접 점(ICP) 잔차를 결합하는 손실을 사용하여 점 클라우드에 점진적으로 등록된다. 모델은 몸체 φb와 얼굴 표정의 오프셋 φf에 대한 깊은 변이형 자동인코더(VAE)로 구현된 비선형 형태 공간을 가지며, 가충적인 포즈 공간 변형 함수 D, 골격 K와 J 관절, 센터 예측기 C, 및 블렌드 스킨 함수 M를 포함한다. 훈련 동안, 동일한 피사체의 모든 고해상도 스캔(몸체 전체와 얼굴 및 손에 대한 클로즈업 모두)이 사용되며, 필터 F에 의해 적절하게 마스킹된 잔차로 사용된다. 모델 구축을 위해, 우리는 N명의 캡쳐된 피사체를 사용하며, B개의 전체 바디 스캔, F개의 클로즈업 핸드 스캔, 그리고 H개의 클로즈업 헤드 스캔이 있다. 학습 동안, 우리는 각 스캔에서의 포즈 추정 θ에 대한 손실 함수를 최소화하는 것과 다른 모델 파라미터(φ, γ, ψ, ω)에 대해 최적화하는 것 사이를 번갈아 가며 진행한다. 작동시, 예를 들어 포즈와 형태 추정을 위해, 모델은 파라미터 α = (θ, β)에 의해 제어되며, 이는 kinematic 포즈 θ와 VAE 잠재 공간에 대한 몸체 형태와 얼굴 표정 β = (βf, βb)를 포함한다. 여기서 인코더-디코더는 φ = (φf, φb)에 의해 주어진다.](GHUM%20Implicit%20Generative%20Models%20of%203D%20Human%20Shape%20%20257c9f5a7ad740c99a468568d12272b0/Untitled%201.png)

우리의 엔드-투-엔드 통계적 3D 조인트 인간 형태 모델 구축의 개요. 우리는 다양한 자세와 부드러운 조직 변형을 노출하는 고해상도 3D 바디 스캔을 포함한 'A' 포즈와 임의의 포즈로 시작한다. 또한, 우리는 상세한 표정과 다른 제스처, 물체를 잡는 손의 클로즈업 스캔을 수집한다. 몸체 랜드마크는 여러 가상의 관점에서 사실적인 3D 복원을 렌더링하고, 그림에서 그들을 감지하고 삼각화함으로써 자동으로 식별된다. 예술가가 설계한 전체 몸체 관절 메시는 가능한 한 정합성이 있는 표면 이전[41]에 따라 스펄스 랜드마크 상응성과 밀도있는 최근접 점(ICP) 잔차를 결합하는 손실을 사용하여 점 클라우드에 점진적으로 등록된다. 모델은 몸체 φb와 얼굴 표정의 오프셋 φf에 대한 깊은 변이형 자동인코더(VAE)로 구현된 비선형 형태 공간을 가지며, 가충적인 포즈 공간 변형 함수 D, 골격 K와 J 관절, 센터 예측기 C, 및 블렌드 스킨 함수 M를 포함한다. 훈련 동안, 동일한 피사체의 모든 고해상도 스캔(몸체 전체와 얼굴 및 손에 대한 클로즈업 모두)이 사용되며, 필터 F에 의해 적절하게 마스킹된 잔차로 사용된다. 모델 구축을 위해, 우리는 N명의 캡쳐된 피사체를 사용하며, B개의 전체 바디 스캔, F개의 클로즈업 핸드 스캔, 그리고 H개의 클로즈업 헤드 스캔이 있다. 학습 동안, 우리는 각 스캔에서의 포즈 추정 θ에 대한 손실 함수를 최소화하는 것과 다른 모델 파라미터(φ, γ, ψ, ω)에 대해 최적화하는 것 사이를 번갈아 가며 진행한다. 작동시, 예를 들어 포즈와 형태 추정을 위해, 모델은 파라미터 α = (θ, β)에 의해 제어되며, 이는 kinematic 포즈 θ와 VAE 잠재 공간에 대한 몸체 형태와 얼굴 표정 β = (βf, βb)를 포함한다. 여기서 인코더-디코더는 φ = (φf, φb)에 의해 주어진다.

사람 모델은 J 관절을 포함하는 골격(K)을 가진 관절 메시로 표현되며, 피부 변형은 리니어 블렌딩 스키닝(LBS)을 기반으로 합니다. 비선형 모델은 얼굴 표정을 구동합니다. 모델 X는 M(α = (θ, β), φ, γ, ψ, ω)로 공식화됩니다:

![Untitled](GHUM%20Implicit%20Generative%20Models%20of%203D%20Human%20Shape%20%20257c9f5a7ad740c99a468568d12272b0/Untitled%202.png)

- X˜(βb)는 아이덴티티 기반 나머지 모양으로, βb는 체형 가변성을 인코딩합니다.
- ∆X˜f(βf)는 저차원 잠재 코드 βf로 제어되는 중립 머리 포즈에서의 표정입니다.
- c = C(X˜)는 체형에 따라 달라지는 골격 관절 중심입니다.
- θ는 각 관절의 회전 자유도(DOF)와 루트의 병진 변수로 구성된 스켈레톤 포즈 파라미터의 벡터입니다.
- ω는 최대 I 조인트의 영향을 받는 버텍스별 스키닝 가중치입니다.
- ∆X˜(θ)는 스키닝 아티팩트를 보정하기 위해 나머지 셰이프에 추가된 포즈 의존적 보정 블렌드 셰이프입니다.

휴먼 모델인 GHUM과 GHUML은 아티스트가 정의한 리깅된 템플릿 메시를 사용하여 초기화됩니다. 파이프라인은 메시 토폴로지와 조인트 계층구조 K를 고정된 상태로 유지하면서 모든 파라미터(θ, φ, γ, ψ, ω)를 추정합니다. 휴식에서 포즈된 메시로의 변환은 휴식 포즈 θ¯에서 월드 변환 행렬의 역을 곱하여 구성됩니다.

![우리는 바디 스캔과 클로즈업 핸드 및 헤드 스캔을 융합하여 중립 A 포즈에서 전체 몸체 형태를 추정한다. 단일 바디 스캔에서의 몸체 형태 추정에 비해, 우리는 추가적인 헤드와 핸드 형태 디테일을 활용할 수 있다.](GHUM%20Implicit%20Generative%20Models%20of%203D%20Human%20Shape%20%20257c9f5a7ad740c99a468568d12272b0/Untitled%203.png)

우리는 바디 스캔과 클로즈업 핸드 및 헤드 스캔을 융합하여 중립 A 포즈에서 전체 몸체 형태를 추정한다. 단일 바디 스캔에서의 몸체 형태 추정에 비해, 우리는 추가적인 헤드와 핸드 형태 디테일을 활용할 수 있다.

![Caesar에서의 평가. 왼쪽: GHUM 및 GHUML에 대한 등록에 대한 당점 유클리드 오류. 오른쪽: (위에서 아래로, 등록, GHUM 및 GHUML) VAE 기반 모델은 몸체 형태를 매우 잘 나타낼 수 있다. GHUML과 비교하여 추가적인, 예를 들어 근육이나 허리, 부드러운 조직 디테일이 GHUM에 의해 보존된다.](GHUM%20Implicit%20Generative%20Models%20of%203D%20Human%20Shape%20%20257c9f5a7ad740c99a468568d12272b0/Untitled%204.png)

Caesar에서의 평가. 왼쪽: GHUM 및 GHUML에 대한 등록에 대한 당점 유클리드 오류. 오른쪽: (위에서 아래로, 등록, GHUM 및 GHUML) VAE 기반 모델은 몸체 형태를 매우 잘 나타낼 수 있다. GHUML과 비교하여 추가적인, 예를 들어 근육이나 허리, 부드러운 조직 디테일이 GHUM에 의해 보존된다.

### 3. End-to-End Statistical Model Learning

이러한 모델을 재구성하기 위해 버텍스별 유클리드 거리 오차를 사용하여 모델 출력과 입력 데이터 간의 불일치를 측정합니다. 포즈 매개변수(θ)와 통계적 모양 매개변수가 번갈아 추정 및 업데이트됩니다.

파이프라인에는 두 가지 자동 인코더, 즉 변형 체형 자동 인코더와 변형 얼굴 표정 자동 인코더가 포함되어 있습니다.

가변 체형 자동 인코더는 시저 데이터세트에 등록된 다중 피사체 형상 스캔과 GHS3D 스캔을 사용합니다. 이러한 스캔은 체형 변형을 위한 컴팩트한 잠재 공간을 구축하는 데 사용됩니다. 체형을 표현하기 위해 저차원 잠재 코드와 함께 변형 자동 인코더가 사용됩니다. 인코더와 디코더는 파라메트릭 ReLU 활성화 기능을 사용합니다. 인코더는 평균과 분산(µ, Σ)을 출력하고, 이 값은 재매개변수화 트릭을 사용하여 잠재 공간으로 변환되어 샘플링된 코드(βb)를 얻습니다. 손실 함수에 쿨백-라이블러 발산을 통합하여 잠재 공간을 정규화합니다.

![포즈 공간 변형 아키텍처 스케치와 PSD의 이점을 보여주는 그림, 여기서 비수동 관절점 주위, 예를 들어 오른쪽 엉덩이와 허벅지, 그리고 가슴과 겨드랑이. 그림을 간단하게 만들기 위해, 여기서 우리는 입력 특성으로 θ를 사용한다, 대신에 Ri(θi) - Ri(θ¯i).](GHUM%20Implicit%20Generative%20Models%20of%203D%20Human%20Shape%20%20257c9f5a7ad740c99a468568d12272b0/Untitled%205.png)

포즈 공간 변형 아키텍처 스케치와 PSD의 이점을 보여주는 그림, 여기서 비수동 관절점 주위, 예를 들어 오른쪽 엉덩이와 허벅지, 그리고 가슴과 겨드랑이. 그림을 간단하게 만들기 위해, 여기서 우리는 입력 특성으로 θ를 사용한다, 대신에 Ri(θi) - Ri(θ¯i).

변형 얼굴 표정 자동 인코더는 복잡한 얼굴 표정을 지원하는 것을 목표로 합니다. 이 인코더는 GHS3D에서 사용할 수 있는 수천 개의 표정 모션 시퀀스 스캔을 사용합니다. 머리의 관절 외에도 스키닝 가중치, 포즈 공간 변형 등의 파라미터가 파이프라인 내에서 업데이트됩니다. 관절로 인해 발생하지 않는 얼굴 표정의 경우, 비선형 임베딩 βf는 체형 자동 인코더와 동일한 네트워크 구조로 구축됩니다. VAE에 대한 입력은 모든 관절 동작이 제거된 중립 머리 포즈에서의 표정 ∆X¯f입니다.

### 4. Experiments

데이터 세트: 저자는 4,329명의 피험자의 다양한 신체 및 얼굴 모양이 포함된 Caesar 데이터 세트를 사용했습니다. 또한 48명의 피험자(여성 24명, 남성 24명)로부터 60Hz로 작동하는 독점 시스템을 사용하여 데이터를 캡처했습니다. 이 피험자들은 55개의 신체 포즈, 60개의 손 포즈, 40개의 얼굴 표정 시퀀스를 시연했습니다. 데이터에는 다양한 BMI, 키, 연령이 포함되어 있습니다.

![Caesar에서의 샘플 등록(좌측 상단) 및 우리의 GHS3D. 미묘한 얼굴 디테일을 캡처하는 등록의 품질, 그리고 관절의 결과로 다른 몸체 부분의 부드러운 조직 변형을 확인하십시오.](GHUM%20Implicit%20Generative%20Models%20of%203D%20Human%20Shape%20%20257c9f5a7ad740c99a468568d12272b0/Untitled%206.png)

Caesar에서의 샘플 등록(좌측 상단) 및 우리의 GHS3D. 미묘한 얼굴 디테일을 캡처하는 등록의 품질, 그리고 관절의 결과로 다른 몸체 부분의 부드러운 조직 변형을 확인하십시오.

![VAE 및 PCA 모델의 분석은 저차원 체계에서 비선형 표현의 장점을 보여준다](GHUM%20Implicit%20Generative%20Models%20of%203D%20Human%20Shape%20%20257c9f5a7ad740c99a468568d12272b0/Untitled%207.png)

VAE 및 PCA 모델의 분석은 저차원 체계에서 비선형 표현의 장점을 보여준다

등록: 저자들은 3D 형상 등록에 널리 사용되는 방법인 반복적 최접점(ICP) 및 모따기 거리를 사용하여 포인트 클라우드와 모델을 정렬하는 데 오류가 적었다고 보고했습니다.

모델 평가: 파이프라인을 사용하여 전체 해상도 및 저해상도 인체 모델(GHUM 및 GHUML)을 구축했습니다. 두 모델 모두 테스트 데이터의 등록에 대한 메시의 평균 정점 기반 유클리드 거리를 사용하여 평가되었습니다. 두 모델 모두 다양한 체형을 표현하고, 자연스러운 얼굴 표정을 생성하며, 눈에 띄는 피부 아티팩트 없이 매끄러운 포즈를 취할 수 있었습니다.

GHUM과 GHUML 비교: 저해상도 모델인 GHUML은 입술 변형, 팔과 손가락의 근육 돌출, 지방 조직으로 인한 주름의 디테일이 GHUM에 비해 다소 떨어집니다. 그러나 피드 포워드 평가 모드에서는 GHUM보다 두 배 빠릅니다.

![왼쪽에서 오른쪽으로, 등록, GHUM, 그리고 SMPL. GHUM은 이 동작 시퀀스에 대해 더 적은 골반 아티팩트로 스키닝을 생산한다(평균적으로 0.76 mm의 더 낮은 오류).](GHUM%20Implicit%20Generative%20Models%20of%203D%20Human%20Shape%20%20257c9f5a7ad740c99a468568d12272b0/Untitled%208.png)

왼쪽에서 오른쪽으로, 등록, GHUM, 그리고 SMPL. GHUM은 이 동작 시퀀스에 대해 더 적은 골반 아티팩트로 스키닝을 생산한다(평균적으로 0.76 mm의 더 낮은 오류).

![그림 1에서와 같이 평가 및 렌더링, GHUM 및 GHUML의 핸드 재구성에 중점을 둔다. GHUM이 GHUML 위에 손바닥의 굴곡 영역 주위의 추가적인 변형 디테일을 보존하는 것을 주목하십시오. 얼굴 표정에 대해서는 Sup. Mat.를 참조하십시오.](GHUM%20Implicit%20Generative%20Models%20of%203D%20Human%20Shape%20%20257c9f5a7ad740c99a468568d12272b0/Untitled%209.png)

그림 1에서와 같이 평가 및 렌더링, GHUM 및 GHUML의 핸드 재구성에 중점을 둔다. GHUM이 GHUML 위에 손바닥의 굴곡 영역 주위의 추가적인 변형 디테일을 보존하는 것을 주목하십시오. 얼굴 표정에 대해서는 Sup. Mat.를 참조하십시오.

VAE 평가: 저자들은 체형과 얼굴 표정 모두에 대해 VAE의 품질을 평가했습니다. VAE는 체형에 대해 16디멘드 또는 64디멘드 잠상 표현을, 얼굴 표정에 대해 20디멘드 임베딩을 지원할 수 있었습니다. 20차원 VAE는 96개의 선형 PCA 베이스를 사용하는 것과 유사한 재구성 오차가 있습니다.

GHUM과 SMPL 비교: 저자들은 다양한 피사체와 포즈에 대해 GHUM 모델의 스키닝 품질을 널리 사용되는 3D 인체 모델인 SMPL과 비교했습니다. GHUM은 평균 재구성 오류를 낮추며 SMPL보다 우수한 성능을 보였습니다.

단안 이미지에서 3D 포즈 및 모양 재구성: 또한 다양한 데이터 세트에서 학습된 운동학 사전을 사용하고 골격 관절 재현 오류와 시맨틱 신체 부위 정렬을 사용하여 포즈 및 모양 매개변수를 최적화하는 GHUM을 사용한 이미지 재구성에 대해 설명했습니다.

![GHUM을 사용하여 의미론적 몸체 부분 정렬 손실 아래에서 비선형 포즈 및 형태 최적화에 의존한 단안 3D 인간 포즈 및 형태 재구성.](GHUM%20Implicit%20Generative%20Models%20of%203D%20Human%20Shape%20%20257c9f5a7ad740c99a468568d12272b0/Untitled%2010.png)

GHUM을 사용하여 의미론적 몸체 부분 정렬 손실 아래에서 비선형 포즈 및 형태 최적화에 의존한 단안 3D 인간 포즈 및 형태 재구성.

애플리케이션 사용 사례: 저자들은 의류 가상 의류 시착, 피트니스, AR 및 VR, 특수 효과, 게임과 같은 분야에서 모델의 잠재적 활용 가능성을 강조했습니다. 하지만 현재 이 모델은 시각적 감시, 사람 식별 또는 딥페이크 제작과 같은 애플리케이션에 사용하기에는 충분히 상세하지 않다고 지적합니다.

### 5. Conclusions

저자들은 각각 중간 해상도와 낮은 해상도의 두 가지 새로운 생성형 3D 인체 형태 및 포즈 모델인 GHUM과 GHUML을 소개했습니다. 이 모델은 얼굴과 손의 클로즈업뿐만 아니라 전신 스캔을 포함한 60,000개 이상의 인체 스캔으로 구성된 새로운 데이터 세트인 GHS3D를 기반으로 구축되었습니다.

![Untitled](GHUM%20Implicit%20Generative%20Models%20of%203D%20Human%20Shape%20%20257c9f5a7ad740c99a468568d12272b0/Untitled%2011.png)

최초로 모든 모델 구성 요소를 동시에 학습할 수 있는 새로운 엔드투엔드 딥러닝 프레임워크가 제시되었습니다. 여기에는 비선형 형상 공간, 포즈 공간 변형 보정, 골격 관절 중심 추정기, 표면 블렌드 스키닝 함수가 포함됩니다.

저해상도 및 중간 해상도 영역 모두에서 모델의 성능을 테스트하기 위해 광범위한 실험을 수행했습니다. 여기에는 등록, 제약 조건 하에서의 3D 형상 피팅, 단안 이미지로부터의 3D 포즈 및 형상 추정이 포함되었습니다.

이 연구에서 주목할 만한 결론은 잘 훈련된 저해상도 모델(정점이 약 3,000개)이 단순성을 고려할 때 예상치 못한 놀라운 인체 형상 표현 능력을 보여줄 수 있다는 것입니다.

마지막으로 저자들은 이 모델을 추가 연구에 활용할 수 있도록 하여 이 분야의 발전을 촉진할 수 있도록 하겠다고 약속했습니다.

- SMPL과 비교
    
    SMPL은 Skinned Multi-Person Linear Model의 약자로, 3차원 인간 형태 및 포즈 모델링에 널리 사용되는 기범입니다. SMPL 모델은 선형 혼합 스킨 모델을 사용하여 복잡한 인간 몸체의 모양과 움직임을 근사화합니다. SMPL의 간단하고 효율적인 선형 구조는 많은 그래픽 및 비전 애플리케이션에서 이용되어 왔습니다.
    
    그러나 SMPL에는 몇 가지 제한 사항이 있습니다. SMPL은 선형 방식으로 스킨을 처리하기 때문에, 특히 조인트 주변에서 비선형적인 스킨 변형을 완전히 모델링하는 데 어려움이 있습니다. 또한, SMPL의 형상 공간은 주로 몸체의 크기와 체형에 집중되어 있으며, 세부적인 형상, 특히 얼굴이나 손에 대한 형상을 모델링하는 데는 한계가 있습니다.
    
    이에 비해, GHUM 및 GHUML은 종단 간 딥러닝 프레임워크를 사용하여 비선형 형태 공간, 포즈 공간 변형 보정, 골격 관절 중심 추정기, 표면 혼합 스키닝 함수 등 모든 모델 구성 요소의 매개변수를 결합하여 학습합니다. 이를 통해, GHUM 및 GHUML은 보다 세밀한 형상, 특히 얼굴이나 손의 세부 형상을 더 잘 재현할 수 있습니다. 또한, GHUM 및 GHUML은 스키닝 아티팩트를 더 적게 생성하면서도, 다양한 형태와 포즈에 대해 더 자연스러운 움직임을 제공합니다. 그러나 이러한 성능 향상은 더 복잡한 모델 구조와 더 큰 계산 요구사항을 필요로 합니다.