# Kandinsky: an Improved Text-to-Image Synthesis with Image Prior and Latent Diffusion

[https://arxiv.org/abs/2310.03502](https://arxiv.org/abs/2310.03502)

[https://github.com/ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2)

- Oct 2023

### 1 Introduction

최근에는 텍스트-이미지 모델의 생성 기능이 크게 발전했습니다. 이제 이러한 모델은 놀라운 속도로 사실적인 품질의 이미지를 생성할 수 있습니다. 이러한 모델은 다양한 분야에서 활용되고 있으며, 사용자 친화적인 웹 플랫폼부터 고급 AI 그래픽 툴에 이르기까지 다양한 기능을 갖추고 있습니다.

![칸딘스키 모델의 이미지 사전 체계 및 추론 체계.](Kandinsky%20an%20Improved%20Text-to-Image%20Synthesis%20with%20bcb54052d04c4980a8773a465b808bc7/Untitled.png)

칸딘스키 모델의 이미지 사전 체계 및 추론 체계.

이 연구는 혁신적인 잠재 확산 아키텍처 설계에 초점을 맞춰 급성장하는 이 분야에 대한 새로운 접근 방식을 소개합니다. 이 백서에서는 새로운 "칸딘스키" 아키텍처에 대해 자세히 설명하고 이 모델의 기능을 통합한 데모 시스템에 대한 개요를 제공합니다. 성능을 검증하기 위해 이미지 생성 품질을 평가하는 실험을 수행했습니다. 놀랍게도, 그 결과 다른 오픈 소스 모델과 비교했을 때 선도적인 프리셰트 시작 거리(FID) 점수를 기록했습니다. 이 아키텍처를 더욱 개선하기 위해 이전 설계에 대한 심층적인 분석을 수행하여 최적의 구성을 결정할 수 있었습니다.

이미지 사전 확산과 잠재 확산을 통합하는 동종 최초의 텍스트-이미지 아키텍처를 도입했습니다.
Stable Diffusion, IF, DALL-E 2와 같은 최고 모델과 견주어도 손색이 없을 뿐만 아니라 FID 메트릭에 기반한 오픈 소스 모델의 새로운 표준을 제시하는 실험 결과.
텍스트를 이미지로 변환하는 이 최첨단 방법을 캡슐화한 소프트웨어 도구로, 현장에서 뛰어난 성능을 발휘하는 사전 학습된 모델이 포함되어 있습니다. 이러한 모델은 Apache 2.0 라이선스에 따라 아낌없이 제공되므로 비상업적 및 상업적 목적 모두에 대한 접근성을 보장합니다.
웹 기반 이미지 편집기 애플리케이션이 추가로 개발되었습니다. 텍스트 프롬프트에서 즉석에서 이미지를 생성할 수 있으며 영어와 러시아어를 모두 지원합니다. 또한 이 도구에는 이미지 수정 기능도 포함되어 있습니다. 이 기능에 대한 시각적 쇼케이스는 YouTube를 통해 확인할 수 있습니다.

### 2 Related Work

텍스트-이미지 생성 모델의 초기 단계에서는 DALL-E 및 CogView와 같은 초기 디자인이 자동 회귀 방식에 의존했습니다. 그러나 이러한 초기 모델은 콘텐츠 수준 아티팩트에 대한 문제에 직면했습니다. 이로 인해 이 분야에서 기본으로 인정받게 된 DALL-E 2, Imagen, Stable Diffusion과 같은 확산 기반 모델이 도입되었습니다. 이러한 모델은 일반적으로 픽셀 수준과 잠재 수준 전략의 두 가지 범주로 분류됩니다.

확산 기반 모델에 대한 관심이 높아지면서 새로운 디자인과 아키텍처가 탄생했습니다. 그 결과 드림부스와 드림포즈 등 오픈 소스 생성 모델을 활용하는 다양한 애플리케이션이 등장했습니다. 이러한 애플리케이션은 이미지 생성 기술을 사용하여 눈에 띄는 기능을 제공함으로써 확산 기반 이미지 생성 방법의 매력과 빠른 발전을 강화했습니다. 그 결과 3D 객체 합성, 비디오 생성, 제어 가능한 이미지 편집과 같은 분야에서 획기적인 애플리케이션이 이 분야의 리더로 부상했습니다.

다양한 이미지 생성 기법 중에서도 디퓨전 모델은 뛰어난 성능으로 두각을 나타내고 있습니다. 확산 모델은 무조건 및 조건부 이미지 생성 작업 모두에서 최고 수준의 결과를 일관되게 제공합니다. 특히 이 모델은 적대적 학습 없이도 더 높은 충실도와 다양성을 가진 이미지를 생성함으로써 생성적 적대적 네트워크(GAN)를 능가합니다. 또한 확산 모델은 인페인팅, 아웃페인팅, 초고해상도 등 여러 이미지 처리 작업에서 탁월한 성능을 발휘합니다.

확산 모델과 텍스트-이미지 생성의 융합은 이러한 모델의 강력한 성능으로 인해 연구 분야에서 상당한 주목을 받고 있습니다. GLIDE, Imagen, DALL-E 2, eDiff-I와 같은 초기 모델은 주로 저해상도 이미지를 생성한 다음 초고해상도 확산 모델을 사용하여 이미지를 향상시켰습니다. 또한 이러한 모델에는 Imagen의 T5 대형 언어 모델과 GLIDE 및 DALL-E 2의 CLIP과 같은 다양한 텍스트 인코더가 사용되었습니다.

### 3 Demo System

칸딘스키 모델의 사용자 지향 구현

칸딘스키 모델은 다양한 사용자 중심 솔루션에 통합되어 주요 이미징 구성 요소로 사용되고 있습니다. 이러한 통합은 모델의 다양한 추론 방법에 의해 주도되었으며, 그 중 일부는 최적의 성능을 위해 특수한 프런트엔드 기능을 필요로 합니다. 이러한 노력의 결과, 추론을 위한 두 가지 주요 플랫폼, 즉 텔레그램 봇과 퓨전브레인 웹사이트가 탄생했습니다.

![칸딘스키 모델을 사용한 추론 체계의 예시.](Kandinsky%20an%20Improved%20Text-to-Image%20Synthesis%20with%20bcb54052d04c4980a8773a465b808bc7/Untitled%201.png)

칸딘스키 모델을 사용한 추론 체계의 예시.

FusionBrain 웹 기반 이미지 편집기

FusionBrain은 이미지 업로드 및 저장 기능, 이동 가능한 보기 창, 이미지 삭제 도구, 확대/축소 기능, 다양한 스타일 선택 등 다양한 기능을 갖춘 온라인 이미지 편집기입니다. 이미지 생성을 위해 FusionBrain은 세 가지 주요 기능을 제공합니다:

- 텍스트-이미지 생성: 사용자는 러시아어 또는 영어로 텍스트 프롬프트를 입력하고 사용 가능한 옵션에서 원하는 종횡비를 선택하면 시스템이 입력에 따라 이미지를 생성합니다.
- 인페인팅: 사용자는 이미지의 특정 부분을 지울 수 있습니다. 지워진 부분은 텍스트 프롬프트의 안내에 따라 또는 텍스트 입력 없이 다시 채울 수 있습니다.
- 아웃페인팅: 이 플랫폼에서는 사용자가 슬라이딩 창을 사용하여 기존 이미지를 확장할 수 있습니다. 이 창이 이미지의 일부와 겹치는 경우 텍스트 프롬프트가 표시되거나 표시되지 않은 창 부분을 채울 수 있습니다.

이러한 인페인팅과 아웃페인팅 기능은 이미지 편집 기능에 있어, 플랫폼의 가장 큰 특징입니다.

텔레그램 봇의 이미징 기능

텔레그램 봇은 다양한 이미지 생성 도구를 제공합니다:

- 텍스트-이미지 생성: 퓨전브레인과 유사하게, 이 기능은 사용자가 제공한 텍스트 프롬프트를 이미지로 바꿔줍니다.
- 이미지와 텍스트 융합: 여기에서 사용자는 텍스트 프롬프트와 함께 이미지를 입력할 수 있습니다. 그러면 시스템이 주어진 텍스트와 원본 이미지를 기반으로 새로운 이미지를 생성합니다.
- 이미지 융합: 사용자는 기본 이미지와 보조 '안내' 이미지를 입력할 수 있습니다. 그러면 시스템이 이 두 이미지를 병합하여 일관성 있는 혼합 이미지를 생성합니다.
- 이미지 변형: 사용자가 이미지를 제출하면 시스템이 원본과 유사한 여러 개의 새로운 이미지를 생성하도록 요청할 수 있습니다.

이 두 플랫폼은 칸딘스키 모델에 기반한 포괄적인 이미지 편집 및 생성 기능을 함께 제공합니다.

### 4 Kandinsky Architecture

저희 작업의 핵심 목표는 텍스트-이미지 합성에서 최첨단 성능을 달성하는 것이었습니다. 처음에는 다국어 텍스트-대-이미지 생성 성능을 향상시키기 위해 mT5, XLMR, XLMR-CLIP과 같은 다국어 텍스트 인코더를 실험해 보았습니다. 그러나 이러한 실험을 통해 CLIP 이미지 임베딩이 이미지 품질 면에서 독립형 텍스트 인코더를 능가한다는 사실을 알게 되었습니다. 이러한 인사이트를 바탕으로 XLMR 텍스트 임베딩을 유지하면서 CLIP의 텍스트와 이미지 임베딩 공간 간에 확산 및 선형 매핑을 병합하는 이미지 선행 접근 방식으로 전환하게 되었습니다. 이 접근 방식을 통해 칸딘스키는 두 개의 텍스트 인코더를 사용하게 되었습니다: 이미지 사전 매핑에 연결되는 CLIP 텍스트와 XLMR입니다. 이 두 인코더는 모델 훈련 단계에서 변경되지 않았습니다.

!["파도 위를 미끄러지는 코기"에 대한 칸딘스키 웹 인터페이스: 생성(왼쪽) 및 인/아웃페인팅(오른쪽).](Kandinsky%20an%20Improved%20Text-to-Image%20Synthesis%20with%20bcb54052d04c4980a8773a465b808bc7/Untitled%202.png)

"파도 위를 미끄러지는 코기"에 대한 칸딘스키 웹 인터페이스: 생성(왼쪽) 및 인/아웃페인팅(오른쪽).

이러한 아키텍처 선택은 픽셀 수준 확산 모델과 달리 잠재 확산 모델의 우수한 훈련 효율성에 크게 영향을 받았습니다. 이에 따라 잠재 확산 아키텍처를 채택하여 모델을 텍스트 인코딩, 임베딩 매핑(이미지 이전이라고 함), 잠재 확산의 세 가지 주요 단계로 구분했습니다.

임베딩 매핑 단계에서는 CLIP-ViT-L14 모델이 제공하는 텍스트 및 이미지 임베딩에 대한 확산 프로세스를 사용하여 처음부터 훈련된 트랜스포머-인코더 모델을 배포했습니다. 훈련 요법의 뚜렷한 측면은 시각적 임베딩의 요소별 정규화였습니다. 전체 데이터 세트 통계에 기반한 이 기법은 확산 과정의 수렴을 가속화했습니다. 추론 중에 원래의 클립 이미지 임베딩 공간으로 전환하기 위해 역 정규화를 통합했습니다.

CLIP의 텍스트 및 이미지 임베딩에서 학습을 도출한 이미지 사전 모델은 일련의 포괄적인 실험과 아키텍처 탐색을 거쳤습니다. 인간의 평가 측면에서 눈에 띄는 아키텍처는 특정 매개변수가 있는 1D 확산과 표준 트랜스포머 인코더를 활용했습니다.

잠복 확산 세그먼트는 UNet 모델과 맞춤형 자동 인코더를 통합합니다. 이 모델은 여러 조건 신호, 즉 클립 이미지 임베딩, 클립 텍스트 임베딩 및 XLMR-CLIP 텍스트 임베딩에 의존합니다. CLIP-이미지 임베딩과 XLMR-CLIP 임베딩을 병합하여 잠재 확산 프로세스의 입력으로 사용합니다. 이러한 임베딩을 기반으로 확산 프로세스를 추가로 조절했습니다. 특히 확산 추론 중에 자동 인코더의 양자화 단계를 유지하기로 결정한 결과 생성되는 이미지의 다양성과 품질이 향상되었습니다. 전체 모델에는 33억 개의 파라미터가 포함되어 있습니다.

마지막으로, 이미지 디코딩이 가장 큰 과제라는 점을 인식하고 맞춤형 버전의 MoVQGAN을 설계했으며, 이를 Sber-MoVQGAN이라고 명명했습니다. 이 자동 인코더는 LAION 고해상도 데이터세트에 대해 학습되어 이미지 재구성에서 탁월한 결과를 가져왔습니다. 오픈 사이언스에 대한 노력의 일환으로 모델의 가중치와 코드를 대중이 액세스할 수 있도록 공개했습니다. 다른 자동 인코더와의 비교는 보고서의 전용 표에 나와 있습니다.

### 5 Experiments

잠재 확산 아키텍처의 효과를 측정하기 위해 엄격한 실험 분석을 수행했습니다. COCO-30K 데이터 세트에서 FID-CLIP 곡선을 사용하여 최적의 가이드 스케일 값을 결정하고 칸딘스키 모델을 경쟁 모델과 비교할 수 있었습니다. 선행이 없는 설정, 선형 레이어가 있는 설정, 18개의 잔여 MLP 블록이 있는 설정(ResNet 선행), 트랜스포머 확산 선행이 있는 설정 등 여러 가지 이미지 선행 설정을 테스트했습니다. MoVQ 자동 인코더에서 잠재 양자화의 역할을 이해하는 데 중점을 두었습니다. 잠재 양자화를 켰을 때와 껐을 때의 결과를 비교하여 이미지 품질에 미치는 영향을 파악하고자 했습니다.

![고양이가 있는 500쌍의 이미지에 대해 학습된 원본 이미지 사전 및 선형 사전의 "말을 타는 우주비행사"라는 프롬프트가 포함된 이미지 생성 결과.](Kandinsky%20an%20Improved%20Text-to-Image%20Synthesis%20with%20bcb54052d04c4980a8773a465b808bc7/Untitled%203.png)

고양이가 있는 500쌍의 이미지에 대해 학습된 원본 이미지 사전 및 선형 사전의 "말을 타는 우주비행사"라는 프롬프트가 포함된 이미지 생성 결과.

평가의 깊이를 더하기 위해 우리와 유사한 오픈 소스 모델인 IF 모델 12를 FID 점수를 사용하여 분석했습니다. 그러나 미묘한 사용자 경험을 포착하는 데 있어 자동화된 지표의 단점을 인식하고 블라인드 인적 평가도 수행했습니다. 이 평가는 DrawBench 데이터 세트에서 수행되었으며, 이미지 품질에 대한 인간의 인사이트를 포착하는 것을 목표로 했습니다. 이러한 다각적인 평가를 통해 칸딘스키의 성능에 대한 전체적인 관점을 확보하고 이미지 사전 설계의 잠재력과 효과를 조명할 수 있었습니다.

### 6 Results

실험 결과는 텍스트를 이미지로 변환하는 데 있어 칸딘스키 아키텍처의 뛰어난 능력을 보여주었습니다. 칸딘스키는 COCO-30K 검증 세트에서 8.03의 FID 점수를 기록하여 오픈 소스 시스템 중 최상위권에 속하며 선도적인 모델과 경쟁했습니다. 더 자세히 살펴본 결과, MoVQ에서 양자화를 사용하면 이미지 품질이 소폭 향상되는 것으로 나타났습니다. CLIP 점수와 사람의 평가 측면에서 가장 설득력 있는 결과는 확산 이전을 사용하여 얻은 것이었습니다.

또한 선형 사전을 사용한 구성이 가장 뛰어난 것으로 나타났으며, 최고 FID 점수인 8.03점을 달성했습니다. 이 흥미로운 결과는 시각적 임베딩과 텍스트 임베딩 사이에 선형적인 관계가 존재할 가능성이 있음을 시사합니다. 이러한 가능성을 탐색하기 위해 500개의 고양이 이미지 하위 집합에 대해 선형 매핑을 학습시켰으며, 이를 "고양이 이전"이라는 유머러스한 이름으로 명명했습니다. 이 매핑이 보여준 숙련도는 놀라울 정도로 뛰어났으며, 우리가 제안한 아키텍처의 기능을 더욱 확고히 했습니다.

![Untitled](Kandinsky%20an%20Improved%20Text-to-Image%20Synthesis%20with%20bcb54052d04c4980a8773a465b808bc7/Untitled%204.png)

### 7 Conclusion

새로운 잠재 확산 모델을 활용하여 다양한 이미지 관련 작업을 위해 설계된 최첨단 시스템인 '칸딘스키'를 소개했습니다. 놀랍게도 칸딘스키는 유사한 오픈소스 플랫폼 중 상위권에 랭크되어 있습니다. 사용자들은 웹 애플리케이션과 텔레그램 봇을 통해 칸딘스키의 기능에 쉽게 접근할 수 있으며, 보다 심층적인 분석을 원하시는 분들을 위해 허깅 페이스에서 사전 훈련된 모델을 사용할 수 있도록 하였습니다. 또한, 유연한 라이선스와 함께 소스 코드를 공개적으로 공유하여, 다양한 상업적 활용을 위한 문을 열었습니다. 앞으로도 최신 이미지 인코더의 기능을 탐구하고, 텍스트-이미지 연산을 위한 우수한 UNet 구조를 개발하며, 텍스트 프롬프트 해석을 더욱 심층적으로 연구할 계획입니다. 또한 고해상도 이미지를 생성하고 텍스트 프롬프트 이미지 편집과 같은 혁신적인 기능을 추가하는 데 주력하고 있습니다. 하지만 무엇보다도 시스템의 콘텐츠가 적절하게 유지되도록 하는 것이 가장 중요합니다. 따라서 잠재적으로 유해한 이미지가 생성되는 것을 방지하기 위해 실시간 콘텐츠 중재 메커니즘을 고안하는 데 주력하고 있습니다.

### 8 Limitations

칸딘스키의 유능함에도 불구하고 한계가 없는 것은 아닙니다. 사실적인 이미지를 생성하지만, 생성된 이미지를 제공된 텍스트와 더 잘 일치시킬 수 있는 여지가 있습니다. 또한 추가 연구를 통해 이미지 품질 지표를 개선하여 사람의 인식에 더 강하게 공감할 수 있도록 할 수 있다고 생각합니다.

### 9 Ethical Considerations

책임감 있고 윤리적인 접근 방식을 유지하는 것은 우리에게 매우 중요합니다. 우리는 칸딘스키의 결과물이 해롭거나 불쾌감을 주지 않도록 조치를 취했습니다. 여기에는 학습 데이터를 정리하고 가학적인 텍스트 단서를 사전에 발견하는 것이 포함됩니다. 하지만 완벽한 시스템은 없습니다. 특수하게 제작된 특정 프롬프트는 드물게 부적절한 콘텐츠로 이어질 수 있습니다. 이를 방지하려면 출력을 선별하고 정제하여 의도한 애플리케이션의 표준에 부합하는지 확인하는 추가 보호 계층을 구현하는 것이 좋습니다.