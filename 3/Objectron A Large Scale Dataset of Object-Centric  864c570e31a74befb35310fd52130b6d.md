# Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations

[https://arxiv.org/abs/2012.09988](https://arxiv.org/abs/2012.09988)

[https://github.com/google-research-datasets/Objectron](https://github.com/google-research-datasets/Objectron)

[https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html](https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html)

*이 논문에서는 3D 오브젝트 포즈 주석이 포함된 대규모의 짧은 동영상 모음인 Objectron 데이터셋을 소개합니다. 14,819개의 동영상으로 구성된 이 데이터 세트는 온디바이스 증강 현실(AR) 라이브러리를 기반으로 한 효율적인 데이터 수집 및 주석 프레임워크를 사용하여 생성되었습니다. 또한 이 논문에서는 향후 연구를 위한 기준으로 2단계 3D 객체 감지 모델을 제시합니다. 이 모델은 Objectron 데이터 세트를 기반으로 학습되며 3D 물체 감지를 위한 최첨단 기술을 활용합니다. 저자들은 이 데이터세트를 공개함으로써 3D 물체 형상 이해, 비디오 모델, 물체 검색, 뷰 합성, 3D 재구성과 같은 분야의 추가 연구가 촉진되기를 희망합니다.*

### 1. Introduction

이 글에서는 증강 현실, 로봇 공학, 이미지 검색과 같은 분야에 적용되는 머신 러닝에서 3D 객체 이해를 개선하기 위해 더 나은 데이터 세트의 필요성에 대해 설명합니다. 저자는 대규모의 실제 데이터 세트가 부족한 상황에서 인간이 사물을 인식하는 학습 방식을 모방하여 다양한 관점에서 사물의 3D 구조를 캡처하는 객체 중심 비디오 데이터 세트를 사용할 것을 제안합니다.

![데이터셋의 예시 비디오들. 각 샘플은 객체의 3D 경계 상자로 주석이 달린 비디오입니다.](Objectron%20A%20Large%20Scale%20Dataset%20of%20Object-Centric%20%20864c570e31a74befb35310fd52130b6d/Untitled.png)

데이터셋의 예시 비디오들. 각 샘플은 객체의 3D 경계 상자로 주석이 달린 비디오입니다.

저자들은 다양한 각도에서 캡처한 일반적인 물체가 등장하는 짧은 비디오 클립 모음인 Objectron 데이터세트를 소개합니다. 이 비디오에는 카메라 위치, 희소 포인트 클라우드, 표면 평면을 포함한 증강 현실 메타데이터가 추가되어 있습니다. 또한 데이터 세트의 각 오브젝트에는 위치, 방향, 크기를 설명하는 3D 바운딩 박스가 수동으로 주석으로 추가됩니다. 이 데이터 세트는 5개 대륙 10개 국가에 걸친 다양한 지리적 샘플에서 추출한 14,819개의 주석이 달린 비디오 클립과 400만 개의 이미지로 구성되어 있습니다.

Objectron 데이터 세트의 장점으로는 각 객체에 대한 다중 보기, 각 비디오에서 3D 바운딩 박스의 시간적 일관성(3D 추적 가능), 실제 수집 환경(더 나은 일반화를 위한), 정확한 메트릭 스케일 저장, 카메라 파라미터가 제공되는 보정된 이미지, 조밀하고 연속적인 주석, 각 객체 카테고리의 다양한 인스턴스 등이 있습니다.

![우리의 데이터셋은 객체 중심의 비디오로, 같은 객체의 다른 각도에서 다른 뷰를 캡처합니다.](Objectron%20A%20Large%20Scale%20Dataset%20of%20Object-Centric%20%20864c570e31a74befb35310fd52130b6d/Untitled%201.png)

우리의 데이터셋은 객체 중심의 비디오로, 같은 객체의 다른 각도에서 다른 뷰를 캡처합니다.

마지막으로 저자는 3D 객체 감지를 위해 데이터 세트로 수행한 실험을 언급하며, 방향이 지정된 3D 경계 상자의 3D 교차점(IoU)을 계산하는 새로운 방법을 제안합니다. 이러한 실험은 향후 연구의 기초가 될 수 있습니다.

### 2. Previous Work

저자는 3D 물체 감지에 사용되는 여러 기존 데이터 세트를 검토하고 이를 제안한 Objectron 데이터 세트와 대조합니다:

- BOP 챌린지: 더 작은 데이터 세트를 결합한 3D 물체 감지를 위한 벤치마크. 이 이미지는 대부분 통제된 환경에서 가져온 것으로, 산업 환경에서 흔히 볼 수 있는 혼란과 폐색이 특징입니다.
- T-LESS: 텍스처나 색상이 부족한 산업 환경에서 흔히 볼 수 있는 물체를 특징으로 합니다.
- Rutgers APC: 아마존 피킹 챌린지에서 사용된 14개의 텍스처 오브젝트가 포함되어 있습니다.
- LineMOD: 오브젝트 포즈 추정에 널리 사용됩니다.
- IC-BIN: LineMOD와 유사하지만 카테고리가 추가되었습니다.
- YCB: 제어된 설정에서 물체의 비디오를 포함합니다.

이에 비해 Objectron은 제어되지 않은 실제 환경에서 일반적인 물체에 대한 더 큰 규모의 고해상도 비디오를 제공합니다.

- ObjectNet3D: 이미지에서 3D 오브젝트 포즈를 포함합니다.
- 파스칼3D+: 파스칼 VOC와 일부 ImageNet 이미지에 3D 포즈 주석을 추가합니다.

이 두 데이터 세트 모두 더 많은 카테고리와 인스턴스를 포함하지만 이미지만 포함합니다. 3D 포즈 주석은 9개 대신 최대 6-DoF(자유도)까지만 지원하며 카메라 내재값이 포함되지 않아 물체의 스케일을 복구할 수 없습니다.

- Pix3D: 픽셀 수준의 2D-3D 포즈 정렬을 포함합니다.
- 3D 오브젝트: 10개의 일상적인 물체 카테고리에 대한 이산화된 시점 주석을 제공합니다.
- ScanNet: 시맨틱 주석이 있지만 3D 포즈 정보가 없는 실내 장면의 대규모 비디오 데이터 세트입니다.
- Rio: 물체의 3D 포즈로 주석이 달린 실내 스캔이 포함되어 있습니다.

이에 비해 오브젝트론은 객체 중심적이며 훨씬 더 많은 샘플을 포함합니다.

ShapeNet 및 HyperSim과 같은 합성 데이터 세트도 존재합니다. 셰이프넷은 오브젝트의 CAD 모델이 많지만 시각적 품질이 부족하고 실제 애플리케이션에 잘 일반화되지 않습니다. HyperSim은 오브젝트 포즈 주석이 있는 사실적인 장면을 생성하지만 데이터 세트의 에셋이 공개되지 않아 데이터를 재현하기가 어렵습니다. 이러한 합성 데이터 세트의 실제 일반화 기능은 알려지지 않았습니다. 오브젝트론은 이러한 문제를 크게 개선하여 보다 실제적이고 실용적인 애플리케이션을 제공합니다.

### 3. Data Collection and Annotation

이 섹션에서는 저자가 Objectron 데이터 세트에 대한 데이터를 수집하고 주석을 단 방법에 대해 자세히 설명합니다:

3.1. 객체 범주:
저자들은 3D 물체 감지에 어려움을 겪는 다양한 크기의 강체 및 비강체 일반 물체를 다양한 범주로 분류했습니다. 예를 들어 "컵", "병", "책", "시리얼 상자" 등이 있으며, 대칭이 있거나 텍스트가 뚜렷한 것이 특징입니다. 또한 증강 현실 및 이미지 검색과 같은 애플리케이션을 구현하기 위해 '신발', '의자'와 같은 카테고리도 포함되었습니다.

3.2. 데이터 수집:
데이터는 고급 휴대전화를 사용한 비디오 녹화를 통해 수집되었으며, 이를 통해 여러 국가에서 데이터 수집 캠페인을 빠르게 시작할 수 있었습니다. 데이터 세트는 5개 대륙 10개 국가에 걸쳐 지리적으로 다양합니다. 이들은 카메라가 물체 주위를 이동하면서 1440x1920 해상도의 10초짜리 동영상을 30fps로 녹화했습니다. 영상과 함께 ARKit 또는 ARCore와 같은 툴을 사용하여 AR 세션을 통해 카메라 포즈, 포인트 클라우드, 표면 평면을 캡처했습니다. 이러한 비디오 녹화와 AR 세션 메타데이터를 데이터세트에 공개하고 모든 프레임에 대한 캘리브레이션, 외생 및 내생 매트릭스를 제공했습니다.

3.3. 데이터 주석:
각 이미지에 3D 바운딩 박스를 주석 처리하는 대신(시간이 많이 걸리는 프로세스), 비디오 클립의 3D 오브젝트에 주석을 달고 클립의 모든 프레임에 이러한 주석을 적용하여 이미지당 주석 처리 비용을 절감했습니다. 이 접근 방식은 비디오 시퀀스의 모든 프레임에 주석이 달린 이미지를 생성하고 바운딩 박스에 대한 정확한 메트릭 크기를 제공했습니다.

![데이터 주석. 주석이 달린 3D 상자는 여러 뷰에서 확인되고, 그런 다음 시퀀스의 모든 이미지에 적용됩니다.](Objectron%20A%20Large%20Scale%20Dataset%20of%20Object-Centric%20%20864c570e31a74befb35310fd52130b6d/Untitled%202.png)

데이터 주석. 주석이 달린 3D 상자는 여러 뷰에서 확인되고, 그런 다음 시퀀스의 모든 이미지에 적용됩니다.

3.4. 주석 분산:
주석의 정확도는 비디오 전체에서 추정된 카메라 포즈의 안정성과 주석 작성자의 정밀도에 따라 달라졌습니다. 연구진은 동영상의 길이가 2% 미만으로 드리프트되는 것을 관찰했으며, 10초 미만의 짧은 시퀀스를 캡처하여 이 문제를 더욱 최소화했습니다. 어노테이터의 정확도를 평가하기 위해 8명의 어노테이터가 동일한 시퀀스에 다시 주석을 달았습니다. 의자 방향, 이동, 축척에 대한 표준편차는 각각 4.6°, 1cm, 4cm로 평가자 간 차이가 미미한 것으로 나타났습니다.

### 4. Objectron Dataset

섹션 4에서는 저자가 Objectron 데이터 세트의 세부 사항을 자세히 살펴봅니다:

Objectron 데이터 세트에는 자전거, 책, 병, 카메라, 시리얼 상자, 의자, 컵, 노트북, 신발 등 9가지 카테고리의 비디오가 포함되어 있습니다. 자전거나 노트북처럼 고정되어 있지 않은 물체도 있지만, 모든 물체는 비디오 녹화 중에 고정된 상태로 유지됩니다. 카메라는 각 비디오에서 물체 주위를 움직이며 다양한 각도에서 물체를 포착합니다.

![서로 다른 주석자에 의해 주석이 달린 3D 경계 상자의 오버레이는 다른 평가자들의 주석이 매우 가깝다는 것을 보여줍니다.](Objectron%20A%20Large%20Scale%20Dataset%20of%20Object-Centric%20%20864c570e31a74befb35310fd52130b6d/Untitled%203.png)

서로 다른 주석자에 의해 주석이 달린 3D 경계 상자의 오버레이는 다른 평가자들의 주석이 매우 가깝다는 것을 보여줍니다.

![객체 카테고리별 샘플의 시점 분포. 상단 행은 극 그래프에서의 방위각 분포를 보여주며, 하단 행은 고도 분포를 나타냅니다.](Objectron%20A%20Large%20Scale%20Dataset%20of%20Object-Centric%20%20864c570e31a74befb35310fd52130b6d/Untitled%204.png)

객체 카테고리별 샘플의 시점 분포. 상단 행은 극 그래프에서의 방위각 분포를 보여주며, 하단 행은 고도 분포를 나타냅니다.

이 데이터 세트에는 14,819개의 주석이 달린 비디오에서 400만 개의 주석이 달린 이미지에 나타나는 총 17,095개의 객체 인스턴스가 포함되어 있습니다. 이 데이터는 10개의 다른 국가에서 수집되어 다양한 사물, 텍스트, 언어, 지역 환경을 포함하고 있습니다.

데이터 세트의 각 샘플에는 고해상도 이미지와 함께 카메라 포즈, 포인트 클라우드 데이터, 환경의 평면 표면, 각 오브젝트에 대해 수동으로 주석이 달린 3D 바운딩 박스가 포함되어 있습니다. 이러한 3D 바운딩 박스는 카메라 포즈에 대한 오브젝트의 방향, 이동 및 크기를 자세히 설명합니다.

또한 데이터 세트에는 계산된 3D 키포인트, 투영된 2D 키포인트, 각 오브젝트의 바운딩 박스에 대한 방위각과 고도도 포함되어 있습니다. 각 비디오 프레임에는 카메라 포즈, 투영 및 뷰 매트릭스가 포함됩니다.

![폴리곤 클리핑 알고리즘을 사용한 정확한 3D IoU 계산.](Objectron%20A%20Large%20Scale%20Dataset%20of%20Object-Centric%20%20864c570e31a74befb35310fd52130b6d/Untitled%205.png)

폴리곤 클리핑 알고리즘을 사용한 정확한 3D IoU 계산.

오브젝트 주석에는 카메라 중심을 기준으로 한 오브젝트의 회전 및 이동과 오브젝트의 배율이 포함됩니다. 시점 분포를 더 잘 이해하기 위해 저자는 카메라 중심을 기준으로 각 오브젝트 인스턴스의 방위각을 계산했으며, 방위각 0도는 오브젝트의 정면 뷰를 나타냅니다. 다양한 물체 카테고리에 대한 방위각 분포는 일부 카테고리의 경우 전면 및 상단 뷰에 대한 특정 편향성을 보여주었습니다.

![대칭 객체의 3D IoU 계산: 3D IoU를 최대화하기 위해 대칭축 Y 축을 따라 경계 상자를 회전시킵니다.](Objectron%20A%20Large%20Scale%20Dataset%20of%20Object-Centric%20%20864c570e31a74befb35310fd52130b6d/Untitled%206.png)

대칭 객체의 3D IoU 계산: 3D IoU를 최대화하기 위해 대칭축 Y 축을 따라 경계 상자를 회전시킵니다.

### 5. Baseline Experiments and Evaluations

섹션 5에서는 Objectron 데이터 세트에서 수행한 기준 실험 및 평가에 대해 설명합니다.

3D 물체 감지 알고리즘의 성능을 평가하기 위해 3D IoU(교집합에 대한 교차), 2D 투영 오류, 시점 오류, 극좌표 및 방위각 오류, 회전 오류 등 여러 2D 및 3D 지표에 대한 평균 정밀도를 계산하는 평가 코드가 포함된 데이터 세트가 공개됩니다. 저자는 방향이 지정된 상자에 대한 3D IoU를 계산하는 새로운 방법을 제시합니다.

5.1: Jaccard 인덱스라고도 하는 IoU 메트릭은 물체 감지와 같은 작업을 위한 표준 평가 도구입니다. 이 지표는 예측된 상자와 기준 실측 상자, 두 상자의 교차 부피를 계산합니다. 이 값은 축척, 회전, 이동의 변화에 변하지 않으며, IoU는 일정하게 유지됩니다. 오브젝트론의 경우, 저자는 서덜랜드-호지먼 폴리곤 클리핑 알고리즘을 사용하여 축이 정렬되지 않은 박스에 대해 3D IoU를 계산하는 정확한 알고리즘을 제안합니다.

5.2: 저자들은 MobilePose라는 최신 모델을 사용하여 3D 오브젝트 감지 및 시점 추정을 위한 기준 결과를 제공합니다. 이 네트워크는 모바일 장치에서 실시간으로 작동하도록 설계되었습니다. 저자들은 MobileNetV2를 백엔드로 사용하고 네트워크에 주의 헤드와 회귀 헤드를 추가했습니다. 이 설정은 9개의 2D 투영 키포인트를 예측한 다음 EPNP 알고리즘을 사용하여 3D로 변환합니다.

또한 3D 물체 감지를 위한 새로운 2단계 아키텍처를 제안했습니다. 첫 번째 단계에서는 SSD 모델을 사용하여 물체의 2D 크롭을 추정합니다. 두 번째 단계에서는 EfficientNet-Lite를 사용하여 2D 크롭에서 3D 바운딩 박스의 키포인트를 회귀합니다. 그런 다음 이 키포인트는 EPnP 알고리즘을 사용하여 3D로 변환됩니다.

![MobilePose[15] 모델의 아키텍처.](Objectron%20A%20Large%20Scale%20Dataset%20of%20Object-Centric%20%20864c570e31a74befb35310fd52130b6d/Untitled%207.png)

MobilePose[15] 모델의 아키텍처.

![두 단계 모델의 아키텍처. 빨간 블록은 1 × 1 합성곱 계층, 녹색 블록은 깊이 합성곱 계층, 그리고 파란 블록은 스킵 연결을 위한 덧셈 계층입니다. 끝에 있는 검은 블록은 완전 연결 계층입니다.](Objectron%20A%20Large%20Scale%20Dataset%20of%20Object-Centric%20%20864c570e31a74befb35310fd52130b6d/Untitled%208.png)

두 단계 모델의 아키텍처. 빨간 블록은 1 × 1 합성곱 계층, 녹색 블록은 깊이 합성곱 계층, 그리고 파란 블록은 스킵 연결을 위한 덧셈 계층입니다. 끝에 있는 검은 블록은 완전 연결 계층입니다.

실험 결과에 따르면 데이터 수집가가 일반적으로 비디오를 캡처하기 위해 물체를 내려다보고 걸어 다니기 때문에 이 모델이 방위각보다 고도를 더 정확하게 추정하는 것으로 나타났습니다. 이 모델은 '컵'의 회전을 추정하는 데는 성능이 좋지 않습니다. 그러나 저자는 아핀 변환이나 자르기와 같은 데이터 증강 기법이 모델을 일반화하고 성능을 개선하는 데 도움이 될 수 있다고 제안합니다. 제안된 두 모델은 모두 가볍고 모바일 장치에서 실시간 성능을 발휘할 수 있습니다.

### 6. Details of the Objectron data format

데이터 세트는 공개 액세스를 위해 구글의 오브젝트론 버킷에 저장되어 있으며, 파싱 및 평가를 위한 추가 코드는 깃허브에서 확인할 수 있습니다. 데이터 세트의 각 샘플은 원시 비디오 파일(MOV 형식, 30fps, 1440x1920 해상도), AR 메타데이터 및 주석 결과를 제공합니다. AR 메타데이터에는 카메라 변환, 뷰, 프로젝션, 고유 매트릭스 등의 중요한 데이터가 포함되어 있습니다. 또한 세계 좌표계 및 표면 평면의 스파스 포인트 클라우드도 포함됩니다.

각 오브젝트 인스턴스에 대한 주석 데이터에는 월드 좌표계와 카메라 좌표계의 3D 버텍스와 함께 바운딩 박스의 방향, 이동, 스케일과 같은 풍부한 정보가 포함됩니다. 또한 이미지 평면에 깊이가 있는 2D 투영도 포함됩니다. 각 인스턴스에는 오브젝트의 카테고리에 해당하는 레이블 문자열이 있습니다.

연구진은 원시 데이터 외에도 이미지 및 비디오 모델에 대해 각각 Tensorflow tf.Example 및 tf.SequenceExample 형식으로 사전 처리된 데이터 세트도 제공했습니다. 두 형식 모두 텐서플로 레코드로 저장됩니다. 팀은 예제 파이프라인을 구현하여 파이토치, 텐서플로우, 잭스 트레이닝 파이프라인에 이 데이터를 효율적으로 공급했습니다.

### 7. Conclusion

연구진은 다양한 자연 환경에서 촬영한 14,819개의 짧은 동영상으로 구성된 대규모 객체 중심 데이터 세트인 Objectron 데이터 세트를 소개했습니다. 각 비디오에는 주석이 달린 객체 포즈 데이터가 함께 제공됩니다.

연구팀은 온디바이스 AR 라이브러리를 활용하는 효율적이고 확장 가능한 데이터 수집 및 주석 프레임워크를 개발했습니다. 이 논문에서는 Objectron 데이터 세트로 훈련된 2단계 3D 오브젝트 감지 모델의 결과도 제시합니다. 이 모델은 향후 이 데이터세트를 활용한 연구의 벤치마크 역할을 합니다.

이 데이터세트를 공개함으로써 저자들은 3D 물체 형상 이해, 비디오 모델, 물체 검색, 뷰 합성, 3D 재구성 분야의 추가 연구에 영감을 불어넣고 활성화하는 것을 목표로 하고 있습니다. 이러한 분야의 경계를 넓히고 새로운 애플리케이션 개발에 박차를 가하는 것이 목표입니다.