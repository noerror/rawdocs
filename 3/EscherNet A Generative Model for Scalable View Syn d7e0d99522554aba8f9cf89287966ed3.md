# EscherNet: A Generative Model for Scalable View Synthesis

[https://arxiv.org/abs/2402.03908](https://arxiv.org/abs/2402.03908)

[https://kxhit.github.io/EscherNet](https://kxhit.github.io/EscherNet)

- Feb 2024

![유연한 수의 레퍼런스 뷰(보라색으로 강조 표시)를 기반으로 임의의 카메라 포즈로 일관된 타깃 뷰(파란색으로 강조 표시)를 유연하게 생성할 수 있는 확산 모델인 에셔넷(EscherNet)을 소개합니다. 에셔넷은 여러 오브젝트와 풍부한 텍스처가 포함된 합성 이미지와 실제 이미지에서 카메라 제어의 놀라운 정밀도와 강력한 일반화를 보여줍니다.](EscherNet%20A%20Generative%20Model%20for%20Scalable%20View%20Syn%20d7e0d99522554aba8f9cf89287966ed3/Untitled.png)

유연한 수의 레퍼런스 뷰(보라색으로 강조 표시)를 기반으로 임의의 카메라 포즈로 일관된 타깃 뷰(파란색으로 강조 표시)를 유연하게 생성할 수 있는 확산 모델인 에셔넷(EscherNet)을 소개합니다. 에셔넷은 여러 오브젝트와 풍부한 텍스처가 포함된 합성 이미지와 실제 이미지에서 카메라 제어의 놀라운 정밀도와 강력한 일반화를 보여줍니다.

### **1. 서론**

뷰 합성(view synthesis)은 컴퓨터 비전과 컴퓨터 그래픽스에서 중요한 작업 중 하나입니다. 이 작업은 참조되는 여러 시점들을 바탕으로 장면을 다른 관점에서 재렌더링하는 과정을 포함하며, 인간 시각의 적응성을 모방하는 데 목적이 있습니다. 뷰 합성의 능력은 일상 생활에서의 객체 조작이나 탐색뿐만 아니라, 깊이, 관점, 몰입감을 가진 창작물을 상상하고 만들어내는 인간의 창의성을 발휘하는 데에도 필수적입니다.

이 논문은 뷰 합성 문제에 대해 새롭게 접근합니다. 특히, **"일반적인 3D 표현을 어떻게 학습하여 확장 가능한 뷰 합성을 용이하게 할 수 있을까?"**라는 질문을 던지며, 이를 두 가지 주요 관찰을 통해 탐구합니다:

1. **기존 연구의 한계**: 최근 뷰 합성 연구는 주로 훈련 속도나 렌더링 효율성 개선에 초점을 맞추어 왔습니다. 이러한 연구들은 대부분 볼륨 렌더링에 의존하며, 결과적으로 각각의 방법들은 특정 장면에 국한되어 사용되는 경향이 있습니다. 이는 장면의 3D 공간 좌표에 강하게 결합되어 있기 때문입니다. 본 논문에서는 이러한 접근 방식에서 벗어나, 장면의 색상과 기하학만을 사용하여 학습하는 새로운 패러다임을 제안합니다. 이는 구체적인 좌표 시스템이나 실제 3D 기하학 없이도 암시적 표현을 학습할 수 있게 하며, 장면 특정적인 제약을 극복할 수 있는 확장성을 제공합니다.
2. **새로운 접근 방식**: 뷰 합성은 조건부 생성 모델링 문제로 취급될 수 있습니다. 이는 참조 뷰가 제한적일 때 여러 가능한 예측을 제공하며, 사용 가능한 정보가 많아질수록 생성된 장면이 실제와 더 가까워지게 됩니다. 현재의 3D 생성 모델은 대부분 단일 참조 뷰를 지원하지만, 이 논문에서는 입력 정보의 양에 따라 유연하게 조정할 수 있는 생성 공식이 더 바람직하다고 주장합니다.

이러한 관점에서, 이 논문은 **EscherNet**이라는 새로운 접근 방식을 소개합니다. EscherNet은 이미지-대-이미지 조건부 확산 모델로, 참조 뷰와 타겟 뷰 사이의 복잡한 관계를 포착하기 위해 트랜스포머 아키텍처를 활용합니다. 특히, EscherNet의 핵심 혁신은 카메라 위치 인코딩(CaPE) 설계에 있으며, 이는 4도 및 6도의 카메라 포즈를 나타내는 데 사용됩니다. 이러한 방식을 통해 EscherNet은 참조-대상 및 대상-대상 뷰 일관성을 자연스럽게 통합하고, 특정 좌표 시스템이나 실제 3D 기하학에 의존하지 않으면서도 확장 가능한 뷰 합성을 달성합니다.

### **2. 관련 연구**

뷰 합성과 관련된 연구는 크게 세 분야로 나눌 수 있습니다: 신경 3D 표현 학습, 새로운 뷰 합성 방법, 그리고 3D 확산 모델에 대한 연구입니다. 각각의 분야는 3D 시각화와 생성을 위한 다양한 접근 방식을 탐구하고 있으며, 이는 EscherNet 개발의 기반이 되었습니다.

1. 신경 3D 표현(Neural 3D Representations)

- **목표**: 3D 객체나 장면을 디지털 형태로 표현하고, 이를 학습하여 컴퓨터가 이해할 수 있도록 하는 것입니다.
- **접근 방식**:
    - 초기 연구는 3D 데이터(예: 복셀, 포인트 클라우드)에 직접 최적화하여 명시적인 3D 표현 학습에 초점을 맞췄습니다.
    - 다른 연구들은 신경망을 훈련시켜 3D 공간 좌표를 서명 거리 함수나 점유율에 매핑함으로써 암시적인 3D 표현을 학습하는 데 중점을 두었습니다.
- **한계**: 이러한 방식은 대부분 실제 3D 기하학에 크게 의존하며, 그로 인해 주로 소규모 합성 3D 데이터에 국한됩니다.

2. 새로운 뷰 합성(Novel View Synthesis)

- **발전**: NeRF(Neural Radiance Fields)의 성공 이후, 훈련 및 렌더링 효율을 향상시키기 위한 다양한 방법들이 개발되었습니다. 이들은 공간 분해, 코드북, 해시 테이블 또는 가우시안을 사용하여 NeRF의 일반화 능력을 향상시키려고 시도했습니다.
- **도전**: PixelNeRF와 같은 방법들은 다양한 장면에서의 일반화 능력을 향상시키기 위해 여러 장면을 함께 최적화하려고 했지만, 볼륨 렌더링에 필요한 높은 계산 요구 사항에 제약을 받았습니다.
    
    ![D 표현 개요. EscherNet은 카메라 포즈 P T 1:M을 기반으로 M개의 타깃 뷰 XT 1:M 세트를 생성하고, N개의 레퍼런스 뷰 XR 1:N과 카메라 포즈 P R 1:N에서 얻은 정보를 활용합니다. 에셔넷은 특정 좌표계와 관계없이 카메라 포즈 P R과 P T 사이의 상대적인 카메라 변환만 고려함으로써 암시적 3D 표현을 학습하는 새로운 방법을 제시하여 다중 뷰 포즈 이미지로 쉽게 확장할 수 있습니다.](EscherNet%20A%20Generative%20Model%20for%20Scalable%20View%20Syn%20d7e0d99522554aba8f9cf89287966ed3/Untitled%201.png)
    
    D 표현 개요. EscherNet은 카메라 포즈 P T 1:M을 기반으로 M개의 타깃 뷰 XT 1:M 세트를 생성하고, N개의 레퍼런스 뷰 XR 1:N과 카메라 포즈 P R 1:N에서 얻은 정보를 활용합니다. 에셔넷은 특정 좌표계와 관계없이 카메라 포즈 P R과 P T 사이의 상대적인 카메라 변환만 고려함으로써 암시적 3D 표현을 학습하는 새로운 방법을 제시하여 다중 뷰 포즈 이미지로 쉽게 확장할 수 있습니다.
    

3. 3D 확산 모델(3D Diffusion Models)

- **진보**: 2D 생성 모델, 특히 확산 모델의 발전은 현실적인 객체와 장면을 생성할 수 있는 능력을 보여주었습니다. 이는 3D 생성 모델 설계에 영감을 주었습니다.
- **접근 방식**: 초기 3D 확산 모델은 텍스트에서 3D로의 생성을 위해 2D 확산 모델에서 가이드하는 방식을 최적화함으로써, 점수 증류 샘플링(SDS)을 사용했습니다. 하지만 이는 계산적으로 매우 집약적입니다.
- **한계**: 이러한 접근 방식은 대부분 한계가 있으며, 특히 3D 이해가 제한적이어서 비현실적인 3D 생성 결과를 초래할 수 있습니다.

이러한 배경 하에, EscherNet은 이전 연구들의 한계를 극복하고자 하는 목적을 가지고 있습니다. 볼륨 렌더링이나 실제 3D 기하학에 의존하지 않으면서도, 다양한 뷰에서 일관된 3D 장면을 생성할 수 있는 새로운 방법론을 제시합니다. 이는 뷰 합성 및 3D 생성에 대한 새로운 접근 방식을 탐색하는 데 기여할 것입니다.

### **3. EscherNet**

EscherNet은 뷰 합성 문제를 새로운 방식으로 해결하기 위해 제안된 모델입니다. 이 모델은 조건부 생성 모델링 문제로서의 뷰 합성을 재구성하며, 다양한 참조 시점에서 임의의 수의 대상 시점으로의 이미지 생성을 가능하게 합니다. EscherNet의 핵심은 변화하는 참조 뷰 수를 기반으로 일관된 3D 노벨 뷰 생성을 달성하는 것입니다.

![EscherNet 아키텍처 세부 사항. EscherNet은 최소한의 중요한 수정만으로 안정적인 확산 아키텍처 설계를 채택합니다. 경량 비전 인코더는 N개의 기준 뷰에서 하이 레벨 및 로우 레벨 신호를 모두 캡처합니다. U-Net에서는 M개의 목표 뷰 내에서 자체 주의를 적용하여 목표 간 일관성을 유지하고, M개의 목표 뷰와 N개의 기준 뷰(이미지 인코더로 인코딩)에서 교차 주의를 적용하여 기준과 목표 간 일관성을 유지합니다. 각 주의 블록에서는 키와 쿼리에 CaPE를 사용하여 특정 좌표계와 무관하게 상대적인 카메라 포즈를 통해 주의 맵을 학습할 수 있습니다.](EscherNet%20A%20Generative%20Model%20for%20Scalable%20View%20Syn%20d7e0d99522554aba8f9cf89287966ed3/Untitled%202.png)

EscherNet 아키텍처 세부 사항. EscherNet은 최소한의 중요한 수정만으로 안정적인 확산 아키텍처 설계를 채택합니다. 경량 비전 인코더는 N개의 기준 뷰에서 하이 레벨 및 로우 레벨 신호를 모두 캡처합니다. U-Net에서는 M개의 목표 뷰 내에서 자체 주의를 적용하여 목표 간 일관성을 유지하고, M개의 목표 뷰와 N개의 기준 뷰(이미지 인코더로 인코딩)에서 교차 주의를 적용하여 기준과 목표 간 일관성을 유지합니다. 각 주의 블록에서는 키와 쿼리에 CaPE를 사용하여 특정 좌표계와 무관하게 상대적인 카메라 포즈를 통해 주의 맵을 학습할 수 있습니다.

**문제 정의 및 표기법**

- **문제 설정**: EscherNet에서는 뷰 합성을 X_T(대상 뷰)가 주어진 X_R(참조 뷰)와 P_R(참조 뷰의 카메라 포즈), P_T(대상 뷰의 카메라 포즈)에 조건을 달아 생성되는 조건부 확률 모델로 설정합니다.
- **목표**: 모델은 참조 뷰와 그에 해당하는 카메라 포즈로부터 대상 뷰를 생성할 수 있어야 하며, 이 과정에서 참조 뷰와 대상 뷰 사이, 그리고 대상 뷰들 사이의 일관성을 유지해야 합니다.

**아키텍처 설계**

- **기반 모델**: EscherNet은 기존 2D 확산 모델을 기반으로 하여, 대규모 훈련을 통해 강력한 웹 스케일 사전 지식을 활용합니다.
- **카메라 포즈 인코딩**: 각 뷰/이미지에 대해 카메라 포즈를 인코딩하는 방식을 채택하여, 어떤 수의 뷰로부터 어떤 수의 대상 뷰로의 합성이 가능하도록 합니다.

**다중 뷰 생성**

- EscherNet은 어떠한 2D 확산 모델과도 결합할 수 있으며, 특히 StableDiffusion v1.5를 사용하여 설계되었습니다. 이는 텍스트-이미지 생성에서 다중 뷰 생성으로의 적용을 위한 여러 키 수정을 포함합니다.

**참조 뷰 조건화**

- 참조 뷰의 고수준 의미와 저수준 텍스처 세부 정보를 모두 포착하기 위해, 참조 이미지를 토큰 집합으로 표현하는 방식을 선택했습니다. 이는 모델이 참조 뷰의 수에 관계없이 유연하게 다룰 수 있도록 합니다.

**카메라 위치 인코딩 (CaPE)**

- CaPE는 EscherNet의 중요한 혁신 중 하나로, 카메라 포즈를 효과적으로 인코딩하여 트랜스포머 아키텍처 내에서 참조 및 대상 뷰 토큰 사이의 상대적 카메라 변환을 직접 인코딩할 수 있게 합니다. 이는 4도 및 6도 카메라 포즈를 모두 지원합니다.

EscherNet은 이러한 설계를 통해 참조 뷰의 수에 관계없이 어떤 수의 대상 뷰도 생성할 수 있는 유연성을 제공합니다. 이는 특히 참조 뷰의 수가 증가함에 따라 생성 품질이 향상되는 것으로 나타났으며, 이는 모델이 원래 설계 목표와 잘 부합함을 보여줍니다.

### **4. 실험**

EscherNet의 성능을 평가하기 위해, 다양한 데이터셋과 기준 모델을 사용한 실험이 수행되었습니다. 이 실험들은 EscherNet이 기존의 3D 확산 모델들과 신경 렌더링 방법들을 얼마나 효과적으로 뛰어넘는지를 측정하고, 뷰 합성 및 3D 생성에서의 그 성능을 입증합니다.

**훈련 데이터셋**

- **Objaverse-1.0**: EscherNet은 800K 객체를 포함하는 Objaverse-1.0 데이터셋을 사용하여 훈련되었습니다. 이 데이터셋은 객체 중심 뷰 합성에 적합하며, 모델을 다른 3D 확산 모델 기준과 공정하게 비교할 수 있는 기반을 제공합니다.
    
    ![NeRF 합성 드럼 장면에서 생성된 뷰 시각화. 신경 렌더링 방식은 의미 있는 콘텐츠를 생성하지 못하는 반면, 에셔넷은 매우 제한된 레퍼런스 뷰가 제공되더라도 그럴듯한 뷰 합성을 생성합니다. 그러나 참조 뷰가 10개 이상일 경우, 장면별 방법은 렌더링 품질이 크게 향상되는 것으로 나타났습니다. 드럼 장면의 모든 테스트 뷰에서 평균 PSNR을 평균한 값입니다. 다른 장면 및/또는 더 많은 레퍼런스 뷰에 대한 결과는 부록 D에 나와 있습니다.](EscherNet%20A%20Generative%20Model%20for%20Scalable%20View%20Syn%20d7e0d99522554aba8f9cf89287966ed3/Untitled%203.png)
    
    NeRF 합성 드럼 장면에서 생성된 뷰 시각화. 신경 렌더링 방식은 의미 있는 콘텐츠를 생성하지 못하는 반면, 에셔넷은 매우 제한된 레퍼런스 뷰가 제공되더라도 그럴듯한 뷰 합성을 생성합니다. 그러나 참조 뷰가 10개 이상일 경우, 장면별 방법은 렌더링 품질이 크게 향상되는 것으로 나타났습니다. 드럼 장면의 모든 테스트 뷰에서 평균 PSNR을 평균한 값입니다. 다른 장면 및/또는 더 많은 레퍼런스 뷰에 대한 결과는 부록 D에 나와 있습니다.
    

**새로운 뷰 합성 결과**

- **비교 대상**: EscherNet은 Google Scanned Objects(GSO) 데이터셋과 RTMV 데이터셋에서 기존의 3D 확산 모델 및 신경 렌더링 방법과 비교되었습니다.
- **성능 평가**: EscherNet은 기존 모델들과 비교하여 뛰어난 성능을 보였으며, 특히 제한된 참조 뷰에서도 설득력 있는 뷰 합성 결과를 생성할 수 있음을 입증하였습니다.
    
    ![GSO 및 RTMV 데이터 세트에 대한 새로운 뷰 합성 시각화. 에셔넷은 제로-1-3-XL보다 우수한 생성 품질과 세밀한 카메라 제어를 제공합니다. 특히, 추가 뷰로 컨디셔닝할 경우 EscherNet은 생성된 뷰와 실사 텍스처의 유사성이 향상되어 배낭 끈이나 거북이 등껍질과 같이 더욱 정교한 텍스처 디테일을 보여줍니다.](EscherNet%20A%20Generative%20Model%20for%20Scalable%20View%20Syn%20d7e0d99522554aba8f9cf89287966ed3/Untitled%204.png)
    
    GSO 및 RTMV 데이터 세트에 대한 새로운 뷰 합성 시각화. 에셔넷은 제로-1-3-XL보다 우수한 생성 품질과 세밀한 카메라 제어를 제공합니다. 특히, 추가 뷰로 컨디셔닝할 경우 EscherNet은 생성된 뷰와 실사 텍스처의 유사성이 향상되어 배낭 끈이나 거북이 등껍질과 같이 더욱 정교한 텍스처 디테일을 보여줍니다.
    

**3D 생성 결과**

- **데이터셋**: GSO 데이터셋에서 단일 및 소수 이미지 3D 생성 성능이 평가되었습니다.
- **성능 비교**: EscherNet은 기존의 최고 수준의 3D 생성 모델들과 비교하여 현저히 우수한 3D 재구성 품질을 달성했습니다. 이는 EscherNet이 참조 뷰의 수에 관계없이 유연하게 다양한 3D 일관된 뷰를 생성할 수 있음을 보여줍니다.
    
    ![GSO의 단일 뷰 3D 재구성 시각화. 밀도가 높고 일관된 새로운 뷰를 생성하는 EscherNet의 기능은 완전하고 잘 제한된 3D 지오메트리의 재구성을 크게 향상시킵니다. 반면, One-2-3-45-XL과 DreamGaussian-XL은 훨씬 더 큰 사전 훈련된 모델을 활용함에도 불구하고 지나치게 매끄럽고 노이즈가 많은 재구성을 생성하는 경향이 있으며, 희박한 고정 뷰 합성에 의해 제약되는 SyncDreamer는 특히 소파와 종의 하단 부분에서 지오메트리를 엄격하게 제한하는 데 어려움을 겪습니다.](EscherNet%20A%20Generative%20Model%20for%20Scalable%20View%20Syn%20d7e0d99522554aba8f9cf89287966ed3/Untitled%205.png)
    
    GSO의 단일 뷰 3D 재구성 시각화. 밀도가 높고 일관된 새로운 뷰를 생성하는 EscherNet의 기능은 완전하고 잘 제한된 3D 지오메트리의 재구성을 크게 향상시킵니다. 반면, One-2-3-45-XL과 DreamGaussian-XL은 훨씬 더 큰 사전 훈련된 모델을 활용함에도 불구하고 지나치게 매끄럽고 노이즈가 많은 재구성을 생성하는 경향이 있으며, 희박한 고정 뷰 합성에 의해 제약되는 SyncDreamer는 특히 소파와 종의 하단 부분에서 지오메트리를 엄격하게 제한하는 데 어려움을 겪습니다.
    

**텍스트-3D 생성 결과**

- EscherNet은 텍스트에서 3D 생성 문제에도 적용될 수 있는 유연성을 가지고 있습니다. 이는 텍스트에서 이미지로의 생성을 수행한 후, 생성된 이미지를 사용하여 3D 뷰를 생성하는 두 단계 접근 방식을 통해 가능합니다.
- **실험 결과**: EscherNet은 이러한 접근 방식을 통해, 심지어 분포 외의 내용에서도 일관된 3D 새로운 뷰를 생성할 수 있음을 보여줍니다.
    
    ![MVDream(위)과 SDXL(아래)을 사용한 텍스트-3D 시각화. EscherNet은 사용자가 제공한 텍스트 프롬프트로 생성된 합성 이미지에 대해 설득력 있고 사실적인 뷰 합성을 제공합니다. 추가 결과는 부록 E에 나와 있습니다.](EscherNet%20A%20Generative%20Model%20for%20Scalable%20View%20Syn%20d7e0d99522554aba8f9cf89287966ed3/Untitled%206.png)
    
    MVDream(위)과 SDXL(아래)을 사용한 텍스트-3D 시각화. EscherNet은 사용자가 제공한 텍스트 프롬프트로 생성된 합성 이미지에 대해 설득력 있고 사실적인 뷰 합성을 제공합니다. 추가 결과는 부록 E에 나와 있습니다.
    

EscherNet의 실험 결과는 이 모델이 기존의 3D 확산 모델과 신경 렌더링 방법들을 상당한 차이로 능가하며, 제한된 참조 뷰에서도 신뢰할 수 있는 뷰 합성을 제공할 수 있음을 입증합니다. 또한, EscherNet은 참조 뷰의 수에 유연하게 대응할 수 있는 능력을 가지고 있으며, 이는 3D 생성과 뷰 합성 분야에서의 응용 가능성을 크게 확장시킵니다. EscherNet의 성공은 카메라 위치 인코딩과 같은 혁신적인 설계 요소들이 기여한 바 크며, 이는 향후 연구와 개발에 중요한 영감을 제공합니다.

### **5. 결론**

EscherNet은 변화하는 참조 뷰 수에서 암시적 3D 표현을 학습하여 일관된 3D 새로운 뷰 합성을 달성하는 다중 뷰 조건부 확산 모델입니다. EscherNet은 Stable Diffusion의 2D 아키텍처를 활용하고, 카메라 위치 임베딩(CaPE)을 통해 혁신을 도입하여 뷰 합성과 3D 시각을 전진시키는 유망한 방법을 제공합니다.