# Generative Image Dynamics

[https://arxiv.org/abs/2309.07906](https://arxiv.org/abs/2309.07906)

[https://generative-dynamics.github.io/](https://generative-dynamics.github.io/)

- Sep 2023

![Untitled](Generative%20Image%20Dynamics%20f0948d67712842aaa7d50974da90adca/Untitled.png)

### 1. Introduction

주위를 둘러보면 바람에 흔들리는 나무나 깜박이는 촛불처럼 자연의 거의 모든 사물은 어떤 식으로든 움직이고 있습니다. 아무것도 움직이지 않거나 움직임이 이상하게 보이는 사진을 보면 이상하거나 비현실적으로 느껴질 수 있습니다. 인간은 사물을 보는 것만으로도 사물이 어떻게 움직이는지 쉽게 상상할 수 있지만, 컴퓨터는 이를 똑같이 수행하기 어렵습니다.

![우리의 접근법은 장면 동력에 대한 생성 이미지 공간 전제를 모델링합니다: 단일 RGB 이미지에서, 우리의 모델은 푸리에 영역에서 조밀하게 장기 동작 궤적을 모델링하는 모션 표현인 신경 스토카스틱 모션 텍스처를 생성합니다. 우리의 모션 전제는 단일 그림을 무려 나는 비디오로 바꾸거나, 대화형 사용자 자극(예: 객체에서 점을 드래그하고 놓기)에 반응하여 객체 동력을 시뮬레이션하는 것과 같은 응용 프로그램을 가능하게 한다는 것을 보여줍니다. 오른쪽에서는 입력 그림에 표시된 스캔라인을 따라 10초 동안의 비디오를 통한 시간-시간 X-t 슬라이스를 사용하여 출력 비디오를 시각화합니다.](Generative%20Image%20Dynamics%20f0948d67712842aaa7d50974da90adca/Untitled%201.png)

우리의 접근법은 장면 동력에 대한 생성 이미지 공간 전제를 모델링합니다: 단일 RGB 이미지에서, 우리의 모델은 푸리에 영역에서 조밀하게 장기 동작 궤적을 모델링하는 모션 표현인 신경 스토카스틱 모션 텍스처를 생성합니다. 우리의 모션 전제는 단일 그림을 무려 나는 비디오로 바꾸거나, 대화형 사용자 자극(예: 객체에서 점을 드래그하고 놓기)에 반응하여 객체 동력을 시뮬레이션하는 것과 같은 응용 프로그램을 가능하게 한다는 것을 보여줍니다. 오른쪽에서는 입력 그림에 표시된 스캔라인을 따라 10초 동안의 비디오를 통한 시간-시간 X-t 슬라이스를 사용하여 출력 비디오를 시각화합니다.

모든 물체는 얼마나 무겁거나 얼마나 신축성이 있는지와 같은 특정 자연 법칙에 따라 움직입니다. 이러한 속성은 물체를 밀거나 당길 때 물체가 어떻게 움직일지를 결정합니다. 우리 주변의 모든 물체에 대해 이러한 속성을 측정하는 것은 정말 어렵습니다. 하지만 좋은 소식은 항상 그럴 필요는 없다는 것입니다. 대신 현실 세계에서 사물이 어떻게 움직이는지 관찰하고 배울 수 있습니다. 예를 들어, 우리는 나무가 어떻게 흔들리는지, 나뭇잎이 바람에 바스락거리는지 알고 있습니다. 그리고 이러한 움직임을 예측할 수 있기 때문에 같은 동작을 하는 컴퓨터 프로그램을 만들 수 있습니다.

최근의 기술 발전으로 텍스트를 읽는 것만으로도 사실적인 이미지와 동영상을 만들 수 있는 컴퓨터 프로그램을 만들 수 있게 되었습니다. 우리 팀은 이러한 발전을 활용하여 한 장의 사진만 보고도 장면이 어떻게 움직일지 예측할 수 있는 컴퓨터 프로그램을 만들고자 했습니다.

수많은 동영상의 모션 데이터를 사용하여 프로그램을 훈련시켰습니다. 그런 다음 사진을 출발점으로 삼아 프로그램의 각 작은 부분이 다음에 어떻게 움직일지 예측합니다. 이번 연구에서는 바람에 흔들리는 나무나 꽃과 같은 자연 장면에 초점을 맞췄습니다.

우리의 방법은 단순히 새로운 이미지를 생성하는 것이 아니라 움직임을 예측하는 데 초점을 맞추기 때문에 다른 접근 방식과 다릅니다. 이 접근 방식을 사용하면 애니메이션을 더 잘 제어할 수 있고 애니메이션이 더 자연스럽게 보입니다. 이를 통해 부드럽게 반복되는 동영상을 만들거나, 동영상 속 움직임을 편집하거나, 사용자가 사진과 상호작용하고 반응을 볼 수 있습니다. 정지된 사진을 모든 것이 사실적으로 움직이는 미니 영화로 바꾸는 것과 같다고 생각하면 됩니다.

### 2. Related Work

텍스트로 이미지 및 동영상 만들기:

최근의 기술은 텍스트 지침을 기반으로 사실적인 이미지를 만들 수 있습니다. 일부는 여기서 더 나아가 이러한 이미지를 기반으로 비디오 클립을 만들기도 합니다. 하지만 이러한 동영상은 사물이 이상하게 움직이거나 텍스처가 이상하게 변하거나 사물의 크기가 마술처럼 변하는 것처럼 보이는 등 약간 이상하게 보일 수 있습니다.

사진에 생동감 불어넣기:

동영상을 처음부터 새로 만드는 대신, 하나의 사진을 움직이게 하여 생동감을 불어넣는 기법도 있습니다. 3D-Unet이라는 일종의 컴퓨터 모델을 사용하는 것이 인기 있는 방법입니다. 그러나 텍스트로 만든 동영상과 마찬가지로 이러한 애니메이션도 때때로 문제가 발생할 수 있습니다. 더 나은 방법으로는 '안내' 비디오, 특정 이동 규칙 또는 사용자의 입력을 사용하여 이미지가 어떻게 움직여야 하는지 결정하는 방법이 있습니다. 이러한 기법을 사용하면 애니메이션을 보다 사실적으로 만들 수 있지만 추가 단계가 필요하거나 움직임을 표시하는 방법이 제한될 수 있습니다.

모션 이해하기:

사물이 움직이는 방식을 연구하고 표현하는 방법에는 여러 가지가 있습니다. 우리와 유사한 일부 방법은 푸리에와 같은 특수 수학적 기법을 사용하여 동영상에서 움직임을 표시하고 편집합니다. 다른 방법들은 사물이 다음에 어떻게 움직일지 예측합니다. 저희는 이미지 속 물체와 장면을 기반으로 움직임을 이해하여 자연스럽게 어떻게 움직일지 예측하려고 합니다. 인간과 동물을 대상으로 한 연구도 있습니다.

움직이는 패턴으로서의 동영상:

해변의 파도나 흔들리는 나무와 같은 일부 장면을 움직이는 패턴 또는 '동적 텍스처'로 생각해 보세요. 이는 자연스럽고 부드럽게 보이는 움직이는 배경화면과 같습니다. 이러한 움직이는 패턴을 만드는 방법에는 여러 가지가 있으며, 특히 반복해서 매끄럽게 재생되는 루핑 비디오가 대표적입니다. 이러한 방법과 달리, 저희의 접근 방식은 이러한 패턴에 대해 미리 학습한 다음 단일 사진에 적용합니다.

### 3. Overview

나무의 정지 사진이 있다고 상상해 보세요. 이 시스템의 목표는 이 사진을 나무가 바람에 흔들리는 것처럼 보이는 동영상으로 변환하는 것입니다. 꽃이 춤추거나 촛불이 깜빡이는 등 모든 장면에서 이 작업을 수행하고자 합니다.

모션 예측: 먼저 특수 도구(잠재 확산 모델)를 사용하여 사진 속 장면이 자연스럽게 어떻게 움직일지 추측합니다. 이 예측을 "신경 확률론적 모션 텍스처"라고 합니다. 사진 속 사물이 시간이 지남에 따라 어떻게 흔들리거나 펄럭일지 알려주는 청사진이라고 생각하면 됩니다.

예측을 움직임으로 전환하기: 그런 다음 이 모션 텍스처를 "모션 변위 필드"라고 하는 일련의 모션 지침으로 변환합니다. 이 지침은 그림의 각 부분을 움직여 생동감 있게 보이도록 하는 방법을 안내합니다.

그림에 애니메이션 적용: 이러한 모션 지침이 준비되면 시스템은 다른 도구를 사용하여 원본 사진의 픽셀을 프레임 단위로 이동하고 조정하여 실제와 같은 동영상을 만듭니다.

저희 시스템은 단 하나의 사진에서 움직임을 이해하기 때문에 여러 가지 멋진 작업을 할 수 있습니다. 다양한 속도로 사진에 애니메이션을 적용하거나 비디오 루프를 부드럽게 만들 수 있습니다. 심지어 사용자가 사진과 상호작용하여 사진 속 사물이 사용자의 동작에 반응하여 움직이도록 만들 수도 있습니다!

### 4. Neural stochastic motion textures

4.1. 모션 텍스처란 무엇인가요?

흔들리는 깃발 사진이 있다고 상상해 보세요. 모션 텍스처는 깃발이 흔들리는 것처럼 보이도록 하기 위해 깃발의 각 부분이 시간에 따라 어떻게 움직여야 하는지를 보여주는 일종의 맵입니다. 이는 주어진 시간에 그림의 각 픽셀이 어디에 있어야 하는지를 알려주는 "변위 맵"을 통해 수행됩니다.

4.2. 확률론적 모션 텍스처:

자연은 예측할 수 없습니다. 나뭇잎은 매번 같은 방식으로 펄럭이지 않습니다. 이러한 무작위성을 표현하기 위해 모션 텍스처에 "확률성"(또는 무작위성)을 도입합니다. 사물이 움직이는 방식이 고정되어 있는 것이 아니라 자연스러운 패턴을 기반으로 다양한 움직임을 포함합니다. 하지만 무작위 노이즈를 추가하는 것만으로는 충분하지 않으며, 혼란스럽거나 이상하게 보일 수 있습니다. 그래서 유니티는 컴퓨터 그래픽 분야의 기술을 사용하여 더 스마트한 접근 방식을 사용합니다. 한 가지 주요 아이디어는 "푸리에 변환"이라는 것을 사용하는 것입니다. 푸리에 변환은 복잡한 동작을 더 단순한 조각으로 분해하는 데 도움이 되는 도구라고 생각하면 됩니다(노래를 개별 음으로 분해하는 것처럼). 저희는 이러한 "음표"(또는 주파수) 몇 개만으로도 대부분의 자연스러운 움직임을 표현하기에 충분하다는 것을 발견했습니다.

![왼쪽: 우리는 실제 비디오의 데이터 세트에서 추출된 x 및 Y 동작 구성 요소에 대한 평균 동작 전력 스펙트럼을 시각화합니다, 파란색과 녹색 곡선으로 표시됩니다. 자연 진동 동작은 주로 저주파 구성 요소로 구성되므로 우리는 빨간색 점으로 표시된 첫 번째 K = 16 항을 사용합니다. 오른쪽: 이미지 폭과 높이로 진폭을 조정한 후 3Hz (K = 16)에서 푸리에 항의 진폭의 히스토그램을 보여줍니다 (파란색), 또는 빈도 적응형 정규화 (빨간색). 우리의 적응형 정규화는 계수가 극단 값에 집중되는 것을 방지합니다.](Generative%20Image%20Dynamics%20f0948d67712842aaa7d50974da90adca/Untitled%202.png)

왼쪽: 우리는 실제 비디오의 데이터 세트에서 추출된 x 및 Y 동작 구성 요소에 대한 평균 동작 전력 스펙트럼을 시각화합니다, 파란색과 녹색 곡선으로 표시됩니다. 자연 진동 동작은 주로 저주파 구성 요소로 구성되므로 우리는 빨간색 점으로 표시된 첫 번째 K = 16 항을 사용합니다. 오른쪽: 이미지 폭과 높이로 진폭을 조정한 후 3Hz (K = 16)에서 푸리에 항의 진폭의 히스토그램을 보여줍니다 (파란색), 또는 빈도 적응형 정규화 (빨간색). 우리의 적응형 정규화는 계수가 극단 값에 집중되는 것을 방지합니다.

4.3. 좋은 동작 예측하기:

이제 사진 속 사물이 어떻게 움직여야 하는지 예측하기 위해 '잠재 확산 모델'이라는 특수 컴퓨터 모델을 사용합니다. 이 모델은 실제 동영상에서 학습하여 사물이 어떻게 움직이는지 이해합니다. 이 모델은 사진을 가져와서 더 간단한 형태로 압축한 다음 가장 적합한 모션 텍스처를 추측합니다. 또한 모델이 특정 요구 사항에 맞게 잘 작동하도록 하기 위해 몇 가지 조정 기능을 도입했습니다:

![동작 예측 모듈. 우리는 주파수 조정된 디노이징 모델을 통해 신경 스토카스틱 동작 텍스처 S를 예측합니다. 확산 네트워크 ϵθ의 각 블록은 2D 공간 레이어와 빈도 교차 주의 레이어 (오른쪽, 빨간색 상자)를 교차하며, 잠재 특징 z n을 순차적으로 디노이징합니다. 디노이즈된 특징은 VAE 디코더 D에 공급되어 S를 생성합니다. 훈련 중에는 다운 샘플링된 입력 I0를 실제 동작 텍스처에서 VAE 인코더 E를 통해 인코딩된 잡음이 많은 잠재 특징과 연결하고, 가우스 잡음 z N으로 잡음이 많은 특징을 교체합니다 (왼쪽).](Generative%20Image%20Dynamics%20f0948d67712842aaa7d50974da90adca/Untitled%203.png)

동작 예측 모듈. 우리는 주파수 조정된 디노이징 모델을 통해 신경 스토카스틱 동작 텍스처 S를 예측합니다. 확산 네트워크 ϵθ의 각 블록은 2D 공간 레이어와 빈도 교차 주의 레이어 (오른쪽, 빨간색 상자)를 교차하며, 잠재 특징 z n을 순차적으로 디노이징합니다. 디노이즈된 특징은 VAE 디코더 D에 공급되어 S를 생성합니다. 훈련 중에는 다운 샘플링된 입력 I0를 실제 동작 텍스처에서 VAE 인코더 E를 통해 인코딩된 잡음이 많은 잠재 특징과 연결하고, 가우스 잡음 z N으로 잡음이 많은 특징을 교체합니다 (왼쪽).

- 주파수 적응형 노멀라이제이션입니다: 모션 텍스처가 주파수마다 다른 패턴을 가지고 있다는 것을 발견했습니다. 따라서 이러한 차이를 올바르게 처리하도록 모델을 조정했습니다.
- 주파수 조정 노이즈 제거: 이것은 서로 다른 주파수에 걸친 예측이 함께 의미가 있도록 하는 멋진 방법입니다. 깃발이 어떤 부분에서는 빠르게 흔들리고 다른 부분에서는 느리게 흔들리는 것을 원하지 않습니다. 따라서 이 단계를 통해 전체 모션이 조율되고 사실적으로 보이도록 합니다.

### 5. Image-based rendering

뉴럴 스토캐스틱 모션 텍스처

- 모션 텍스처: 모션 텍스처는 시간에 따라 변하는 2D 변위 맵의 시퀀스로, 입력 이미지에서 미래 시점의 픽셀 위치를 설명합니다.
- 스토캐스틱 모션 텍스처: 자연스러운 움직임은 서로 다른 속성을 가진 몇 가지 고조파 오실레이터의 조합으로 표현할 수 있습니다. 노이즈 필드를 통합하면 모션을 무작위로 만들 수 있지만, 이로 인해 때때로 비현실적인 애니메이션이 발생할 수 있습니다. 모션을 주파수 도메인으로 변환하면 더 적은 수의 예측으로 애니메이션을 만들 수 있으며 시간이 지나도 일관성이 유지됩니다.
    
    ![렌더링 모듈. 우리는 동작 감지 딥 이미지 기반 렌더링 모듈을 사용하여 누락된 컨텐츠를 채우고 왜곡된 입력 이미지를 세밀하게 만듭니다. 여기서 다중 스케일 기능은 입력 이미지 I0에서 추출됩니다. 그런 다음 동작 필드 Ft와 함께 기능에 소프트맥스 물금을 적용하여 시간 0에서 t까지 (동작에서 파생된 가중치 W에 따라). 왜곡된 기능은 이미지 합성 네트워크에 공급되어 세밀한 이미지 Iˆt를 생성합니다.](Generative%20Image%20Dynamics%20f0948d67712842aaa7d50974da90adca/Untitled%204.png)
    
    렌더링 모듈. 우리는 동작 감지 딥 이미지 기반 렌더링 모듈을 사용하여 누락된 컨텐츠를 채우고 왜곡된 입력 이미지를 세밀하게 만듭니다. 여기서 다중 스케일 기능은 입력 이미지 I0에서 추출됩니다. 그런 다음 동작 필드 Ft와 함께 기능에 소프트맥스 물금을 적용하여 시간 0에서 t까지 (동작에서 파생된 가중치 W에 따라). 왜곡된 기능은 이미지 합성 네트워크에 공급되어 세밀한 이미지 Iˆt를 생성합니다.
    
- 확산 모델로 모션 예측하기: 이 논문에서는 잠재 확산 모델(LDM)을 사용하여 모션을 예측합니다. 이 모델은 효율적이고 품질을 유지합니다. 연구진은 이 모델을 실제 동영상에서 모션 텍스처를 예측하는 데 적용했습니다. 여기에는 모션 텍스처의 주파수별 분포라는 문제가 있습니다. 이를 해결하기 위해 정규화 기법과 조정된 노이즈 제거 전략을 도입하여 예측 정확도를 향상시켰습니다.

이미지 기반 렌더링

- 이 방법은 예측된 모션 텍스처로부터 미래 프레임을 생성하기 위해 역시간 고속 푸리에 변환(FFT)을 사용하여 미래 시점의 모든 입력 픽셀의 위치를 결정합니다.
- 딥 이미지 기반 렌더링 방법을 사용하여 입력 이미지의 픽셀이 예측된 모션에 따라 "스플래트"됩니다. 포워드 워핑(이러한 스플래팅)은 이미지에 구멍과 같은 문제를 일으킬 수 있습니다.
이 문제를 해결하기 위해 모션에 따라 픽셀에 가중치를 부여하는 "소프트맥스 스플래팅"이라는 전략을 사용합니다. 큰 움직임은 전경 오브젝트로 간주하고, 작거나 움직임이 없는 오브젝트는 배경으로 간주합니다.
    
    ![왼쪽에서 오른쪽으로, 우리는 RGB 픽셀 공간에서 평균 스플래팅 (a)으로 렌더링된 미래 프레임을 보여줍니다. 학습 가능한 가중치 [42]로 소프트맥스 스플래팅 (b) 및 우리의 동작 인식 기능 스플래팅 (c).](Generative%20Image%20Dynamics%20f0948d67712842aaa7d50974da90adca/Untitled%205.png)
    
    왼쪽에서 오른쪽으로, 우리는 RGB 픽셀 공간에서 평균 스플래팅 (a)으로 렌더링된 미래 프레임을 보여줍니다. 학습 가능한 가중치 [42]로 소프트맥스 스플래팅 (b) 및 우리의 동작 인식 기능 스플래팅 (c).
    
- 훈련에는 실제 비디오 프레임을 사용하여 모델에 모션을 렌더링하는 방법을 가르치는 것이 포함됩니다. 그 결과 이 방법이 다른 방법보다 이미지 구멍이나 아티팩트와 같은 문제를 더 잘 처리하는 것으로 나타났습니다.

### 6. Applications

이미지-비디오:

- 제시된 기법을 사용하면 정적인 사진에 애니메이션을 적용할 수 있습니다. 이는 모션 텍스처를 예측한 다음 이미지 기반 렌더링 방법으로 애니메이션을 생성하여 수행됩니다.
- 모션 필드를 변경하여 슬로우 모션과 같이 속도를 조정할 수 있습니다. 또한 예측된 모션 텍스처의 강도를 조정하여 애니메이션 모션을 늘리거나 줄일 수 있습니다.

심리스 루핑:

- 시작과 끝이 눈에 띄지 않고 반복되는 동영상을 만들어 연속적인 경험을 제공하는 것을 목표로 합니다.
- 이미 끊김 없이 반복되는 동영상을 찾기가 어렵기 때문에 일반 클립으로 이러한 동영상을 제작하는 방법을 고안했습니다.
- '모션 셀프 가이드'라는 방법을 사용하여 동영상의 시작과 끝이 위치와 속도 모두 일치하도록 합니다.
- 다른 방법과 비교한 결과, 이 방법이 더 적은 왜곡으로 더 나은 루핑을 제공하는 것으로 나타났습니다.

단일 이미지에서 인터랙티브 다이내믹을 구현합니다:

- 이전 연구에 따르면 비디오에서 물체의 움직임은 물체의 물리적 특성과 연관될 수 있습니다. 예를 들어, 물체의 진동이나 진동 방식은 물체의 재질이나 구조를 나타낼 수 있습니다.
- 이 아이디어를 사용하여 사용자가 사진 속 물체를 찌르거나 당기는 등의 상호작용을 할 때 물체가 어떻게 반응할지 시뮬레이션할 수 있습니다.
- 물체의 움직임을 고조파 진동자의 조합으로 모델링하여 실시간으로 반응을 시뮬레이션할 수 있습니다. 이는 수학 공식과 방법을 통해 이루어집니다.
- 인상적인 특징은 기존 기술에는 비디오가 필요했지만, 이 방법은 단 하나의 이미지로 인터랙티브한 장면을 시뮬레이션할 수 있다는 점입니다.

이 요약은 제시된 모션 표현 및 애니메이션 파이프라인의 다양한 애플리케이션에 대한 개요를 제공합니다. 이 시스템은 정지 이미지에 애니메이션을 적용하고, 끊김 없이 반복되는 비디오를 생성하며, 단일 이미지에서 인터랙티브 다이내믹을 시뮬레이션할 수 있습니다.

### 7. Experiments

구현 세부 사항:

![슬라이딩 윈도우 FID 및 DT-FVD. 우리는 다른 방법으로 생성된 비디오의 크기 30 프레임의 슬라이딩 윈도우 FID와 크기 16 프레임의 DT-FVD를 보여줍니다.](Generative%20Image%20Dynamics%20f0948d67712842aaa7d50974da90adca/Untitled%206.png)

슬라이딩 윈도우 FID 및 DT-FVD. 우리는 다른 방법으로 생성된 비디오의 크기 30 프레임의 슬라이딩 윈도우 FID와 크기 16 프레임의 DT-FVD를 보여줍니다.

- 이 팀은 모션 텍스처 예측을 위해 VAE가 지원하는 특정 모델(LDM)을 사용했습니다.
- 256 x 160 크기의 이미지로 모델을 학습시켰는데, 이 학습에는 16개의 Nvidia A100 GPU에서 약 6일이 소요되었습니다.
- 애니메이션 제작에 필수적인 렌더링 모듈은 Nvidia V100 GPU에서 25FPS로 실시간 실행됩니다.

데이터 및 기준선:

- 주로 나무나 꽃이 흔들리는 등 진동 동작이 있는 자연스러운 장면에 중점을 두었습니다.
- 이러한 장면에 대한 2,631개의 비디오를 수집했습니다. 10%는 테스트를 위해 따로 보관했습니다.
- 광학 흐름 방법을 사용하여 실측 모션 데이터를 생성하여 13만 개 이상의 이미지-모션 쌍 샘플을 생성했습니다.
- 그런 다음 이 접근 방식을 여러 최신 애니메이션 및 비디오 예측 기법과 비교했습니다.

메트릭:

- 두 가지 주요 평가 방법이 사용되었습니다:
    - FID, KID, FIDsw 메트릭을 사용한 개별 프레임의 품질.
    - FVD, DT-FVD 및 새로운 슬라이딩 윈도우 FVD를 사용한 비디오의 전반적인 품질 및 일관성.

정량적 결과:

- 이 접근 방식은 사실감과 시간적 일관성 측면에서 이전의 단일 이미지 애니메이션 방식보다 우수한 것으로 나타났습니다.
- 이 방법은 시간이 지나도 눈에 띄는 품질 저하 없이 일관성을 유지하는 비디오를 생성했습니다.
    
    ![다른 접근법으로 생성된 비디오의 X-t 슬라이스. 왼쪽에서 오른쪽으로: 입력 이미지 및 해당 X-t 비디오 슬라이스에서 지상 진실 비디오, 세 가지 기준선 [26, 28, 83]에서 생성된 비디오, 마지막으로 우리의 접근법으로 생성된 비디오.](Generative%20Image%20Dynamics%20f0948d67712842aaa7d50974da90adca/Untitled%207.png)
    
    다른 접근법으로 생성된 비디오의 X-t 슬라이스. 왼쪽에서 오른쪽으로: 입력 이미지 및 해당 X-t 비디오 슬라이스에서 지상 진실 비디오, 세 가지 기준선 [26, 28, 83]에서 생성된 비디오, 마지막으로 우리의 접근법으로 생성된 비디오.
    

질적 결과:

- X-t 슬라이스를 사용하여 결과를 시각화한 결과, 애니메이션이 실제 동영상과 매우 흡사한 것으로 나타났습니다.
- 다른 기술은 사실적인 움직임을 캡처하는 데 실패하거나 고스트 현상이나 색상 왜곡과 같은 시각적 문제가 나타났습니다.
    
    ![생성된 미래 프레임 및 해당 동작 필드의 시각적 비교. 기준 이미지에서의 차이점을 검사함으로써 우리는 우리의 접근법이 기준선과 비교하여 더 현실적인 질감과 동작을 생성한다는 것을 관찰합니다. 우리는 전체 결과를 위해 부록 비디오를 참조하도록 독자에게 권장합니다.](Generative%20Image%20Dynamics%20f0948d67712842aaa7d50974da90adca/Untitled%208.png)
    
    생성된 미래 프레임 및 해당 동작 필드의 시각적 비교. 기준 이미지에서의 차이점을 검사함으로써 우리는 우리의 접근법이 기준선과 비교하여 더 현실적인 질감과 동작을 생성한다는 것을 관찰합니다. 우리는 전체 결과를 위해 부록 비디오를 참조하도록 독자에게 권장합니다.
    

절제 연구:

- 모션 예측 및 렌더링 모듈에서 디자인 선택을 테스트했습니다.
- 다양한 주파수 대역을 사용하고 모션 텍스처와 렌더링 방법을 수정하는 등 다양한 변형을 시도했습니다.
- 전체 모델 구성이 최고의 성능을 제공했습니다.

연구팀은 단일 이미지 애니메이션을 위한 효과적인 접근 방식을 개발하고 기존 방법과 성능을 비교하여 양적, 질적으로 우수함을 입증했습니다.

### 8. Discussion and Conclusion

한계:

- 이 방법은 저주파 확률적 모션에 맞춰져 있으므로 진동하지 않는 모션이나 고주파 진동을 효과적으로 캡처하지 못할 수 있습니다.
- 실제 동영상에 큰 움직임이나 큰 변위가 있는 경우 동영상 품질이 저하될 수 있습니다.
- 원본 이미지에서 대량의 새 콘텐츠를 생성해야 하는 경우 애니메이션 품질이 저하될 수 있습니다.

결론:

- 연구팀은 단 하나의 이미지로 자연스러운 진동을 애니메이션화하는 기술을 도입했습니다.
- 실제 동영상에서 학습한 확률론적 모션 텍스처라는 신경 표현을 사용하여 미래의 움직임을 예측하고 애니메이션을 만들었습니다.
- 이 방법은 이전 방법보다 더 사실적인 애니메이션을 제공합니다.
- 단일 이미지에서 인터랙티브 애니메이션을 생성하는 데에도 응용할 수 있습니다.