# DreaMoving: A Human Dance Video Generation Framework based on Diffusion Models

[https://arxiv.org/abs/2312.05107](https://arxiv.org/abs/2312.05107)

[https://dreamoving.github.io/dreamoving/](https://dreamoving.github.io/dreamoving/)

- Dec 2023

### 1 Introduction

인간 중심 콘텐츠의 텍스트-비디오(T2V) 모델과 과제

이 소개에서는 동영상 생성을 크게 발전시킨 Stable-VideoDiffusion1 및 Gen22와 같은 텍스트-비디오(T2V) 모델의 최근 발전을 강조합니다. 이러한 발전에도 불구하고 사람 중심의 콘텐츠, 특히 캐릭터 댄스를 생성하는 것은 여전히 어려운 과제입니다. 이러한 어려움은 두 가지 주요 요인에 기인합니다:

오픈소스 휴먼 댄스 비디오 데이터 세트의 부족: 사람의 춤 영상이 포함된 공개적으로 사용 가능한 데이터 세트가 부족합니다. 이러한 한계는 T2V 모델이 인간의 춤 동작을 효과적으로 학습하고 복제하는 데 방해가 됩니다.

정확한 텍스트 설명 확보의 어려움: 춤 동작을 텍스트 형식으로 정확하게 설명하는 것은 어렵기 때문에 T2V 모델의 훈련이 복잡해집니다. 이러한 모델은 프레임 내 및 프레임 간에 일관된 품질의 비디오를 생성하고, 더 긴 길이를 유지하며, 콘텐츠의 다양성을 보장하기 위해 상세한 설명이 필요합니다.

개인화 및 제어 가능성의 과제

또한 이 논문은 학계에서 큰 관심을 받고 있는 인간 중심 비디오 생성에서 개인화 및 제어 가능성의 중요성을 강조합니다. 컨트롤넷, 드림부스, LoRA와 같은 기존 연구는 이미지 생성의 구조를 제어하고 외관을 조작하는 데 있어 진전을 이루었습니다. 하지만 이러한 방법은 모션 패턴을 제어하는 데는 부족한 경우가 많으며, 다양한 타겟 아이덴티티에 따라 특정 하이퍼파라미터를 조정해야 하므로 컴퓨팅 요구 사항이 증가할 수 있습니다.

드레아무빙: 새로운 프레임워크

이 백서에서는 이러한 문제를 해결하기 위해 확산 모델(DM)을 기반으로 하는 새로운 프레임워크인 DreaMoving을 소개합니다. DreaMoving은 향상된 제어 및 개인화 기능을 제공하여 인간 댄스 비디오 생성의 한계를 뛰어넘는 것을 목표로 합니다. 이 백서의 다음 섹션에서는 DreaMoving의 구성에 대해 자세히 설명하고 그 적용 결과를 제시합니다.

### 2. Architecture

DreaMoving의 아키텍처 요약
DreaMoving의 구조 개요

휴먼 댄스 비디오 생성을 위한 프레임워크인 DreaMoving은 안정-확산 모델을 기반으로 하며 세 가지 주요 네트워크로 구성됩니다:

노이즈 제거 U-넷: 이 네트워크는 DreaMoving의 핵심입니다. 이 네트워크는 더 나은 비디오 품질과 움직임의 일관성을 위해 AnimateDiff와 유사한 모션 블록으로 향상되었습니다.

비디오 제어 네트워크: 노이즈 제거 U-Net용 플러그인으로, 비디오의 모션 측면을 제어하는 데 중점을 둡니다.

콘텐츠 가이드: 또 다른 노이즈 제거 U-Net용 플러그인으로, 동영상에서 캐릭터와 배경의 모양을 관리합니다.

데이터 수집 및 전처리

DreaMoving을 효과적으로 학습시키기 위해 인터넷에서 약 1,000개의 고품질 인간 댄스 비디오를 수집했습니다. 그런 다음 이 비디오를 약 6,000개의 짧은 클립으로 분할했습니다. 텍스트 설명의 경우 Minigpt-v2 모델을 사용하여 키프레임에 대한 자세한 캡션을 생성하여 전체 비디오 클립을 표현했습니다.

모션 블록 통합

시간적 일관성과 모션 충실도를 향상시키기 위해 AnimateDiff에서 채택한 모션 블록을 디노이징 U-Net과 컨트롤넷에 모두 통합했습니다. 이러한 블록은 처음에 AnimateDiff 모델을 기반으로 한 다음 개인 무용 비디오 데이터에 따라 미세 조정되었습니다.

콘텐츠 가이더 개발

콘텐츠 가이드는 사람의 모습과 배경을 포함하여 생성된 비디오의 콘텐츠를 제어합니다. 정확한 사람 모습 안내를 위한 이미지 프롬프트와 배경 생성을 위한 텍스트 프롬프트를 결합합니다. 이 시스템은 얼굴 특징과 선택적 신체 특징에 이미지 인코딩을 사용하고, 텍스트 설명과 결합하여 최종 콘텐츠 임베딩을 생성합니다.

모델 훈련 단계

콘텐츠 가이더 훈련: 이미지 인코더로 OpenCLIP ViT-H14를 사용하여 독립적으로 훈련하고 얼굴 특징에 대해 Arcface 모델로 보완합니다.

롱 프레임 사전 훈련: 이 단계에서는 모션 모듈의 시퀀스 길이를 확장하고 노이즈 제거 U-Net의 모션 모듈만 훈련했습니다.

비디오 제어망 훈련: 사전 훈련 후, 사람의 댄스 비디오 데이터와 사람의 포즈 또는 깊이 입력을 사용하여 노이즈 제거 U-Net으로 비디오 컨트롤넷을 훈련했습니다.

표현 미세 조정: 이 마지막 단계에서는 노이즈 제거 U-Net의 모션 블록을 미세 조정하여 사람의 표정 생성을 향상시키는 데 중점을 두었습니다.

모델 추론 및 제어

추론하는 동안 입력에는 텍스트 프롬프트, 참조 이미지, 포즈 또는 깊이 시퀀스가 포함됩니다. 비디오 컨트롤넷의 제어 스케일과 콘텐츠 가이더에서 얼굴/신체 안내의 강도를 조정할 수 있습니다. 이 시스템은 텍스트 프롬프트를 통해 콘텐츠를 완벽하게 제어할 수 있으며, 비디오의 다양한 측면에 대한 사용자 지정 설정이 가능합니다.

### 3. Results

DreaMoving은 고품질의 충실한 비디오를 제작할 수 있는 능력을 입증했습니다. 이 프레임워크는 다음과 같은 다양한 유형의 안내 입력과 함께 작동할 수 있습니다:

텍스트 프롬프트만: 사용자는 간단한 텍스트 설명을 입력할 수 있으며, DreaMoving은 이러한 텍스트 지침만으로 비디오를 생성할 수 있습니다. 이에 대한 예는 원본 문서의 그림 2에 나와 있습니다.

이미지 프롬프트만: 프레임워크는 이미지 프롬프트를 기반으로 비디오를 생성할 수도 있습니다. 이 기능은 사용자가 동영상에 등장하는 인물의 신원을 유지하고자 할 때 유용합니다.

텍스트 및 이미지 프롬프트 결합: 사용자는 텍스트와 이미지 프롬프트를 모두 입력할 수 있는 옵션이 있습니다. 이를 통해 그림 4에서 볼 수 있듯이 인물의 얼굴과 의상을 포함하여 동영상 콘텐츠를 보다 세부적으로 제어할 수 있습니다.

개인화 및 일반화 기능

DreaMoving은 고도의 개인화 기능을 제공합니다:

얼굴 식별 보존: 사용자는 콘텐츠 가이더에 특정 얼굴 이미지를 입력하면 그림 3과 같이 특정 인물의 신원이 유지되는 동영상을 생성할 수 있습니다.

외형 사용자 지정: 사용자는 동영상에 등장하는 캐릭터의 얼굴 특징과 의상을 모두 정의하여 더욱 맞춤화된 결과를 얻을 수 있습니다.

또한 DreaMoving은 인상적인 일반화 기능을 보여줍니다:

보이지 않는 스타일라이즈드 이미지 처리: 이 프레임워크는 훈련 중에 접해보지 못한 양식화된 이미지도 효과적으로 처리할 수 있습니다. 그림 5에서 볼 수 있듯이 DreaMoving은 이러한 새로운 이미지의 스타일과 콘텐츠에 맞는 비디오를 생성할 수 있습니다.
요약하면, DreaMoving은 다양한 유형의 입력 프롬프트를 활용하고 다양한 스타일과 도메인에 적응하면서 충실도가 높은 개인화되고 다양한 비디오 콘텐츠를 제작할 수 있는 능력이 뛰어납니다.