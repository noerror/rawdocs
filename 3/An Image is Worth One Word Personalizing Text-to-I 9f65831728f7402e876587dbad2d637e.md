# An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion

[https://arxiv.org/pdf/2208.01618.pdf](https://arxiv.org/pdf/2208.01618.pdf)

텍스트-이미지 모델을 사용하면 자연어를 사용하여 이미지를 만들 수 있습니다. 그러나 구체적이고 고유한 개념을 생성하거나 모양을 수정하는 것은 어려울 수 있습니다. 이 연구는 창의적인 자유를 얻기 위한 간단한 접근 방식을 제시합니다. 이 방법은 고정된 텍스트-이미지 모델의 임베딩 공간에 3~5개의 개념 이미지만 사용하여 새로운 '단어'를 통해 개념을 표현하는 방법을 학습합니다. 이러한 '단어'는 자연어 문장과 결합하여 개인화된 창작물을 만들 수 있습니다. 단일 단어 임베딩만으로도 고유한 개념을 포착하기에 충분한 경우가 많습니다. 이 접근 방식은 다양한 기준선보다 성능이 뛰어나며 다양한 작업에서 잘 작동합니다. 코드, 데이터 및 새로운 단어는 [https://textual-inversion.github.io](https://textual-inversion.github.io/) 을 참조하세요.

1

영화 '타이타닉'에서 로즈는 잭에게 특정 스타일로 자신을 그려달라고 부탁합니다. 이 장면은 인간이 단순한 언어를 통해 복잡한 개념을 어떻게 전달할 수 있는지를 잘 보여줍니다. 텍스트-이미지 모델은 최근 자연어 설명에서 이미지를 생성하는 데 큰 가능성을 보여주었습니다. 그러나 이러한 모델에 새로운 개념을 도입하는 것은 재교육이나 미세 조정으로 인해 사전 지식을 잊어버릴 수 있기 때문에 어려운 일입니다.

제안된 솔루션은 사전 학습된 텍스트-이미지 모델의 텍스트 임베딩 공간에서 새로운 단어를 찾아 새롭고 구체적인 개념에 대한 '의사 단어'를 생성하는 것입니다. 이렇게 하면 생성 모델은 그대로 유지되어 일반화 기능은 그대로 유지됩니다. '텍스트 반전'이라고 하는 이 프로세스는 작은 이미지 세트를 사용하여 새로운 개념을 표현하기 위해 단일 단어 임베딩을 최적화합니다.

이 접근 방식은 다양한 컨셉과 프롬프트에 효과적이며, 사용자가 새로운 장면에 독특한 오브젝트를 삽입하고, 스타일을 변형하고, 포즈를 바꾸고, 편견을 줄이고, 새로운 제품을 상상할 수 있도록 해줍니다. 이 방법은 사용자가 제공한 캡션을 사용하여 생성한 이미지에 비해 시각적 충실도가 높고 더욱 강력한 편집이 가능합니다.

2

텍스트 안내 이미지 합성은 GAN을 사용하여 캡션에서 이미지를 생성합니다. 최근의 발전은 대규모 자동 회귀 또는 확산 모델을 사용하여 인상적인 결과를 만들어 냈습니다. 이러한 모델은 이미지 편집, 도메인 적응, 비디오 조작 및 스타일 전송과 같은 다양한 애플리케이션에 텍스트 기반 인터페이스를 사용합니다. 제안된 접근 방식은 특정 개념을 설명하기 위해 새로운 의사 단어를 도입하여 고정된 모델의 어휘를 확장합니다.

GAN 반전에서는 최적화 기반 기법을 통해 잠재 표현을 찾거나 인코더를 사용하여 이미지를 조작합니다. 확산 기반 반전은 네트워크를 통해 이미지의 노이즈를 제거하면서 콘텐츠를 보존하는 데 중점을 둡니다. 제안된 방법은 사용자가 제공한 개념을 반전시켜 모델 어휘에 새로운 의사 단어로 표현하여 보다 직관적인 편집을 가능하게 합니다.

![텍스트 임베딩 및 반전 프로세스의 개요입니다. 플레이스홀더 단어가 포함된 문자열 가 포함된 문자열은 먼저 토큰(예: 사전의 단어 또는 하위 단어 색인)으로 변환됩니다. 이러한 토큰은 다음과 같이 변환됩니다. 연속 벡터 표현("임베딩", v)으로 변환됩니다. 마지막으로, 임베딩 벡터는 다음과 같이 변환됩니다. 단일 컨디셔닝 코드 cθ(y)로 변환되어 생성 모델을 안내합니다. 의사-워터와 연결된 임베딩 벡터 v∗ 를 재구성 목표를 사용하여 의사 단어 S∗와 연관된 임베딩 벡터 v∗ 를 최적화합니다.](An%20Image%20is%20Worth%20One%20Word%20Personalizing%20Text-to-I%209f65831728f7402e876587dbad2d637e/Untitled.png)

텍스트 임베딩 및 반전 프로세스의 개요입니다. 플레이스홀더 단어가 포함된 문자열 가 포함된 문자열은 먼저 토큰(예: 사전의 단어 또는 하위 단어 색인)으로 변환됩니다. 이러한 토큰은 다음과 같이 변환됩니다. 연속 벡터 표현("임베딩", v)으로 변환됩니다. 마지막으로, 임베딩 벡터는 다음과 같이 변환됩니다. 단일 컨디셔닝 코드 cθ(y)로 변환되어 생성 모델을 안내합니다. 의사-워터와 연결된 임베딩 벡터 v∗ 를 재구성 목표를 사용하여 의사 단어 S∗와 연관된 임베딩 벡터 v∗ 를 최적화합니다.

머신 러닝의 개인화는 추천 시스템, 연합 학습, 비전 및 그래픽에 적용되는 등 오랜 목표였습니다. 이 작업은 개체 검색 및 세분화를 위해 CLIP의 텍스트 임베딩 공간에서 의사 단어를 식별하는 PALAVRA와 가장 관련이 있습니다. 하지만 팔라브라의 접근 방식은 새로운 장면에서 그럴듯하게 재구성하거나 합성하는 데 필요한 세부 사항을 포착하지 못합니다.

3

목표는 사전 학습된 텍스트-이미지 모델의 중간 표현으로 인코딩하여 사용자가 지정한 새로운 개념을 언어에 따라 생성할 수 있도록 하는 것입니다. 제안된 방법은 입력 텍스트를 연속 벡터 표현으로 변환하는 텍스트 인코더의 단어 임베딩 단계를 사용합니다.

이 방법은 자동 인코더와 확산 모델로 구성된 잠재 확산 모델(LDM)을 사용하여 구현됩니다. 자동 인코더는 이미지를 잠재 코드로 매핑하거나 그 반대로 매핑하는 방법을 학습하고, 확산 모델은 학습된 잠재 공간 내에서 코드를 생성합니다. 이 모델은 클래스 레이블, 세분화 마스크 또는 텍스트 임베딩에 따라 조건이 지정될 수 있습니다.

사용자가 지정한 개념에 대한 새로운 임베딩을 찾기 위해 작은 이미지 세트가 사용되며, LDM 손실을 최소화하여 임베딩을 최적화합니다. 최적화 목표가 정의되고 텍스트 인코더와 노이즈 제거 네트워크가 모두 고정된 상태에서 원본 LDM 모델과 동일한 훈련 체계가 사용됩니다.

이 방법은 LDM 논문의 원본 하이퍼 파라미터를 사용하여 적용되며, GPU와 특정 학습 속도를 사용하여 실험을 수행합니다. 실험 결과 이 접근 방식은 대부분의 경우 잘 작동하지만, 일부 개념의 경우 더 나은 결과를 얻기 위해 더 적은 단계 또는 더 높은 학습 속도가 필요할 수 있습니다.

4

이 연구에서는 하나의 의사 단어로 객체의 변형을 캡처하는 방법인 텍스트 반전을 시연합니다. 이 접근 방식을 휴먼 캡션 및 DALLE-2와 비교합니다. 그 결과 텍스트 반전이 단 하나의 단어 임베딩을 사용하여 인간 캡션 및 DALLE-2보다 고유한 디테일을 더 잘 캡처하는 것으로 나타났습니다.

텍스트 가이드 합성은 학습된 의사 단어를 사용하여 새로운 장면을 구성하는 모델의 능력을 보여줍니다. 텍스트 반전은 PALAVRA와 같은 기준 모델과 비교했을 때, 하나의 의사 단어를 여러 세대에 걸쳐 최적화할 수 있는 반면 기준 모델은 새로 생성할 때마다 많은 비용이 드는 최적화가 필요합니다.

![학습된 두 개의 의사 단어를 사용하여 구문을 생성합니다. 이 모델은 두 개념을 결합한 프롬프트를 사용할 때 두 개념을 결합하는 프롬프트를 사용할 때 두 개념의 의미를 결합할 수 있습니다. 이 모델은 추론 능력에 제한이 있습니다. 두 개념을 나란히 배치하는 것과 같이 더 복잡한 관계형 프롬프트에 대한 추론 능력에는 제한이 있습니다. 이미지 크레딧: @QinniArt (왼쪽), @Leslie Manlapig(오른쪽). 각각 비상업적/비인쇄용으로 승인된 복제물입니다.](An%20Image%20is%20Worth%20One%20Word%20Personalizing%20Text-to-I%209f65831728f7402e876587dbad2d637e/Untitled%201.png)

학습된 두 개의 의사 단어를 사용하여 구문을 생성합니다. 이 모델은 두 개념을 결합한 프롬프트를 사용할 때 두 개념을 결합하는 프롬프트를 사용할 때 두 개념의 의미를 결합할 수 있습니다. 이 모델은 추론 능력에 제한이 있습니다. 두 개념을 나란히 배치하는 것과 같이 더 복잡한 관계형 프롬프트에 대한 추론 능력에는 제한이 있습니다. 이미지 크레딧: @QinniArt (왼쪽), @Leslie Manlapig(오른쪽). 각각 비상업적/비인쇄용으로 승인된 복제물입니다.

스타일 전이는 특정 스타일을 나타내는 의사 단어를 찾는 모델의 능력을 보여줌으로써 추상적인 아이디어를 포착하는 모델의 능력을 확장합니다. 개념 구성은 모델이 여러 개의 새로운 유사 단어를 동시에 추론할 수 있지만 이들 간의 관계에는 어려움을 겪는다는 것을 보여줍니다.

편향성 감소는 텍스트 반전이 편향된 개념에 대해 "더 공정한" 단어를 학습하여 보다 포용적인 세대를 이끌어낼 수 있음을 보여줍니다. 다운스트림 애플리케이션은 혼합 잠재 확산과 같은 동일한 초기 LDM 모델을 기반으로 구축된 모델에서 의사 단어가 사용될 수 있음을 보여줍니다.

![Untitled](An%20Image%20is%20Worth%20One%20Word%20Personalizing%20Text-to-I%209f65831728f7402e876587dbad2d637e/Untitled%202.png)

이미지 큐레이션은 부분적으로 수동으로 이루어지며, 프롬프트당 16개의 후보가 생성되고 최상의 결과가 선택됩니다. 그러나 CLIP을 사용하여 이미지 순위를 매기는 선택 프로세스를 자동화할 수 있습니다.

5

이 연구에서 연구자들은 미지의 잠재 공간에서 디자인 선택을 탐색하고 텍스트 임베딩 공간에 많은 GAN 반전 원리가 적용된다는 사실을 발견했습니다. 그러나 기존의 GAN 반전 솔루션은 이러한 맥락에서 잘 작동하지 않는 경우가 많습니다. 이들은 클립 간 거리와 텍스트 프롬프트를 사용하여 재구성 및 편집 가능성을 기반으로 잠재 공간 임베딩을 평가합니다.

확장된 잠재 공간, 점진적 확장, 정규화, 이미지별 토큰, 휴먼 캡션, 참조 설정 등 다양한 설정이 테스트됩니다. 그 결과 단일 단어 방식이 다중 단어 기준선에 비해 재구성 및 편집 가능성 모두에서 우수한 성능을 보였습니다. 이 기준선은 왜곡과 편집 가능성 사이의 상충 관계를 보여주며, 사람 설명은 원하는 만큼 개념의 유사성을 포착하지 못합니다.

사용자 연구에서는 CLIP 기반 지표를 확인하고 개념 재현 및 편집을 위한 사람 기반 캡션의 유사한 한계를 강조합니다.

6

이 연구에서는 개인화된 언어 안내 이미지 생성을 위한 '텍스트 반전'이라는 방법을 소개합니다. 이 방법은 자유도가 높지만 정확한 모양을 학습하는 데 어려움이 있고 최적화 시간이 오래 걸립니다. 연구진은 향후 연구에서 이러한 한계를 개선하고자 합니다.

텍스트-이미지 모델은 오해의 소지가 있는 콘텐츠, 잘못된 정보, 편견을 조장하는 데 사용될 수 있지만, 특정 개념을 더 정확하게 설명할 수 있다면 이러한 편견을 줄이는 데 도움이 될 수 있습니다. 또한 저작권 침해를 위해 예술적 스타일을 오용할 가능성도 우려됩니다.

결론적으로, 텍스트 반전은 다양한 텍스트-이미지 모델에 적용할 수 있는 개인화된 이미지 생성을 위한 유망한 접근 방식입니다. 연구진은 이 연구가 향후 개인화된 이미지 생성 작업의 기반을 마련하고 예술적 영감에서 제품 디자인에 이르기까지 다양한 응용 분야를 제공할 수 있다고 믿습니다.