# GLIGEN:Open-Set Grounded Text-to-Image Generation

*이 백서에서는 다양한 제거 연구를 사용하여 언어 유도 이미지 생성(GLIGEN) 모델을 분석합니다. 저자는 게이트 자기 주의, 널 캡션 사용, 푸리에 대 MLP 임베딩 등의 효과를 평가합니다. 또한 이미지 인페인팅 및 키포인트 접지와 같은 작업에서 GLIGEN 모델을 추가로 평가하여 유연성과 강력한 성능을 입증합니다. 저자들은 GLIGEN 모델 내의 학습 과정을 시각화하여 시간이 지남에 따라 공간 대응을 학습하고 이미지 품질을 개선하는 방법을 관찰함으로써 결론을 내립니다. LVIS 데이터 세트와 COCO2017 검증 세트에 대한 테스트에서 입증된 바와 같이, 모델을 사전 학습하면 성능이 더욱 향상됩니다.*

[https://arxiv.org/pdf/2301.07093.pdf](https://arxiv.org/pdf/2301.07093.pdf)

[https://gligen.github.io/](https://gligen.github.io/)

![GLIGEN은 다양한 접지 조건을 제공하여 고정된 텍스트-이미지 생성 모델에 다양한 접지 기능을 제공합니다. GLIGEN은 (a) 텍스트 엔티티 + 박스, (b) 이미지 엔티티 + 박스, (c) 이미지 스타일 및 텍스트 + 박스, (d) 키포인트, (e) 뎁스 맵, (f) 엣지 맵, (g) 노멀 맵 및 (h) 시맨틱 맵을 지원합니다.](GLIGEN%20Open-Set%20Grounded%20Text-to-Image%20Generation%202b4844b8ac4b4a3884d2e5d65f767313/Untitled.png)

GLIGEN은 다양한 접지 조건을 제공하여 고정된 텍스트-이미지 생성 모델에 다양한 접지 기능을 제공합니다. GLIGEN은 (a) 텍스트 엔티티 + 박스, (b) 이미지 엔티티 + 박스, (c) 이미지 스타일 및 텍스트 + 박스, (d) 키포인트, (e) 뎁스 맵, (f) 엣지 맵, (g) 노멀 맵 및 (h) 시맨틱 맵을 지원합니다.

1. Introduction

이 논문에서는 텍스트 이미지 합성으로 알려진 텍스트에서 이미지를 생성하는 분야의 발전에 대해 설명합니다. 지난 몇 년 동안 생성적 적대 신경망(GAN)은 이미지 생성 및 조작에 대한 제어를 제공하면서 이 분야에서 가장 큰 성공을 거두었습니다. 다른 텍스트 조건부 모델은 안정적인 학습 목표와 이미지-텍스트 쌍 데이터에 대한 대규모 학습을 통해 뛰어난 이미지 품질과 개념 범위를 보여주었습니다.

그러나 이러한 모델은 텍스트를 입력으로만 사용하기 때문에 경계 상자나 키포인트로 쉽게 수행할 수 있는 이미지 내 물체의 위치와 같은 정확한 세부 정보를 제공하기 어렵다는 한계가 있습니다. 또한 현재 모델은 이미지 생성 프로세스를 제어하기 위해 여러 입력을 결합하지 않습니다.

저자들은 텍스트-이미지 확산 모델에 경계 상자, 참조 이미지, 키포인트 등의 추가 입력을 허용하여 이를 개선하는 방법을 제안합니다. 문제는 이 새로운 정보를 도입하면서 사전 학습된 모델의 광범위한 개념 지식을 보존하는 것입니다.

이 문제를 해결하기 위해 원래 모델 가중치를 동결하고 새로운 입력을 처리할 수 있는 새로운 학습 가능한 레이어를 추가할 것을 제안합니다. 이렇게 하면 모델이 새로운 정보를 기존 모델에 점진적으로 융합하여 더 나은 이미지 품질과 제어 기능을 제공할 수 있습니다.

또한 저자들은 특정 데이터 세트에 대해서만 학습한 경우에도 보이지 않는 물체에도 일반화할 수 있다는 사실을 발견했습니다. 이는 접지된 개체와 관련된 각 구문(경계 상자당 하나의 구문)을 인코딩하고 인코딩된 위치 정보와 함께 인코딩된 토큰을 새 레이어에 공급함으로써 달성할 수 있었습니다.

저자들은 사전 학습된 모델의 가중치를 유지하고 새로운 레이어를 점진적으로 통합하여 텍스트-이미지 합성을 더 잘 제어할 수 있는 새로운 방법을 제안하여 기여하고 있습니다. 이 모델은 바운딩 박스 입력으로 텍스트-이미지 생성을 달성하고 레이아웃-이미지 작업에서 이전의 최첨단 모델보다 뛰어난 성능을 발휘합니다.

2. Related Work

이 논문에서는 자동 회귀 모델과 확산 모델을 비교하면서 대규모 텍스트-이미지 생성 모델의 발전에 대해 설명합니다. DALL-E 및 Parti와 같은 모델은 효과가 입증되었지만 일반적으로 캡션만 입력으로 사용하므로 이미지에서 객체의 위치와 같은 세부 사항의 정밀도가 제한됩니다. Make-A-Scene은 시맨틱 맵을 포함하지만 제한된 범주 내에서 작동하며, eDiff-I는 수정된 주의력 맵을 사용하지만 이 연구에서 사용된 방법은 더 간단한 인터페이스를 제공하고 다양한 다른 컨디셔닝 입력을 허용합니다.

또한 이 논문에서는 객체 카테고리로 레이블이 지정된 경계 상자를 기반으로 이미지를 생성하는 레이아웃에서 이미지를 생성하는 방법도 검토합니다. 이 접근 방식에 대한 기존 방법(예: Layout2Im 및 LostGAN)은 일반적으로 훈련 세트에서 관찰된 시각적 개념(일반적으로 COCO 데이터 세트의 약 80개 범주)으로 제한됩니다. 반면, 저자들의 방법은 오픈 세트 기반 이미지 생성을 위한 최초의 작업으로, 훈련 데이터에서 반드시 볼 수 없는 더 넓은 범위의 개념을 사용할 수 있습니다.

이 연구에서는 다양한 형태의 조건부 정보를 탐색하는 생성적 적대 신경망(GAN)과 확산 모델에 대한 다른 조건부 이미지 생성 방법도 살펴봅니다. 그러나 이러한 모델은 일반적으로 처음부터 학습됩니다. 저자의 방법은 대규모 웹 데이터에 대해 사전 학습된 기존 모델을 기반으로 하며, 비용 효율적인 방식으로 새로운 오픈 세트 기반 이미지 생성 기능을 구현하는 것을 목표로 합니다.

3. Preliminaries on Latent Diffusion Models

이 논문에서는 텍스트-이미지 작업에 대한 확산 기반 방법의 사용에 대해 설명하며, 특히 잠재 확산 모델(LDM)과 그 후속 모델인 안정 확산 모델이 특히 강력하다는 점을 강조합니다. 이러한 모델은 두 단계를 거치는데, 먼저 양방향 매핑 네트워크를 구축하여 이미지(x)의 잠재 표현(z)을 얻은 다음 이 잠재 표현에 대해 확산 모델을 훈련합니다.

이러한 모델의 훈련 목표는 노이즈 zT에서 시작하여 점차적으로 노이즈가 적은 샘플을 생성하는 것이며, 모든 시간 단계에서 캡션에 따라 조건이 지정됩니다. 훈련은 각 단계마다 노이즈와 노이즈 제거 자동 인코더 함수의 출력 간의 차이를 최소화하는 것을 목표로 하며, 이는 세타(θ)로 파라미터화됩니다. 이 노이즈 제거 프로세스는 이미지의 잠재적 표현에 대해 수행됩니다.

네트워크의 아키텍처는 조건을 인코딩하여 보다 깨끗한 버전의 z를 생성하는 방법을 중심으로 구성됩니다. 여기에는 ResNet 및 트랜스포머 블록으로 구성된 디노이즈 자동 인코더와 조건 인코딩 컴포넌트가 포함됩니다. 원래 LDM에서는 BERT와 유사한 네트워크를 처음부터 학습시켜 각 캡션을 일련의 텍스트 임베딩으로 인코딩한 다음 모델에 공급하여 캡션을 대체합니다. 안정 확산 모델에서는 캡션 기능이 고정 CLIP 텍스트 인코더를 통해 인코딩됩니다.

이러한 모델은 캡션 정보만을 기반으로 노이즈를 제거할 수 있으며 대규모 학습에서 인상적인 결과를 보여주었지만, 저자의 논문에서 초점을 맞추고 있는 추가 접지 입력이 가능한 이미지를 합성하는 것이 과제로 남아 있습니다.

4. Open-set Grounded Image Generation

이 연구에서는 GLIGEN이라는 근거 기반 텍스트-이미지 생성 시스템을 소개합니다. 이 시스템은 언어-대-이미지 확산 모델을 확장하여 외부 접지를 허용합니다. 이 근거 정보는 e(의미 정보)와 l(공간 정보)로 표현됩니다. 이들은 이미지에 어떤 객체가 있어야 하고 어디에 배치해야 하는지에 대한 지침을 모델에 제공하는 데 사용됩니다.

![접지 토큰 구성 프로세스 그림 텍스트 대소문자가 있는 바운딩 박스.](GLIGEN%20Open-Set%20Grounded%20Text-to-Image%20Generation%202b4844b8ac4b4a3884d2e5d65f767313/Untitled%201.png)

접지 토큰 구성 프로세스 그림 텍스트 대소문자가 있는 바운딩 박스.

작성자는 주로 공간 정보에는 바운딩 박스를, 의미적 표현에는 텍스트를 사용합니다. 이미지 캡션과 접지 엔티티를 확산 모델에 입력하면 캡션과 접지 토큰으로 처리됩니다. 그런 다음 이러한 토큰을 사용하여 이미지 생성을 안내합니다.

![사전 학습된 text2img 모델의 경우 텍스트 피처가 각 교차 주의 레이어에 공급됩니다. 새로운 조건부 정보를 받아들이기 위해 새로운 게이트형 셀프 어텐션 레이어가 삽입됩니다.](GLIGEN%20Open-Set%20Grounded%20Text-to-Image%20Generation%202b4844b8ac4b4a3884d2e5d65f767313/Untitled%202.png)

사전 학습된 text2img 모델의 경우 텍스트 피처가 각 교차 주의 레이어에 공급됩니다. 새로운 조건부 정보를 받아들이기 위해 새로운 게이트형 셀프 어텐션 레이어가 삽입됩니다.

또한 모델이 학습에서 본 엔티티를 넘어 새로운 엔티티로 일반화할 수 있는 폐쇄 집합에서 개방 집합 접근 방식으로의 전환에 대해서도 논의합니다. 이를 위해 캡션과 동일한 텍스트 인코더를 사용하여 명사 엔티티를 처리합니다.

이 논문에서는 접지 조건을 제공하는 다른 방법도 살펴봅니다. 이러한 방법 중 하나는 엔티티를 설명할 때 텍스트 대신 이미지를 사용하는 것입니다. 또한 공간 구성에 경계 상자 대신 키포인트를 사용할 것을 제안합니다. 세밀한 제어를 위해 에지 맵이나 심도 맵과 같이 공간적으로 정렬된 조건 맵을 제안합니다.

학습 접근 방식은 기존의 사전 학습된 모델을 사용하고 원래 모델 가중치를 방해하지 않고 새로운 모듈을 추가하는 지속적 학습을 기반으로 합니다. 모델에 새로운 게이트형 셀프 어텐션 레이어가 추가되어 공간 접지 기능을 사용할 수 있습니다. 이 레이어를 통해 시각적 기능이 조건부 정보를 활용할 수 있으므로 접지 프로세스가 개선됩니다.

추론에서는 근거 추론 단계와 표준 추론 단계의 2단계 프로세스를 제안합니다. 이 접근 방식은 이미지 생성과 접지 사이의 균형을 맞추고 생성된 이미지의 시각적 품질을 향상시킵니다.

전반적으로 GLIGEN은 대규모 언어 대 이미지 생성 모델에 외부 접지 정보를 주입하는 혁신적인 방법을 제시하여 복잡한 언어 명령을 기반으로 보다 사실적이고 정확한 이미지를 생성할 수 있는 능력을 향상시킵니다.

5. Experiments

이 작업은 텍스트 설명과 특정 접지 지침(예: 바운딩 박스)을 기반으로 이미지를 생성하는 모델을 제시합니다. 모델의 성능은 폐쇄형 및 개방형 설정 모두에서 테스트되고 다른 접지 양식으로 확장됩니다.

폐쇄형 접지 텍스트2이미지 생성 평가에서는 텍스트2이미지 생성의 표준 벤치마크인 COCO2014 데이터 세트에 대해 모델을 훈련합니다. 다양한 접지 지침이 테스트됩니다: 감지 데이터(COCO2014D), 감지 및 캡션 데이터(COCO2014CD), 접지 데이터(COCO2014G). 이 모델의 이미지 합성 품질과 접지 정확도는 미세 조정된 기준 모델과 비슷한 수준으로 높은 것으로 나타났습니다.

![COCO의 로컬라이제이션 주석만 사용하여 학습한 경우라도 일반화할 수 있습니다.](GLIGEN%20Open-Set%20Grounded%20Text-to-Image%20Generation%202b4844b8ac4b4a3884d2e5d65f767313/Untitled%203.png)

COCO의 로컬라이제이션 주석만 사용하여 학습한 경우라도 일반화할 수 있습니다.

다음으로, 이 모델을 탐지 주석에 대해서만 학습된 레이아웃2이미지 생성 방법과 비교합니다. 이 모델은 풍부한 시각적 시맨틱을 갖춘 사전 학습된 대규모 생성 모델을 기반으로 구축되어 이미지 품질과 접지 정확도 모두에서 최첨단 성능을 달성합니다.

모델의 오픈 세트 성능은 COCO 훈련 모델부터 시작하여 테스트됩니다. 이 모델은 COCO 카테고리를 넘어서는 근거 개체를 생성할 수 있으며, 1203개의 롱테일 개체 카테고리가 포함된 LVIS 데이터 세트에서 유망한 성능을 보여줍니다. 이 모델은 강력한 제로 샷 생성 성능을 보여주며, 심지어 지도 기준 모델을 능가하는 성능을 보여 보이지 않는 데이터로 일반화할 수 있는 능력을 보여줍니다.

모델의 오픈 세트 기능을 더 연구하기 위해 다양한 대규모 데이터 세트를 사용하여 학습 데이터를 확장합니다. 학습 데이터가 확장됨에 따라 특히 희귀한 개념에 대한 모델의 제로 샷 성능이 향상됩니다.

![기준 텍스트2이미지 생성. 베이스라인은 기준이 되는 기능이 부족하고 CLIP 텍스트 공간으로 인해 여러 객체가 있는 문장에서 '우산'과 같은 객체를 놓칠 수 있으며 공간적으로 사실과 반대되는 개념을 생성하는 데 어려움을 겪습니다.](GLIGEN%20Open-Set%20Grounded%20Text-to-Image%20Generation%202b4844b8ac4b4a3884d2e5d65f767313/Untitled%204.png)

기준 텍스트2이미지 생성. 베이스라인은 기준이 되는 기능이 부족하고 CLIP 텍스트 공간으로 인해 여러 객체가 있는 문장에서 '우산'과 같은 객체를 놓칠 수 있으며 공간적으로 사실과 반대되는 개념을 생성하는 데 어려움을 겪습니다.

마지막으로 모델은 텍스트 양식 기반 이상으로 확장됩니다. 여기에는 이미지 기반 생성, 텍스트 및 이미지 기반 생성, 키포인트 기반 생성, 공간 정렬 조건 맵 기반 생성이 포함됩니다. 이를 통해 다양한 크리에이티브 생성 시나리오를 구현할 수 있습니다. 또한 이미지 생성 과정에서 추가 접지 정보의 사용을 제어하기 위해 스케줄링 샘플링 기법이 사용되어 이미지 품질과 생성의 다양성을 향상시킵니다.

![예약 샘플링. 한 도메인(예: 사람)에서 학습된 모델을 시각적으로 개선하거나 한 도메인(예: 사람)에서 학습된 모델을 다른 도메인으로 확장할 수 있습니다.](GLIGEN%20Open-Set%20Grounded%20Text-to-Image%20Generation%202b4844b8ac4b4a3884d2e5d65f767313/Untitled%205.png)

예약 샘플링. 한 도메인(예: 사람)에서 학습된 모델을 시각적으로 개선하거나 한 도메인(예: 사람)에서 학습된 모델을 다른 도메인으로 확장할 수 있습니다.

6. Conclusion

연구진은 사전 학습된 텍스트-이미지 확산 모델을 이미지에 근거를 두거나 이미지를 특정 설명된 특성과 연결할 수 있는 기능으로 향상시키기 위해 고안된 방법인 GLIGEN을 도입했습니다. 이 방법은 접지용 바운딩 박스를 사용하여 성공적으로 시연되었습니다. 이 방법은 간단하고 효과적이며 키포인트, 참조 이미지, 에지 맵 및 심도 맵과 같은 공간 정렬 조건과 같은 다른 접지 조건으로 확장할 수 있습니다. GLIGEN의 다목적성은 텍스트-이미지 합성 분야의 발전과 사전 학습된 모델의 적용 범위를 넓힐 수 있는 유망한 길을 제시합니다.

A. Implementation and training details

이 텍스트에서는 안정적 확산 모델을 기반으로 이미지 접지를 위한 GLIGEN이라는 시스템을 구현하는 구체적인 방법을 설명합니다. 접지에는 이미지의 요소를 특정 특성으로 연결하는 것이 포함됩니다. 다음은 간단한 설명입니다:

텍스트가 있는 상자 접지 토큰: 설명된 각 요소(접지 텍스트)는 CLIP과 같은 텍스트 인코더를 사용하여 텍스트 임베딩으로 바뀝니다. CLIP 모델의 '문장 끝'(EOS) 토큰 임베딩은 전체 입력 텍스트 설명에 대한 정보를 포함하고 있기 때문에 접지 텍스트 임베딩으로 사용됩니다. 연구원들은 푸리에 임베딩을 사용하여 경계 상자 좌표를 인코딩한 다음 이를 텍스트 임베딩과 결합하여 다층 퍼셉트론(MLP)에 공급하여 접지 토큰을 생성합니다.

![상자에 대한 세 가지 유형의 접지 데이터.](GLIGEN%20Open-Set%20Grounded%20Text-to-Image%20Generation%202b4844b8ac4b4a3884d2e5d65f767313/Untitled%206.png)

상자에 대한 세 가지 유형의 접지 데이터.

이미지가 포함된 박스 그라운딩 토큰: 이미지로 그라운딩하는 과정도 비슷합니다. 이미지 인코더를 사용하여 이미지 임베딩을 생성한 다음 텍스트에 대해 취한 접근 방식과 일치하는 텍스트 특징 공간으로 변환합니다.

키포인트 그라운딩 토큰: 이와 유사하게 처리되지만 동일한 사람에게 속한 키포인트를 연결하기 위해 사람 토큰 임베딩 벡터가 추가됩니다. 이는 이미지에 여러 사람을 생성해야 할 때 유용하며, 모델이 어떤 키포인트가 어떤 사람에 해당하는지 알 수 있게 해줍니다.

공간 정렬 조건용 토큰: 에지 맵, 심도 맵, 시맨틱 맵 및 노멀 맵을 포함하는 조건입니다. 이러한 조건의 텐서 크기를 조정한 다음 처리하여 64개의 접지 토큰을 출력합니다.

게이트형 셀프 어텐션 레이어: 팀은 원래 확산 모델과 유사한 자기 관심 레이어를 삽입했지만 접지 토큰을 시각적 토큰과 동일한 차원으로 변환하는 선형 투영 레이어를 추가했습니다.

훈련은 많은 수의 반복과 특정 학습 속도를 사용하여 수행되었으며, 배치 크기와 GPU 설정은 실험에 따라 달라졌습니다. 접지 프로세스는 접지 데이터, 감지 데이터, 감지 및 캡션 데이터의 세 가지 유형의 데이터로 테스트되었습니다.

텍스트 및 이미지 입력, 키포인트, 공간적으로 정렬된 조건에 따라 이미지를 그라운딩하기 위해 GLIGEN 시스템이 어떻게 구현되는지에 대한 자세한 설명은 다음과 같습니다. 이 시스템은 대규모로 훈련되고 다양한 유형의 데이터로 테스트되었습니다.

연구원들은 언어 유도 이미지 생성(GLIGEN) 모델의 성능에 대한 여러 변수의 영향을 조사하고 있습니다. 이들은 COCO 데이터 세트와 프레셰 시작 거리(FID) 및 YOLO 평균 정밀도(AP) 지표를 사용하여 평가합니다.

게이티드 셀프 어텐션과 게이티드 크로스 어텐션 비교: 이 팀은 접지 지침을 통합하기 위해 게이트형 자기 주의를 사용합니다. 게이트형 교차 주의로 전환하면 FID는 비슷하게 유지되지만 YOLO AP는 감소하여 셀프 어텐션의 고유한 정보 공유가 유용하다는 것을 알 수 있습니다.

널 캡션과 명사 엔티티 문장 비교: 감지 주석만 있는 상황에서는 모든 명사 엔티티에서 문장을 생성하는 대신 널 캡션을 사용하기로 선택합니다. 후자의 접근 방식은 사전 학습된 텍스트 인코더가 이러한 색다른 캡션에 익숙하지 않기 때문에 FID가 더 나빠질 수 있습니다.

푸리에 임베딩과 MLP 임베딩 비교: 푸리에 임베딩을 MLP 임베딩으로 대체하면 이미지 품질은 비슷하게 유지되지만 레이아웃 대응이 크게 악화되어 푸리에 임베딩의 가치를 확인할 수 있습니다.

이미지 인페인팅: GLIGEN은 각 단계에서 알려진 영역을 분포의 샘플로 대체하여 이미지 인페인팅(누락된 영역 채우기)을 수행합니다. 특히 누락된 객체가 이미지에 이미 존재하는 경우 기준선보다 더 나은 성능을 발휘합니다.

![키포인트 결과. 저희 모델은 키포인트를 기반으로 더 높은 품질의 더 높은 품질의 이미지를 생성하며, 캡션을 사용하여 장면과 성별 등의 세부 사항을 장면이나 성별과 같은 세부 정보를 지정할 수 있습니다.](GLIGEN%20Open-Set%20Grounded%20Text-to-Image%20Generation%202b4844b8ac4b4a3884d2e5d65f767313/Untitled%207.png)

키포인트 결과. 저희 모델은 키포인트를 기반으로 더 높은 품질의 더 높은 품질의 이미지를 생성하며, 캡션을 사용하여 장면과 성별 등의 세부 사항을 장면이나 성별과 같은 세부 정보를 지정할 수 있습니다.

키포인트를 사용한 접지: 모델이 경계 상자 대신 사람의 키포인트를 접지 조건으로 사용할 수 있음을 보여줍니다. 고전적인 이미지 간 변환 모델인 pix2pixHD와 비교했을 때 GLIGEN은 훨씬 더 나은 이미지 품질을 생성합니다.

사전 훈련의 이점: LVIS 데이터 세트와 COCO2017 밸 세트에서 테스트한 결과, 사전 훈련된 모델은 특히 미세 조정 후 훨씬 향상된 성능을 보여 사전 훈련의 이점을 강조합니다.

![이미지 접지 인페인팅. 참조 이미지를 사용하여 채우려는 구멍을 접지할 수 있습니다.](GLIGEN%20Open-Set%20Grounded%20Text-to-Image%20Generation%202b4844b8ac4b4a3884d2e5d65f767313/Untitled%208.png)

이미지 접지 인페인팅. 참조 이미지를 사용하여 채우려는 구멍을 접지할 수 있습니다.

GLIGEN 분석: 주의 지도를 시각적으로 검사하고 학습 가능한 매개변수 γ가 훈련 중에 어떻게 변화하는지 분석하여 모델이 공간 대응을 학습하고 시간이 지남에 따라 세부 사항을 미세 조정하는 방법에 대한 통찰력을 제공합니다.

- 요약
    
    소개 및 모델 개요: 이 논문은 언어 유도 이미지 생성의 개념과 그 중요성을 설명하는 것으로 시작됩니다. 그리고 연구에 사용할 모델인 GLIGEN에 대해 설명합니다.
    
    게이트 자기 주의 Gated Self-Attention: 저자들은 먼저 모델에서 게이트 자기 주의의 중요성에 대해 살펴보고, 접지 지시를 흡수하는 데 있어 게이트 자기 주의의 역할을 강조합니다. 또한 절제 연구를 통해 게이트형 자기 주의와 게이트형 교차 주의를 비교하여 자기 주의가 YOLO AP 점수 측면에서 더 나은 성과를 거둔다는 것을 보여줍니다.
    
    캡션 없음: 이 논문에서는 감지 주석만 사용할 수 있는 경우 널 캡션을 사용하는 결정에 대해 논의합니다. 대체 접근 방식도 테스트되었지만 더 나쁜 FID 점수를 보여 널 캡션 접근 방식이 더 나은 성능을 보입니다.
    
    푸리에와 MLP 임베딩 비교: 저자는 실험을 위해 푸리에 임베딩을 MLP 임베딩으로 대체합니다. 그 결과 이미지 품질은 비슷하게 유지되지만, 레이아웃 대응은 MLP 임베딩에서 더 나빠지는 것으로 나타났습니다.
    
    인페인팅 작업: GLIGEN 모델의 인페인팅 작업 수행 능력을 살펴봅니다. 저자들은 각 이미지에서 하나의 객체를 마스킹하고 모델이 누락된 영역을 다시 칠하도록 하는 실험을 수행했습니다. 그 결과, 기준선과 비교했을 때, 인페인팅된 물체가 누락된 영역을 더 촘촘하게 채우는 것으로 나타났습니다.
    
    이미지 접지 인페인팅 및 키포인트 접지: 저자는 인페인팅을 위해 누락된 영역에 텍스트를 그라운딩하고 참조 이미지를 그라운딩할 수 있음을 보여줌으로써 모델의 기능을 더욱 확장합니다. 또한 사람의 키포인트로 모델을 평가하여 모델의 유연성을 보여줍니다.
    
    정량적 결과: 이 논문은 다양한 데이터 세트에 대해 사전 학습된 모델을 사용한 더 많은 연구를 제공합니다. 이 연구들은 사전 학습이 모델의 성능을 크게 향상시킬 수 있음을 보여줍니다.
    
    GLIGEN에 대한 분석: 마지막으로 저자들은 GLIGEN, 특히 박스 접지 모델에 대한 분석을 수행합니다. 게이트 자기 주의 레이어 내의 주의 지도를 시각화하고 훈련 과정에서 학습 가능한 파라미터 γ가 어떻게 변화하는지 살펴봅니다.
    
- 공간 정보
    
    이 논문에서 공간 정보는 이미지 내 객체('토큰'이라고 함)의 위치를 의미합니다.
    
    저자는 이 공간 정보를 처리하기 위해 자기 주의 메커니즘, 특히 게이트형 자기 주의와 게이트형 교차 주의 메커니즘을 사용합니다. 자기 주의에서는 입력된 모든 토큰이 다른 모든 토큰과 상호 작용할 수 있으므로 토큰 간의 공간적 관계를 학습하고 처리 중에 고려할 수 있습니다.
    
    앞서 언급한 제거 연구에서는 공간 정보를 인코딩하기 위해 푸리에 임베딩을 사용하는 것에 대해서도 논의합니다. 이 기술은 토큰의 공간 좌표를 복잡한 공간 관계를 포착할 수 있는 고차원 표현으로 변환합니다. 이 논문에서는 푸리에 임베딩을 사용하면 이 작업에 다층 퍼셉트론(MLP)을 사용하는 것보다 우수한 결과를 얻을 수 있다는 사실을 밝혀 공간 정보를 캡처하는 데 푸리에 임베딩이 효과적이라는 것을 보여주었습니다.
    
    이 논문은 또한 게이트 자기 주의 레이어 내의 주의 지도를 제시하여 다양한 접지 토큰에 대한 반응으로 다양한 공간 영역에 대한 주의를 시각적으로 확인할 수 있습니다. 이는 모델이 공간 정보를 사용하여 주의를 유도하고 궁극적으로 이미지를 생성하는 방법을 보여줍니다.
    
    따라서 본질적으로 공간 정보는 물체의 상대적 위치를 인코딩하여 적용되므로 모델은 훈련 및 생성 과정에서 이러한 공간 관계를 고려하고 학습할 수 있습니다.
    
- 위치 임베딩
    
    임베딩은 데이터를 머신러닝 모델에 입력할 수 있는 형태로 변환하는 방법입니다. 공간 정보의 경우 임베딩을 사용하여 이미지에서 객체의 위치나 위치를 모델에서 처리할 수 있는 형태로 변환할 수 있습니다.
    
    이 백서의 맥락에서 저자는 푸리에 임베딩을 사용하여 공간 정보를 인코딩합니다. 푸리에 임베딩은 주기 함수를 사인과 코사인으로 알려진 단순한 진동 함수의 합으로 분해하는 데 사용되는 수학적 도구인 푸리에 급수를 기반으로 합니다. 공간 정보의 경우 푸리에 임베딩은 주파수가 다른 사인 및 코사인 함수 집합을 사용하여 위치 좌표를 더 높은 차원의 공간에 매핑합니다. 그 결과 복잡한 공간 관계를 포착할 수 있는 풍부한 표현이 가능합니다.
    
    더 자세히 설명하면 다음과 같습니다:
    
    한 쌍의 좌표(x, y)로 표현되는 2D 위치가 있다고 가정해 보겠습니다. 푸리에 임베딩은 일련의 사인 및 코사인 함수를 사용하여 이 쌍을 더 높은 차원의 공간에 매핑합니다. 이는 다음과 같이 보일 수 있습니다:
    
    ```arduino
    
    [sin(x), cos(x), sin(2x), cos(2x), ..., sin(nx), cos(nx),
     sin(y), cos(y), sin(2y), cos(2y), ..., sin(ny), cos(ny)]
    ```
    
    여기서 n은 사용하려는 서로 다른 주파수의 수입니다. 이렇게 하면 각 좌표 쌍에 대해 고차원 벡터가 효과적으로 생성되며, 이를 모델에 입력할 수 있습니다.
    
    이 푸리에 기반 표현은 단순한 x, y 좌표 쌍으로는 포착할 수 없는 위치 간의 상호 작용과 관계를 포착할 수 있다는 점에서 큰 장점이 있습니다. 푸리에 임베딩은 사인 및 코사인 함수의 주기적 특성으로 인해 가까운 관계와 먼 관계를 모두 표현하고 구분할 수 있어 공간 데이터에 특히 효과적입니다.
    
    이 논문에서 푸리에 임베딩은 이미지 생성을 목적으로 공간 정보를 인코딩하는 데 있어 다층 퍼셉트론(MLP)을 사용하는 것보다 우수한 것으로 입증되었습니다.