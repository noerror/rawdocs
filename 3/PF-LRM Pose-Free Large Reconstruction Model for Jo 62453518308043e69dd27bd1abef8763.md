# PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction

[https://arxiv.org/abs/2311.12024](https://arxiv.org/abs/2311.12024)

[https://totoro97.github.io/pf-lrm/](https://totoro97.github.io/pf-lrm/)

- Nov 2023

### 1 INTRODUCTION

3D 재구성은 이미징 및 컴퓨터 그래픽과 같은 다양한 분야에서 사용되는 컴퓨터 비전의 핵심 문제입니다. 사진 측량과 같은 전통적인 방법과 최신 신경 재구성 기술은 이미지에서 충실도가 높은 기하학적 구조와 외관을 재구성하는 데 중점을 두고 크게 발전해 왔습니다. 이러한 방법에는 일반적으로 보정된 카메라 포즈가 있는 이미지가 필요하며, 보통 SfM(Structure-from-Motion) 솔버를 사용하여 계산합니다. 하지만 SfM은 밀집된 시점을 가정하기 때문에 이커머스, 소비자 캡처, 동적 장면 재구성에서 흔히 볼 수 있는 드문드문한 시점과 이미지 간 겹침이 거의 없는 시나리오에서는 실패합니다.

![(위쪽 블록) 야생에서 볼 수 없는 이미지에 대한 모델의 일반화 가능성을 입증하기 위해 이전/현재 3D 인식 생성 작업에서 2~4개의 비포즈 이미지를 가져와 PFLRM을 사용하여 NeRF를 공동으로 재구성하고 피드포워드 방식으로 상대적인 포즈를 추정합니다. (아래쪽 블록) 실제 캡처에 대한 모델의 일반화 가능성도 보여줍니다. 생성/합성 이미지의 소스: 열 1(위쪽에서 아래쪽), Magic3D(Lin 외., 2023b), DreamFusion(Poole 외., 2022), Wonder3D(Long 외., 2023); 열 2(위쪽에서 아래쪽), Zero-1-to-3(Liu 외., 2023a), 2023a), SyncDreamer(Liu 외, 2023b), Consistent-1-to-3(Ye 외, 2023); 열 3(위에서 아래로), MVDream(Shi 외, 2023b), NeRF(Mildenhall 외, 2020), Zero123++(Shi 외, 2023a). 실제 이미지의 출처: 1열 1행, HuMMan 데이터세트(Cai 외, 2022); 1열 2행, RelPose++(Lin 외, 2023a); 기타, 휴대폰 캡처.](PF-LRM%20Pose-Free%20Large%20Reconstruction%20Model%20for%20Jo%2062453518308043e69dd27bd1abef8763/Untitled.png)

(위쪽 블록) 야생에서 볼 수 없는 이미지에 대한 모델의 일반화 가능성을 입증하기 위해 이전/현재 3D 인식 생성 작업에서 2~4개의 비포즈 이미지를 가져와 PFLRM을 사용하여 NeRF를 공동으로 재구성하고 피드포워드 방식으로 상대적인 포즈를 추정합니다. (아래쪽 블록) 실제 캡처에 대한 모델의 일반화 가능성도 보여줍니다. 생성/합성 이미지의 소스: 열 1(위쪽에서 아래쪽), Magic3D(Lin 외., 2023b), DreamFusion(Poole 외., 2022), Wonder3D(Long 외., 2023); 열 2(위쪽에서 아래쪽), Zero-1-to-3(Liu 외., 2023a), 2023a), SyncDreamer(Liu 외, 2023b), Consistent-1-to-3(Ye 외, 2023); 열 3(위에서 아래로), MVDream(Shi 외, 2023b), NeRF(Mildenhall 외, 2020), Zero123++(Shi 외, 2023a). 실제 이미지의 출처: 1열 1행, HuMMan 데이터세트(Cai 외, 2022); 1열 2행, RelPose++(Lin 외, 2023a); 기타, 휴대폰 캡처.

도전 과제

새로운 스파스 메서드를 포함한 기존의 많은 재구성 방법은 정확한 카메라 포즈에 의존하여 작동합니다. 뷰가 희박하고 기준선이 넓은 시나리오에서는 SfM 솔버가 실패하기 쉽기 때문에 이러한 방법은 신뢰할 수 없게 됩니다.

우리의 기여: PF-LRM

이 연구에서는 삼면 신경 방사 필드(NeRF)를 사용하여 카메라 포즈와 물체의 모양과 외관을 예측하는 카테고리에 구애받지 않는 접근 방식인 PF-LRM을 소개합니다. 이 방법은 최소 2~4개의 희박한 입력 이미지로 사실적인 3D 오브젝트와 정확한 포즈를 재구성할 수 있습니다. 핵심 혁신은 2D 멀티뷰 이미지 토큰과 3D 삼면 NeRF 토큰을 모두 처리하는 확장 가능한 단일 스트림 트랜스포머 모델로, 2D와 3D 요소 간의 정보 교환을 용이하게 합니다. 이 접근 방식은 이미지에서 직접 포즈 파라미터를 회귀하는 대신 2D 패치 중심에 해당하는 3D 오브젝트 포인트를 예측함으로써 이전 방법과 차별화됩니다. 이 방법은 트랜스포머의 토큰 단위 연산과 더 잘 호환되므로 더 정확한 결과를 얻을 수 있습니다.

모델 훈련 및 성능

약 5억 9천만 개의 파라미터를 가진 대규모 트랜스포머 모델인 PF-LRM은 약 100만 개의 개체를 포함하는 대규모 데이터 세트에서 훈련되었습니다. 이 모델은 2~4개의 다양한 수의 비포즈 입력 이미지에서도 잘 작동하며, 특히 매우 희박한 입력 이미지에서도 포즈 추정 및 새로운 뷰 합성에서 최첨단 결과를 달성합니다. 이 모델은 기존의 기준 방법보다 훨씬 뛰어난 성능을 발휘합니다.

애플리케이션

이 논문에서는 텍스트/이미지를 3D로 변환하는 것과 같은 PF-LRM 모델의 잠재적 응용 분야도 강조하여 다양한 맥락에서 이 모델의 다양성과 유용성을 보여줍니다.

### 2 RELATED WORK

희소 포즈 이미지로부터의 NeRF

- 배경: 원래의 신경 방사장(NeRF) 기법은 정확한 재구성을 위해 많은 수의 포즈 이미지가 필요했습니다.
- 최근의 발전: 최근의 연구는 정규화 전략이나 광범위한 데이터 세트에서 학습하는 방법을 제안하는 스파스 뷰 NeRF 재구성에 초점을 맞추고 있습니다. 이러한 방법은 일반적으로 각 입력 이미지에 대해 정확한 카메라 포즈를 가정하지만, 희소 이미지에서 이러한 포즈를 얻는 것은 여전히 어려운 과제입니다.

모션으로부터 구조화(SfM)

- 접근 방식: SfM 기법은 여러 뷰에서 2D 특징이 일치하는 것을 식별하여 카메라 포즈와 희박한 3D 장면 구조를 동시에 해결합니다.
- 한계: 이 방법은 시각적으로 충분히 겹치고 뷰 간에 뚜렷한 특징이 있을 때 효과적입니다. 하지만 오브젝트의 여러 측면에서 촬영한 이미지가 몇 개 밖에 없는 등 입력이 극히 희박한 경우에는 겹침이 부족하기 때문에 어려움을 겪습니다.

RGB 이미지에서 신경망 포즈 예측하기

- 메서드: 일부 방법은 3D 형태 정보를 통합하지 않고 네트워크 예측에서 직접 카메라 포즈를 회귀시키려고 시도했습니다.
- 한계 및 개선 사항: 이러한 방법은 진일보한 방법이지만, 카메라 포즈와 3D 모양에 대한 공동 추론을 통합하면 결과가 크게 개선되는 것으로 나타났습니다.

우리의 접근 방식: 포즈 예측을 위한 새로운 방법

- 방법론: 카메라 포즈를 직접 회귀하는 대신 이미지 패치 토큰에서 각 뷰에 대한 거친 포인트 클라우드를 예측한다는 점에서 기존 기법과 다릅니다. 이러한 포인트는 3D-2D 대응을 설정하는 데 도움이 되며, 이를 통해 차별적인 PnP(Perspective-n-Point) 솔버를 사용하여 포즈를 풀 수 있습니다.
- 장점: 이 접근 방식은 다른 방식에서 사용하는 프레임 간 씬 흐름과 달리 겹치는 부분이 거의 없는 희소 뷰 입력에 더 적합합니다. 트랜스포머 기반 모델은 확장성이 뛰어나며 다양한 대규모 데이터 세트에 대해 훈련되어 견고성과 일반화가 우수합니다.

다른 작업과의 차별성

- 고유한 특징: 카메라 포즈와 3D NeRF 재구성을 동시에 복구하는 단일 단계 추론 파이프라인으로 차별화됩니다. 이는 2단계 프로세스가 필요하거나 사람의 이미지와 같은 특정 유형의 이미지를 위해 설계된 다른 방법과는 대조적입니다. 이 접근 방식을 사용하면 희소 뷰 입력을 더 잘 처리할 수 있으며 확장성과 일반화 기능이 향상됩니다.

![파이프라인 개요. 포즈가 잡히지 않은 희소 입력 이미지가 주어지면 대형 트랜스포머 모델을 사용하여 삼면 NeRF를 재구성하는 동시에 모든 소스 뷰의 상대적인 카메라 포즈를 기준 뷰에 대해 추정합니다. 훈련 중에 삼면체 토큰은 실사 카메라 포즈를 사용하여 새로운 시점의 렌더링 손실로 감독됩니다. 카메라 등록의 경우 카메라 포즈를 직접 회귀시키는 대신 이미지 토큰을 포인트 클라우드(오른쪽 상단) 형태의 거친 3D 지오메트리에 매핑하여 패치 중심에 해당하는 각 패치 토큰에서 3D 포인트를 예측합니다. 그런 다음 차등 가능한 PnP 솔버를 사용하여 이러한 예측된 3D-2D 대응으로부터 카메라 포즈를 얻습니다(3.3절).](PF-LRM%20Pose-Free%20Large%20Reconstruction%20Model%20for%20Jo%2062453518308043e69dd27bd1abef8763/Untitled%201.png)

파이프라인 개요. 포즈가 잡히지 않은 희소 입력 이미지가 주어지면 대형 트랜스포머 모델을 사용하여 삼면 NeRF를 재구성하는 동시에 모든 소스 뷰의 상대적인 카메라 포즈를 기준 뷰에 대해 추정합니다. 훈련 중에 삼면체 토큰은 실사 카메라 포즈를 사용하여 새로운 시점의 렌더링 손실로 감독됩니다. 카메라 등록의 경우 카메라 포즈를 직접 회귀시키는 대신 이미지 토큰을 포인트 클라우드(오른쪽 상단) 형태의 거친 3D 지오메트리에 매핑하여 패치 중심에 해당하는 각 패치 토큰에서 3D 포인트를 예측합니다. 그런 다음 차등 가능한 PnP 솔버를 사용하여 이러한 예측된 3D-2D 대응으로부터 카메라 포즈를 얻습니다(3.3절).

### 3 METHOD

이 방법의 목표는 카메라 포즈를 알 수 없는 이미지 세트에서 물체의 3D 모델을 재구성하는 것입니다. 하나의 이미지를 레퍼런스 뷰로 지정하고, 이 방법은 3D 삼면 신경 방사 필드(NeRF)와 이 레퍼런스에 대한 다른 이미지의 상대적인 카메라 포즈를 예측합니다. 이는 이미지와 삼면 NeRF 토큰을 모두 처리하는 트랜스포머 모델을 사용하여 수행됩니다.

프로세스 세부 사항

- 이미지 토큰화 및 보기 인코딩: 이미지는 사전 학습된 비전 트랜스포머를 사용하여 토큰화됩니다. 각 이미지는 토큰으로 분류되며, 참조 이미지 토큰과 소스 이미지 토큰을 구분하는 학습 가능한 기능이 추가됩니다. 또한 이 모델은 각 뷰에 대한 이해를 높이기 위해 카메라 내재성을 통합합니다.
- 트라이플레인 토큰화 및 위치 임베딩: 삼면 NeRF가 토큰화되고 위치 임베딩이 학습되어 이 토큰을 해당 삼면 토큰에 매핑함으로써 정보 교환 프로세스를 지원합니다.
- 단일 스트림 트랜스포머 모델: 이 모델은 처리를 위해 일련의 트랜스포머 레이어(자체 주의 및 MLP 레이어)를 사용하여 삼면과 이미지 토큰을 입력으로 받습니다. 출력 토큰은 볼류메트릭 NeRF 렌더링과 뷰별 포즈 예측에 사용됩니다.

차별적인 볼륨 렌더링을 통한 NeRF 감독

- 신경 차등 볼륨 렌더링 기법을 사용하여 삼면 NeRF를 감독합니다. 이 프로세스에는 트라이플레인 NeRF에서 새로운 시점으로 이미지를 렌더링하고 감독을 위해 L2 및 LPIPS 렌더링 손실을 조합하여 사용하는 것이 포함됩니다.

차별적인 PnP 솔버를 통한 포즈 예측

- 이 방법은 NeRF 토큰에 의해 컨텍스트화된 뷰별 이미지 패치 토큰에서 상대적인 카메라 포즈를 추정합니다. 여기에는 이미지 패치 토큰에서 각 뷰의 거친 포인트 클라우드를 예측하고, 이를 이미지 패치 센터와 결합하여 미분 가능한 PnP 솔버를 사용하여 포즈 예측에 필요한 3D-2D 대응을 설정하는 작업이 포함됩니다.

손실 함수 및 구현 세부 사항

- 훈련 목표는 렌더링 손실과 뷰별 거친 지오메트리 예측, 불투명도 예측, 차등 PnP 손실과 관련된 손실을 결합하는 것입니다. 36개의 자체 주의 레이어로 구성된 이 모델은 삼면체 모양을 예측하고 훈련에 AdamW 옵티마이저를 사용하므로 상당한 양의 컴퓨팅 리소스(약 1주일 동안 128개의 Nvidia A100 GPU)가 필요합니다.

요약하면, 이 방법은 이미지 및 삼면체 NeRF 토큰화, 단일 스트림 트랜스포머 처리, 차별적인 렌더링 및 PnP 해결 기술의 혁신적인 사용을 통합하여 카메라 포즈를 알 수 없는 이미지에서 3D 재구성을 위한 효율적인 접근 방식을 제공합니다.

### 4 EXPERIMENTS

4.1 실험 설정

훈련 데이터 세트

- 이 모델은 오브제버스의 렌더링과 MVImgNet의 실제 포즈 캡처로 구성된 대규모 멀티뷰 포즈 데이터 세트(총 약 100만 개의 오브젝트)로 훈련됩니다.

평가 데이터 세트

- 일반화 테스트를 위해 OmniObject3D, Google Scanned Objects(GSO), Amazon 버클리 오브젝트(ABO), Common Objects 3D(CO3D), DTU 등 여러 데이터 세트가 사용되었습니다. 이러한 데이터 세트는 희박한 이미지 입력으로 모델의 성능을 평가할 수 있는 다양한 오브젝트와 조건을 제공합니다.

기준선 및 지표

- 모델의 성능은 포즈 추정 및 재구성 품질에 중점을 두고 FORGE, RelPose++, HLoc과 같은 다양한 기준선과 비교됩니다. 사용된 메트릭에는 이미지 품질 평가를 위한 쌍별 상대 포즈 오류, PSNR, SSIM 및 LPIPS가 포함됩니다.

4.2 실험 결과

4.2.1 포즈 예측 품질

- 이 모델은 모든 데이터 세트에서 포즈 예측 정확도와 렌더링 품질이 기준선을 크게 뛰어넘는 우수한 성능을 보여주었습니다. 여기에는 다른 방식에 비해 회전 오차가 크게 줄어들어 스파스 뷰 시나리오에서 견고함을 보여주었습니다.

![보이지 않는 OmniObject3D(Wu 외, 2023), GSO(Downs 외, 2022), ABO(Collins 외, 2022) 데이터세트에 대한 교차 데이터세트 일반화. 예측된 포즈(두 번째 열)에서 예측된 NeRF의 렌더링은 입력된 비포즈 이미지(첫 번째 열)와 거의 일치하여 두 예측의 뛰어난 정확도를 보여줍니다. 또한 재구성된 NeRF의 새로운 뷰 렌더링(네 번째 열)과 그에 해당하는 실측 데이터(세 번째 열)를 통해 고품질 NeRF 재구성을 보여주며, RGBD 융합을 사용하여 NeRF에서 렌더링된 멀티뷰 RGBD 이미지를 융합하여 메시를 쉽게 추출(마지막 열)할 수 있음을 보여줍니다(Curless & Levoy, 2023). 더 많은 시각적 예는 부록의 그림 7에서 확인할 수 있습니다.](PF-LRM%20Pose-Free%20Large%20Reconstruction%20Model%20for%20Jo%2062453518308043e69dd27bd1abef8763/Untitled%202.png)

보이지 않는 OmniObject3D(Wu 외, 2023), GSO(Downs 외, 2022), ABO(Collins 외, 2022) 데이터세트에 대한 교차 데이터세트 일반화. 예측된 포즈(두 번째 열)에서 예측된 NeRF의 렌더링은 입력된 비포즈 이미지(첫 번째 열)와 거의 일치하여 두 예측의 뛰어난 정확도를 보여줍니다. 또한 재구성된 NeRF의 새로운 뷰 렌더링(네 번째 열)과 그에 해당하는 실측 데이터(세 번째 열)를 통해 고품질 NeRF 재구성을 보여주며, RGBD 융합을 사용하여 NeRF에서 렌더링된 멀티뷰 RGBD 이미지를 융합하여 메시를 쉽게 추출(마지막 열)할 수 있음을 보여줍니다(Curless & Levoy, 2023). 더 많은 시각적 예는 부록의 그림 7에서 확인할 수 있습니다.

4.2.2 재구성 품질

- 이 모델을 통해 재구성된 NeRF는 PSNR 측면에서 FORGE를 능가하는 높은 품질을 보여주었습니다. 이는 모델이 정확한 포즈를 예측할 뿐만 아니라 고품질의 3D 재구성을 생성한다는 것을 나타냅니다.

4.3 견고성 테스트

- 가변 입력 뷰 수: 이 모델은 다양한 수의 입력 뷰에서도 성능을 유지하여 적응력을 보여줍니다.
불완전한 세분화 마스크: 모델은 세분화 마스크에서 어느 정도의 노이즈에 대해 견고성을 보였지만, 노이즈가 매우 심한 마스크에서는 성능이 떨어졌습니다.

4.4 제거 연구

- 다양한 버전의 모델을 테스트하여 모델 크기, NeRF 예측 및 포즈 예측의 영향을 평가했습니다. 더 큰 모델이 더 나은 성능을 보였으며, 이는 규모가 도움이 된다는 것을 나타냅니다. NeRF 예측을 제거하면 불안정성이 개선되었고, 관절 포즈 예측은 3D 재구성 품질을 향상시켰습니다.

![GSO 데이터에 대한 제거 연구(Downs et al., 2022). '우리 방식(L)'은 가장 선명한 디테일로 최고의 재구성 품질을 제공하는 반면, 모델 크기를 줄이면('우리 방식(S)') 텍스처가 흐려지는 결과를 초래합니다. 포즈 예측 분기를 더 제거하면('포즈 예측이 없는 우리 (S)') 텍스처가 더 나빠집니다. 다른 제거 변형, 특히 포즈 예측이 없는 변형의 공정한 비교를 위해 입력 이미지에 해당하는 동일한 실측 포즈(예측된 포즈가 아닌)를 사용하여 재구성된 NeRF를 렌더링합니다.](PF-LRM%20Pose-Free%20Large%20Reconstruction%20Model%20for%20Jo%2062453518308043e69dd27bd1abef8763/Untitled%203.png)

GSO 데이터에 대한 제거 연구(Downs et al., 2022). '우리 방식(L)'은 가장 선명한 디테일로 최고의 재구성 품질을 제공하는 반면, 모델 크기를 줄이면('우리 방식(S)') 텍스처가 흐려지는 결과를 초래합니다. 포즈 예측 분기를 더 제거하면('포즈 예측이 없는 우리 (S)') 텍스처가 더 나빠집니다. 다른 제거 변형, 특히 포즈 예측이 없는 변형의 공정한 비교를 위해 입력 이미지에 해당하는 동일한 실측 포즈(예측된 포즈가 아닌)를 사용하여 재구성된 NeRF를 렌더링합니다.

4.5 애플리케이션

- 이 모델은 포즈가 잡히지 않은 몇 장의 이미지에서 NeRF를 재구성할 수 있기 때문에 텍스트/이미지-3D 생성 같은 애플리케이션에 적합합니다. 이는 멀티뷰 이미지 생성기 또는 텍스트-이미지 생성기를 사용하여 파이프라인에 통합한 다음 PF-LRM으로 3D 재구성을 수행할 수 있습니다.

요약하자면, 이 실험은 다양한 까다로운 데이터 세트와 시나리오에서 모델의 견고성, 포즈 추정의 정확성, 고품질 3D 재구성을 보여줍니다. 또한 다양한 입력 조건에 대한 확장성과 적응성은 3D 생성 및 재구성에 대한 실용적인 응용 가능성을 보여줍니다.

### 5 CONCLUSION

카메라 파라미터를 공동으로 추정하고 3D 형상을 신경 방사 필드(NeRF)로 재구성하기 위해 대형 트랜스포머 기반 모델을 사용하는 3D 재구성에 대한 혁신적인 접근 방식을 제시합니다. 이 모델의 혁신적인 자기 주의 사용은 삼면체 토큰과 이미지 패치 토큰 간의 효과적인 통신을 가능하게 합니다. 그 결과, 차별화 가능한 원근법(PnP) 솔버를 사용하여 포즈 추정을 위한 NeRF 재구성 품질이 향상되고 표면 포인트를 정확하게 예측할 수 있습니다. 광범위한 멀티뷰 데이터 세트로 훈련된 이 모델은 포즈 예측 정확도와 재구성 품질에서 기준선 방법을 크게 능가하며 텍스트/이미지-3D 생성에 적용할 수 있는 가능성을 보여줍니다.

한계와 향후 방향

- 배경 정보: 현재 모델은 카메라 포즈 추정을 위한 풍부한 정보 소스가 될 수 있는 배경 단서를 활용하지 않습니다. 향후 작업에는 공간 왜곡이 있는 배경을 처리하는 것이 포함될 수 있습니다.
- 뷰 종속 효과: 이 모델은 포인트별 색상 모델링 접근 방식 때문에 뷰 종속 효과를 고려하지 않습니다. 스파스 뷰에서 뷰에 따라 달라지는 모양을 처리하는 것은 향후 연구할 영역입니다.
- 해상도 향상: 거친 모델링 또는 기타 고용량 표현을 통해 예측된 삼면 NeRF의 해상도를 개선하여 세부적인 지오메트리와 텍스처 재구성을 향상시킬 수 있는 가능성이 있습니다.
- 카메라 고유 특성: 이 모델은 알려진 카메라 특성을 가정합니다. 향후에는 카메라 고유 특성을 예측하는 기능도 개발될 수 있습니다.
- 포즈 감독 없이 훈련: 현재 이 모델은 훈련 중에 실측 포즈 감독이 필요합니다. 흥미로운 방향은 이러한 요구 사항을 제거하여 자연 상태의 비디오를 포함한 더 광범위한 훈련 데이터를 활용하는 것입니다.

윤리 및 환경 문제
얼굴과 손 영역의 해상도가 제한적이기는 하지만 사람의 이미지를 재구성하는 모델의 능력은 특히 저작권 및 개인정보 보호와 관련하여 윤리적 고려 사항을 제기합니다. 또한 이 모델을 훈련하는 데 필요한 상당한 컴퓨팅 리소스는 환경 문제를 야기할 수 있으며, 이는 대규모 언어 모델에서 볼 수 있는 추세를 반영합니다.

재현성

모델의 설계, 학습 아키텍처 및 손실 함수에 대한 자세한 정보와 함께 부록에서 확장된 세부 정보를 제공합니다. 이러한 투명성은 연구의 재현성을 돕고 구현 세부 사항에 대한 명확성을 보장하는 것을 목표로 합니다.

요약하자면, 이 연구는 3D 재구성의 중요한 진전으로, 공동 카메라 포즈 추정 및 형상 재구성을 위한 강력하고 확장 가능한 솔루션을 제공합니다. 이 연구는 유망한 결과를 보여줌과 동시에, 현재의 한계를 해결하고 모델의 기능과 응용 분야를 확장할 수 있는 추가 연구와 개선의 길을 열어줍니다.