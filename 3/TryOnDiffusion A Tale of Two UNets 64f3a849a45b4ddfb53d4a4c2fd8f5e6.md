# TryOnDiffusion: A Tale of Two UNets

[https://arxiv.org/abs/2306.08276](https://arxiv.org/abs/2306.08276)

![TryOnDiffusion은 의복 세부사항을 1024×1024 해상도에서 보존하면서 상당한 체형 및 자세 변경을 동반한 의류 입어보기 결과를 생성합니다. 결과의 모서리에는 입력 이미지(대상 사람과 다른 사람이 입은 의류)가 표시됩니다.](TryOnDiffusion%20A%20Tale%20of%20Two%20UNets%2064f3a849a45b4ddfb53d4a4c2fd8f5e6/Untitled.png)

TryOnDiffusion은 의복 세부사항을 1024×1024 해상도에서 보존하면서 상당한 체형 및 자세 변경을 동반한 의류 입어보기 결과를 생성합니다. 결과의 모서리에는 입력 이미지(대상 사람과 다른 사람이 입은 의류)가 표시됩니다.

### 1. Introduction

이 논문에서는 체형, 포즈, 시야의 변화가 큰 경우에도 높은 해상도와 디테일로 의상이 사람에게 어떻게 보일지 시각화하는 새로운 방법인 TryOnDiffusion을 소개합니다. 이 기술은 체형과 포즈의 변화를 정확하게 표현하는 방식으로 옷을 조정해야 하는 기존 가상 의류 트라이온 기술의 문제를 해결합니다. 기존 방식은 픽셀 변위를 추정하고 픽셀을 워프한 다음 블렌딩하기 때문에 비현실적인 왜곡이나 오류가 발생하는 경우가 많습니다.

TryOnDiffusion은 교차 주의 기능을 통해 통신하는 두 개의 서브 유니넷을 결합하는 새로운 아키텍처인 병렬 유니넷(Parallel-UNet)을 활용하여 더 나은 솔루션을 제공합니다. 이 방법은 암시적 워핑을 사용하고 워핑과 블렌딩 프로세스를 결합하여 최종 결과물을 개선합니다. 이 방법은 오클루전이 심하고 포즈 차이가 극심한 경우에 효과적입니다.

TryOnDiffusion은 의상의 디테일을 유지하면서 다양하고 복잡한 신체 포즈와 모양에 대해 1024×1024 해상도로 고품질 결과를 생성할 수 있습니다. 이 고해상도를 달성하기 위해 캐스케이드 디퓨전 모델을 사용합니다. 비전문가 15명을 대상으로 한 사용자 연구에서 TryOnDiffusion은 다른 상위 3개 방법과 비교했을 때 92.72%의 확률로 가장 우수한 방법으로 선택되었습니다.

### 2. Related Work

이 논문의 '관련 작업' 섹션에서는 두 가지 주요 영역에 대해 설명합니다: 이미지 기반 가상 체험 및 확산 모델.

![전체 파이프라인 (상단): 전처리 단계에서 대상 사람은 사람 이미지에서 분리되어 "의복을 고려하지 않은 RGB" 이미지를 만들고, 대상 의류는 의류 이미지에서 분리되고, 사람과 의류 이미지 모두에 대한 자세가 계산됩니다. 이러한 입력은 128×128 Parallel-UNet(핵심 기여)으로 전달되어 128 × 128 입어보기 이미지를 만들며 이는 추가로 256×256 Parallel-UNet에 입어보기 조건 입력과 함께 입력으로 전달됩니다. 256×256 Parallel-UNet에서 출력되는 결과는 1024×1024 이미지를 만드는 표준 초고해상도 확산에 전달됩니다. 하단에는 128×128 Parallel-UNet의 구조가 시각화되어 있습니다. 세부사항은 본문을 참조하십시오. 256×256 Parallel-UNet은 128 버전과 유사하며, 완전성을 위해 보충 자료에서 제공됩니다.](TryOnDiffusion%20A%20Tale%20of%20Two%20UNets%2064f3a849a45b4ddfb53d4a4c2fd8f5e6/Untitled%201.png)

전체 파이프라인 (상단): 전처리 단계에서 대상 사람은 사람 이미지에서 분리되어 "의복을 고려하지 않은 RGB" 이미지를 만들고, 대상 의류는 의류 이미지에서 분리되고, 사람과 의류 이미지 모두에 대한 자세가 계산됩니다. 이러한 입력은 128×128 Parallel-UNet(핵심 기여)으로 전달되어 128 × 128 입어보기 이미지를 만들며 이는 추가로 256×256 Parallel-UNet에 입어보기 조건 입력과 함께 입력으로 전달됩니다. 256×256 Parallel-UNet에서 출력되는 결과는 1024×1024 이미지를 만드는 표준 초고해상도 확산에 전달됩니다. 하단에는 128×128 Parallel-UNet의 구조가 시각화되어 있습니다. 세부사항은 본문을 참조하십시오. 256×256 Parallel-UNet은 128 버전과 유사하며, 완전성을 위해 보충 자료에서 제공됩니다.

이미지 기반 가상 시착:
여기에는 한 쌍의 이미지를 기반으로 소스 의상을 착용한 대상 인물의 이미지를 생성하는 것이 포함됩니다. 다양한 방법이 제안되었지만, 일반적으로 모델을 워핑하고 블렌딩하는 2단계 프로세스를 거칩니다. 최초의 중요한 연구인 VITON은 박판-스플라인(TPS) 워핑을 사용하는 파이프라인을 도입했으며, 이후 ClothFlow, VITON-HD, HR-VITON, SDAFN과 같은 다양한 연구를 통해 개선되었습니다. 그러나 이러한 방법들은 종종 정렬 불량 문제에 직면합니다. TryOnGAN은 포즈 조건이 적용된 StyleGAN2를 페어링되지 않은 패션 이미지로 훈련하여 이를 극복하지만, 디테일이 있거나 패턴이 있는 의복의 경우 의복 디테일이 손실되는 경우가 많습니다. 이 논문에서는 단일 네트워크 패스에서 암시적 워핑과 블렌딩을 수행하여 의복 디테일 보존을 개선하는 새로운 아키텍처를 제안합니다.

확산 모델:
확산 모델은 훈련 안정성과 모드 커버리지로 인해 다양한 이미지 생성 작업에서 탁월한 성능을 발휘하는 강력한 생성 모델 제품군입니다. 고해상도, 컬러화, 새로운 시각 합성, 텍스트-이미지 생성 작업에서 우수한 성능을 보였습니다. 그러나 이러한 모델의 대부분은 채널별 연결이 있는 기존의 UNet 아키텍처를 사용하며, 이는 입력 및 출력 픽셀이 완벽하게 정렬된 이미지 간 변환에 적합합니다. 그러나 이 방법은 가상 시착의 경우처럼 의류 뒤틀림과 같이 매우 비선형적인 변환이 필요한 작업에는 직접 적용할 수 없습니다. 이 논문에서는 이러한 문제를 극복하기 위해 교차 어텐션을 통해 암시적 의류 워핑을 가능하게 하는 병렬-유넷(Parallel-UNet) 아키텍처를 제안합니다.

### 3. Method

이 백서의 '방법' 섹션에서는 제안된 가상 시착 시스템의 프로세스를 설명하며, 이 시스템은 사람(Ip)과 다른 사람(Ig)이 착용한 의상의 이미지를 촬영하고 해당 의상을 착용한 사람의 새로운 이미지(Itr)를 생성합니다.

전처리: 여기에는 사람 및 의상 이미지 모두에 대한 휴먼 파싱 맵과 2D 포즈 키포인트를 예측하는 작업이 포함됩니다. 의상 이미지의 경우, 의상은 파싱 맵을 사용하여 분할됩니다. 사람 이미지의 경우 의복에 구애받지 않는 RGB 이미지가 생성됩니다. 저자들은 보다 적극적인 방법을 사용하여 의상 정보를 제거합니다. 또한 포즈 키포인트를 네트워크에 입력하기 전에 정규화합니다.

확산 모델: 확산 모델은 반복적인 노이즈 제거 과정을 통해 학습하는 일종의 생성 모델입니다. 데이터 샘플을 가우시안 노이즈로 변환하는 포워드 프로세스와 이 노이즈를 원래 샘플로 다시 변환하는 학습 가능한 리버스 프로세스로 구성됩니다. 조건부 확산 모델은 다양한 유형의 입력에 대해 작동하도록 훈련할 수 있습니다.

시험용 캐스케이드 확산 모델: 저자가 제안한 솔루션에는 캐스케이드 확산 모델이 포함됩니다. 이는 하나의 기본 확산 모델과 두 개의 초해상도 확산 모델로 구성됩니다. 기본 모델은 트라이온 조건부 입력에서 128x128 트라이온 결과를 생성합니다. 그런 다음 초해상도 모델이 이 결과의 해상도를 높입니다. 무작위 가우시안 노이즈가 노이즈 컨디셔닝 증강의 한 형태로 트라이온 조건부 입력에 추가됩니다. 추론하는 동안 모델은 이전 단계의 출력을 가져와 해상도를 높이고 1024x1024의 해상도로 최종 시도 결과를 합성합니다.

"Parallel-UNet" 하위 섹션에서는 저자가 개발한 가상 체험 시스템의 핵심 구성 요소에 대해 설명합니다.

병렬-유넷: 128x128 및 256x256 병렬-유넷 모델은 노이즈가 있는 이미지 zt와 트라이온 조건부 입력 ctryon을 노이즈 증강 수준과 함께 처리합니다. 256x256 모델에는 128x128 트라이온 결과도 입력으로 포함됩니다.

Parallel-UNet의 두 가지 핵심 설계 요소는 암시적 워핑과 단일 패스에서 워핑과 블렌딩의 조합입니다:

암시적 워핑: 의류 워핑과 관련된 복잡한 변형을 처리하기 위해 저자는 교차 주의 메커니즘을 사용할 것을 제안합니다. 이 메커니즘은 대상 인물과 원본 의상 간의 유사성을 계산하여 시착 작업에 대한 대응을 표현하는 학습 가능한 방법을 제공합니다. 주의 지도는 노이즈가 있는 이미지와 분할된 의상 이미지의 평면화된 특징에서 생성됩니다. 주의 메커니즘은 모델이 다양한 표현 하위 공간에서 학습할 수 있도록 다중 헤드로 만들어졌습니다.

단일 패스로 워프와 블렌드를 결합합니다: 이는 의복과 사람을 각각 처리하는 두 개의 개별 UNet을 통해 이루어집니다. 사람 UNet은 의상에 구애받지 않는 RGB 이미지와 노이즈가 있는 이미지를 입력으로 받습니다. 의복-UNet은 세그먼트화된 의복 이미지를 입력으로 받고 교차 주의를 통해 의복 특징을 대상 이미지에 융합합니다.

또한 저자들은 사람과 의상의 포즈가 워프 및 블렌드 프로세스를 안내하는 데 필수적이라는 점에 주목합니다. 포즈 임베딩을 계산하기 위해 선형 레이어에 공급된 포즈는 주의 메커니즘을 사용하여 사람-유넷에 융합됩니다. 임베딩은 모든 스케일에 걸쳐 FiLM을 사용하여 변조됩니다.

### 4. Experiments

이 섹션의 '실험'에서는 가상 체험 시스템을 테스트하는 데 사용되는 데이터 및 평가 프로세스에 대해 설명합니다.

![TryOnGAN [26], SDAFN [2] 및 HR-VITON [25]과의 비교. 첫 번째 열은 입력 (사람, 의류) 쌍을 보여줍니다. TryOnDiffusion은 극단적인 체형 및 자세 변경 하에서도 텍스트와 기하학적 패턴을 포함한 의복 세부사항을 잘 변형시킵니다.](TryOnDiffusion%20A%20Tale%20of%20Two%20UNets%2064f3a849a45b4ddfb53d4a4c2fd8f5e6/Untitled%202.png)

TryOnGAN [26], SDAFN [2] 및 HR-VITON [25]과의 비교. 첫 번째 열은 입력 (사람, 의류) 쌍을 보여줍니다. TryOnDiffusion은 극단적인 체형 및 자세 변경 하에서도 텍스트와 기하학적 패턴을 포함한 의복 세부사항을 잘 변형시킵니다.

데이터 세트: 개발자는 4백만 개의 샘플로 구성된 훈련 데이터 세트를 사용했으며, 각 샘플에는 같은 사람이 다른 포즈로 같은 의상을 입은 두 개의 이미지가 포함되어 있습니다. 테스트 데이터 세트는 6,000개의 짝을 이루지 않은 샘플로 구성되었으며, 각기 다른 의상을 다른 포즈로 착용한 여러 사람의 이미지가 포함되어 있습니다. 모든 이미지는 감지된 2D 사람 포즈를 기준으로 1024x1024로 잘라내고 크기를 조정했습니다. 데이터 세트에는 다양한 포즈, 체형, 피부 톤, 다양한 텍스처 패턴의 의상을 입은 남성과 여성이 모두 포함되었습니다. 또한 저자들은 VITON-HD 데이터 세트에서 시스템을 테스트했습니다.

![VITON-HD 데이터셋 [6]에서 최신 기술 방법과의 비교. 모든 방법은 동일한 4M 데이터셋에서 훈련되었고 VITON-HD에서 테스트되었습니다.](TryOnDiffusion%20A%20Tale%20of%20Two%20UNets%2064f3a849a45b4ddfb53d4a4c2fd8f5e6/Untitled%203.png)

VITON-HD 데이터셋 [6]에서 최신 기술 방법과의 비교. 모든 방법은 동일한 4M 데이터셋에서 훈련되었고 VITON-HD에서 테스트되었습니다.

![Ablation 연구의 정성적 결과. 왼쪽: 암시적 변형을 위한 교차 주의 대 비교. 오른쪽: 변형과 혼합을 위한 한 개의 네트워크 대 두 개의 네트워크. 초록 상자가 강조하는 차이를 보려면 확대하십시오.](TryOnDiffusion%20A%20Tale%20of%20Two%20UNets%2064f3a849a45b4ddfb53d4a4c2fd8f5e6/Untitled%204.png)

Ablation 연구의 정성적 결과. 왼쪽: 암시적 변형을 위한 교차 주의 대 비교. 오른쪽: 변형과 혼합을 위한 한 개의 네트워크 대 두 개의 네트워크. 초록 상자가 강조하는 차이를 보려면 확대하십시오.

구현 세부 사항: 저자들은 아담 옵티마이저를 사용하여 각각 500,000회씩 세 가지 모델을 학습시켰으며, 학습 속도는 처음 10,000회 반복 동안 선형적으로 증가했습니다. 훈련 중에 '컨디셔닝 드롭아웃'이라는 기법을 사용했으며, 다양한 단계 수에 대해 DDPM 방법을 사용하여 모델을 샘플링했습니다. 노이즈 컨디셔닝 증강 수준은 훈련 중에 균일한 분포에서 샘플링되었으며 추론 중에는 일정한 값으로 설정되었습니다.

다른 방법과의 비교: 저자들은 이 방법을 다른 세 가지 방법과 비교했습니다: TryOnGAN, SDAFN, HR-VITON이며, 공정한 비교를 위해 데이터 세트에 대해 세 가지 방법을 모두 다시 훈련했습니다. 각 방법의 결과는 원래의 해상도로 표시되었습니다.

정량적 비교: 저자들은 페어링되지 않은 테스트 데이터 세트로 인해 프레쳇 시작 거리(FID) 및 커널 시작 거리(KID) 메트릭을 사용하여 시스템을 평가했습니다. 이 방법은 자사 데이터 세트와 VITON-HD 데이터 세트 모두에서 테스트했을 때 다른 방법에 비해 훨씬 더 나은 성능을 보였습니다.

'사용자 연구' 섹션에서는 가상 체험 시스템을 평가하는 데 사용되는 추가 평가 방법에 대해 설명합니다.

사용자 연구: 시스템에 대한 사람의 피드백을 수집하기 위해 두 가지 연구가 수행되었습니다. 첫 번째 연구에서는 비전문가 평가자 15명이 무작위로 선정된 2804개의 입력 쌍을 사용하여 시스템을 평가했습니다. 병렬-유넷 방식이 92.72%의 입력에서 가장 우수한 것으로 평가되었습니다. 두 번째 연구에서는 동일한 프로세스를 반복했지만 좀 더 까다로운 포즈가 포함된 2,000개의 입력 쌍을 사용했습니다. 이 경우 이 시스템은 95.8%의 입력에서 최고로 평가되었습니다.

정성적 비교: 시스템과 기준선의 시각적 비교는 그림 3과 4에 나와 있습니다. 비교에 사용된 입력 쌍은 신체 포즈, 모양, 복잡한 의복 소재에 차이가 있습니다. 저자들은 다른 모델은 의상의 텍스처 패턴을 유지하는 데 어려움을 겪거나 뒤틀림 아티팩트가 발생하는 반면, 병렬-유넷 방식은 까다로운 포즈나 복잡한 소재에서도 소스 의상의 미세한 디테일을 보존하고 인물과 매끄럽게 블렌딩한다는 사실을 발견했습니다.

절제 연구: 저자들은 두 가지 제거 연구를 수행했는데, 첫 번째 연구는 암시적 뒤틀림에 대한 교차 주의와 연결의 사용을 비교했습니다. 그 결과, 크로스 어텐션이 신체 포즈와 형태가 크게 변하는 상황에서도 의상의 디테일을 더 잘 보존하는 것으로 나타났습니다. 두 번째 제거 연구에서는 단일 네트워크 패스에서 워핑과 블렌딩을 결합하는 것과 두 작업을 순차적으로 처리하는 것을 비교했습니다. 그 결과 두 작업을 순차적으로 처리하면 의상 경계 근처에 아티팩트가 발생하는 반면, 단일 네트워크에서는 더 멋진 블렌딩이 생성된다는 사실을 발견했습니다.

한계: 긍정적인 결과에도 불구하고 이 방법에는 전처리 과정에서 세분화 맵과 포즈 추정에 오류가 있을 경우 의상이 누출될 수 있는 아티팩트가 발생할 수 있는 등의 한계가 있습니다. 다른 한계로는 의류에 구애받지 않는 RGB를 사용하여 신원을 불완전하게 표현하고, 복잡한 배경에서 성능이 저하될 가능성이 있으며, 전신 시착 실험이 부족하다는 점 등이 있습니다. 실패 사례의 몇 가지 예는 그림 6에서 확인할 수 있습니다.

![오류가 있는 의복 분할(왼쪽) 또는 의복이 의복-중립 RGB 이미지로 유출됨(오른쪽)으로 인해 실패가 발생합니다.](TryOnDiffusion%20A%20Tale%20of%20Two%20UNets%2064f3a849a45b4ddfb53d4a4c2fd8f5e6/Untitled%205.png)

오류가 있는 의복 분할(왼쪽) 또는 의복이 의복-중립 RGB 이미지로 유출됨(오른쪽)으로 인해 실패가 발생합니다.

이러한 한계에도 불구하고 저자들은 그림 7에서 다양한 인물과 의상을 대상으로 한 TryOnDiffusion의 유망한 결과를 보여주며, 보충 자료에서 더 많은 결과를 확인할 수 있습니다.

![Untitled](TryOnDiffusion%20A%20Tale%20of%20Two%20UNets%2064f3a849a45b4ddfb53d4a4c2fd8f5e6/Untitled%206.png)

### 5. Summary and Future Work

'요약 및 향후 작업' 섹션에서 저자는 사람 이미지와 의상 이미지에서 가상 시착을 합성하는 새로운 방법을 요약합니다. 새로운 체형과 포즈에 대한 워프의 품질과 의복의 보존 측면에서 최신 기술보다 훨씬 뛰어난 결과물을 얻을 수 있었다고 설명합니다. 두 개의 유니넷이 동시에 학습되고 한 유니넷이 교차 주의를 통해 다른 유니넷에 정보를 전송하는 혁신적인 병렬 유니넷 아키텍처가 이러한 고품질 결과의 원동력입니다.

저자들은 이 아키텍처가 일반적인 이미지 편집 애플리케이션에 상당한 영향을 미칠 것으로 보고 있으며, 앞으로 이 분야에 대한 연구를 계속할 계획입니다. 또한 비디오 입력을 처리하도록 아키텍처를 확장하여 이 기술의 응용 가능성을 더욱 넓힐 계획입니다. 이 연구는 가상 트라이온 기술 분야에서 중요한 진전을 이루었으며 향후 이 분야의 연구 및 개발을 위한 토대를 마련했습니다.