# LoRA: Low-Rank Adaptation of Large Language Models

[https://arxiv.org/pdf/2106.09685.pdf](https://arxiv.org/pdf/2106.09685.pdf)

[https://github.com/huggingface/blog/blob/main/lora.md](https://github.com/huggingface/blog/blob/main/lora.md)

간단히 말해, 자연어 처리 모델은 많은 데이터를 학습하여 특정 작업에 맞게 조정됩니다. 1,750억 개의 매개 변수가 있는 GPT-3처럼 모델이 커질수록 미세 조정이 더 어려워지고 비용이 많이 듭니다. 이 문제를 해결하기 위해 연구원들은 원래 모델 가중치를 고정하고 모델의 각 레이어에 더 작고 학습 가능한 행렬을 추가하는 방법인 저순위 적응(LoRA)을 제안합니다. 이렇게 하면 학습 가능한 파라미터의 수와 메모리 요구 사항이 크게 줄어듭니다.

LoRA는 더 적은 수의 파라미터와 더 빠른 학습에도 불구하고 여러 모델에서 전체 미세 조정과 동등하거나 그 이상의 성능을 발휘합니다. 또한 다른 방법과 달리 추론하는 동안 추가 시간이 추가되지 않습니다. 연구원들은 LoRA가 언어 모델을 조정하는 데 효과적인 이유에 대해서도 탐구합니다. 연구원들은 LoRA를 PyTorch 모델과 통합하는 데 도움이 되는 패키지를 출시하고 구현과 모델 체크포인트를 제공했습니다.

1 INTRODUCTION

자연어 처리에서는 사전 학습된 대규모 언어 모델을 여러 작업에 맞게 조정하는 것이 매우 중요합니다. 미세 조정은 모든 파라미터를 업데이트하지만 GPT-3처럼 파라미터가 많은 모델에는 비효율적입니다. 파라미터를 줄이기 위한 기존 기술은 효율성이나 모델 품질을 희생하는 경우가 많습니다.

![Untitled](LoRA%20Low-Rank%20Adaptation%20of%20Large%20Language%20Models%2032d19804c7aa47d9817b679b375a0cea/Untitled.png)

낮은 순위 적응(LoRA)은 사전 학습된 가중치를 동결한 채 순위 분해 행렬을 최적화하여 일부 고밀도 레이어를 간접적으로 학습하는 새로운 접근 방식입니다. LoRA는 스토리지 및 컴퓨팅 효율이 뛰어나며 여러 가지 장점을 제공합니다:

1. 사전 학습된 모델을 여러 작업에 공유할 수 있으며, 각 작업에 대해 작은 LoRA 모듈을 사용하여 스토리지 및 작업 전환 오버헤드를 줄일 수 있습니다.
2. LoRA는 낮은 순위의 작은 행렬만 최적화되므로 하드웨어 요구 사항을 최대 3배까지 낮춰 학습 효율을 높입니다.
3. 완전히 미세 조정된 모델에 비해 추론 지연 시간이 발생하지 않습니다.
LoRA는 접두사 튜닝과 같은 다른 방법과 결합할 수 있습니다.

전반적으로 LoRA는 모델 품질 저하 없이 다양한 작업에 대규모 언어 모델을 적용할 수 있는 효율적이고 다재다능한 방법입니다.
2 PROBLEM STATEMENT

언어 모델링은 자연어 처리의 일반적인 작업으로, 작업별 프롬프트에 따라 조건부 확률을 최대화하는 작업을 포함합니다. 요약이나 기계 독해와 같은 작업에 GPT-3와 같은 사전 학습된 모델을 적용하는 경우, 기존의 미세 조정 방식은 각 작업에 대해 서로 다른 매개변수 세트를 학습해야 합니다. 이는 특히 대규모 모델의 경우 어려울 수 있습니다.

이 백서에서는 작업별 매개변수를 인코딩하기 위해 낮은 순위 표현을 사용하는 보다 효율적인 접근 방식을 제안합니다. 이 새로운 방법은 학습 가능한 파라미터의 수를 크게 줄여 메모리 및 컴퓨팅 효율을 높입니다. 예를 들어, GPT-3 175B를 사용할 경우 학습 가능한 파라미터 수를 원래 파라미터의 0.01%까지 줄일 수 있습니다.

3 AREN’T EXISTING SOLUTIONS GOOD ENOUGH

대규모 언어 모델을 효율적으로 적용하는 것은 전이 학습의 초점이었습니다. 두 가지 일반적인 전략은 어댑터 레이어를 추가하고 입력 레이어 활성화를 최적화하는 것입니다. 하지만 두 가지 접근 방식 모두 특히 지연 시간에 민감한 대규모 시나리오에서는 한계가 있습니다.

어댑터 계층은 파라미터가 거의 없지만 순차적으로 처리되기 때문에 추론 지연 시간이 발생합니다. 배치 크기가 작은 온라인 추론 설정에서는 이로 인해 지연 시간이 눈에 띄게 증가할 수 있습니다. 이 문제는 모델이 여러 디바이스에 분산되어 있는 경우 더욱 심각해집니다.

접두사 튜닝과 같이 입력 프롬프트를 직접 최적화하는 것은 최적화하기 어렵고 학습 가능한 파라미터의 수에 따라 성능이 달라집니다. 또한 적응을 위해 시퀀스 길이의 일부를 예약하면 다운스트림 작업을 처리하는 데 사용할 수 있는 길이가 줄어들어 성능에 영향을 미칠 수 있습니다.

이러한 과제는 성능 저하 없이 대규모 언어 모델 적응의 효율성을 개선하는 것을 목표로 하는 로우랭크 적응(LoRA)과 같은 대체 방법의 필요성을 강조합니다.

4 OUR METHOD

LoRA(로우랭크 적응)는 트랜스포머와 같은 대규모 언어 모델을 효율적으로 적응시키기 위해 고안된 방법입니다. 사전 학습된 가중치 행렬에 대한 업데이트를 저순위 분해로 표현하여 추가적인 추론 대기 시간 없이 메모리와 스토리지 사용량을 줄이는 것이 핵심입니다. 트랜스포머 아키텍처에 적용하면 LoRA는 다운스트림 작업에 대한 주의 가중치를 조정하여 매개변수 효율성을 개선하는 데 중점을 둡니다.

LoRA의 주요 이점은 다음과 같습니다:

1. 메모리 및 스토리지 사용량 대폭 감소: LoRA를 사용하면 VRAM 사용량을 최대 2/3까지 줄일 수 있어 더 적은 수의 GPU로 더 효율적인 트레이닝이 가능하고 I/O 병목 현상을 줄일 수 있습니다.
2. 더 빠른 모델 전환: LoRA를 사용하면 작업별 가중치를 빠르게 교체할 수 있으므로 다양한 맞춤형 모델을 온디맨드 방식으로 배포할 수 있습니다.
3. 훈련 중 속도 향상: LoRA는 대부분의 파라미터에 대해 기울기 계산이 필요하지 않기 때문에 전체 미세 조정에 비해 훈련 속도가 25% 향상될 수 있습니다.

하지만 LoRA는 단일 포워드 패스에서 A와 B 매트릭스가 다른 여러 작업에 대한 입력을 일괄 처리할 수 없다는 등의 몇 가지 제한 사항이 있습니다. 이러한 한계에도 불구하고 LoRA는 성능 저하 없이 대규모 언어 모델을 특정 작업에 맞게 조정할 수 있는 보다 효율적인 방법을 제공합니다.

5 EMPIRICAL EXPERIMENTS

연구원들은 RoBERTa, DeBERTa, GPT-2, GPT-3 등 다양한 모델과 작업에서 LoRA의 성능을 평가합니다. 미세 튜닝(FT), 바이어스 전용(BitFit), 접두사 임베딩 튜닝(PreEmbed), 접두사 레이어 튜닝(PreLayer), 어댑터 튜닝과 같은 다양한 기준선과 비교합니다. 실험은 자연어 이해(NLU)에서 생성(NLG)에 이르는 광범위한 작업을 GLUE, WikiSQL, SAMSum과 같은 데이터 세트를 사용하여 수행합니다.

LoRA는 다른 효율적인 적응 접근 방식과 비교했을 때 경쟁력 있는 성능을 보여줍니다. 세 가지 데이터 세트 모두에서 미세 조정 기준선과 일치하거나 이를 능가하며 확장성과 작업 성능이 더 뛰어납니다. 하지만 모든 방법이 훈련 가능한 파라미터가 많다고 해서 단조롭게 이점을 얻는 것은 아닙니다. 접두사 포함 튜닝이나 접두사 레이어 튜닝과 같은 일부 접근 방식은 너무 많은 특수 토큰을 사용할 때 성능이 저하됩니다. 이는 입력 분포가 사전 학습 데이터 분포에서 더 멀어지기 때문일 수 있습니다.

6 RELATED WORKS

이 연구는 일반 도메인 데이터에 대해 사전 학습되고 특정 작업에 맞게 미세 조정된 GPT-3와 같은 Transformer 언어 모델에 초점을 맞추고 있습니다. 미세 조정은 성능을 극대화하기 위해 모든 매개변수를 재학습하는 경우가 많지만, GPT-3는 크기가 크기 때문에 이 과정이 어렵습니다. 따라서 파라미터를 효율적으로 조정하는 방법이 필요합니다. 한 가지 접근 방식은 신경망의 기존 레이어 사이에 어댑터 레이어를 삽입하는 것입니다. 머신 러닝에서는 낮은 순위 구조가 일반적이며, 저자들은 LoRA라는 낮은 순위 적응 방법을 제안합니다. 이 방법은 다른 텐서 곱 기반 방법과 결합하여 효율성을 향상시킬 수 있습니다. 또한 저자는 미세 조정의 대안으로 입력 단어 임베딩을 최적화하는 접근 방식을 비교합니다.

7 UNDERSTANDING THE LOW-RANK UPDATES

LoRA는 작업 성능에 부정적인 영향을 주지 않으면서 사전 훈련된 트랜스포머에서 훈련 가능한 파라미터를 줄이는 효과적인 방법입니다. 연구진은 어떤 가중치 행렬을 조정해야 하는지, 최적의 조정 행렬에 순위 결핍이 있는지, 조정 행렬과 원래 가중치 사이의 연관성을 이해하는 것을 목표로 삼았습니다.

연구 결과, Wq 가중치와 Wv 가중치를 모두 적용하는 것이 최고의 성능을 이끌어내는 것으로 나타났습니다. 놀랍게도 적응 행렬의 순위(r)가 작아도 성능이 좋았으며, 이는 낮은 순위의 적응 행렬로도 충분하다는 것을 시사합니다. 적응 행렬은 특정 다운스트림 작업에는 중요하지만 사전 학습에서는 강조되지 않은 원래 가중치의 특정 기능을 증폭하는 것으로 보입니다. 이 방법을 사용하면 해석 가능성을 개선하고 실험 속도를 높이며 하드웨어 요구 사항을 줄일 수 있습니다.

8 CONCLUSION AND FUTURE WORK

LoRA는 트랜스포머와 같은 대규모 언어 모델을 위한 효율적인 적응 전략입니다. 일반적으로 이러한 모델을 미세 조정하는 데 드는 하드웨어 및 스토리지 비용을 줄이면서도 고품질의 성능을 유지할 수 있습니다. 또한 LoRA를 서비스로 사용할 경우 작업 전환이 빨라져 더욱 다양한 용도로 활용할 수 있습니다.

LoRA의 향후 연구 방향에는 다른 효율적인 적응 방법과의 결합, 미세 조정 또는 LoRA의 기본 메커니즘 이해, 가중치 행렬을 선택하는 더 나은 방법 찾기, 추가 개선을 위한 적응 행렬(∆W)의 순위 결핍 탐색 등이 포함됩니다. LoRA의 원리는 레이어가 촘촘한 모든 신경망에 적용될 수 있으며, 연구와 적용에 있어 많은 가능성을 열어줍니다.

- Using LoRA for Efficient Stable Diffusion Fine-Tuning
    
    [https://github.com/huggingface/blog/blob/main/lora.md](https://github.com/huggingface/blog/blob/main/lora.md)
    
    LoRA는 GPT-3와 같은 대규모 언어 모델을 보다 효율적으로 미세 조정하기 위해 Microsoft 연구진이 개발한 기술입니다. 이러한 모델은 특정 작업에 맞게 조정하는 데 비용이 많이 들 수 있으므로 이 문제를 해결하기 위해 LoRA가 만들어졌습니다. 이 방법은 사전 학습된 모델 가중치를 동결하고 각 트랜스포머 블록 내에 학습 가능한 레이어를 추가하여 학습 가능한 매개변수의 수와 GPU 메모리 요구량을 줄이는 것입니다.
    
    ![Untitled](LoRA%20Low-Rank%20Adaptation%20of%20Large%20Language%20Models%2032d19804c7aa47d9817b679b375a0cea/Untitled%201.png)
    
    원래 대규모 언어 모델을 위해 개발된 LoRA는 안정적 확산 미세 조정과 같은 다른 영역에서도 사용할 수 있습니다. 이 기술은 이미지 표현과 텍스트 프롬프트를 연결하는 교차 주의 레이어에 적용될 수 있습니다. 요컨대, LoRA는 이미지와 텍스트 간의 관계를 구축하는 데 도움이 됩니다.
    
    LoRA를 안정적으로 확산하기 위해 가장 먼저 적용한 사람은 류시모씨이며, 류시모씨의 깃허브 프로젝트에서 예시와 토론을 확인할 수 있습니다. 이전에는 크로스 어텐션 레이어에서 LoRA를 사용하려면 복잡한 해킹이 필요했지만, 이제는 보다 사용자 친화적인 접근 방식을 사용할 수 있습니다. 이를 통해 모델을 보다 창의적이고 유연하게 사용할 수 있습니다.
    
    LoRA 교육 지원은 드림부스 및 전체 미세 조정 방법 모두에 대해 제공되며, 다음과 같은 몇 가지 이점이 있습니다:
    
    1. 더 빠른 훈련.
    2. 더 낮은 컴퓨팅 요구 사항(예: 11GB VRAM GPU로 모델 미세 조정).
    3. 학습된 가중치가 훨씬 작아져 모델 공유가 더 쉬워집니다.
    
    이제 사용자는 전체 사본 대신 하나의 작은 파일로 미세 조정된 모델을 공유할 수 있어 스토리지 및 다운로드 비용을 절감할 수 있습니다. 따라서 미세 조정된 모델을 훨씬 더 쉽고 편리하게 공유하고 사용할 수 있습니다.
    
    요약하자면, LoRA는 대규모 언어 모델을 특정 작업이나 도메인에 맞게 조정할 수 있는 보다 효율적이고 유연한 방법을 제공하여 학습 프로세스의 속도를 높이고 컴퓨팅 요구 사항을 줄여줍니다.
    
- Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning
    
    [https://github.com/cloneofsimo/lora](https://github.com/cloneofsimo/lora)
    
    주요 특징
    
    - Low-rank Adaptation을 통해 Dreambooth 방식보다 2배 빠른 안정적인 확산 모델 미세 조정
    - 매우 작은 최종 결과(1MB ~ 6MB)를 얻고 쉽게 공유하고 다운로드할 수 있습니다.
    - 호환 가능diffusers
    - 인페인팅 지원
    - 때때로 전체 미세 조정보다 더 나은 성능 (하지만 광범위한 비교를 위해 향후 작업으로 남겨둠)
    - 체크포인트 병합 + LoRA를 함께 병합하여 레시피 구축
    - 더 나은 결과를 얻기 위해 CLIP + Unet + 토큰을 미세 조정하는 파이프라인.
    - 즉시 사용 가능한 다중 벡터 중추 튜닝 반전
    
    안정성 AI와 허깅페이스 덕분에 많은 사람들이 더 나은 이미지 생성을 위해 안정적인 확산 모델을 미세 조정하는 것을 즐길 수 있게 되었습니다. 하지만 이 과정은 느리고 모델이 커질 수 있습니다. 텍스트 반전이 대안이 될 수 있지만 품질이 낮은 이미지를 생성합니다.
    
    효율적인 미세 조정 방법인 LoRA는 이 문제에 대한 해결책입니다. LoRA는 전체 모델을 미세 조정하는 대신 모델의 '잔여'를 미세 조정하는 데 초점을 맞추기 때문에 모델 크기가 훨씬 작아집니다.
    
    이 리포지토리는 Stability AI의 Stable-diffusion을 사용하며, 이를 중심으로 다양한 미세 조정 방법을 구축합니다:
    
    - 드림부스: LoRA를 적용하여 훈련 프로세스를 규칙화하고 모델의 일반화 기능을 유지하면서 높은 충실도를 유지합니다.
    - 텍스트 반전: LoRA를 사용하지 않지만 개념적 아이디어를 반전시키는 데 강력하며 충실도에는 덜 집중합니다.
    - 피벗튜닝: 보다 효율적이고 효과적인 접근을 위해 두 가지 방법을 결합합니다.
    
    이 방법을 사용하면 더 작고 관리하기 쉬운 모델로 작업하면서 더 나은 결과를 얻을 수 있습니다.