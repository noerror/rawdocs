# Shap·E: Generating Conditional 3D Implicit Functions

*잠복 확산 모델을 사용하는 3D 제너레이티브 모델링의 획기적인 방법인 Shap-E를 소개합니다. 암시적 함수에서 작동하여 복잡한 3D 모델을 생성할 수 있으며, NeRF와 텍스처 메시로 렌더링할 수 있습니다. #AI #MachineLearning #3DModeling*

[https://github.com/openai/shap-e/](https://github.com/openai/shap-e/tree/main)

[https://arxiv.org/pdf/2305.02463.pdf](https://arxiv.org/pdf/2305.02463.pdf)

Shap-E는 새로운 3D 제너레이티브 모델로 3D 에셋 제작을 위해 새로 개발되었습니다. 한 가지 유형의 출력만 제공하는 이전 모델과 달리 Shap-E는 텍스처 메시와 뉴럴 래디언스 필드를 모두 렌더링할 수 있는 파라미터를 생성할 수 있습니다.

Shap-E를 두 단계로 훈련합니다. 첫 번째 단계에서는 3D 에셋을 결정론적으로 암시적 함수의 파라미터로 변환하도록 인코더를 훈련합니다. 두 번째 단계에서는 이러한 인코더 출력을 사용하여 조건부 확산 모델을 훈련합니다.

쌍을 이루는 3D 및 텍스트 데이터로 구성된 대규모 데이터 세트로 Shap-E를 훈련시키면 이 모델은 단 몇 초 만에 복잡하고 다양한 3D 에셋을 생성할 수 있습니다. Shap-E는 더 복잡한 다중 표현 출력 공간에서 작동함에도 불구하고 더 빠르게 수렴하고 유사하거나 더 높은 품질의 샘플을 생성함으로써 포인트 클라우드를 생성하는 이전 모델인 Point-E보다 더 나은 성능을 발휘합니다.

모델 가중치, 추론용 코드, 샘플 출력은 GitHub([https://github.com/openai/shap-e](https://github.com/openai/shap-e))에서 확인할 수 있습니다.

### 1 Introduction

최근 제너레이티브 이미지 모델의 발전으로 오디오, 비디오, 3D 자산과 같은 다른 데이터 유형에 대해서도 유사한 모델을 개발하는 데 관심이 쏠리고 있습니다. 이미지와 오디오는 픽셀 그리드나 샘플 배열로 직접 표현할 수 있지만, 3D 자산에 대해 효율적이고 사용 가능한 표현을 만드는 것은 더 어렵습니다.

이 문제를 해결하기 위해 널리 사용되는 솔루션은 3D 좌표를 색상이나 밀도와 같은 특정 데이터에 매핑하여 3D 자산을 인코딩하는 암시적 신경 표현(implicit neural representations,INRs)입니다. 이는 해상도와 무관하며 스타일 전송이나 모양 편집과 같은 다양한 애플리케이션에 사용할 수 있습니다. 이 연구에서는 두 가지 유형의 INR에 중점을 둡니다: 3D 장면과 텍스처를 표현하는 방식이 다른 신경 방사 필드(NeRF)와 DMTet입니다.

INR은 유연성에도 불구하고 데이터 세트의 모든 샘플에 대해 얻는 데 비용이 많이 들 수 있으며, 수많은 파라미터로 인해 다운스트림 모델 훈련이 복잡해질 수 있습니다. 일부 연구자들은 이러한 문제를 해결하기 위해 자동 인코더 또는 메타 학습을 사용하기도 하고, 다른 연구자들은 그라데이션 기반 메타 학습 없이 인코더를 훈련시켜 NeRF 파라미터를 직접 생성하기도 합니다.

이 연구에서는 이러한 접근 방식을 결합하고 확장하여 다양하고 복잡한 3D 표현을 생성하는 제너레이티브 모델인 Shap-E를 소개합니다. 트랜스포머 기반 인코더를 사용하여 INR 파라미터를 생성한 다음 인코더의 출력에 대해 확산 모델을 훈련합니다. 이전 모델과 달리 Shap-E는 NeRF와 메시를 동시에 표현할 수 있는 INR을 생성하여 더 많은 렌더링 및 애플리케이션 옵션을 제공합니다.

대규모 데이터 세트에 대해 학습할 경우 Shap-E는 텍스트 프롬프트를 기반으로 다양한 3D 에셋을 생성할 수 있으며, 최근의 3D 생성 모델인 Point-E보다 더 빠르게 수렴하고 유사하거나 더 나은 결과를 얻을 수 있습니다. Shap-E와 Point-E의 성능은 비슷한 경우가 많지만, 특히 텍스트 캡션을 조건으로 할 때 눈에 띄는 차이가 있습니다. Shap-E의 샘플 품질은 최적화 기반 방법만큼 높지는 않지만 훨씬 빠르므로 유리한 절충안이 될 수 있습니다.

Shap-E의 모델, 코드 및 샘플은 [https://github.com/openai/shap-e](https://github.com/openai/shap-e) 에서 확인할 수 있습니다.

### 2 Background

이 논문에서는 3D 도형과 그 속성을 모델링하는 다양한 방법에 대해 논의하고 있습니다.

신경 방사 필드(NeRF): NeRF는 3D 장면을 함수로 표현하는 기법입니다. 이 함수는 3D 공간 좌표와 보는 방향을 RGB 색상과 밀도 값에 매핑합니다. 이미지를 생성하기 위해 카메라에서 뷰포트의 픽셀을 통해 광선을 투사하고 광선을 따라 지점에서 함수를 쿼리하여 색상을 계산합니다. 광선을 따라 함수의 적분을 근사화하기 위해 일련의 증가 값을 사용합니다. 이 논문에서는 얇은 피처를 캡처하기 위한 2단계 렌더링 절차를 제안하는데, 먼저 값 시퀀스를 광선을 따라 균일하게 샘플링한 다음 밀도가 높은 지점을 중심으로 다시 샘플링하는 방식입니다.

부호화된 거리 함수 및 텍스처 필드(STF): STF는 3D 모양을 표현하는 또 다른 방법입니다. 이 방법은 3D 좌표를 거리 값에 매핑하는 부호화된 거리 함수(SDF)를 사용하여 좌표에서 도형 표면의 가장 가까운 점까지의 거리를 나타냅니다. 이 방법에는 텍스처 정보도 포함되며, 각 표면 포인트에 대한 RGB 색상을 예측하기 위해 별도의 모델을 학습시킵니다.

확산 모델: 확산 모델은 고차원 연속 분포를 모델링하는 데 사용됩니다. 데이터 샘플로 시작하여 점차 가우시안 노이즈를 추가하여 점점 더 노이즈가 많은 버전의 샘플을 만듭니다. 이 모델은 노이즈가 없는 원본 샘플을 예측하도록 훈련됩니다. 새로운 샘플을 생성하려면 노이즈 샘플에서 시작하여 점차적으로 노이즈를 제거하는 프로세스를 거꾸로 진행합니다. 또한 일부 정보에 따라 모델을 컨디셔닝하고 그에 따라 모델 예측을 조정하여 샘플의 품질을 개선하는 방법도 있습니다.

잠재 확산: 이는 이미지를 위한 2단계 생성 기법입니다. 인코더와 디코더는 이미지의 잠재적 표현을 생성하고 재구성하도록 훈련됩니다. 그런 다음 인코딩된 데이터 세트 샘플에 대해 확산 모델을 훈련합니다. 새로운 샘플을 생성하기 위해 확산 모델은 잠재 샘플을 생성하고 디코더는 이를 이미지로 변환합니다. 이 접근 방식은 지각 손실이나 GAN 기반 목표 대신 기본 재구성 손실을 사용하고 잠상을 고정 범위에 고정하고 확산 스타일 노이즈를 추가함으로써 단순화됩니다.

### 3 Related Work

이 요약에서는 머신러닝 기법을 사용한 3D 모델 제작의 진행 상황에 대해 설명합니다. 논의된 접근 방식은 크게 명시적, 암시적, 이 두 가지를 혼합한 세 가지 범주로 분류할 수 있습니다.

명시적 표현: 이 방법에는 명시적 3D 표현에 대해 학습된 자동 인코더가 사용되며, 출력 표현이 고정된 해상도에 묶여 있거나 3D 에셋을 표현할 수 있는 기능이 제한되는 경우가 많습니다. 포인트 클라우드 자동 인코더, 생성적 적대 신경망(GAN), 가우시안 혼합 모델(GMM), 확산 모델 사용 등 다양한 기술이 언급되고 있습니다. 이러한 기법은 장점에도 불구하고 3D 에셋을 완벽하게 표현하기에는 부족한 경우가 많습니다.

암시적 표현: 이는 저자의 방법과 더 유사합니다. 명시적인 3D 데이터 대신 '잠재적' 표현을 기반으로 모델을 조정합니다. 이 방법은 보다 표현력이 풍부하고 규모가 큰 모델을 만들 수 있다는 장점이 있습니다. 언급된 몇 가지 기술에는 부호화된 거리 함수(SDF) 샘플 그리드 또는 복셀 그리드를 잠재 벡터로 인코딩한 다음 암시적 모델을 컨디셔닝하는 데 사용하는 것이 포함됩니다.

학습된 인코더 없이: 학습된 인코더 없이 3D 표현을 생성하는 데 중점을 둔 작업도 있는데, 이를 잠재 조건부 암시적 3D 표현이라고 합니다. 메타 학습, 그라데이션 기반 최적화 또는 확산과 같은 기술을 사용하여 암시적 함수의 파라미터를 직접 생성합니다. 이러한 방법은 명시적인 입력 표현이 필요하지 않지만 비용이 많이 들고 데이터 세트가 커지면 확장하기가 어려울 수 있습니다.

마지막으로, 이 백서에서는 그라디언트 기반 최적화를 사용하여 개별 샘플을 최적화하는 접근 방식에 대해 언급하며, 종종 암시적 함수를 형성합니다. 여기에는 드림필드, 드림퓨전 또는 메시 자체의 직접 최적화와 같은 기술이 포함됩니다. 그러나 이러한 접근 방식은 종종 비용이 많이 드는 최적화 절차가 필요하고 생성된 각 샘플에 대해 많은 작업을 반복해야 하므로 대규모 데이터 세트에 대해 사전 학습할 수 있는 직접 생성 모델과 대조적입니다.

### 4 Method

이 방법은 2단계 프로세스를 통해 3D 모델을 생성합니다. 먼저 인코더가 조밀한 명시적 3D 에셋 표현을 기반으로 암시적 함수의 파라미터를 생성하도록 훈련됩니다. 인코더는 3D 에셋의 잠재적 표현을 출력하고, 이 표현은 다층 퍼셉트론(MLP)의 가중치로 변환됩니다. 둘째, 인코더에서 생성된 잠재적 표현에 대해 확산 모델을 학습시킵니다. 이 모델은 이미지 또는 텍스트 설명에 따라 조건이 설정됩니다.

사용되는 데이터 세트는 해당 렌더링, 포인트 클라우드, 텍스트 캡션이 포함된 3D 에셋으로 구성됩니다. 훈련 과정에는 더 나은 결과를 얻기 위해 원본 후처리를 약간 수정하는 작업이 포함됩니다. 예를 들어 포인트 클라우드의 뷰와 포인트 수를 늘리고 렌더링 중에 조명과 머티리얼을 단순화합니다.

3D 인코더는 3D 에셋의 포인트 클라우드와 렌더링된 뷰를 모두 가져와서 에셋을 암시적 함수로 나타내는 MLP의 파라미터를 출력하는 방식으로 작동합니다. 포인트 클라우드와 입력 뷰는 크로스 어텐션을 통해 처리된 다음 잠재 표현을 출력하는 트랜스포머 백본이 이어집니다.

![인코더는 16k 해상도의 RGB 포인트 클라우드와 각 전경에 대한 증강 공간 좌표로 렌더링된 RGBA 이미지를 모두 수집합니다. 픽셀. 그런 다음 MLP의 파라미터를 출력하고, 이 파라미터는 NeRF와 부호화된 텍스처 필드 (STF) 역할을 합니다.](Shap%C2%B7E%20Generating%20Conditional%203D%20Implicit%20Function%2023c27f47c0fc4010bc21b0002dd9d44d/Untitled.png)

인코더는 16k 해상도의 RGB 포인트 클라우드와 각 전경에 대한 증강 공간 좌표로 렌더링된 RGBA 이미지를 모두 수집합니다. 픽셀. 그런 다음 MLP의 파라미터를 출력하고, 이 파라미터는 NeRF와 부호화된 텍스처 필드 (STF) 역할을 합니다.

이 방법에는 NeRF(신경 방사 필드) 렌더링과 STF(공간 트랜스포머 네트워크) 렌더링을 사용한 디코딩이 추가로 포함됩니다. NeRF 렌더링에는 광선을 샘플링하고 실제 색상과 예측 색상 간의 L1 손실을 최소화하는 작업이 포함됩니다. 반면 STF 렌더링은 MLP에 추가 출력 헤드를 추가하여 SDF 값과 텍스처 색상을 예측합니다. MLP는 NeRF 렌더링 목표만을 사용하여 사전 학습됩니다.

제너레이티브 모델의 경우 트랜스포머 기반 확산 아키텍처가 채택되지만 포인트 클라우드는 잠재 벡터 시퀀스로 대체됩니다. 잠재 벡터는 트랜스포머에 공급되는 시퀀스이며, 각 토큰은 MLP 가중치 매트릭스의 다른 행에 해당합니다. 이미지 조건부 및 텍스트 조건부 생성에는 서로 다른 조건부 전략이 사용됩니다.

### 5 Results

이 연구에서는 인코더 트레이닝 프로세스 전반에 걸쳐 두 가지 렌더링 기반 지표, 즉 피크 신호 대 잡음비(PSNR)와 재구성된 NeRF 및 STF 렌더링의 CLIP R-정확도를 추적하여 3D 인코더를 평가했습니다. 그 결과 미세 조정을 통해 STF 렌더링의 품질이 향상되고 증류 과정에서 NeRF 재구성 품질이 저하되었음에도 불구하고 NeRF 품질이 약간 향상되는 것으로 나타났습니다.

그런 다음 연구진은 잠재 확산 모델을 동일한 아키텍처, 훈련 데이터 세트 및 컨디셔닝 모드를 공유하는 방법인 Point-E와 비교했습니다. 이 비교는 암시적 신경 표현과 명시적 표현을 생성할 때의 효과를 분리하는 데 도움이 되었습니다. 텍스트 조건부 Shap-E 모델은 훈련이 끝날 무렵 텍스트 캡션을 과도하게 맞추기 시작했지만 두 가지 지표 모두에서 비교 대상인 Point-E 모델보다 우수한 성능을 보였습니다. 반면에 이미지 조건부 Shap-E 모델과 Point-E 모델은 최종 평가 성능에서 약간의 차이만 있을 뿐 비슷한 성능을 보였습니다.

Shap-E를 더 광범위한 3D 생성 기법과 비교했을 때, 추가적인 업샘플링 확산 모델이 필요하지 않았기 때문에 Shap-E가 Point-E보다 더 빠른 추론을 제공한다는 것을 알 수 있었습니다. 그러나 최적화 기반 방법의 우수한 샘플 품질은 상당한 추론 비용을 수반한다는 점에 주목했습니다. 명시적 모델링과 암시적 모델링의 차이에도 불구하고 두 방법 모두 동일한 데이터와 모델 아키텍처에서 뚜렷한 특징을 학습할 수 있었습니다.

### 6 Conclusion

Shap-E의 텍스트 조건부 모델은 단순한 속성을 가진 단일 객체와 관련된 프롬프트를 이해하는 데는 능숙하지만, 개념을 구성하고 두 개 이상의 객체를 요구할 때 원하는 객체 수를 정확하게 생성하는 데는 어려움을 겪습니다. 이러한 한계는 충분한 쌍을 이루는 훈련 데이터가 부족하기 때문일 수 있으며, 주석이 달린 대규모 3D 데이터 세트가 이러한 문제를 극복하는 데 도움이 될 수 있음을 시사합니다.

Shap-E는 인식 가능한 3D 자산을 생성할 수 있지만, 결과 샘플이 거칠고 세밀한 디테일이 부족할 수 있습니다. 이는 부분적으로 인코더가 디테일한 텍스처를 잃기 때문입니다. 인코더를 개선하면 이러한 손실된 생성 품질을 일부 복구할 수 있습니다.

연구진은 Shap-E를 최적화 기반 3D 생성 기술과 결합하여 최적의 결과를 얻을 수 있다고 제안합니다. 예를 들어, Shap-E로 생성된 NeRF 또는 메시를 최적화 기반 접근 방식의 시작점으로 사용하여 더 빠른 수렴을 이끌어낼 수 있습니다.

이 연구에서는 3D 암시적 함수의 공간에 대한 잠복 확산 모델인 Shap-E를 제시했습니다. 이 모델은 NeRF와 텍스처 메시를 모두 렌더링할 수 있으며 동일한 데이터 세트, 모델 아키텍처, 학습 리소스가 주어졌을 때 유사한 명시적 생성 모델과 일치하거나 더 나은 성능을 발휘합니다. 또한 이 연구는 텍스트 조건부 모델이 중간 표현으로 이미지 없이도 다양하고 흥미로운 오브젝트를 생성할 수 있다는 사실을 밝혀냈으며, 특히 명시적 표현보다 더 많은 유연성을 제공할 수 있는 3D 영역에서 암시적 표현 생성의 잠재력을 강조합니다.