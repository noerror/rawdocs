# Tex2Shape: Detailed Full Human Body Geometry From a Single Image

[https://github.com/thmoa/tex2shape](https://github.com/thmoa/tex2shape)

[https://arxiv.org/pdf/1904.08645.pdf](https://arxiv.org/pdf/1904.08645.pdf)

이 논문에서는 한 장의 사진에서 얼굴, 머리카락, 옷, 심지어 주름까지 포함한 상세한 인체 전체 형태를 추론하는 새로운 방법을 소개합니다. 이 모델은 입력 이미지의 가려진 부분에서도 세부적인 결과를 생성할 수 있으며 대화형 프레임 속도로 작동합니다.

이 방법의 기본 아이디어는 모양 회귀를 정렬된 이미지 간 변환 문제로 변환하는 것입니다. 이 메서드의 입력은 기성 메서드에서 얻은 가시 영역의 부분 텍스처 맵입니다. 이 부분 텍스처로부터 모델은 상세한 노멀 및 벡터 변위 맵을 추정합니다. 그런 다음 이 맵을 저해상도 스무스 바디 모델에 적용하여 디테일과 의상을 추가할 수 있습니다.

이 모델은 합성 데이터로만 학습되었음에도 불구하고 실제 사진에도 잘 일반화됩니다. 이 논문에서 제공하는 다양한 결과는 제안된 방법의 다양성과 견고성을 입증하며, 단일 이미지에서 전체 인체 형태를 추론하는 데 효과적임을 보여줍니다.

![Untitled](Tex2Shape%20Detailed%20Full%20Human%20Body%20Geometry%20From%20a%201765df0b146b45ec9ab751cdb83f4370/Untitled.png)

1. Introduction

이 논문에서 저자는 단일 이미지에서 상세한 전신 인체 형상을 자동으로 재구성하는 문제를 다룹니다. 이 기술은 가상 및 증강 현실, 장면 분석, 가상 시착 등 다양한 분야에서 활용될 수 있습니다. 기존의 대부분의 작업은 포즈와 대략적인 신체 형태를 복구하는 데 중점을 두지만 머리카락, 얼굴, 옷의 주름과 주름을 포함한 세부적인 형태를 제공하지 않습니다.

저자들은 까다로운 형태 회귀 문제를 이미지 간 변환 문제로 변환하는 방법인 Tex2Shape를 소개합니다. 저자는 DensePose를 사용하여 입력 및 출력 쌍을 포즈에 독립적인 SMPL 모델의 UV 매핑에 매핑합니다. 입력은 부분 텍스처 맵 이미지이며, 모델은 형상을 UV 공간 변위와 노멀 맵으로 회귀합니다.

Tex2Shape는 다양한 의상, 포즈, 모양을 가진 사람의 2,043개 3D 스캔 데이터 세트를 학습합니다. 각 스캔에 SMPL을 비강제적으로 등록하고, 모델 모양 매개변수에 맞게 최적화하며, 자유형 변위를 변위 맵에 저장합니다. 부분 텍스처 맵을 매핑하여 노멀 및 변위 맵을 완성하기 위해 Pix2Pix 네트워크를 사용하고, SMPL 체형 파라미터를 추정하기 위해 또 다른 작은 네트워크를 사용합니다.

단순함에도 불구하고 Tex2Shape는 단 50밀리초 만에 단일 이미지에서 풀 3D 의류, 머리카락, 얼굴 디테일을 효과적으로 회귀시킵니다. 또한 입력 이미지의 가려진 부분에 대한 형상 디테일을 환각화할 수 있습니다. 이 방법은 이미지 간 변환 접근 방식을 사용하여 세부적인 신체 형태를 추론하는 첫 번째 시도이며, 저자들은 Tex2Shape를 연구 목적으로 사용할 수 있도록 공개했습니다.

2. Related Work

이 백서에서는 포즈 재구성과 함께 접근하는 경우가 많은 인체 형상 재구성 분야의 다양한 방법을 검토합니다. 다음은 다양한 영역에 대한 간략한 개요입니다:

1. 포즈 및 모양 재구성: 많은 방법이 파라메트릭 신체 모델을 사용하여 검색 공간을 제한하거나 사전 스캔한 정적 템플릿을 사용하여 포즈와 비강체 표면 변형을 캡처합니다. 최근의 연구는 SMPL 모델을 네트워크 아키텍처에 통합하여 프로세스를 더욱 자동적이고 강력하게 만들었습니다. 그러나 이러한 방법의 대부분은 포즈 감지에 초점을 맞추고 있으며, 형태 추정은 모델 공간으로 제한되는 경우가 많습니다.
2. 얼굴 재구성: 단안 얼굴 재구성 방법은 지오메트리 개선을 위해 음영 기반 정교화를 사용합니다. 최근의 작업은 신경망에 차별적 얼굴 렌더러를 통합하여 기본 모델에 대한 지오메트리 및 알베도의 인스턴스 보정을 추정하거나 비디오에서 처음부터 아이덴티티 지오메트리 및 알베도 기준을 학습합니다.
3. 의류 재구성 및 모델링: 의복 아래에서 체형을 추정하거나 복잡한 의복을 알몸에서 오프셋으로 모델링하는 여러 가지 방법이 제안되었습니다. 일부 기술은 의류의 사실적인 애니메이션에 초점을 맞추고 의류를 개별적으로 또는 포즈의 함수로 예측합니다. 학습 기반 노멀과 깊이 복구 또는 단일 의상에 대한 메시를 시연하는 기술도 있습니다.

이러한 방법과 달리 저자들은 이미지 간 매핑을 학습하여 단일 이미지에서 상세한 전신 모양을 재구성하는 최초의 접근 방식인 Tex2Shape를 제안합니다. 이 방법은 까다로운 형태 회귀 문제를 정렬된 이미지 간 변환 문제로 변환하고 단일 이미지에서 풀 3D 의류, 머리카락, 얼굴 디테일을 효과적으로 회귀합니다.

3. Method

이 작업은 피사체의 체형을 반영하고 머리카락, 옷, 옷 주름과 같은 디테일을 포함하는 한 장의 사진으로 피사체의 애니메이션 가능한 3D 모델을 만드는 것을 목표로 합니다. 목표는 입력 이미지에 보이는 신체 부위와 보이지 않는 신체 부위 모두에 디테일이 포함되도록 하고, 정확한 3D 포즈 추정 없이 완전 자동 재구성을 달성하는 것입니다.

이를 위해 저자는 SMPL 신체 모델을 기반으로 법선과 벡터 변위(UV 형상 이미지)를 추론하기 위해 Pix2Pix 스타일의 컨볼루션 신경망을 훈련합니다. 입력 이미지를 출력 UV 모양 이미지와 정렬하기 위해 기성 방법을 사용하여 가시 영역의 부분 UV 텍스처 맵을 추출합니다. 또한 두 번째 소형 CNN은 입력 이미지에서 SMPL 형상 파라미터를 추론합니다.

이 논문은 다음과 같이 구성되어 있습니다:

섹션 3.1: 이 작업에 사용된 파라메트릭 바디 모델에 대해 설명합니다.
섹션 3.2: 모양, 법선, 변위의 파라미터화에 대해 설명합니다.
프로세스에 대한 개요는 백서의 그림 2에서 확인할 수 있습니다.

3.1. Parametric body model

백서의 섹션 3.1에서는 이 작업에 사용된 파라메트릭 신체 모델인 피부 다인 선형 모델(SMPL)에 대해 설명합니다. SMPL은 최소한의 옷을 입은 피사체를 스캔하여 학습한 파라미터화된 신체 모델입니다. 포즈(θ)와 모양(β)의 함수로 정의되며, N = 6890개의 정점과 F = 13776개의 면으로 구성된 메시를 반환합니다. 모양(β)은 훈련 데이터 피사체의 처음 10개의 주성분에 해당합니다.

단안 이미지에 내재된 스케일의 모호함을 해결하기 위해 저자는 이 작업에서 β를 신체 높이와 무관하게 만들었습니다. 이 방법은 표준화된 높이로 β를 추정하며 포즈(θ)와는 무관합니다. 3.2절에 설명된 대로, SMPL 형상 공간을 벗어난 디테일은 UV 변위와 노멀 맵(UV 형상 이미지)을 통해 추가됩니다.

데이터 세트 생성 프로세스(섹션 4 참조)에서 SMPL은 카메라 앞에서 포즈를 취하는 사람의 이미지를 합성하는 데 사용됩니다.

3.2. UV parameterization

3.2절에서는 SMPL 모델의 해상도 제한과 메시의 불규칙한 2D 그리드를 극복하는 데 사용되는 방법인 UV 파라미터화에 대해 설명합니다. UV 매핑은 표면을 이미지에 래핑하여 표면에 정의된 함수를 이미지로 표현할 수 있도록 합니다. U와 V는 이미지의 두 축을 나타냅니다. 매핑은 인접한 정점의 이심 보간을 통해 맵의 각 픽셀을 표면의 한 지점에 할당합니다. 이를 통해 UV 맵 해상도에 비례하는 기하학적 디테일로 메시를 증강할 수 있습니다.

저자는 노멀 맵과 벡터 변위 맵이라는 두 가지 UV 맵을 사용하여 SMPL 모델을 증강합니다. 노멀 맵에는 셰이딩을 통해 시각적 디테일을 추가하거나 향상시킬 수 있는 새로운 표면 노멀이 포함되어 있습니다. 벡터 변위 맵에는 기본 표면을 변위하는 3D 벡터가 포함되어 있습니다. 변위와 노멀은 모두 SMPL 모델의 표준 T 포즈에 정의됩니다.

신경망에 대한 입력은 5.3절에 설명된 대로 입력 사진에서 보이는 픽셀의 부분 텍스처 맵입니다.

4. Dataset Generation

데이터 세트 생성 섹션에서는 저자가 실제 사람의 3D 스캔에서 다양한 데이터 세트를 합성하여 모델을 훈련하는 과정을 설명합니다. 이 데이터 세트는 실제 조명 아래에서 다양한 포즈를 취한 사람의 합성 이미지와 노멀 맵, 변위 맵, SMPL 형상 파라미터 β로 구성됩니다. Twindom에서 총 1826개의 스캔을 제공했으며, 렌더피플닷컴과 axyzdesign.com에서 각각 163개와 54개의 스캔을 구매했습니다.

![Untitled](Tex2Shape%20Detailed%20Full%20Human%20Body%20Geometry%20From%20a%201765df0b146b45ec9ab751cdb83f4370/Untitled%201.png)

이러한 스캔은 동일한 메시 레이아웃을 공유하지 않으므로 작성자는 각 스캔에 대해 SMPL 모델을 비강제적으로 등록합니다. 이렇게 하면 모든 정점이 데이터 세트 전체에서 동일한 컨텍스트 정보를 공유하고 SMPL 모델을 사용하여 스캔의 포즈를 변경할 수 있습니다. 그러나 옷을 입은 사람을 비강체적으로 등록하는 것은 어려운 문제이며, 종종 부자연스러운 모양을 초래합니다.

저자는 긴 머리, 치마, 드레스를 입은 사람을 등록하는 데 어려움이 있어 현재 데이터 세트가 남성에게 약간 편향되어 있다는 점에 주목하면서 2043개의 고품질 등록을 수동으로 선택했습니다. 2043개의 스캔 중 20개의 스캔은 검증을 위해, 55개의 스캔은 테스트를 위해 예약되어 있습니다.

그런 다음 저자는 비강체 등록 절차에 대해 자세히 설명하고 모델 학습을 위한 페어링된 데이터 세트의 합성에 대해 설명합니다.

4.1. Scan registration

섹션 4.1에서는 스캔에서 미세한 기하학적 디테일을 캡처하는 데 필수적인 스캔 등록 프로세스에 대해 설명합니다. 저자들은 SMPL 모델의 각 면을 4개로 세분화하여 27,554개의 정점과 55,104개의 면을 가진 새로운 메시를 생성합니다. 이 고해상도 메쉬는 미세한 기하학적 디테일을 설명하는 데 더 적합합니다.

등록 프로세스는 견고성을 높이기 위해 단계적으로 수행됩니다. 단계는 다음과 같습니다:

포즈 재구성: 작성자는 스캔 대상의 포즈를 재구성하는 것으로 시작합니다. 여러 대의 카메라에서 스캔을 렌더링하고 2D 재투영 오류를 최소화하여 3D 랜드마크를 찾아 2D 조인트 OpenPose 감지를 수행합니다.

SMPL 포즈 파라미터 최적화: 다음으로, 추정된 3D 조인트 위치를 설명하기 위해 SMPL 포즈 파라미터 θ를 최적화합니다.

셰이프 파라미터 최적화: 그런 다음 형상 파라미터 β를 최적화하여 스캔에서 SMPL 표면까지의 거리를 최소화합니다. 이 단계에서는 SMPL 모델이 나체 형태만 안정적으로 설명할 수 있으므로 스캔 외부의 정점에 대해 더 높은 비용을 지불하여 SMPL 정점이 스캔 내부에 유지되도록 합니다.

세밀한 디테일 복구: 마지막으로, 세밀한 디테일을 복구하기 위해 SMPL 정점의 위치를 최적화합니다.

이렇게 등록된 모델은 세분화된 SMPL 메시 레이아웃으로 스캔의 고주파 디테일을 설명하며, 등록된 모델을 다시 배치할 수 있습니다.

4.2. Spherical harmonic lighting

4.2장에서는 구형 고조파 조명을 사용하여 사람의 이미지를 사실적인 조명과 합성하여 쌍을 이루는 데이터 세트를 만드는 방법에 대해 설명합니다.

구형 고조파(SH)는 구의 표면 위에 정의된 직교 기저 함수입니다. 렌더링에서 SH는 빛이 씬에 비추는 방향을 설명하는 데 사용됩니다. 저자는 표준 절차에 따라 색상당 처음 9개의 SH 컴포넌트를 사용하여 조명을 설명합니다.

다양하고 사실적인 조명 조건을 생성하기 위해 이전 작업과 유사한 접근 방식에 따라 Laval Indoor HDR 데이터 세트의 이미지를 디퓨즈 SH 계수로 변환합니다. 추가 증강을 위해 계수를 Y축을 중심으로 무작위로 회전시킵니다.

구형 하모닉 조명을 사용하여 합성 이미지에 사실적인 조명 조건을 생성할 수 있으며, 이는 모델 학습을 위한 고품질의 페어링된 데이터 세트를 생성하는 데 필수적입니다.

4.3. UV map synthetization

이 섹션에서는 저자가 데이터 세트를 완성하기 위해 UV 맵을 생성하는 방법을 설명합니다. 이들은 고해상도 등록을 사용하여 상세한 UV 변위와 노멀 맵을 렌더링합니다. 이러한 맵은 표준 해상도 네이키드 SMPL을 보강하는 데 사용되므로 더 높은 메시 해상도나 버텍스별 오프셋이 필요하지 않습니다. 이 접근 방식을 통해 제작자는 3D 등록의 세부 사항을 설명하고 옷 주름과 같은 미세한 기하학적 디테일이 있는 사람 이미지를 합성할 수 있습니다.

5. Model and Training

이 모델은 노멀 및 변위 맵용과 SMPL 형상 매개변수 β용의 두 개의 CNN으로 구성됩니다. Tex2Shape 네트워크는 U-Net 생성기와 PatchGAN 판별기로 구성된 조건부 생성적 적대적 네트워크(Generative Adversarial Network)입니다. β 네트워크는 1024×1024 DensePose 감지를 입력으로 받습니다. 목표는 인지 품질이 높은 결과를 생성하는 것이며, 손실에 대해 구조적 유사성 지수(SSIM)와 MS-SSIM으로 실험합니다. 입력 부분 텍스처 맵은 입력 이미지의 픽셀을 DensePose 감지를 기반으로 UV 공간으로 변환하여 생성됩니다.

![Untitled](Tex2Shape%20Detailed%20Full%20Human%20Body%20Geometry%20From%20a%201765df0b146b45ec9ab751cdb83f4370/Untitled%202.png)

6. Experiments

섹션 6에서는 저자들이 제안한 방법을 정성적, 정량적으로 평가합니다. 저자들은 이 방법의 다양성과 견고성을 입증하고 네 가지 데이터 세트에 대한 최신 방법과 비교합니다. 또한 다양한 감독 손실의 영향을 연구하고, UV 매핑을 위한 다양한 방법을 평가하며, 다양한 가시성 수준에 대한 견고성을 측정합니다. 또한 피사체 간 의복 이동이라는 방법의 잠재적 적용 가능성을 보여줍니다. 결과는 UV 매핑에 사용된 방법에 따라 색상으로 구분되며, 모든 결과는 인터랙티브 프레임 속도로 계산되었습니다. 저자들은 이 방법의 변위 맵, 노멀 맵, β 추정에 평균 50ms가 소요되며, DensePose를 사용한 UV 매핑은 실시간으로 수행할 수 있다고 보고합니다.

6.1. Qualitative results and comparisons

섹션 6.1에서는 저자들이 제안한 방법과 피플스냅샷 데이터 세트에서 단안 인체 형상 재구성을 위한 네 가지 관련 방법 간의 질적 비교를 제시합니다. 이들은 자신들의 방법을 동일한 피사체의 120개 이미지를 사용하는 BodyNet, SiCloPe, HMR 및 최적화 기반 방법과 비교합니다. 저자들은 이 방법이 가장 높은 수준의 디테일을 제공하며, 단일 이미지 입력에서 머리카락, 얼굴 디테일, 옷 주름을 포함한 다양한 의상을 사실적인 3D 모델로 추론할 수 있음을 보여줍니다. 또한 3DPW, DeepFashion, 피플스냅샷 데이터세트에 대한 질적 결과도 보여줍니다. 저자들은 이 방법이 다양한 실제 조건에 성공적으로 일반화된다고 결론지었습니다.

6.2. Type of supervision

저자들은 제안한 방법을 피플스냅샷 데이터세트에서 단안 인체 형상 재구성을 위한 다른 네 가지 관련 방법과 비교하고, 그 결과를 나란히 비교합니다. 이 방법은 디테일 측면에서 다른 방법보다 성능이 뛰어나고 인터랙티브한 프레임 속도로 실행됩니다. 또한 저자들은 다양한 감독 손실의 효과를 연구하고 MS-DSSIM 손실이 복잡한 의상을 더 안정적으로 재구성한다는 사실을 발견했습니다.

6.3. Impact of UV mapping

섹션 6.3에서는 UV 매핑 선택이 제안된 방법에 미치는 영향을 평가합니다. (1) 스캔에서 계산된 지상 실측 UV 매핑 사용, (2) 3D 포즈 디텍터 결과에서 텍스처 재현을 시뮬레이션하는 자유 형식 오프셋이 없는 네이키드 SMPL 모델의 UV 좌표 렌더링 사용, (3) UV 매핑에 DensePose 사용 등 세 가지 변형 네트워크가 훈련됩니다. 그 결과 GT-UV와 DensePose 변형은 거의 동일한 반면, 3D 포즈 변형은 디테일이 부족하고 얼굴 영역에 노이즈가 발생하는 것으로 나타났습니다. GT-UV 및 DensePose 변형은 실측 결과와 거의 유사합니다. DensePose 및 3D 포즈 매핑 변형은 합성 데이터로만 학습하면서 실제 영상에 직접 사용할 수 있습니다.

6.4. Impact of visibility

섹션 6에서는 저자들이 제안한 방법에 대한 정성적, 정량적 평가를 제시합니다. 피플스냅샷 데이터 세트에서 단안 인체 형상 재구성을 위한 네 가지 관련 방법과 그들의 방법을 비교합니다. 그 결과, 120개의 프레임을 입력으로 사용하는 방법과 비교했을 때에도 다른 방법에 비해 가장 높은 수준의 디테일을 제공하는 것으로 나타났습니다. 또한 다양한 감독 손실의 영향과 UV 매핑이 이 방법에 미치는 영향도 평가합니다. 저자들은 다양한 포즈와 카메라까지의 거리로 인해 발생하는 다양한 가시성 설정에 대한 방법의 견고함을 보여줍니다.

6.5. Garment transfer

이 섹션에서는 기존의 단안 인체 형상 재구성 방법과 정성적, 정량적 비교를 통해 제안한 방법을 평가합니다. 또한 다양한 유형의 감독 및 UV 매핑의 영향과 다양한 가시성 설정에서 방법의 견고성을 연구합니다. 마지막으로 의류 전송 또는 가상 시착에 이 방법을 적용할 수 있는 가능성을 보여줍니다. 전반적으로 이 결과는 제안된 방법의 다양성, 견고성 및 높은 품질을 보여줍니다.

7. Discussion and Conclusion

이 논문에서 저자는 단일 입력 이미지에서 사람의 전신 형태를 추론하기 위해 Tex2Shape라는 방법을 제안합니다. 이 방법은 DensePose에서 생성된 부분 텍스처 맵을 사용하여 UV 공간에서 노멀 및 변위 맵의 형태로 디테일을 추정합니다. 추정된 UV 맵을 사용하면 높은 메시 해상도 없이도 빈도가 높은 디테일로 SMPL 바디 모델을 증강할 수 있습니다. 저자들은 Tex2Shape가 합성 데이터로만 훈련된 상태에서 실제 영상에 강력하게 일반화된다는 것을 보여줍니다. 또한 이 방법이 디테일과 품질 측면에서 최첨단 방법보다 성능이 뛰어나다는 것을 보여줍니다. 그러나 이 방법은 훈련 세트에 포함되지 않는 머리카락과 옷, 특히 긴 머리와 드레스에는 한계가 있습니다. 향후 연구에서는 모든 유형의 의상과 액세서리를 허용하는 형상 표현을 추가로 연구하고자 합니다.