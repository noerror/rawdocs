# Neural Volumes: Learning Dynamic Renderable Volumes from Images

[https://arxiv.org/abs/1906.07751](https://arxiv.org/abs/1906.07751)

[https://arxiv.org/pdf/1906.07751.pdf](https://arxiv.org/pdf/1906.07751.pdf)

- Jun 2019

### 1 INTRODUCTION

폴리곤 메시는 사실적인 장면에서 3D 지오메트리를 표현하는 데 널리 사용됩니다. 그러나 오클루전, 반사율 가변성, 동적 변화와 같은 문제로 인해 복잡한 실제 현상을 정확하게 캡처하는 데 어려움을 겪는 경우가 많습니다. 이 논문에서는 이러한 문제를 해결하기 위해 머신러닝을 활용하여 기하학적 재구성의 부정확성을 보정하는 새로운 체적 표현을 소개합니다.

![우리의 시스템에 의해 캡처되고 모델링된 객체들의 렌더링. 우리의 방법에 대한 입력은 동기화되고 보정된 멀티뷰 비디오로 구성됩니다. 우리는 레이 행진 알고리즘을 사용하여 엔드-투-엔드로 인코더-디코더 네트워크를 훈련함으로써 장면의 동적, 부피적 표현을 구축합니다.](Neural%20Volumes%20Learning%20Dynamic%20Renderable%20Volumes%2003b559cf1fdd4bf98f417db7d6851dcb/Untitled.png)

우리의 시스템에 의해 캡처되고 모델링된 객체들의 렌더링. 우리의 방법에 대한 입력은 동기화되고 보정된 멀티뷰 비디오로 구성됩니다. 우리는 레이 행진 알고리즘을 사용하여 엔드-투-엔드로 인코더-디코더 네트워크를 훈련함으로써 장면의 동적, 부피적 표현을 구축합니다.

최근 몇 년 동안 전통적인 물리학에서 영감을 받은 표현에 의존하는 대신 머신러닝을 사용하여 3D 씬에서 빛의 이동을 모델링하는 데 상당한 진전이 있었습니다. 이러한 기술은 복잡한 장면을 고품질로 렌더링할 수 있는 가능성을 보여주었습니다. 하지만 시스템 훈련 시 사용할 수 있는 관점이 제한적이고 신경 아키텍처에 3D 인식 기능이 부족하다는 한계가 있습니다.

이 연구에서는 기하학적 변형과 외형 변형을 동시에 학습하는 새로운 방법을 제안합니다. 이 방법은 색상과 불투명도의 반투명 3D 모델과 적분 투영을 통한 렌더링을 포함하는 지오메트리의 체적 표현을 사용합니다. 이 방법은 잠재적인 솔루션의 범위를 넓히는 방식으로 그라디언트 정보를 분산시킵니다.

과적합을 방지하기 위해 신경망 디코더를 사용하여 여러 시점에 걸쳐 일반화되는 솔루션을 발견하도록 장려합니다. 이 디코더는 볼륨 항목의 기울기에 공간적 규칙성을 도입하여 보다 일반화할 수 있는 솔루션을 생성합니다. 또한 인코더 네트워크를 사용하여 각 프레임에서 장면 상태를 인코딩하는 저차원 잠재 공간을 생성하여 시퀀스를 공동으로 재구성할 수 있습니다.

볼류메트릭 표현의 가장 큰 단점은 규칙적인 격자 구조로 인해 해상도가 제한된다는 점입니다. 이를 극복하기 위해 학습 알고리즘이 사용 가능한 메모리를 더 잘 활용할 수 있도록 하는 워핑 기법이 사용됩니다.

요약하자면, 이 새로운 접근 방식은 여러 시점에 걸쳐 잘 일반화되고 동적 장면을 재구성하며 새로운 콘텐츠 생성이 가능하고 엔드투엔드 학습이 가능한 체적 표현을 제공합니다. 이미지 수준의 감독만 필요하며 실시간 렌더링과 즉각적인 조정이 가능하므로 가상 현실의 인터랙티브 애플리케이션에 적합합니다.

### 2 RELATED WORK

이 논문에서 소개하는 접근 방식은 체적 재구성, 변형 가능한 볼륨, 신경 렌더링, 새로운 뷰 합성 등 다양한 영역에서 활용됩니다.

고전적인 표면 및 체적 재구성 기법은 컴퓨터 비전에서 중요한 역사를 가지고 있습니다. 초기 작업은 주로 포인트 기반 및 표면 기반 재구성에 중점을 두었으며, 멀티뷰의 경우 상당한 관심을 기울였습니다. 그러나 이러한 방법의 대부분은 머리카락이나 연기와 같이 얇은 구조와 밀도가 높은 반투명 물질을 캡처하는 데 어려움을 겪었습니다.

복셀 컬러링 및 공간 조각과 같은 체적 재구성 방법은 멀티뷰 이미지에서 복셀 그리드의 점유율과 색상을 복구하는 데 집중함으로써 이러한 문제를 피할 수 있었습니다. 이 백서에서 제안하는 접근 방식은 이러한 방법에서 영감을 얻었습니다. 그러나 입력 이미지를 더 잘 일치시키기 위해 생성 모델을 활용하여 기존 방법의 일반적인 한계를 극복하고 더 복잡한 재료와 미세한 형상을 캡처할 수 있다는 점에서 차이가 있습니다.

최근의 체적 재구성에 관한 연구에서는 광선이 처음 닿는 표면과 해당 픽셀의 색상 사이의 비용 함수인 광선 전위 개념을 탐구했습니다. 광선 전위를 사용한 이전 작업과 현재 작업의 차이점은 현재 작업에서는 일반적으로 재구성 연구의 초점인 점유 확률이 아닌 복셀 투명도를 모델링한다는 것입니다. 렌더링에 초점을 맞춘 이 새로운 접근 방식은 연기와 같은 반투명 머티리얼이 포함된 동적 장면을 재구성하고 렌더링할 수 있게 해줍니다.

변형이 가능한 비강체 오브젝트는 최적화와 학습 기반 접근 방식 모두에 어려움을 줍니다. DynamicFusion과 같은 이전 작업에서는 TSDF(Truncated Signed Distance Function) 표현을 사용하여 기본 3D 템플릿 표면과 시간에 따라 달라지는 워프를 생성하여 뎁스 프레임을 융합했습니다. 이 작업은 또한 워프 필드 개념을 고전적인 볼륨 레이 마칭 프로세스에 통합하여 모션을 모델링할 뿐만 아니라 동적 비균일 샘플링 그리드를 시뮬레이션하여 복셀 그리드의 유효 해상도를 높였습니다. 이 접근 방식은 2D에서 이미지를 정렬하는 워프를 학습하는 데 사용되는 유사한 기법에서 영감을 얻었습니다.

딥러닝을 사용한 뉴럴 렌더링은 체적 표현을 활용하는 일부 방법과 함께 점점 더 많은 관심을 받고 있는 분야입니다. 이러한 방법 중 일부는 좋은 결과를 제공하지만 이미지와 복셀 재구성 간의 대응이 필요하거나 정적 오브젝트 및 장면으로 제한되는 등 특정 한계가 있습니다. 이 작업은 머신러닝을 사용하여 RGBA 볼륨을 생성한 다음 학습된 매개변수 없이 레이 마칭 알고리즘을 사용하여 렌더링한다는 점에서 이전 접근 방식과 차별화됩니다. 이 전략은 해석 가능한 볼륨을 제공함으로써 더 나은 시점 일반화로 이어질 수 있습니다.

새로운 뷰 합성은 주어진 입력 이미지 세트에서 새로운 뷰의 RGB 이미지를 생성하는 데 중점을 둡니다. 대부분의 방법은 일반적으로 기하학적 프록시를 사용하고 어떤 형태의 블렌딩을 수행하여 최종 픽셀 색상을 생성합니다. 그러나 이 백서에서 제안하는 접근 방식은 시퀀스에서 작동하고 애니메이션이 가능한 모델을 생성하므로 대부분의 새로운 뷰 합성 기법과 차별화됩니다. 또한 이 방법은 단일 생성 프레임워크를 사용하여 동적 장면의 새로운 뷰를 생성하며, 모델의 잠재 장면 임베딩을 통해 새로운 임베딩 시퀀스를 통해 새로운 애니메이션을 보다 유연하게 생성할 수 있습니다.

이 논문에서 사용한 방법은 딥어프런스 모델을 비롯한 다른 많은 기술처럼 정밀한 메시 트래킹이나 다른 형태의 전처리에 의존하지 않습니다. 대신 원시 이미지만을 사용하여 엔드투엔드로 학습하므로 메시로 모델링하기 어려운 머리카락과 같은 복잡한 표면을 표현하는 데 유리합니다.

### 3 OVERVIEW

제안된 방법은 내부 3D 체적 표현을 통합하는 이미지 감독만을 사용하여 새로운 뷰에서 이미지를 렌더링하는 엔드투엔드 파이프라인을 도입합니다. 이 접근 방식에는 두 가지 주요 구성 요소가 있습니다:

입력 이미지를 3D 볼륨인 V(x)로 변환하는 인코더-디코더 네트워크.
카메라 파라미터 세트가 주어지면 볼륨 V에서 이미지를 렌더링하는 차등 레이마칭 단계.
이 접근 방식은 최종 레이어가 자유 파라미터가 없는 고정 함수 볼륨 렌더링 작업인 자동 인코더로 개념화할 수 있습니다.

3D 볼륨 모델은 3D 위치(x ∈ R³)를 해당 지점의 로컬 RGB 색상과 차등 불투명도(V: R³ → R⁴)에 매핑합니다. 구체적으로 V(x) = [V_rgb(x), V_α(x)]로, 여기서 V_rgb(x) ∈ R³은 x의 색을 나타내고 V_α(x) ∈ R은 0(완전 투명도)에서 ∞에 이르는 차등 불투명도입니다.

반투명 볼륨은 이산 볼륨 표현을 부드럽게 하여 학습을 위한 그라데이션이 흐르도록 하고, 머리카락과 같이 제한된 해상도에서 반투명하게 보이는 반투명 오브젝트나 얇은 구조물을 모델링할 수 있도록 하는 두 가지 용도로 사용됩니다.

그림 2는 파이프라인을 시각적으로 보여줍니다. 첫 번째 단계는 공연의 다양한 시점에서 동기화되고 보정된 비디오 스트림 세트를 캡처하는 것입니다. 그런 다음 인코더 네트워크가 각 시점의 카메라 하위 집합에서 이미지를 가져와 해당 시점의 장면 상태를 나타내는 잠재 코드 z를 생성합니다. 그런 다음 볼륨 디코더 네트워크가 이 잠상 코드에서 3D 볼륨을 생성하여 각 포인트 x에 RGBα 값을 제공합니다. 마지막으로 누적 광선 마칭 알고리즘이 특정 시점의 볼륨을 렌더링합니다.

![우리의 방법의 파이프라인. 우리는 멀티뷰 캡처 시스템에서 시작하며, 이 중 일부 카메라를 우리의 인코더의 입력으로 선택합니다. 인코더는 잠재 코드 z를 생성하고, 이는 공간의 각 점에 대한 RGB와 α값, 그리고 RGBα 볼륨에 인덱스하는 데 사용되는 워프 필드로 디코딩됩니다. 디코더는 추가 제어 신호(예: 머리 포즈) c를 선택적으로 사용할 수 있습니다. 그런 다음 우리는 볼륨을 렌더링하기 위해 누적 레이 행진 알고리즘을 사용합니다. 최종 출력은 이미지 공간에서의 RGB 이미지와 관련된 알파 마스크와 추정된 배경입니다. 우리는 이러한 구성요소를 합성하고 렌더링된 이미지와 대상 이미지 사이의 L2 손실을 네트워크 매개 변수에 대해 최소화합니다.](Neural%20Volumes%20Learning%20Dynamic%20Renderable%20Volumes%2003b559cf1fdd4bf98f417db7d6851dcb/Untitled%201.png)

우리의 방법의 파이프라인. 우리는 멀티뷰 캡처 시스템에서 시작하며, 이 중 일부 카메라를 우리의 인코더의 입력으로 선택합니다. 인코더는 잠재 코드 z를 생성하고, 이는 공간의 각 점에 대한 RGB와 α값, 그리고 RGBα 볼륨에 인덱스하는 데 사용되는 워프 필드로 디코딩됩니다. 디코더는 추가 제어 신호(예: 머리 포즈) c를 선택적으로 사용할 수 있습니다. 그런 다음 우리는 볼륨을 렌더링하기 위해 누적 레이 행진 알고리즘을 사용합니다. 최종 출력은 이미지 공간에서의 RGB 이미지와 관련된 알파 마스크와 추정된 배경입니다. 우리는 이러한 구성요소를 합성하고 렌더링된 이미지와 대상 이미지 사이의 L2 손실을 네트워크 매개 변수에 대해 최소화합니다.

시스템은 각 입력 이미지를 재구성하고 전체 훈련 세트에서 제곱 픽셀 재구성 손실을 최소화하여 엔드투엔드로 훈련됩니다. 훈련 중에는 전체 파이프라인이 인코더-디코더 네트워크의 가중치를 훈련하는 데 활용됩니다. 추론 시에는 훈련 이미지에서 생성된 잠재 코드 시퀀스 또는 새로 생성된 시퀀스에서 잠재 코드 스트림 z가 생성되고 실시간으로 디코딩 및 렌더링됩니다.

### 4 ENCODER NETWORK

우리 시스템에서 새로운 시퀀스 생성을 가능하게 하는 주요 구성 요소는 인코더-디코더 아키텍처입니다. 이 설정은 일관된 잠재 표현 z ∈ R²⁵⁶를 사용하여 씬의 상태를 인코딩합니다. 이 잠재 공간에서의 트래버스는 새로운 볼륨 시퀀스로 디코딩할 수 있으며, 이를 통해 모든 시점에서 렌더링할 수 있습니다. 이는 재생만 허용하거나 생성 프로세스를 제한적으로 제어할 수 있는 특수한 프레임별 메시 구조에 의존하는 방법과 대조적입니다..

![인코더 아키텍처. 입력은 K-camera의 부분집합에서 온 이미지들로, 카메라별 CNN을 통과합니다. 잠재 변수 z는 L-차원 조건 변수 c와 연결되어 디코더로 전달됩니다(§5 참조).](Neural%20Volumes%20Learning%20Dynamic%20Renderable%20Volumes%2003b559cf1fdd4bf98f417db7d6851dcb/Untitled%202.png)

인코더 아키텍처. 입력은 K-camera의 부분집합에서 온 이미지들로, 카메라별 CNN을 통과합니다. 잠재 변수 z는 L-차원 조건 변수 c와 연결되어 디코더로 전달됩니다(§5 참조).

또한 이 표현 방식은 재생 중에 씬 상태의 일부만 변경되는 조건부 디코딩(예: 음성 중 표정이나 뷰에 따라 달라지는 외형 효과)이 가능합니다. 인코더-디코더 아키텍처는 훈련 중에 조건 변수의 쌍을 이루는 샘플을 사용할 수 있는 경우 이 기능을 자연스럽게 지원합니다.

잠재 공간을 구성하기 위해 특정 시점의 장면 상태는 멀티 카메라 캡처 시스템의 뷰 하위 집합을 사용하여 컨볼루션 신경망(CNN)에 의해 인코딩됩니다. 각 카메라 뷰는 전용 브랜치를 통과한 후 다른 브랜치의 뷰와 연결되고 최종 모양으로 인코딩됩니다.

모든 카메라 뷰를 입력으로 사용하는 것이 정보 이론적으로 최적이지만, K = 3 뷰를 사용하는 것이 실용적이고 계산적으로 더 효율적이라는 것이 입증되었습니다. 커버리지를 최대화하기 위해 대략 직교하는 카메라의 하위 집합이 선택됩니다. 구체적으로 이 시스템은 정면, 왼쪽, 오른쪽에서 가장 많은 카메라 뷰를 사용하고 이미지를 8배에서 334×512픽셀로 다운샘플링합니다.

잠재 공간을 통과하는 동안 그럴듯한 샘플을 생성하려면 생성 모델이 훈련 샘플 간에 잘 일반화해야 합니다. 이는 일반적으로 부드러운 잠재 공간을 학습함으로써 달성할 수 있습니다. 평활화를 촉진하기 위해 변형 아키텍처가 사용됩니다. 인코더는 대각선 256차원 가우시안(즉, µ와 σ)의 파라미터를 출력하고 표준 정규 분포로부터의 KL-분산을 정규화에 사용합니다.

또한 가변 아키텍처는 인코더와 함께 훈련할 때 디코더가 컨디셔닝 변수를 활용하도록 보장합니다. 변형 병목 현상은 비정보 잠재 차원을 최대화하고, 컨디셔닝 변수와 관련된 정보는 잠재 공간 밖으로 투사되어 디코더가 재구성 시 컨디셔닝 변수를 사용하도록 합니다.

이 컨디셔닝 방법은 사용자가 렌더링된 출력을 제어하기 위해 사용할 수 있는 모든 보조 정보를 사용하여 적용할 수 있습니다. 뷰 컨디셔닝은 스페큘러와 같이 뷰에 따라 달라지는 효과를 올바르게 렌더링할 수 있기 때문에 특히 중요합니다. VR에서 볼 때 뷰 벡터 형태의 보조 정보는 가상 씬에서 헤드셋의 상대적 방향에서 얻을 수 있습니다.

### 5 VOLUME DECODERS

이 섹션에서는 볼륨 디코더라고 하는 다양한 신경망 아키텍처를 사용하여 RGBα 볼륨 함수 V를 파라미터화하는 다양한 방법을 살펴봅니다. 여기에는 복셀 그리드 및 MLP(다층 퍼셉트론)와 같은 볼륨에 대한 가능한 표현과 워핑 필드를 사용하여 효과적인 해상도를 향상시키는 방법이 포함됩니다. 또한 뷰에 따라 달라지는 외관을 모델링하기 위한 뷰 컨디셔닝에 대해서도 설명합니다.

5.1 MLP 디코더

상태 z를 갖는 점 x의 체적 함수 V(x; z)에 대한 한 가지 잠재적 모델은 비선형성을 갖는 일련의 완전히 연결된 레이어로 생성된 암시적 모델입니다. 이 접근 방식은 복셀 그리드 해상도나 저장 공간의 제약을 받지 않는다는 장점이 있습니다. 그러나 실제로 고품질 재구성에 필요한 크기의 MLP는 비현실적입니다. 또한 광선 이동 프로세스에서 각 광선을 따라 매 단계마다 MLP를 평가해야 하므로 실시간 애플리케이션의 MLP 복잡도에 대한 상한선을 설정해야 합니다.

5.2 복셀 그리드 디코더

MLP로 전체 볼륨을 암시적으로 모델링하는 대신 볼륨 함수를 복셀의 개별적인 3D 그리드로 모델링할 수 있습니다. 이 명시적인 3D 복셀 그리드는 신경망의 출력 텐서로 생성됩니다. 중심이 xo이고 측면이 W 크기인 3D 큐브에 대한 볼륨 디코더를 정의합니다. 실제로는 컨볼루션 아키텍처 또는 완전히 연결된 일련의 레이어를 사용하여 g(z)를 구현합니다.

RGBα 값에 소프트플러스 함수를 적용하여 음수가 아닌지 확인합니다. 두 가지 디코더 변형은 그림 4에 나와 있습니다.

![Voxel grid 디코더들. (a) 컨볼루션 디코더와 (b) 선형 기반 디코더. L은 조건 변수의 크기를 나타냅니다.](Neural%20Volumes%20Learning%20Dynamic%20Renderable%20Volumes%2003b559cf1fdd4bf98f417db7d6851dcb/Untitled%203.png)

Voxel grid 디코더들. (a) 컨볼루션 디코더와 (b) 선형 기반 디코더. L은 조건 변수의 크기를 나타냅니다.

5.3 워핑 필드

복셀 그리드만으로는 단일 복셀만큼 작은 디테일만 표현할 수 있고 고해상도에서 평가 및 저장하는 데 계산 비용이 많이 들기 때문에 한계가 있습니다. 또한 장면의 대부분이 빈 공간으로 구성된 일반적인 장면에서는 낭비가 심합니다.

이러한 문제를 해결하기 위해 유니티는 워핑 필드를 사용하여 복셀 볼륨의 유효 해상도를 변경하고 모션을 보다 자연스럽게 모델링할 것을 제안합니다. 워핑 공식에서는 템플릿 RGBα 볼륨 T(x)와 워프 볼륨 W-1(x)을 생성합니다.

역 워프 필드는 W-1 (x) → y x, y ∈ R3로 정의됩니다. 역 워프를 선택하면 추가 메모리 없이 출력 공간의 작은 복셀 영역을 템플릿 공간의 더 큰 영역에 매핑하여 해상도 증가 변환을 표현할 수 있습니다. 따라서 역 워프는 균일한 그리드 샘플링보다 더 높은 해상도로 출력 공간의 디테일을 표현할 수 있지만 학습 중에 사용 가능한 그라데이션을 제공하는 데 필요한 출력 공간의 모든 곳에 잘 정의된 상태를 유지합니다.

최종 볼륨 값을 생성하기 위해 먼저 역워프 값을 평가한 다음 워프된 지점에서 템플릿 볼륨 T를 샘플링합니다(VRGBα(x) = TRGBα(W-1(x)).

5.4 아핀 워프의 혼합물

워프 필드 디코더의 아키텍처는 모델의 품질에 큰 영향을 미치기 때문에 중요합니다. 워프 필드 디코딩에 대한 직접적인 접근 방식은 디컨볼루션을 사용하여 각 출력 지점에서 자유롭게 변화하는 템플릿 샘플 포인트를 가진 워프 필드를 생성하는 것입니다. 그러나 이러한 매개변수화는 너무 유연하기 때문에 과적합이 발생하고 새로운 뷰에 대한 일반화가 제대로 이루어지지 않을 수 있습니다. 따라서 워프 필드의 기본 구성 요소는 어파인 워프가 되어야 한다고 제안합니다. 단일 어파인 워프만으로는 비선형 굽힘을 모델링할 수 없기 때문에 어파인 워프의 공간적 혼합을 사용하여 역 워프 필드를 생성합니다.

아핀 혼합은 다음과 같이 표현합니다:
W-1(x) = Σ Ai(x) ai(x),
여기서 Ai(x)는 θ 번째 아핀 변환이고, {Ri, si, ti}는 θ 번째 아핀 변환 파라미터의 회전, 스케일링, 이동을 정의하며, ai(x)는 θ 번째 워프의 가중치 볼륨입니다. 워프 후 공간 혼합 가중치 wi("워프된 가중치")를 샘플링합니다.

변환 파라미터 {Ri, si, ti}와 가중치 볼륨 wi를 계산하기 위해 인코딩 z 뒤에 완전히 연결된 2개의 레이어를 사용합니다. 가중치를 출력하기 전에 가중치 볼륨의 값에 exp(-)를 적용하여 가중치가 음수가 아닌지 확인합니다. 실제로 16개의 워프를 혼합하면 충분한 표현력을 얻을 수 있습니다. 워프 필드를 표현하기 위해 32^3 복셀 그리드를 사용하는데, 이는 낮은 해상도가 학습에 도움이 되는 삼선 보간을 통해 부드러움을 제공하기 때문입니다.

5.5 뷰 컨디셔닝

뷰에 따라 달라지는 외관을 모델링하기 위해 시점에 따라 RGB 디코더 네트워크를 컨디셔닝하도록 선택합니다. 이를 통해 특정 함수 형태를 지정하지 않고도 데이터 기반 방식으로 스페큘러를 모델링할 수 있습니다. 이를 위해 인코딩과 함께 카메라의 정규화된 방향을 디코더에 입력합니다. 뷰 컨디셔닝 모델의 경우, 시점에 따라 RGB 값만 컨디셔닝하기 때문에 별도의 컨볼루션 브랜치를 사용하여 RGB 값과 α 값을 생성합니다.

### 6 ACCUMULATIVE RAY MARCHING

앞뒤 애디티브 블렌딩을 모방하는 반투명 볼륨의 렌더링 프로세스를 제안합니다. 카메라 광선이 불균일한 머티리얼의 볼륨을 통과할 때 경로를 따라 각 지점에서 머티리얼의 로컬 컬러와 밀도에 따라 컬러를 누적합니다.

6.1 반투명 볼륨 렌더링

볼륨 함수 V(x)에서 모델링하는 볼륨을 통해 광선을 행진하여 이미지를 생성합니다. 오클루전을 고려하기 위해 광선은 색상뿐만 아니라 불투명도도 누적합니다. 누적된 불투명도가 1에 도달하면(광선이 불투명 영역을 통과할 때와 마찬가지로) 광선에 더 이상 색을 누적할 수 없습니다. 카메라의 초점면에 있는 픽셀 p의 색상 Irgb(p)는 광선이 단위 방향 rd로 행진하여 얻습니다. 렌더링 프로세스는 볼륨을 통과하는 광선의 경로에 대한 적분으로 주어지며, 누적된 불투명도는 오클루전을 모델링하기 위해 1로 고정됩니다.

알고리즘 1은 볼륨 V와 교차하는 광선의 출력 색상 계산을 보여줍니다. 특히 이 이미지 형성 모델은 차별화 가능하므로 볼륨 V의 파라미터를 대상 이미지와 일치하도록 최적화할 수 있습니다. 실제로는 스텝 크기를 볼륨 크기의 1/128로 설정하여 512 × 600 해상도에서 가상 현실에서 실시간 스테레오를 구현할 수 있었습니다.

6.2 하이브리드 렌더링

반투명 볼륨 표현은 다용도로 사용할 수 있지만, 일부 유형의 씬 콘텐츠는 래핑되지 않은 텍스처 맵과 결합된 표면 기반 표현을 사용하여 고해상도로 더 효율적으로 표현할 수 있습니다. 예를 들어 사람 얼굴의 미세한 디테일을 표현할 수 있는 특수 캡처 시스템을 사용할 수 있으며 일반적으로 1024^2보다 큰 텍스처 해상도를 사용합니다.

위에서 설명한 볼륨 표현은 기존 메시 기반 표현과 자연스럽게 통합될 수 있습니다. 알고리즘 1에 설명된 대로 렌더링하되, 메시와 교차하는 모든 광선에 대해 tmax를 메시 깊이로 설정합니다. 광선이 tmax에 도달하면 남은 색상 처리량은 교차점에 있는 메시의 색상으로 채워집니다.

8.6에서 설명한 것처럼 이 하이브리드 렌더링 프로세스를 사용하여 학습한 반투명 볼륨은 메시가 더 충실하게 표현되는 영역에서 메시가 가려지는 것을 자연스럽게 방지합니다.

### 7 END-TO-END TRAINING

이 섹션에서는 인코더-디코더 네트워크의 가중치 θ를 훈련하는 방법에 대한 자세한 내용을 설명합니다. 카메라별 색상 보정 행렬과 정적 배경 이미지의 추정, 손실 함수의 구성, 정확도를 향상시키는 재구성 선행에 대해 설명합니다.

7.1 컬러 보정

소니 카메라는 기하학적으로 보정되었지만 서로에 대한 상대적인 색상 보정은 이루어지지 않았습니다. 카메라 간에 일관된 색상 표현을 보장하기 위해 카메라별 및 채널별 게인 д 및 바이어스 b를 도입하여 재구성된 이미지를 실측 이미지와 비교하기 전에 적용합니다.

7.2 배경

훈련 데이터에는 알고리즘이 재구성을 시도할 수 있는 정적 배경이 포함되는 경우가 많습니다. 알고리즘이 관심 객체를 재구성하는 데 초점을 맞추기 위해 카메라별 배경 이미지 I(bg)rgb를 추정합니다. 특정 뷰의 최종 이미지는 모든 픽셀을 레이마칭하고 볼륨을 종료할 때 남은 불투명도에 따라 결과물인 Irgb(p)를 해당 배경 픽셀 I(bg)rgb(p)와 결합하여 얻습니다.

7.3 재구성 선행사

프리어를 사용하지 않고 재구성된 볼륨에는 보정 오류 또는 뷰 종속 효과로 인해 연기 같은 아티팩트가 포함되는 경향이 있습니다. 이러한 아티팩트를 완화하기 위해 두 가지 프리어를 도입했습니다.

첫 번째 선행은 로그 복셀 불투명도의 전체 변화를 정규화하여 희박한 공간 그라데이션을 적용함으로써 불투명 영역과 투명 영역 사이의 선명한 경계를 복구하는 데 도움이 됩니다.

두 번째 선행은 최종 이미지 불투명도 Iα(p)에 대한 베타 분포(Beta(0.5, 0.5))입니다. 이 선행은 대부분의 광선이 물체나 배경에 부딪혀야 한다는 직관에 따라 출구 불투명도의 엔트로피를 줄입니다.

7.4 훈련 하이퍼파라미터

전체 훈련 목표는 손실 함수와 잠재 인코딩 z와 가변 자동 인코더에 사용되는 표준 정규 분포 사이의 KL 발산 및 앞서 설명한 전제 조건과 결합합니다.

손실 함수를 최소화하기 위해 아담 옵티마이저를 사용합니다. 인코더-디코더 네트워크 가중치는 10^-4의 학습률로 훈련되고, 추정 배경 이미지와 카메라당 게인 및 바이어스는 각각 10^-1 및 10^-3의 별도 학습률로 훈련됩니다. 메모리 사용량을 줄이기 위해 이미지에서 픽셀을 무작위로 샘플링하며, 훈련 프로세스는 단일 NVIDIA Tesla V100에서 약 10일이 소요됩니다.

### 8 EXPERIMENTS

우리는 모델을 평가하기 위해 정량적 실험과 정성적 실험을 모두 수행합니다. 실험에는 1283 크기의 컨볼루션 볼륨 디코더가 사용됩니다. 이 모델의 설계 선택은 품질과 속도 사이의 균형을 맞추고 새로운 관점에 대한 일반화를 보여줍니다. 보풀, 연기, 머리카락과 같이 일반적으로 재구성하기 어려운 오브젝트에 대한 모델의 성능을 시연합니다. 기존의 트라이앵글 래스터화 및 잠재 공간에서 보간을 사용한 애니메이션을 사용한 모델의 성능도 소개합니다.

우리는 34개의 컬러 카메라로 구성된 멀티 카메라 시스템을 사용하여 데이터를 캡처합니다. 카메라 시스템은 아이코사헤드 체커보드 패턴을 사용하여 보정됩니다. 원시 이미지와 카메라 보정 데이터는 이 방법의 유일한 입력입니다. 정량적 평가를 위해 훈련 카메라와 7대의 보류된 검증 카메라 세트에서 픽셀 재구성의 평균 제곱 오차를 비교합니다.

8.1 워핑 방법

워핑이 없는 모델, 컨볼루션 신경망에 의해 생성된 워프가 있는 모델, 어파인 혼합 가중치를 계산하기 전에 워프를 적용하지 않는 모델, 제안한 어파인 혼합 워프 모델 등 여러 대안과 우리의 워핑 방법을 비교합니다. 특히 움직이는 손, 흔들리는 머리카락, 드라이아이스 연기 등 복잡한 데이터 세트에서 아핀 혼합 모델이 다른 모델보다 성능이 뛰어나다는 것을 확인했습니다.

![워핑 방법 평가. 이 실험에서는 워핑에 대한 다양한 기법을 평가합니다. 우리는 이동하는 손, 흔들리는 머리카락, 건조한 얼음 연기와 같은 세 가지 다른 데이터셋에서 모델을 평가합니다: 워핑 없음, 컨볼루션 워핑(전치 컨볼루션 사용), 컨볼루션 워핑(삼선형 업샘플링 후 컨볼루션 사용), 워프 없는 어파인 혼합, 공간 가중치 볼륨 wi가 평가되기 전에 워프되지 않음 (Eq. (13)), 그리고 워프 공간 어파인 혼합, Eq. (6)에서 설명된 방식. 모든 경우에서, 우리의 워프 공간 어파인 혼합은 다른 모델들을 능가합니다.](Neural%20Volumes%20Learning%20Dynamic%20Renderable%20Volumes%2003b559cf1fdd4bf98f417db7d6851dcb/Untitled%204.png)

워핑 방법 평가. 이 실험에서는 워핑에 대한 다양한 기법을 평가합니다. 우리는 이동하는 손, 흔들리는 머리카락, 건조한 얼음 연기와 같은 세 가지 다른 데이터셋에서 모델을 평가합니다: 워핑 없음, 컨볼루션 워핑(전치 컨볼루션 사용), 컨볼루션 워핑(삼선형 업샘플링 후 컨볼루션 사용), 워프 없는 어파인 혼합, 공간 가중치 볼륨 wi가 평가되기 전에 워프되지 않음 (Eq. (13)), 그리고 워프 공간 어파인 혼합, Eq. (6)에서 설명된 방식. 모든 경우에서, 우리의 워프 공간 어파인 혼합은 다른 모델들을 능가합니다.

8.2 뷰 컨디셔닝

스페큘러와 같이 뷰에 따라 달라지는 효과를 모델링하는 것은 렌더링에 매우 중요합니다. 렌더링된 뷰의 시점에 따라 RGB 디코더를 컨디셔닝함으로써 네트워크는 시야각에 따라 장면의 특정 부분의 색상을 조정할 수 있습니다. 뷰 컨디셔닝 모델은 뷰에 따라 달라지는 장면의 일부 모습을 표현할 수 있기 때문에 뷰 컨디셔닝이 적용되지 않은 모델보다 새로운 관점에서 장면을 더 효과적으로 모델링할 수 있습니다.

8.3 선행 및 배경 추정

재구성된 볼륨에 여러 가지 선행 요소를 사용하여 재구성의 아티팩트를 줄입니다. 배경 추정과 프리어가 재구성된 볼륨의 품질에 미치는 영향을 평가했습니다. 평가는 알려진 배경 이미지, 학습된 배경 이미지, 배경 모델 없음의 세 가지 시나리오에서 각각 전제 조건의 유무에 따라 수행되었습니다. 그 결과 전처를 사용할 때 검증 관점에서 성능이 향상되는 것으로 나타났습니다. 흥미롭게도 배경 이미지를 학습하는 것이 알려진 배경 이미지를 사용하는 것보다 약간 더 나은 성능을 보였습니다.

8.4 정성적 결과

정성적 결과는 이 방법으로 렌더링한 결과와 실사 이미지를 비교한 것입니다. 보풀, 연기, 사람의 피부와 머리카락과 같은 복잡한 현상을 효과적으로 모델링할 수 있지만, 종종 옅은 스모키 패턴으로 나타나는 일부 아티팩트가 발생하기도 합니다. 카메라 뷰가 많을수록 이러한 아티팩트가 줄어드는 경향이 있습니다. 또한 시간 경과에 따른 템플릿 볼륨의 변화와 이것이 렌더링되는 최종 왜곡 볼륨과 어떻게 일치하는지 보여줍니다.

![질적 결과. 이 그림에서는 3개의 데이터셋에 대해 3개의 유효성 검사 뷰포인트에 대한 그라운드 트루스, 재구성된 이미지, 각 픽셀에 대한 평균 제곱근 오차의 시각화를 보여줍니다.](Neural%20Volumes%20Learning%20Dynamic%20Renderable%20Volumes%2003b559cf1fdd4bf98f417db7d6851dcb/Untitled%205.png)

질적 결과. 이 그림에서는 3개의 데이터셋에 대해 3개의 유효성 검사 뷰포인트에 대한 그라운드 트루스, 재구성된 이미지, 각 픽셀에 대한 평균 제곱근 오차의 시각화를 보여줍니다.

![시간을 통해 변화하는 워핑과 템플릿 볼륨. 이 그림은 두 개의 시퀀스에서 여러 프레임을 나타내며, (a) 최종 워핑된 볼륨, (b) 워핑 이전에 학습된 템플릿 볼륨을 보여줍니다. 얼굴 시퀀스의 경우, 대부분의 변화는 워핑을 통해 설명되며 얼굴은 템플릿 공간에서 정적입니다. 그러나 추적이 명시적으로 강제되지 않는 경우, 네트워크는 모션을 더 간결하게 표현할 수 있도록 다른 템플릿으로 표현하는 것을 배울 수 있습니다. 이는 손 시퀀스의 손가락 움직임에서 확인할 수 있습니다. 템플릿에서는 텍스처 세부 사항이 필요한 영역에 공간이 할당되며 평평한 영역은 압축되는 것도 알 수 있습니다. 손 예제에서의 지문과 팔이 이에 해당합니다.](Neural%20Volumes%20Learning%20Dynamic%20Renderable%20Volumes%2003b559cf1fdd4bf98f417db7d6851dcb/Untitled%206.png)

시간을 통해 변화하는 워핑과 템플릿 볼륨. 이 그림은 두 개의 시퀀스에서 여러 프레임을 나타내며, (a) 최종 워핑된 볼륨, (b) 워핑 이전에 학습된 템플릿 볼륨을 보여줍니다. 얼굴 시퀀스의 경우, 대부분의 변화는 워핑을 통해 설명되며 얼굴은 템플릿 공간에서 정적입니다. 그러나 추적이 명시적으로 강제되지 않는 경우, 네트워크는 모션을 더 간결하게 표현할 수 있도록 다른 템플릿으로 표현하는 것을 배울 수 있습니다. 이는 손 시퀀스의 손가락 움직임에서 확인할 수 있습니다. 템플릿에서는 텍스처 세부 사항이 필요한 영역에 공간이 할당되며 평평한 영역은 압축되는 것도 알 수 있습니다. 손 예제에서의 지문과 팔이 이에 해당합니다.

![여러 객체에 대한 학습된 재구성을 보여주며, (a) 렌더링된 색과 불투명도, (b) 다른 수준의 차등 불투명도에 대한 Vα (x)의 등위면, (c) 차등 불투명도의 등고선 그림, (d) x-y 절단에서 볼륨의 내부를 보여주는 슬라이스 렌더, (e) 같은 뷰포인트에서 완전한 렌더, 그리고 (f) 워프되지 않은 템플릿을 보여줍니다.](Neural%20Volumes%20Learning%20Dynamic%20Renderable%20Volumes%2003b559cf1fdd4bf98f417db7d6851dcb/Untitled%207.png)

여러 객체에 대한 학습된 재구성을 보여주며, (a) 렌더링된 색과 불투명도, (b) 다른 수준의 차등 불투명도에 대한 Vα (x)의 등위면, (c) 차등 불투명도의 등고선 그림, (d) x-y 절단에서 볼륨의 내부를 보여주는 슬라이스 렌더, (e) 같은 뷰포인트에서 완전한 렌더, 그리고 (f) 워프되지 않은 템플릿을 보여줍니다.

8.5 단일 프레임 추정

또한 단일 프레임만을 입력으로 사용하여 모델을 평가했습니다. 이 접근 방식은 모션 규칙성 및 중복성을 활용하는 모델의 능력 저하와 같은 여러 가지 문제를 야기했습니다. 이 실험을 통해 각 모델 구성 요소의 기여도를 평가할 수 있었습니다. 그 결과 단일 프레임에서도 컨볼루션 아키텍처를 통해 오브젝트를 정확하게 복구하고 다시 렌더링할 수 있음을 확인할 수 있었습니다.

![단일 프레임 추정. 이 실험에서는 시퀀스가 아닌 단일 프레임만을 재구성합니다. 비교를 위해, 왼쪽에서 오른쪽으로 첫 세 열은 MVS 기반 재구성을 보여줍니다: (a) COLMAP [Schönberger and Frahm 2016; Schönberger et al. 2016]에 의해 복구된 포인트 클라우드, (b) Poisson 재구성 후의 메시, 그리고 (c) Agisoft Metashape [2019]에 의해 복구된 텍스처화된 메시입니다. 열 (d)는 space carving [Kutulakos and Seitz 2000]을 사용한 색상화된 voxel 점유 재구성을 보여주고, 열 (e)는 voxel 그리드의 '직접' 추정을 보여줍니다(즉, T와 W^-1이 인코더-디코더 네트워크의 결과가 아니라 직접 추정됩니다). 마지막으로, (f)는 단일 프레임만을 훈련시킨 우리의 전체 파이프라인 결과를 보여줍니다. 결과는 인코더-디코더 아키텍처가 정확한 추정치를 복구하는데 도움이 된다는 것을 보여줍니다.](Neural%20Volumes%20Learning%20Dynamic%20Renderable%20Volumes%2003b559cf1fdd4bf98f417db7d6851dcb/Untitled%208.png)

단일 프레임 추정. 이 실험에서는 시퀀스가 아닌 단일 프레임만을 재구성합니다. 비교를 위해, 왼쪽에서 오른쪽으로 첫 세 열은 MVS 기반 재구성을 보여줍니다: (a) COLMAP [Schönberger and Frahm 2016; Schönberger et al. 2016]에 의해 복구된 포인트 클라우드, (b) Poisson 재구성 후의 메시, 그리고 (c) Agisoft Metashape [2019]에 의해 복구된 텍스처화된 메시입니다. 열 (d)는 space carving [Kutulakos and Seitz 2000]을 사용한 색상화된 voxel 점유 재구성을 보여주고, 열 (e)는 voxel 그리드의 '직접' 추정을 보여줍니다(즉, T와 W^-1이 인코더-디코더 네트워크의 결과가 아니라 직접 추정됩니다). 마지막으로, (f)는 단일 프레임만을 훈련시킨 우리의 전체 파이프라인 결과를 보여줍니다. 결과는 인코더-디코더 아키텍처가 정확한 추정치를 복구하는데 도움이 된다는 것을 보여줍니다.

8.6 애니메이션

우리의 재구성 방법을 사용하면 다양한 각도에서 캡처한 데이터를 재생하고 다시 렌더링할 수 있습니다. 또한 잠재 변수를 수정하여 새로운 콘텐츠 시퀀스를 생성함으로써 공연의 콘텐츠를 추정할 수도 있습니다. 예를 들어, 잠재 코드를 보간하여 실시간으로 새로운 시퀀스를 만들 수 있었습니다. 또한 텍스처 메시 표현과 복셀 표현을 결합하여 보다 상세한 결과를 얻는 방법도 시연했습니다.

![새로운 컨텐츠 생성. 첫 번째 행: 잠재 공간에서의 보간. 가장 왼쪽과 가장 오른쪽 프레임은 실제 이미지 인코딩에서 재구성되며, 중간 프레임은 왼쪽과 오른쪽 코드 사이의 선형 보간에서 재구성됩니다. 두 번째 행: 사용자 입력에 기반한 실시간 아바타 구동. 자홍색 점은 사용자의 손의 위치를 나타내며, 아바타의 머리는 이를 따라 회전합니다.](Neural%20Volumes%20Learning%20Dynamic%20Renderable%20Volumes%2003b559cf1fdd4bf98f417db7d6851dcb/Untitled%209.png)

새로운 컨텐츠 생성. 첫 번째 행: 잠재 공간에서의 보간. 가장 왼쪽과 가장 오른쪽 프레임은 실제 이미지 인코딩에서 재구성되며, 중간 프레임은 왼쪽과 오른쪽 코드 사이의 선형 보간에서 재구성됩니다. 두 번째 행: 사용자 입력에 기반한 실시간 아바타 구동. 자홍색 점은 사용자의 손의 위치를 나타내며, 아바타의 머리는 이를 따라 회전합니다.

전체적으로 우리의 모델은 복잡한 오브젝트와 모션을 처리하는 데 강력한 성능을 발휘하여 정확하고 역동적인 3D 표현을 만들어 냈습니다. 추가 개선을 통해 사소한 아티팩트를 줄이고 단일 프레임 추정을 최적화할 수 있습니다.

![메시와 부피적 표현의 결합 (고해상도에서 이미지를 보는 것이 좋음). 초기 텍스처화된 메시 모델이 주어진 경우, 메시와 교차하는 모든 레이는 각 레이를 따라 메시의 깊이로 tmax를 설정합니다. tmax에서 종료하는 레이는 남아 있는 불투명도에 기반하여 메시에서 색을 축적합니다. 왼쪽에서 오른쪽으로: 마스크된 메시 (학습 중에 voxel 볼륨에 위치시킴), voxel 표현, 하이브리드 렌더링, 메시만의 재구성. 학습된 voxel 표현은 메시를 가리는 것을 피해 더 높은 품질의 재구성을 달성합니다.](Neural%20Volumes%20Learning%20Dynamic%20Renderable%20Volumes%2003b559cf1fdd4bf98f417db7d6851dcb/Untitled%2010.png)

메시와 부피적 표현의 결합 (고해상도에서 이미지를 보는 것이 좋음). 초기 텍스처화된 메시 모델이 주어진 경우, 메시와 교차하는 모든 레이는 각 레이를 따라 메시의 깊이로 tmax를 설정합니다. tmax에서 종료하는 레이는 남아 있는 불투명도에 기반하여 메시에서 색을 축적합니다. 왼쪽에서 오른쪽으로: 마스크된 메시 (학습 중에 voxel 볼륨에 위치시킴), voxel 표현, 하이브리드 렌더링, 메시만의 재구성. 학습된 voxel 표현은 메시를 가리는 것을 피해 더 높은 품질의 재구성을 달성합니다.

### 9 DISCUSSION

이 연구에서는 반투명 볼륨 표현을 사용하여 멀티뷰 RGB 이미지에서 오브젝트와 장면을 모델링하는 엔드투엔드 방법을 제시했습니다. 이 모델은 움직이는 머리카락, 흐릿한 장난감, 연기 등 까다로운 물체를 명시적인 추적 없이 실시간으로 재구성할 수 있는 능력을 입증했습니다.

![레이 마칭이 종료되는 위치를 보여주는 깊이 맵. 가슴에서의 구멍은 그 레이들이 전체 볼륨을 통과하고 배경에서만 종료된다는 것을 나타냅니다. 가슴 영역은 텍스처 변화가 제한되어 있고 배경과 비슷한 색상이므로, 이 아티팩트는 새로운 뷰포인트에서 볼 때도 재구성 오차에 큰 영향을 미치지 않습니다.](Neural%20Volumes%20Learning%20Dynamic%20Renderable%20Volumes%2003b559cf1fdd4bf98f417db7d6851dcb/Untitled%2011.png)

레이 마칭이 종료되는 위치를 보여주는 깊이 맵. 가슴에서의 구멍은 그 레이들이 전체 볼륨을 통과하고 배경에서만 종료된다는 것을 나타냅니다. 가슴 영역은 텍스처 변화가 제한되어 있고 배경과 비슷한 색상이므로, 이 아티팩트는 새로운 뷰포인트에서 볼 때도 재구성 오차에 큰 영향을 미치지 않습니다.

하지만 몇 가지 한계가 남아 있습니다. 예를 들어, 텍스처가 제한된 표면의 경우 추정된 볼륨이 해당 표면을 투명하게 잘못 표현할 수 있습니다. 하지만 유니티의 이미지 공간 손실 기능 덕분에 재구성의 품질이 부드럽게 저하됩니다. 또한 현재 방식은 굴절 표면을 고려하지 않습니다. 뷰 컨디셔닝을 통해 둔한 스페큘러 하이라이트는 표현할 수 있지만, 고주파 스페큘러 하이라이트는 올바르게 표현되지 않습니다.

잠상 공간을 사용하면 동적 콘텐츠를 생성할 수 있지만 시간적 역학을 명시적으로 모델링하지 않기 때문에 시각적으로는 정확하지만 모델링한 오브젝트의 실제 동작을 나타내지 못하는 시퀀스가 발생할 수 있습니다.

마지막으로, 일반적으로 볼류메트릭 표현은 해상도와 메모리 요구 사항 간의 세제곱 관계로 인해 해상도 한계에 직면하지만, 유니티는 워핑 필드를 사용하여 단순히 복셀 그리드 해상도를 높이지 않고도 효과적인 해상도를 높일 수 있는 방법을 제시했습니다. 유니티는 이러한 개선이 기존 텍스처 메시 표면으로만 달성할 수 있었던 충실도와 해상도를 달성하는 데 도움이 될 수 있다고 생각합니다.