# InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation

[https://arxiv.org/abs/2307.06942](https://arxiv.org/abs/2307.06942)

### 1 Introduction

이 연구에서는 비디오 언어 모델링을 개선하기 위해 새로운 대규모 비디오 중심 데이터 세트인 InternVid를 도입합니다. HowTo100M, HD-VILA 등 대부분의 기존 데이터 세트는 자동 음성 인식(ASR)으로 생성된 텍스트를 사용하는데, 이 텍스트는 동영상 콘텐츠와 의미적 상관관계가 낮아 동영상 검색 및 질의응답 작업에 영향을 미칩니다.

![InternVid에서의 예시들(각 비디오 클립의 세 프레임을 제공함), 해당 생성된 캡션들, 그리고 ASR 전사 내용. 캡션들에서는 명사를 파란색으로, 동사를 초록색으로 강조했습니다. 비영어 전사 내용들은 LLM [1]을 사용하여 영어로 번역되었습니다.](InternVid%20A%20Large-scale%20Video-Text%20Dataset%20for%20Mul%208ae8de7dc1164348a7c4b07ee667d855/Untitled.png)

InternVid에서의 예시들(각 비디오 클립의 세 프레임을 제공함), 해당 생성된 캡션들, 그리고 ASR 전사 내용. 캡션들에서는 명사를 파란색으로, 동사를 초록색으로 강조했습니다. 비영어 전사 내용들은 LLM [1]을 사용하여 영어로 번역되었습니다.

반면, InternVid는 16개의 시나리오와 약 6,000개의 동작 설명을 포함하는 700만 개 이상의 상관관계가 높은 비디오-텍스트 쌍을 포함합니다. 이 데이터는 멀티스케일 캡션 접근 방식을 사용하여 제작되어 사람의 개입을 최소화하면서 고품질 데이터를 보장합니다.

이 연구는 또한 InternVid와 비디오 언어 트랜스포머(ViT-L)를 사용하여 훈련된 새로운 비디오 언어 모델인 ViCLIP을 제시합니다. 이 모델은 대조 학습과 마스크 모델링 기법을 모두 사용하여 전송 가능한 비디오 언어 표현을 효율적으로 학습할 수 있습니다. 이 모델은 최첨단 동작 인식 및 비디오 검색 성능을 달성하여 비디오 텍스트 이해의 새로운 기준을 제시합니다.

또한 InternVid는 멀티모달 대화 시스템을 개발하고 텍스트-비디오 생성 작업을 향상시키는 데에도 사용할 수 있습니다. 인턴비디오의 하위 집합인 인턴비디오-에스테틱스는 워터마크가 없는 고해상도 비디오를 생성하여 텍스트-비디오 기준선의 시각적 및 정량적 결과를 모두 개선할 수 있습니다.

### 2 Related Work

저자들은 이전 연구에서 사용된 다양한 멀티모달 데이터 세트와 비디오 이해 방법에 대해 설명합니다:

멀티모달 데이터 세트: 비전-텍스트 데이터 쌍은 크로스모달 학습에 매우 중요합니다. 효과적인 비전-언어 모델을 훈련하기 위해서는 비전-텍스트 상관관계가 높은 대규모 데이터 세트가 사용됩니다. 예를 들어, LAION-5B는 대규모 이미지 언어 사전 학습을 위해 방대한 양의 이미지-텍스트 쌍을 제공했습니다. 비디오 중심의 멀티모달 데이터 세트에는 각각 고유한 방법론으로 비디오와 텍스트를 페어링하는 HowTo100M, YT-Temporal, HD-VILA 및 WebVid가 사용되었습니다. 그러나 저자들은 고품질의 설명이 포함된 대규모 비디오 언어 데이터 집합을 소개하는 것을 목표로 합니다.

비디오 이해: 대규모 비디오-텍스트 모델을 사전 학습하고 다운스트림 작업을 위해 미세 조정하는 것은 표준 관행이 되었습니다. 초기 기술에서는 사전 학습된 시각 및 언어 인코더를 사용하여 오프라인 비디오 및 텍스트 특징을 얻었지만, 최근에는 엔드투엔드 학습을 사용하는 방법이 선호되고 있습니다. 일반적인 사전 학습 작업에는 마스크 언어 모델링, 비디오-텍스트 매칭, 비디오-텍스트 대조 학습, 마스크 비디오 모델링 및 비디오-텍스트 마스크 모델링이 포함됩니다. VIOLET, All-in-one, LAVENDER, InternVideo, UMT, MERLOT Reserve, VALOR, mPLUG-2, VLAB 등의 모델이 언급되었으며, 각 모델은 비디오 이해도를 향상시키기 위해 다양한 전략과 방법론을 사용합니다. 그러나 일부 모델은 사용하는 비디오 텍스트 데이터의 품질에 따라 제한이 있어 비디오 전용 작업의 성능에 영향을 미칩니다. 마스킹 모델링과 교차 모드 대조 학습을 성공적으로 결합한 모델은 비디오 전용 작업과 비디오 언어 작업 모두에서 경쟁력 있는 성능을 발휘하는 경향이 있습니다.

### 3 InternVid: A Video-Centric Multimodal Dataset

대규모 비디오 언어 학습에서 중요한 격차를 메우는 비디오 중심 멀티모달 데이터세트로 'InternVid'가 소개됩니다. 이 데이터셋은 액션/활동 기반 쿼리 단어, 다양한 카테고리의 인기 동영상, 프레임별 주석에서 파생된 동영상 설명을 사용하여 시간적 역동성, 다양한 의미, 강력한 동영상-텍스트 상관관계를 강조합니다.

![제안된 다중 스케일 비디오 캡션 파이프라인. 코스 및 세부 스케일에서의 캡션들은 각각 초록색 및 진한 초록색으로 표시되어 있습니다.](InternVid%20A%20Large-scale%20Video-Text%20Dataset%20for%20Mul%208ae8de7dc1164348a7c4b07ee667d855/Untitled%201.png)

제안된 다중 스케일 비디오 캡션 파이프라인. 코스 및 세부 스케일에서의 캡션들은 각각 초록색 및 진한 초록색으로 표시되어 있습니다.

InternVid는 YouTube에서 큐레이션하여 동영상, 오디오, 메타데이터 및 캡션의 종합적인 컬렉션을 제공합니다. 여기에는 다양한 카테고리와 다양한 언어의 동영상이 포함되어 있어 다양성과 풍부함을 모두 보장합니다. 동영상은 액션/활동 기반 쿼리를 기반으로 수집된 다음 장면 차이를 사용하여 클립으로 분류됩니다. 각 비디오는 해당 오디오 및 ASR 자막이 포함된 일련의 클립으로 처리됩니다.

멀티스케일 비디오 캡션은 확장 가능하고 풍부하며 다양한 비디오 캡션을 생성하는 데 사용됩니다. 경량 이미지 캡션 모델인 태그2텍스트는 프레임 단위로 비디오를 설명하기 위해 더 세밀한 규모로 사용됩니다. 개별 이미지 캡션은 사전 학습된 언어 모델을 사용하여 비디오 설명으로 합성됩니다. 더 거친 스케일에서는 비디오의 중앙 프레임에 BLIP2를 사용하여 캡션을 작성합니다.

![ViCLIP의 프레임워크.](InternVid%20A%20Large-scale%20Video-Text%20Dataset%20for%20Mul%208ae8de7dc1164348a7c4b07ee667d855/Untitled%202.png)

ViCLIP의 프레임워크.

InternVid는 다른 인기 있는 비디오 언어 데이터 세트에 비해 더 다양하고 풍부한 것으로 입증되었습니다. 클립 길이, 캡션 길이, 미적 점수, 클립과 캡션의 유사성에서 다양성을 보여줍니다. 또한 이 데이터 세트는 비교 가능한 데이터 세트보다 더 많은 동사를 포함하는 더 높은 '액션성'을 보여줍니다.

![InternVid의 비디오-텍스트 데이터 형식(타입 b). 각 클립의 캡션과 ASR 전사 내용은 각각 검은색과 회색으로 표시되어 있습니다. ASR 전사를 제외하면 비디오-텍스트 데이터 형식(타입 a)을 얻을 수 있습니다. 데이터 형식(타입 c)을 얻기 위해서는, 비디오-텍스트 데이터(타입 a)를 가지고 있는 여러 비디오들을 연결합니다.](InternVid%20A%20Large-scale%20Video-Text%20Dataset%20for%20Mul%208ae8de7dc1164348a7c4b07ee667d855/Untitled%203.png)

InternVid의 비디오-텍스트 데이터 형식(타입 b). 각 클립의 캡션과 ASR 전사 내용은 각각 검은색과 회색으로 표시되어 있습니다. ASR 전사를 제외하면 비디오-텍스트 데이터 형식(타입 a)을 얻을 수 있습니다. 데이터 형식(타입 c)을 얻기 위해서는, 비디오-텍스트 데이터(타입 a)를 가지고 있는 여러 비디오들을 연결합니다.

마지막으로, 생성된 비디오 캡션으로 문맥 내 비디오 학습을 위해 InternVid-ICL이라는 통합 비디오 텍스트 데이터 세트가 생성됩니다. 여기에는 다양성과 시간적 맥락을 높이기 위해 다양한 방식으로 구성된 710만 개의 인터리브 비디오-텍스트 데이터 쌍을 생성하는 작업이 포함됩니다.

### 4 ViCLIP: Learning Video-Text Representation at Scale

"ViCLIP"은 비디오-텍스트 표현을 대규모로 학습하기 위한 새로운 접근 방식으로, 오리지널 CLIP 모델의 원리를 기반으로 합니다. ViCLIP은 비디오 인코더(ViT)와 텍스트 인코더로 구성됩니다. 이러한 모듈은 CLIP 모델의 해당 구성 요소에서 초기화되며, 비디오 인코더는 시공간적 주의를 수용하도록 업데이트됩니다.

비디오 인코더는 입력 비디오에 무작위 패치 마스킹을 적용하여 계산 요구 사항을 크게 줄입니다. 사용된 텍스트 인코더 역시 트랜스포머 기반 모델입니다.

사전 학습의 마지막 단계에서는 마스킹된 토큰뿐만 아니라 모든 시각적 토큰이 비디오 트랜스포머에 공급됩니다. 이 접근 방식은 전체 비디오가 입력으로 사용되는 사전 학습과 다운스트림 애플리케이션 간의 격차를 해소하는 데 도움이 됩니다. 마스킹되지 않은 학습은 0.5에포크 동안 4e-6의 학습률로 수행됩니다.

모델의 훈련 목표는 비디오와 텍스트 정렬을 최적화하여 글로벌 비디오 및 텍스트 기능을 사용하여 InfoNCE 손실을 최소화하는 것입니다. InfoNCE 손실 함수는 학습된 비디오와 텍스트 임베딩 간의 코사인 유사도를 계산하고 최적화 프로세스의 일부로 학습된 온도 스케일링 계수를 적용합니다.

ViCLIP의 구현에는 64개의 NVIDIA A100 GPU가 사용되었으며, 5천만 개의 비디오-텍스트 쌍을 사용하여 3일 동안 학습되었습니다. 훈련과 추론을 모두 가속화하기 위해 딥스피드와 플래시어텐션을 사용합니다.

### 5 Experiments

CLIP을 기반으로 구축된 비디오 텍스트 사전 학습 모델인 ViCLIP에 대한 연구의 일환으로 수행된 실험과 그 결과에 대한 자세한 개요를 요청하신 것 같습니다. 다음은 간략한 요약입니다:

![t2v 베이스라인에서의 샘플들과 다른 샘플들의 비교. 우리는 WebVid10M와 추가적인 InternVid-Aesthetics-18M에서 학습된 다른 방법들의 제로샷 텍스트-투-비디오 생성 결과를 제공합니다. 사용된 프롬프트는: 벌거벗은 머리의 남자가 검은색 티셔츠를 입고 기타를 연주하고 있다.](InternVid%20A%20Large-scale%20Video-Text%20Dataset%20for%20Mul%208ae8de7dc1164348a7c4b07ee667d855/Untitled%204.png)

t2v 베이스라인에서의 샘플들과 다른 샘플들의 비교. 우리는 WebVid10M와 추가적인 InternVid-Aesthetics-18M에서 학습된 다른 방법들의 제로샷 텍스트-투-비디오 생성 결과를 제공합니다. 사용된 프롬프트는: 벌거벗은 머리의 남자가 검은색 티셔츠를 입고 기타를 연주하고 있다.

실험 개요:

연구원들은 생성한 InternVid 데이터 세트의 5가지 하위 집합에 대해 ViCLIP을 학습시키고, 널리 사용되는 비디오 관련 벤치마크에서 성능을 테스트했습니다. 다양한 설정(풀 파인튜닝 및 제로 샷)을 사용했으며 기존 데이터 세트와 결과를 비교했습니다.

주요 결과

제로 샷 액션 인식:
WebVid10M으로 학습했을 때 ViCLIP은 거의 모든 데이터 세트에서 OpenAI의 CLIP, EVA-CLIP-L, EVA-CLIP-E보다 우수한 성능을 보였습니다. 특히 InternVid10M-FLT로 훈련했을 때 ViCLIP은 Kinetics 400 / 600 / 700에서 제로 샷 동작 인식에 대한 새로운 기록을 세웠습니다.

비디오-텍스트 검색:
ViCLIP은 InternVid-50M과 같은 비디오 언어 데이터 세트로 사전 학습했을 때 미세 조정 및 제로 샷 검색 성능이 크게 향상되었습니다. 텍스트-비디오 및 비디오-텍스트 작업 모두에서 R@1 점수가 증가하는 것을 관찰했습니다.

데이터 확장 및 문제:
데이터 규모를 늘리면 모델의 성능이 크게 향상되는 것으로 관찰되었습니다. 그러나 이러한 평가에서는 데이터 규모보다 데이터 품질이 더 중요한 것으로 나타났습니다. 또한 동일한 비디오에서 클립을 샘플링할 때 발생하는 오탐이 비디오-텍스트 대조 학습을 심각하게 방해할 수 있다는 사실도 발견했습니다.

텍스트-비디오 생성:
인턴비드 데이터 세트는 기존의 텍스트-비디오 생성 모델을 개선했습니다. 이 비디오 생성 접근 방식은 WebVid10M과 더불어 InternVid-Aesthetics-18M으로 훈련했을 때 IS, FID 및 CLIPSIM 지표에서 상당한 개선을 보였습니다.

비디오 중심 대화 시스템:
사전 학습된 ViCLIP(InternVid를 통해)을 비디오 채팅에 통합했을 때, 비디오-텍스트 변환 기능이 상당히 우수하여 비디오 캡션을 더욱 개선할 수 있는 잠재력을 보여주었습니다.

결론적으로, 이 연구는 사전 학습 데이터의 규모를 늘리면 학습된 표현의 전달성을 향상시킬 수 있음을 시사합니다. 또한 훈련 중에 직관적인 샘플링 전략을 구현하면 데이터 활용을 개선하여 모델 성능을 더욱 향상시킬 수 있습니다.

### 6 Conclusion

결론적으로, 이 논문에서 제시된 연구는 동영상에 초점을 맞춘 멀티모달 연구 분야에서 새로 개발된 InternVid 데이터 세트의 가치에 대한 설득력 있는 증거를 제공합니다. 7백만 개의 고품질 YouTube 동영상에서 가져온 2억 3천만 개 이상의 비디오 클립으로 구성된 이 대규모 데이터 세트는 비디오 언어 표현 학습, 비디오 검색 성능 및 텍스트-비디오 생성의 발전에 큰 잠재력을 보여줍니다.

이러한 비디오 클립은 기존 모델을 활용하여 클립 수준에서 설명을 생성하는 멀티스케일 접근 방식을 통해 처리되었습니다. 연구 결과, 캡션이 비전 트랜스포머 아키텍처를 사용하는 기본 모델인 ViCLIP과 같은 모델의 학습을 향상시키는 데 효과적이라는 것이 확인되었습니다.

또한 데이터 규모가 크로스모달 임베딩의 학습에 상당한 영향을 미친다는 사실을 발견했습니다. 또한, 미적 점수를 기준으로 필터링된 InternVid 데이터 세트의 하위 집합은 텍스트-비디오 생성 성능이 크게 향상되어 데이터 세트의 다용도성을 보여줍니다.

그러나 이 연구는 오탐 문제를 피하고 방대한 양의 데이터의 잠재력을 최대한 활용하기 위해 훈련 중에 보다 정교한 샘플링 전략을 구현해야 할 필요성도 보여줍니다. 이러한 인사이트는 매우 중요한 의미를 가지며 향후 연구의 초점이 되어야 합니다.

데이터, 주석, 메타데이터, 계산된 점수가 포함된 InternVid 데이터 세트의 공개는 비디오 이해 영역의 연구와 발전을 위한 귀중한 리소스를 제공할 것으로 기대됩니다. 또한 비디오 캡션, 비디오 추론, 멀티모달 대화 시스템 등 향후 다양한 연구와 애플리케이션을 촉진하는 촉매제 역할을 할 것으로 기대합니다.

### A Statistics in InternVid

여기에 제공된 세부 정보는 몇 가지 섹션으로 분류할 수 있습니다:

![비디오 추론 작업. 우리의 대화 시스템은 비디오 내용에 기반한 상식을 이해하고 인과 추론을 만들어내는 능력이 있습니다](InternVid%20A%20Large-scale%20Video-Text%20Dataset%20for%20Mul%208ae8de7dc1164348a7c4b07ee667d855/Untitled%205.png)

비디오 추론 작업. 우리의 대화 시스템은 비디오 내용에 기반한 상식을 이해하고 인과 추론을 만들어내는 능력이 있습니다

A. InternVid의 통계

액션성: InternVid 데이터 세트는 캡션에 약 10배 더 많은 동사를 포함하고 있기 때문에 WebVid10M 데이터 세트보다 훨씬 더 높은 동작 표현을 보여줍니다. 연구원들은 자연어 툴킷(NLTK)을 활용하여 고유 동사를 추출하고 태그를 지정함으로써 각 데이터 세트의 '액션성'을 표시했습니다.

비디오 캡션 및 트랜스크립트 배포: 연구원들은 생성된 캡션과 다국어 대본의 단어 분포를 계산했습니다. 이 분포에는 일반적인 객체, 속성, 위치, 장면, 액션/이벤트 등의 범주가 포함됩니다. 또한 다양한 언어의 단어 분포를 제공하여 여러 국가의 사용 경향을 암시합니다.

![비디오 창작 작업. 이 그림은 우리 데이터셋 내에서의 창조적이고 생성적인 작업, 예를 들어 비디오 컨텐츠에 기반한 시 생성 같은 작업에서 VideoChat-ViCLIP의 성능을 보여줍니다.](InternVid%20A%20Large-scale%20Video-Text%20Dataset%20for%20Mul%208ae8de7dc1164348a7c4b07ee667d855/Untitled%206.png)

비디오 창작 작업. 이 그림은 우리 데이터셋 내에서의 창조적이고 생성적인 작업, 예를 들어 비디오 컨텐츠에 기반한 시 생성 같은 작업에서 VideoChat-ViCLIP의 성능을 보여줍니다.

B. 구현 세부 사항

ViCLIP: 제로샷 액션 인식을 위해 각 비디오에서 8개의 프레임을 샘플링했습니다. 모델의 성능은 Kinetics-400 / -600 / -700의 정확도 상위 1, 5위 평균을 기준으로 측정했습니다. 비디오 검색은 풀 파인 튜닝 설정과 제로 샷 설정 모두에서 구현되었으며, 각각 다른 프레임 샘플링 전략을 사용했습니다.

비디오 생성 기준선: 연구팀은 시공간 모델링 접근법을 사용하여 텍스트-비디오 생성 기준을 구축했습니다. 제안된 모델은 잠재력을 모델링하는 트랜스포머가 있는 U-Net을 사용하여 인터리브 시공간적 주의, 시각적 텍스트에 대한 교차 주의, 피드포워드 네트워크 및 시간적 주의를 적용합니다. 시공간적 주의는 텍스트-이미지 확산 모델의 매개변수를 사용하여 초기화되었고, 새로 추가된 시간적 주의 레이어는 기본 매개변수를 사용했습니다.

![텍스트-비디오 생성 기준의 프레임워크.](InternVid%20A%20Large-scale%20Video-Text%20Dataset%20for%20Mul%208ae8de7dc1164348a7c4b07ee667d855/Untitled%207.png)

텍스트-비디오 생성 기준의 프레임워크.

텍스트-비디오 평가: 텍스트-비디오 모델을 평가하기 위해 UCF-101 및 MSRVTT 데이터 세트에서 제로 샷 실험을 수행했습니다. 합성된 결과의 품질과 텍스트-비디오의 의미적 유사성을 평가하기 위해 여러 지표를 사용했습니다. 사용된 메트릭에는 프레임별-FID, FVD, 시작 점수(IS) 및 클립 유사도(CLIPSIM)가 포함됩니다. 이러한 방법을 통해 모델의 성능을 종합적으로 평가할 수 있었습니다.