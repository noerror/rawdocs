# Hyper-VolTran: Fast and Generalizable One-Shot Image to 3D Object Structure via HyperNetworks

[https://arxiv.org/abs/2312.16218](https://arxiv.org/abs/2312.16218)

- Dec 2023

### 1. Introduction

신경망 3D 재구성의 배경과 과제

신경망 3D 재구성의 최근 발전은 새로운 시각 합성 및 로봇 비전과 같은 분야에 큰 영향을 미쳤습니다. 신경 방사장(NeRF) 및 부호화된 거리 함수(SDF)와 같은 기술은 이미지에서 3D 모델을 생성하는 데 중요한 역할을 해왔습니다. 하지만 이러한 방법은 일반적으로 일관된 뷰와 정확한 카메라 포즈를 가진 많은 수의 이미지가 필요하기 때문에 리소스가 제한된 상황에서는 비현실적일 수 있습니다.

희소 이미지(Sparse Image) 데이터 해결

최근 일부 연구에서는 여러 뷰에서 투영된 신경 인코딩 볼륨을 사용하여 더 적은 수의 이미지로 3D 재구성에 진전을 보이고 있습니다. 하지만 단일 이미지에서 3D 모델을 재구성하는 것은 여전히 어려운 과제입니다. 보이지 않는 시점을 정확하게 예측하려면 강력한 사전 지식 또는 기존 지식이 필요한 경우가 많습니다.

제너레이티브 모델 활용

제너레이티브 모델은 2D 이미지 생성에서 가능성을 보여줬는데, 이는 보이지 않는 관점에 대한 강력한 사전 지식으로 작용할 수 있습니다. 한 가지 접근 방식은 확산 모델을 사용하여 재구성 프로세스를 안내하는 것입니다. 예를 들어, 점수 증류 샘플링(SDS)은 확산 모델의 피드백을 사용하여 NeRF를 훈련합니다. 하지만 이 프로세스는 시간이 오래 걸리고 각 장면에 대해 광범위한 최적화가 필요한 경우가 많습니다.

신속하고 일반화 가능한 3D 재구성의 필요성

실용적이고 효율적인 3D 재구성을 위해서는 광범위한 장면별 최적화 없이 빠르게 처리할 수 있는 방법이 필요합니다. 한 가지 방법은 확산 모델을 사용하여 멀티뷰 이미지를 합성하는 것이지만, 이 방법은 기하학적 일관성을 유지하는 데 어려움을 겪을 수 있습니다.

제안된 솔루션

이 논문에서는 이러한 문제를 해결하기 위해 일반화, 속도, 불일치 처리에 중점을 둔 새로운 접근 방식을 소개합니다. 제안된 방법은 하이퍼네트워크에서 생성된 SDF 네트워크와 볼륨 트랜스포머(VolTran)를 결합하여 일관되지 않은 예제를 처리합니다. 이 방법은 이미지 인코더의 잠재 변수를 사용하여 이미지 표현을 생성한 다음 SDF 가중치를 알려줍니다. 이 접근 방식의 목표는 다음과 같습니다:

- 소수의 합성 데이터를 사용하여 3D 메시 재구성을 위한 일반화 가능한 전제를 제공합니다.
- 다양한 시점에 걸쳐 모양과 색상의 불일치를 처리하기 위해 트랜스포머 모듈을 도입합니다.
- 단 한 번의 피드 포워드 프로세스만으로 최소한의 처리 시간(약 5초)으로 신속한 3D 메시 재구성을 달성할 수 있습니다.

이 방법은 원본 문서의 그림 1(하단)에 설명되어 있으며, 신경망 3D 재구성에서 이 기술의 효율성과 일반화 가능성을 보여줍니다.

![상단: 실행 시간 및 모따기 거리에서 기준선과 제안된 방법을 IoU를 나타내는 버블 영역과 비교한 결과입니다. 아래쪽: 유니티의 파이프라인은 확산 모델에서 멀티뷰를 합성하고 하이퍼네트워크에서 생성된 가중치가 있는 SDF 네트워크를 사용하여 멀티뷰에서 SDF로 매핑함으로써 이미지에서 3D로 전환하는 두 가지 구성 요소로 이루어져 있습니다.](Hyper-VolTran%20Fast%20and%20Generalizable%20One-Shot%20Imag%203de9dd2f31054e7ab925f2a8871981cd/Untitled.png)

상단: 실행 시간 및 모따기 거리에서 기준선과 제안된 방법을 IoU를 나타내는 버블 영역과 비교한 결과입니다. 아래쪽: 유니티의 파이프라인은 확산 모델에서 멀티뷰를 합성하고 하이퍼네트워크에서 생성된 가중치가 있는 SDF 네트워크를 사용하여 멀티뷰에서 SDF로 매핑함으로써 이미지에서 3D로 전환하는 두 가지 구성 요소로 이루어져 있습니다.

### 2. Related Work

2D에서 3D로의 재구성을 위한 확산 모델

몇 장의 2D 이미지로 3D 구조를 재구성하는 것은 본질적으로 어려운 작업입니다. 최근 확산 모델의 발전은 단일 이미지에서 3D 재구성에 필요한 전제를 제공함으로써 이 문제를 해결할 수 있는 가능성을 보여주었습니다. 드림퓨전(DreamFusion), 스코어 자코비안 체인(Score Jacobian Chaining)과 같은 주목할 만한 작품에서는 이러한 모델을 사용하여 3D 장면 생성을 최적화합니다. 또 다른 중요한 발전인 Zero123은 입력 이미지와 카메라 파라미터를 통해 새로운 뷰를 합성하여 보다 일관된 멀티뷰 이미지를 제공합니다. 하지만 이러한 방법은 일반적으로 광범위한 장면별 최적화가 필요하고 여러 오브젝트를 표현하는 데 한계가 있어 일반화 가능성이 부족합니다.

빠른 3D 재구성을 위한 일반화 가능한 프리어

이상적인 3D 재구성 방법은 보이지 않는 오브젝트까지 일반화할 수 있는 단일 모델이며, 장면별 최적화 없이 포워드 패스 접근 방식만 필요합니다. PixelNeRF와 MVSNeRF는 각각 입력 이미지에서 추출한 특징 볼륨과 왜곡된 2D 이미지 특징에서 구축한 비용 볼륨을 사용하여 이 분야의 선구적인 작업입니다. 스파스네우스는 이 개념을 스파스 데이터 시나리오로 확장한 것입니다. 그러나 이러한 방법은 여전히 멀티뷰 입력이 필요하고 단일 이미지 재구성에 어려움을 겪습니다. One2345는 확산 모델을 사용하여 단일 입력에서 예제 이미지를 생성함으로써 이 문제를 해결하지만, 생성된 예제에서 불일치하는 문제에 직면합니다. 자가 지도 학습과 텍스트-3D 데이터를 사용하는 Point-e 및 Shap-e와 같은 대규모 모델은 재구성 속도는 빠르지만 품질이 떨어집니다.

소수 샷 학습의 컨텍스트 기반 학습

소수 샷 학습에서 컨텍스트 정보를 활용하는 것은 다양한 입력에 대한 최적의 성능을 위해 매우 중요합니다. 경사 하강을 통해 모델 파라미터를 업데이트하는 방법도 있지만, 이러한 방법에는 여전히 여러 번의 피드 포워드 작업이 필요합니다. 최근에는 단 한 번의 피드 포워드 작업으로 컨텍스트 이해를 달성하여 추가적인 최적화 단계가 필요 없는 접근 방식으로 초점이 옮겨가고 있습니다. 이는 하이퍼네트워크에서 영감을 얻어 제공된 컨텍스트를 기반으로 신경망 가중치를 생성함으로써 달성할 수 있습니다.

요약하면, 제한된 2D 데이터에서 3D 재구성을 위해 확산 모델과 생성적 접근 방식을 사용하는 데 상당한 발전이 있었지만 일반화, 처리 속도 및 일관성 측면에서 여전히 과제가 남아 있습니다. 최근의 연구는 컨텍스트 기반 학습과 신경망 가중치 생성에 대한 혁신적인 접근 방식을 사용하여 최소한의 추가 처리로 새로운 객체와 장면에 빠르게 일반화할 수 있는 모델을 향해 나아가고 있습니다.

### 3. Proposed Method

![훈련 파이프라인은 단일 이미지에서 시작됩니다. 시점 인식 생성 모델을 사용하여 단일 뷰를 이미지 세트로 확장하는 이 방법은 RGB 및 깊이 회귀 손실이 있는 지도 학습을 사용합니다. 구체적으로 1) N개의 RGB 이미지와 뎁스 맵을 활용하여 추가적인 시점과 카메라 포즈를 생성합니다. 2) 지오메트리 가이드 인코딩은 비용 볼륨 형태의 왜곡된 이미지 특징에서 파생됩니다. 3) 테스트 시간 최적화 대신 시각적 인코딩을 통해 이미지 모양을 고려하여 HyperNetwork 모듈을 한 번만 통과하여 SDF 가중치를 얻습니다. 4) 지오메트리 인코딩된 볼륨과 이미지 특징이 SDF 네트워크와 트랜스포머 모듈로 전달되어 완전한 3D 오브젝트 구조가 드러납니다. 따라서 하이퍼네트워크 설계와 일관된 구조 덕분에 새로운 입력에 빠르게 적응할 수 있는 하이퍼볼트랜 방식은 전 세계적으로 주목받고 있습니다.](Hyper-VolTran%20Fast%20and%20Generalizable%20One-Shot%20Imag%203de9dd2f31054e7ab925f2a8871981cd/Untitled%201.png)

훈련 파이프라인은 단일 이미지에서 시작됩니다. 시점 인식 생성 모델을 사용하여 단일 뷰를 이미지 세트로 확장하는 이 방법은 RGB 및 깊이 회귀 손실이 있는 지도 학습을 사용합니다. 구체적으로 1) N개의 RGB 이미지와 뎁스 맵을 활용하여 추가적인 시점과 카메라 포즈를 생성합니다. 2) 지오메트리 가이드 인코딩은 비용 볼륨 형태의 왜곡된 이미지 특징에서 파생됩니다. 3) 테스트 시간 최적화 대신 시각적 인코딩을 통해 이미지 모양을 고려하여 HyperNetwork 모듈을 한 번만 통과하여 SDF 가중치를 얻습니다. 4) 지오메트리 인코딩된 볼륨과 이미지 특징이 SDF 네트워크와 트랜스포머 모듈로 전달되어 완전한 3D 오브젝트 구조가 드러납니다. 따라서 하이퍼네트워크 설계와 일관된 구조 덕분에 새로운 입력에 빠르게 적응할 수 있는 하이퍼볼트랜 방식은 전 세계적으로 주목받고 있습니다.

Hyper-VolTran이라는 이름의 제안된 방법은 두 개의 스트림으로 구성된 3D 신경 재구성 파이프라인입니다. 이 방법은 단일 뷰 이미지와 깊이 맵으로 시작하여 확산 모델을 사용하여 다중 뷰 이미지를 합성한 다음, 이 이미지를 3D 재구성에 사용합니다. 이 프로세스에는 3D 지오메트리 표현을 생성하고 표면 표현을 위한 부호화된 거리 함수(SDF) 가중치를 추정하는 두 가지 주요 흐름이 포함됩니다.

멀티뷰 이미지 합성

파이프라인은 사전 학습된 생성 모델을 사용하여 단일 입력 이미지를 여러 뷰로 확장하는 것으로 시작됩니다. 이 프로세스에는 입력 이미지와 뎁스 맵의 모양을 정규화하고 다양한 관점에서 여러 개의 RGB 이미지와 뎁스 맵을 생성하는 작업이 포함됩니다. 이렇게 합성된 뷰는 훈련과 테스트 단계에서 모두 사용되며, 테스트 중에는 뎁스 맵이 생략됩니다.

지오메트리 인식 인코딩

이 방법은 신경망 인코딩 볼륨을 사용하여 합성된 뷰와 관련 카메라 포즈에서 3D 지오메트리를 구성합니다. 여기에는 2D 이미지 피처를 기준 뷰의 프러스텀 내의 국소화된 평면으로 워핑하는 작업이 포함됩니다. 신경 인코딩 볼륨은 여러 시점에 걸쳐 분산 함수를 사용하여 계산되므로 복잡한 3D 씬 지오메트리를 인코딩하는 데 도움이 됩니다.

SDF를 사용한 볼륨 렌더링

정확한 표면 재구성을 위해 이 방법에서는 신경 방사 필드(NeRF) 대신 부호화된 거리 함수(SDF)를 사용합니다. 신경 인코딩 볼륨에 의해 정보를 받는 SDF 네트워크는 표면 표현을 위한 SDF를 예측합니다. SDF 네트워크의 가중치는 입력 이미지에 따라 조절되는 하이퍼네트워크의 출력에 따라 동적으로 할당됩니다. 이 접근 방식을 사용하면 추론 중에 새로운 객체에 대한 일반화를 더 잘 수행할 수 있습니다.

SDF 네트워크용 하이퍼네트워크

하이퍼네트워크는 입력 이미지를 기반으로 SDF 네트워크에 대한 가중치를 생성하여 동적으로 가중치를 할당할 수 있습니다. 이 방법은 장면별 최적화가 필요하지 않으며 새로운 객체, 특히 학습 데이터와 유사한 의미를 가진 객체에 더 잘 적응할 수 있습니다.

![텍스트에서 3D 컬러 메쉬로 변환한 Hyper-VolTran의 정성적 결과. 확산 모델에서 생성된 이미지가 입력으로 사용됩니다. 입력 이미지의 주요 물체에만 초점을 맞춥니다.](Hyper-VolTran%20Fast%20and%20Generalizable%20One-Shot%20Imag%203de9dd2f31054e7ab925f2a8871981cd/Untitled%202.png)

텍스트에서 3D 컬러 메쉬로 변환한 Hyper-VolTran의 정성적 결과. 확산 모델에서 생성된 이미지가 입력으로 사용됩니다. 입력 이미지의 주요 물체에만 초점을 맞춥니다.

SDF에서 렌더링

이 방법은 볼륨 렌더링 기법을 채택하여 SDF 표현을 기반으로 색상과 볼륨을 렌더링합니다. 이 프로세스에는 주어진 지점에서의 광도를 계산하고 가중치를 혼합하여 렌더링된 색상과 깊이를 예측하는 과정이 포함됩니다.

VolTran: 멀티뷰 집계 트랜스포머

희박한 입력 데이터로 인한 표면 패치의 불일치를 해결하기 위해 이 방법에서는 VolTran이라는 트랜스포머 모듈을 도입합니다. 이 모듈은 자기 주의를 사용하여 서로 다른 시점의 전역 정보를 인코딩하고 집계 토큰을 사용하여 대상 뷰에 대한 출력을 얻습니다.

학습 및 추론

프레임워크는 색상 및 깊이 예측 손실, 표면 평활도를 위한 에이코널 손실, 간결한 기하학적 표면을 위한 희소성 정규화 용어 등 여러 가지 손실을 사용하여 엔드투엔드로 학습됩니다. 추론하는 동안 이 방법은 단 한 번의 피드 포워드 작업만 필요하므로 기존 방법에 비해 계산 시간이 크게 단축됩니다.

Hyper-VolTran은 멀티뷰 이미지 합성, 지오메트리 인식 인코딩, 고급 렌더링 기술을 결합한 3D 신경 재구성에 대한 혁신적인 접근 방식을 제시합니다. 이 기술은 일반화 및 계산 효율성의 문제를 해결하여 단일 뷰 이미지에서 빠르고 정확한 3D 재구성을 위한 유망한 솔루션을 제공합니다.

### 4. Experiments

구현 세부 사항

Hyper-VolTran 모델은 공개 데이터 세트에서 46K 개의 합성된 3D 장면을 사용하여 훈련되었습니다. 주요 구성 요소에는 기본 멀티뷰 생성 모델(Zero123), 지오메트리 가이드 인코더, SDF 가중치 생성을 위한 CLIP 모델이 포함됩니다. 이 모델은 특정 손실 함수 설정으로 최적화되었으며 이미지 분할을 사용하여 32개의 다른 시점에서 대상 물체를 정확하게 식별하는 테스트를 거쳤습니다.

텍스트 3D 변환 결과

텍스트 대 3D 프로세스에는 텍스트 대 이미지 모델을 사용하여 텍스트 프롬프트를 이미지로 변환한 다음, 대상 객체를 잘라내고 다양한 보기를 합성하는 과정이 포함되었습니다. 그런 다음 이러한 뷰를 Hyper-VolTran 모델에 입력하면 다양한 뷰에서 일관된 텍스처를 가진 고품질 메시를 성공적으로 생성할 수 있었습니다.

이미지에서 3D로 변환한 결과

GSO 데이터 세트의 하위 집합을 사용하여 원샷 이미지-3D 메시 재구성에 대한 모델의 성능을 평가했습니다. Hyper-VolTran은 단일 이미지에서 3D 객체를 재구성하는 데 탁월한 능력을 보여줬으며, 충실도와 정확도 측면에서 One2345, Shape, Point-e, Zero123+SD와 같은 다른 방법보다 뛰어난 성능을 보였습니다. 또한 이 모델은 생성된 이미지의 불일치를 처리하는 데 있어서도 견고함을 보여주었습니다.

![단일 이미지에서 3D 재구성에 대한 정성적 비교(예: One2345 [15], Shap-e [11], Pointe [20], Zero123+SD [21]). 볼트란은 경쟁사보다 더 일관되고 높은 품질의 결과를 제공하며, 일반적으로 입력 세부 사항의 보존 수준이 더 높습니다. 더 많은 결과와 확대된 세부 정보는 보충 자료를 참조하세요.](Hyper-VolTran%20Fast%20and%20Generalizable%20One-Shot%20Imag%203de9dd2f31054e7ab925f2a8871981cd/Untitled%203.png)

단일 이미지에서 3D 재구성에 대한 정성적 비교(예: One2345 [15], Shap-e [11], Pointe [20], Zero123+SD [21]). 볼트란은 경쟁사보다 더 일관되고 높은 품질의 결과를 제공하며, 일반적으로 입력 세부 사항의 보존 수준이 더 높습니다. 더 많은 결과와 확대된 세부 정보는 보충 자료를 참조하세요.

![일관성 없이 생성된 보기의 예와 메시 생성 시 One2345[15]와 제안된 방법을 비교한 결과. One2345는 뷰가 일관성이 없고 어려운 경우 잘 재구성된 메쉬를 생성하지 못합니다.](Hyper-VolTran%20Fast%20and%20Generalizable%20One-Shot%20Imag%203de9dd2f31054e7ab925f2a8871981cd/Untitled%204.png)

일관성 없이 생성된 보기의 예와 메시 생성 시 One2345[15]와 제안된 방법을 비교한 결과. One2345는 뷰가 일관성이 없고 어려운 경우 잘 재구성된 메쉬를 생성하지 못합니다.

정량적 결과

Hyper-VolTran의 효과는 F-Score, 챔퍼 L2 거리, IoU와 같은 메트릭을 사용하여 정량적으로 평가되었습니다. 보이지 않는 물체에 대한 일반화 기능에서 경쟁사보다 뛰어난 성능을 보였으며, PSNR, LPIPS, CLIP 유사성 점수에서 알 수 있듯이 3D 렌더링 품질이 뛰어났습니다. 3D 생성 처리 시간은 단일 A100 GPU에서 약 5초가 소요되었으며, 기본 확산 모델을 사용하여 뷰를 합성하는 데는 추가 시간이 필요했습니다.

![각 모듈에 대한 제거 연구. 각 모듈이 컬러 장면 렌더링에 미치는 영향.](Hyper-VolTran%20Fast%20and%20Generalizable%20One-Shot%20Imag%203de9dd2f31054e7ab925f2a8871981cd/Untitled%205.png)

각 모듈에 대한 제거 연구. 각 모듈이 컬러 장면 렌더링에 미치는 영향.

![확산 모델에서 생성된 샘플 수를 달리한 정성적 결과. 확산 모델에서 더 많은 이미지가 생성될수록 더 나은 형상 품질을 얻을 수 있습니다.](Hyper-VolTran%20Fast%20and%20Generalizable%20One-Shot%20Imag%203de9dd2f31054e7ab925f2a8871981cd/Untitled%206.png)

확산 모델에서 생성된 샘플 수를 달리한 정성적 결과. 확산 모델에서 더 많은 이미지가 생성될수록 더 나은 형상 품질을 얻을 수 있습니다.

분석 및 축소

절제 연구를 통해 HyperNetwork for SDF 및 VolTran 모듈의 중요성이 강조되었습니다. 이 모듈이 없으면 렌더링 품질이 저하되고 노이즈가 증가하여 고품질 결과를 얻는 데 있어 이 모듈의 중요성이 강조되었습니다. 또한 확산 모델에서 생성되는 이미지 수가 중요한 것으로 나타났는데, 이미지 수가 많을수록 지오메트리 표현이 개선되고 이미지 수가 적을수록 품질이 저하되는 것으로 나타났습니다.

전반적으로 Hyper-VolTran은 3D 신경 재구성 분야, 특히 원샷 이미지에서 3D로의 변환에서 상당한 발전을 보여주었습니다. 다양한 오브젝트와 시점을 처리할 수 있는 능력과 효율적인 처리 시간 덕분에 빠르고 정확한 3D 모델 생성을 위한 유망한 툴입니다.

### 5. Conclusions

이 논문에서는 단일 이미지에서 3D 오브젝트 구조를 재구성하는 새로운 접근 방식인 Hyper-VolTran을 소개합니다. 이 방법은 하이퍼네트워크 모듈과 트랜스포머 모듈이라는 두 가지 핵심 구성 요소를 통합합니다. 하이퍼네트워크는 부호화된 거리 함수(SDF) 가중치를 생성하고, 트랜스포머 모듈은 잠재적으로 일치하지 않을 수 있는 여러 뷰의 정보를 통합하는 데 도움을 줍니다.

Hyper-VolTran은 단일 이미지를 3D 모델로 변환하는 작업에서 보이지 않는 물체에 대한 일반화에 탁월합니다. 이 기능은 정량적 데이터와 정성적 평가 모두에서 지원됩니다. 이 접근 방식의 두드러진 특징은 빠른 처리 시간입니다. 약 45초 만에 3D 메시를 생성할 수 있으며, 특히 장면별 최적화가 필요하지 않기 때문에 기존 방식에 비해 크게 개선되었습니다.

하이퍼볼트란은 현재의 최신 기술과 비교했을 때 시간 효율성과 재구성 정확도라는 두 가지 주요 영역에서 뛰어난 성능을 보여줍니다. 따라서 단일 이미지에서 3D를 재구성하는 데 매우 효과적인 도구로, 이 분야의 오랜 과제를 해결할 수 있습니다.

전반적으로 하이퍼볼트란은 단일 이미지에서 3D 오브젝트를 재구성하는 데 있어 상당한 발전을 이루었습니다. 속도와 정확성, 보이지 않는 물체를 처리할 수 있는 능력의 결합으로 3D 모델링 및 컴퓨터 비전 분야에서 유망한 솔루션으로 자리매김하고 있습니다.