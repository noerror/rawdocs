# FastSpeech 2: Fast and High-Quality End-to-End Text-to-Speech

[https://github.com/ming024/FastSpeech2](https://github.com/ming024/FastSpeech2)

[https://arxiv.org/abs/2006.04558](https://arxiv.org/abs/2006.04558)

### 1 INTRODUCTION

신경망을 사용하는 텍스트 음성 변환(TTS) 분야는 지난 몇 년 동안 빠르게 발전해 왔습니다. 타코트론, 딥 보이스 3, 트랜스포머 TTS와 같은 이전 모델은 먼저 텍스트에서 멜 스펙트로그램을 생성한 다음 별도로 학습된 보코더를 사용하여 이 멜 스펙트로그램에서 음성을 합성하는 2단계 프로세스를 사용했습니다. 그러나 이러한 모델은 추론 속도가 느리고 단어를 건너뛰거나 반복하는 등의 견고성 문제가 발생했습니다.

최근에는 이러한 문제를 해결하기 위해 비회귀적 TTS 모델이 개발되었습니다. 이러한 모델은 멜-스펙트로그램을 빠르게 생성하고 견고성 문제를 피하면서 고품질 음성을 생성합니다. 이 중 FastSpeech가 특히 성공을 거두었습니다.

FastSpeech는 자동 회귀 교사 모델을 사용하여 각 음소의 지속 시간과 생성된 멜-스펙트로그램을 제공하여 지식 증류에 사용하므로 TTS의 복잡한 '일대다' 매핑 문제를 해결하는 데 도움이 됩니다. 그러나 이 모델에는 복잡한 2단계의 교사-학생 증류 과정과 부정확한 지속 시간 추출로 인해 음성 품질과 운율이 제한될 수 있다는 단점이 있습니다.

이러한 문제를 극복하기 위해 패스트스피치 2가 제안되었습니다. 단순화된 교사 출력물 대신 원본 타깃으로 훈련함으로써 훈련 과정을 간소화하고 정보 손실을 줄였습니다. 또한 음높이와 에너지를 포함한 음성의 변화 정보를 훈련 과정에 도입합니다. 이를 통해 모델은 텍스트 입력에서 목표 출력을 더 잘 예측할 수 있습니다.

FastSpeech 2s는 여기서 한 걸음 더 나아가 멜-스펙트로그램을 전혀 사용하지 않고 텍스트에서 음성 파형을 직접 생성하여 음성 합성 프로세스를 간소화하고 지연 시간을 줄입니다.

패스트스피치 2와 2s는 다른 TTS 시스템과 비교했을 때 두드러집니다. Merlin이나 Deep Voice와 같은 파라메트릭 음성 합성 시스템과 달리 지속 시간 및 피치와 같은 기능을 완전한 엔드투엔드 방식으로 사용하고 자기 주의 기반 피드 포워드 네트워크를 사용하여 멜 스펙트로그램 또는 파형을 병렬로 생성합니다.

LJSpeech 데이터 세트에 대한 실험 결과, FastSpeech 2와 2s는 자동 회귀 모델의 음성 품질과 일치하며 추론 속도도 훨씬 빠른 것으로 나타났습니다. 또한 FastSpeech 2는 음성 품질 측면에서 기존 FastSpeech 모델을 개선하고 훈련 프로세스를 간소화합니다.

### 2 FASTSPEECH 2 AND 2S

위의 내용은 텍스트 음성 변환(TTS) 작업을 위해 기존 FastSpeech보다 개선된 FastSpeech 2 모델에 대한 자세한 개요를 제공합니다. 이 모델은 TTS 영역에 존재하는 몇 가지 과제, 특히 일대다 매핑 문제를 해결하고 훈련 파이프라인을 간소화하고 음성 품질을 향상시키는 솔루션을 제공합니다.

2.1 동기 부여

FastSpeech 2의 개발 동기는 피치, 지속 시간, 볼륨, 운율과 같은 음성 오디오의 변화로 인해 단일 텍스트 시퀀스가 다양한 가능한 음성 시퀀스에 대응할 수 있는 일대다 매핑 문제의 고유한 과제에서 비롯되었습니다. FastSpeech 모델은 이미 지식 증류와 지속 시간 정보 사용을 도입하여 이 문제를 완화하기 위한 조치를 취했습니다. 그러나 이 모델에는 복잡한 훈련 과정, 교사 모델의 정보 손실, 지속 시간 정보의 부정확성 등의 단점이 있었습니다.

FastSpeech 2는 훈련 파이프라인을 단순화하고, 실측 음성을 훈련 대상으로 사용하여 정보 손실을 방지하며, 지속 시간 정확도를 개선하고 더 많은 분산 정보를 도입함으로써 이러한 문제를 해결합니다.

2.2 모델 개요

FastSpeech 2의 아키텍처는 인코더가 음소 시퀀스를 숨겨진 시퀀스로 변환한 후 분산 적응기가 다양한 분산 정보를 숨겨진 시퀀스에 통합하는 것으로 시작됩니다. 그런 다음 멜-스펙트로그램 디코더가 이 적응된 시퀀스를 멜-스펙트로그램 시퀀스로 변환합니다. 이 모델은 교사-학생 간 증류 파이프라인을 없애고 강제 정렬을 통해 얻은 보다 정확한 음소 지속 시간을 사용합니다. 또한 FastSpeech 2는 피치 및 에너지 예측기를 통합하여 일대다 매핑 문제를 해결하는 데 필수적인 더 많은 분산 정보를 제공합니다.

![Untitled](FastSpeech%202%20Fast%20and%20High-Quality%20End-to-End%20Text%2025e93b28fb62413da475aca2bcb0692a/Untitled.png)

2.3 분산 어댑터

분산 어댑터는 지속 시간, 피치, 에너지와 같은 다양한 분산 정보를 음소 히든 시퀀스에 통합합니다. 여기에는 지속 시간 예측기, 피치 예측기, 에너지 예측기의 세 가지 구성 요소가 포함됩니다. 녹음에서 추출한 지속 시간, 피치, 에너지의 기준값은 훈련 중에 사용됩니다.

지속 시간 예측기 출력은 로그 영역에서 각 음소의 길이를 나타내고, 피치 예측기 출력은 프레임 수준의 기본 주파수 시퀀스를 제공하며, 에너지 예측기 출력은 각 멜-스펙트로그램 프레임의 에너지를 나타냅니다.

또한 FastSpeech 2는 FastSpeech보다 더 정확한 방법인 MFA로 음소 지속 시간을 추출합니다. 또한 원시 파형에서 피치(F0)와 에너지를 추출하여 원핫 벡터 시퀀스로 정량화합니다.

직접 파형 생성

완전한 엔드투엔드 시스템을 향한 한 단계로, 텍스트에서 파형을 직접 생성하여 계단식 멜-스펙트로그램 생성 및 파형 생성의 필요성을 없애는 FastSpeech 2s도 제안합니다. 입력과 출력 간의 정보 격차가 크고 긴 파형 샘플과 제한된 메모리로 인해 전체 텍스트 시퀀스에 대한 학습이 어렵다는 등의 문제를 극복합니다.

FastSpeech 2s는 파형 디코더에서 적대적 훈련을 사용하여 위상 정보를 암시적으로 복구하고 전체 텍스트 시퀀스에 대해 훈련된 멜-스펙트로그램 디코더를 활용하여 텍스트 특징 추출을 지원합니다. 파형 디코더는 WaveNet 구조를 기반으로 하며 비인과적 컨볼루션과 게이트 활성화를 포함합니다. 추론 중에는 파형 디코더만 음성 오디오를 합성하는 데 사용됩니다.

### 3 EXPERIMENTS AND RESULTS

FastSpeech 2 및 FastSpeech 2를 평가하기 위한 설정 및 결과는 다음과 같습니다:

3.1 실험 설정

13,100개의 영어 오디오 클립과 해당 텍스트 트랜스크립트가 포함된 LJSpeech 데이터 세트에서 FastSpeech 2 시스템을 평가했습니다. 발음을 돕기 위해 텍스트는 음소 시퀀스로 변환되었습니다. FastSpeech 2 모델은 인코더와 멜-스펙트로그램 디코더에 4개의 피드포워드 트랜스포머 블록을 사용했습니다. 훈련에는 아담 옵티마이저를 사용하고 특정 학습 속도 스케줄에 따라 48개의 문장으로 구성된 배치 크기의 NVIDIA V100 GPU를 사용했습니다. 훈련은 컨버전스까지 16만 단계 동안 진행되었습니다.

3.2 결과

시스템의 오디오 품질을 평가하기 위해 영어 원어민이 합성된 음성 샘플을 평가하는 평균 의견 점수(MOS) 평가를 실시했습니다. 이 시스템은 타코트론 2, 트랜스포머 TTS, 패스트스피치 등 다른 시스템과 비교되었습니다. FastSpeech 2 및 2s 시스템은 Transformer TTS 및 Tacotron 2와 같은 자동 회귀 모델과 비슷한 성능을 보였으며, 원래 FastSpeech 모델보다 성능이 더 뛰어났습니다.

속도 측면에서 FastSpeech 2는 교사-학생 증류 과정을 제거하여 훈련 파이프라인을 간소화하여 총 훈련 시간을 FastSpeech에 비해 3.22배 단축했습니다. 추론 속도 측면에서도 FastSpeech 2와 2초는 각각 149배, 170배로 Transformer TTS 모델보다 훨씬 빨랐습니다.

FastSpeech 2에 분산 정보(피치 및 에너지 등)를 도입한 결과, 모델 최적화와 일반화 개선에 도움이 되는 것으로 분석되었습니다. 더 많은 분산 정보를 입력으로 제공함으로써 FastSpeech 2와 2s는 더 정확한 피치와 에너지로 음성을 합성할 수 있었습니다. 또한 더 정확한 지속 시간 정보는 음성 품질을 개선하는 데 도움이 되었습니다.

마지막으로, 절제 연구에 따르면 FastSpeech 2와 2s에서 에너지와 피치 분산을 제거하면 음성 품질이 저하되는 것으로 나타나 그 중요성을 입증했습니다. 또한 이 시스템은 피치 입력을 조작하여 합성 음성의 피치를 제어하는 등 편차를 제어할 수 있는 이점을 제공했습니다.

### 4 CONCLUSION

이 글에서 저자는 FastSpeech의 기존 문제를 해결하고 일대다 매핑 문제를 완화하기 위해 고안된 매우 효율적이고 우수한 품질의 엔드투엔드 텍스트 음성 변환(TTS) 시스템인 FastSpeech 2를 소개합니다. FastSpeech 2는 이전 FastSpeech 모델과 달리 실측 멜-스펙트로그램에 직접 모델을 훈련함으로써 훈련 파이프라인을 간소화하고 정보 손실을 방지합니다. 또한 이 시스템은 지속 시간 예측의 정확도를 높이고 피치 및 에너지를 포함한 추가 분산 정보를 도입하여 음성 합성을 더욱 강력하고 자연스러운 사운드로 만듭니다.

FastSpeech 2를 기반으로 저자는 완전한 엔드투엔드 학습 및 추론을 활용하는 비회귀적 텍스트-파형 생성 모델인 FastSpeech 2s도 소개합니다. 실험 결과에 따르면 FastSpeech 2와 2s는 더 간단한 훈련 파이프라인을 유지하면서 음성 품질 측면에서 FastSpeech보다 뛰어난 성능을 발휘합니다. 또한 빠르고 강력하며 제어 가능한 음성 합성을 포함한 FastSpeech의 장점도 그대로 유지합니다.

향후 연구에서는 음성 품질을 더욱 개선하기 위해 더 많은 분산 정보를 통합하는 데 초점을 맞출 예정입니다. 또한 저자들은 더 가벼운 모델을 설계하여 추론 프로세스의 속도를 높일 계획입니다. 이 연구는 자연스러운 인간의 음성에 더 가까이 다가갈 수 있도록 TTS 시스템을 발전시키는 중요한 단계입니다.

- 요약
    
    문제 식별: 저자들은 먼저 패스트스피치 텍스트 음성 변환 시스템의 문제점을 파악했습니다. 그들은 일대다 매핑 문제와 교사-학생 증류 과정을 통한 교육 중 정보 손실 문제에 주목했습니다.
    
    FastSpeech 2 모델 제안: 이러한 문제를 해결하기 위해 이들은 직접 훈련을 위해 실측 멜-스펙트로그램을 사용하는 FastSpeech 2를 제안했습니다. 이를 통해 훈련 파이프라인을 간소화하고 정보 손실을 방지할 수 있습니다. 또한 FastSpeech 2는 일대다 매핑 문제를 완화하기 위해 더 많은 분산 정보(피치, 에너지 및 더 정확한 지속 시간)를 훈련 프로세스에 도입합니다.
    
    FastSpeech 2의 모델 제안: 패스트스피치 2 모델을 기반으로 비회귀적 텍스트-파형 생성 모델인 패스트스피치 2s를 제안했습니다. FastSpeech 2s는 완전한 엔드투엔드 훈련 및 추론의 이점을 활용하여 프로세스를 가속화합니다.
    
    실험 설정: 연구진은 13,100개의 영어 오디오 클립과 해당 텍스트 트랜스크립트로 구성된 LJSpeech 데이터 세트에서 FastSpeech 2와 FastSpeech 2s를 평가했습니다. 평균 절대 오차(MAE)를 사용하여 모델의 출력을 최적화했습니다.
    
    실험 결과 및 분석: 패스트스피치 2와 2s는 음성 품질 측면에서 패스트스피치를 능가하는 것으로 나타났습니다. 또한 훈련 시간이 크게 줄어든 것으로 보아 더 효율적이었습니다. 분산 정보가 모델 최적화 및 일반화에 미치는 영향, 합성 음성의 피치 및 에너지 정확도, 보다 정확한 지속 시간 정보의 효과를 보여주기 위해 분석이 수행되었습니다.
    
    절제 연구: 저자들은 음높이, 에너지 등 다양한 분산 정보의 효과를 확인하기 위해 FastSpeech 2와 2s에서 제거 연구를 수행했습니다. 그 결과 이러한 변수를 제거하면 음성 품질이 저하되는 것을 발견하여 그 중요성을 강조했습니다.
    
    분산 제어: FastSpeech 2 및 2s 모델을 사용하면 피치 입력을 조작하여 합성 음성을 더욱 세밀하게 제어할 수 있으며, 이는 실험을 통해 입증되었습니다.
    
    향후 작업: 연구진은 음성 품질을 더욱 개선하기 위해 더 많은 분산 정보를 통합하고 추론 프로세스의 속도를 높이기 위해 더 가벼운 모델을 설계할 계획입니다.