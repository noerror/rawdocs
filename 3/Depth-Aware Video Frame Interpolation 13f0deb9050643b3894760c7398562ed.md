# Depth-Aware Video Frame Interpolation

*새로운 깊이 인식 #비디오프레임보간 알고리즘은 깊이 정보를 사용하여 인상적인 효율로 부드러운 중간 프레임을 생성합니다. 비디오 처리 기술의 기준을 높이고 있습니다! #AI #MachineLearning #VideoEditing*

[https://github.com/baowenbo/DAIN](https://github.com/baowenbo/DAIN)

[https://arxiv.org/pdf/1904.00830.pdf](https://arxiv.org/pdf/1904.00830.pdf)

[https://arxiv.org/abs/1904.00830](https://arxiv.org/abs/1904.00830)

- Apr 2019

비디오 프레임 보간은 비디오 시퀀스에서 기존 프레임 사이에 추가 프레임을 생성하는 데 사용되는 프로세스입니다. 최근 심층 컨볼루션 신경망 덕분에 성능이 많이 개선되었지만, 영상에 움직임이 많거나 시야에 무언가가 가려져 있는 경우(오클루전)에는 여전히 어려움이 있습니다.

![Untitled](Depth-Aware%20Video%20Frame%20Interpolation%2013f0deb9050643b3894760c7398562ed/Untitled.png)

이 연구는 깊이 정보(기본적으로 비디오에서 물체가 얼마나 멀거나 가까운지 판단하는 정보)를 사용하여 오클루전을 감지할 수 있는 새로운 비디오 프레임 보간 방법을 제시합니다. 연구진은 네트워크에 중간 흐름을 합성하는 레이어를 설계하여 멀리 있는 물체보다 가까운 물체의 우선순위를 정했습니다. 또한 이 시스템은 계층적 기능을 사용하여 연구 중인 픽셀 주변의 픽셀에서 컨텍스트를 수집합니다.

그런 다음 입력 프레임, 뎁스 맵, 컨텍스트 특징을 광학 흐름(장면에서 물체가 겉으로 보이는 움직임의 패턴)과 로컬 보간 커널(특정 작업을 수행하는 작은 프로그램)에 따라 변경하여 출력 프레임을 생성하는 데 도움을 줍니다.

이 방법은 효과적일 뿐만 아니라 작고 효율적이며 전체 프로세스를 완전히 차별화할 수 있어 표준 딥러닝 기법을 사용하여 최적화할 수 있습니다. 정량적 및 정성적 결과 모두 이 새로운 모델이 다양한 데이터 세트에서 기존 비디오 프레임 보간 방법보다 더 나은 성능을 보였습니다.

1. Introduction

비디오 프레임 보간은 컴퓨터 비전에서 중요한 영역으로, 슬로우 모션 생성, 새로운 뷰 합성, 프레임 속도 업컨버전, 비디오 스트리밍의 프레임 복구와 같은 다양한 애플리케이션에 사용됩니다. 프레임 속도가 높은 동영상은 시간적 지터링이나 모션 블러와 같은 문제를 방지하여 시청자에게 더 매력적으로 다가갈 수 있습니다. 그러나 심층 컨볼루션 신경망(CNN)의 발전에도 불구하고 큰 움직임과 오클루전(시야 차단)으로 인해 고품질 프레임을 생성하는 것은 여전히 어려운 과제입니다.

이러한 문제를 해결하기 위해 여러 가지 방법이 제안되었습니다. 일부 접근 방식은 단계별 전략 또는 고급 흐름 추정 아키텍처를 사용하여 큰 움직임을 처리합니다. 오클루전을 처리하는 일반적인 방법은 오클루전 마스크를 추정한 다음 픽셀을 블렌딩하는 것입니다. 최근의 일부 기법은 큰 주변 영역에서 픽셀을 적응적으로 생성하는 방법을 학습합니다. 이러한 방법은 어느 정도 효과적이지만 대량의 학습 데이터와 모델의 오클루전 추론 능력에 의존하기 때문에 다양한 실제 장면에서 잘 작동하지 않을 수 있습니다.

이 연구에서는 깊이 정보를 사용하여 오클루전을 명시적으로 감지하는 방법을 제안합니다. 이 알고리즘은 중간 프레임을 생성할 때 가까이 있는 물체를 우선적으로 고려합니다. 양방향 광학 흐름과 깊이 맵을 추정하고 중간 흐름을 생성합니다. 여러 흐름 벡터가 같은 위치에서 만나면 깊이 값을 기반으로 각 흐름 벡터의 기여도를 계산합니다.

DAIN(Depth-Aware 비디오 프레임 보간)이라고 하는 이 방법은 광학 흐름, 로컬 보간 커널, 뎁스 맵, 컨텍스트 기능을 사용하여 고품질 비디오 프레임을 생성합니다. 사전 학습된 인식 네트워크에 의존하는 대신 계층적 특징을 학습하여 넓은 영역에서 효과적인 컨텍스트 정보를 추출합니다. 그런 다음 추정된 흐름과 로컬 보간 커널을 기반으로 입력 프레임, 컨텍스트 특징, 심도 맵을 워프하여 출력 프레임을 생성합니다.

DAIN은 선명한 물체 모양과 날카로운 모서리를 생성하고 슬로우 모션 비디오의 중간 프레임을 생성할 수 있습니다. 여러 벤치마크 실험을 통해 DAIN이 기존 방법보다 성능이 뛰어나다는 것이 입증되었습니다. 현재의 최신 접근 방식보다 더 효과적이고 효율적이며 컴팩트합니다. 또한 오클루전을 명시적으로 감지하는 방법을 제공하고, 고품질 프레임 합성을 위해 여러 가지 요소를 결합하는 깊이 인식 비디오 프레임 보간 방법을 제안합니다.

2. Related Work

비디오 프레임 보간은 많은 선행 연구를 통해 잘 연구된 분야입니다. 이 개요에서는 머신러닝을 사용하는 최신 기법에 초점을 맞추고 깊이 추정과 같은 관련 주제에 대해서도 논의합니다.

초기 연구에서 Long 등은 CNN을 훈련시켜 중간 프레임을 직접 생성했지만, 일반적인 CNN은 자연 이미지와 비디오의 다중 모드 분포를 정확하게 포착할 수 없어 결과가 흐릿한 경우가 많았습니다. Liu 등은 입력 프레임을 왜곡하기 위해 공간과 시간을 가로지르는 3D 광학 흐름인 딥 복셀 플로우를 제안했습니다. 이 방법은 흐릿함을 줄였지만 움직임이 큰 장면에서는 흐름을 추정하는 것이 여전히 어려웠고 부정확한 흐름은 왜곡과 시각적 아티팩트를 유발할 수 있었습니다.

광학 흐름에 의존하는 대신 AdaConv 및 SepConv와 같은 일부 기술은 적응형 보간 커널을 추정하여 더 넓은 영역에서 픽셀을 생성합니다. 그러나 이러한 방법에는 많은 메모리와 연산 능력이 필요할 수 있습니다. Bao 등은 흐름 기반 접근 방식과 커널 기반 접근 방식을 단일 네트워크에 통합하여 광학 흐름을 사용하여 입력 프레임을 워프한 다음 학습된 보간 커널을 사용하여 샘플링합니다.

기존 방식은 오클루전 마스크를 추정하거나 컨텍스트 특징을 추출하거나 대규모 로컬 보간 커널을 학습하여 간접적으로 오클루전(시야 차단)을 처리하는 경우가 많습니다. 반면, 이 연구에서는 흐름 투영 레이어의 깊이 정보를 사용하여 오클루전을 직접 감지합니다. 또한 출력 프레임을 생성하기 위한 컨텍스트 정보로 깊이 맵을 학습된 계층적 특징과 결합합니다.

깊이는 장면의 3D 지오메트리를 이해하는 데 중요한 시각적 단서이며 이미지 분할 및 오브젝트 감지와 같은 작업에 사용되어 왔습니다. 기존 방법에서는 스테레오 이미지가 필요하지만 최근의 머신러닝 접근 방식은 단일 이미지에서 깊이를 추정하는 것을 목표로 합니다. 이 연구에서는 메가뎁스 데이터세트에서 학습된 모델을 사용하여 입력 프레임에서 깊이 맵을 예측합니다. 이 모델은 워핑과 보간을 위해 상대적인 깊이를 학습합니다.

일부 기술은 작업 간 제약 조건과 일관성을 사용하여 광학 흐름과 깊이를 공동으로 추정하지만, 유니티의 모델은 이 작업도 수행하지만 프레임 보간을 위해 흐름과 깊이를 최적화합니다. 즉, 픽셀 모션과 장면 깊이에 대한 추정치가 실제 값을 반영하지 못할 수 있습니다.

3. Depth-Aware Video Frame Interpolation

이 섹션에서는 흐름 통합을 위한 오클루전을 처리하도록 설계된 '깊이 인식 흐름 투영'이라는 새로운 방법을 중심으로 프레임 보간 알고리즘에 대해 간략하게 설명합니다.

![제안된 깊이 인식 비디오 프레임 보간 모델의 아키텍처. 두 개의 입력 프레임이 주어지면 먼저 광학 흐름과 깊이 맵을 광학 흐름과 깊이 맵을 추정하고 제안된 깊이 인식 흐름 투영 레이어를 사용하여 중간 흐름을 생성합니다. 그런 다음 적응형 워핑 레이어를 채택하여 입력 프레임, 심도 맵, 컨텍스트 피처를 흐름과 공간적으로 다양한 보간을 기반으로 워핑합니다. 커널을 사용합니다. 마지막으로 프레임 합성 네트워크를 적용하여 출력 프레임을 생성합니다.](Depth-Aware%20Video%20Frame%20Interpolation%2013f0deb9050643b3894760c7398562ed/Untitled%201.png)

제안된 깊이 인식 비디오 프레임 보간 모델의 아키텍처. 두 개의 입력 프레임이 주어지면 먼저 광학 흐름과 깊이 맵을 광학 흐름과 깊이 맵을 추정하고 제안된 깊이 인식 흐름 투영 레이어를 사용하여 중간 흐름을 생성합니다. 그런 다음 적응형 워핑 레이어를 채택하여 입력 프레임, 심도 맵, 컨텍스트 피처를 흐름과 공간적으로 다양한 보간을 기반으로 워핑합니다. 커널을 사용합니다. 마지막으로 프레임 합성 네트워크를 적용하여 출력 프레임을 생성합니다.

이 알고리즘의 목표는 주어진 두 입력 프레임 사이에 중간 프레임을 생성하는 것입니다. 양방향 광학 흐름을 추정하고 입력 프레임을 워프하여 이 중간 프레임을 생성하는 데 사용합니다. 이 프로세스에는 일반적으로 순방향 워핑과 역방향 워핑이라는 두 가지 전략이 사용됩니다. 그러나 두 전략 모두 왜곡된 이미지에 구멍이 생기거나 중간 흐름의 근사치를 구하기 어려운 문제가 발생할 수 있습니다. 이 문제를 해결하기 위해 저자들은 다른 연구에서 사용된 흐름 투영 레이어를 채택했지만, 깊이 순서를 사용하여 폐색을 감지함으로써 이를 개선했습니다.

깊이 인식 흐름 투영은 주어진 시간에 해당 위치를 통과하는 흐름 벡터를 반전시켜 특정 위치의 중간 흐름에 근사치를 구합니다. 이 전략을 사용하면 흐름이 더 가까운 오브젝트의 우선 순위를 지정하고 깊이 값이 더 큰 가려진 픽셀의 영향을 줄일 수 있습니다.

또한 저자는 인접한 위치에서 사용 가능한 흐름의 평균을 구하여 중간 흐름에 잠재적인 구멍을 처리합니다. 이 과정을 통해 조밀한 중간 흐름 필드가 생성됩니다. 제안된 깊이 인식 흐름 투영 레이어는 완전히 미분 가능하기 때문에 훈련 중에 흐름 및 깊이 추정 네트워크를 공동으로 최적화할 수 있습니다.

프레임 보간 모델은 흐름 추정, 깊이 추정, 컨텍스트 추출, 커널 추정 및 프레임 합성 네트워크와 같은 여러 하위 모듈로 구성됩니다. 저자들은 흐름 추정에는 PWC-Net이라는 아키텍처를, 깊이 추정에는 모래시계 아키텍처를 사용합니다. 처음부터 학습된 컨텍스트 추출 네트워크를 사용하여 컨텍스트 특징을 학습합니다.

적응형 워핑 레이어는 입력 프레임, 심도 맵, 컨텍스트 특징을 워핑하는 데 사용되며, 프레임 합성 네트워크는 잔여 학습을 통해 출력 프레임을 생성합니다. 훈련 프로세스에는 손실 함수를 최적화하고 모델 훈련을 위해 Vimeo90K 데이터 세트를 사용하는 것이 포함됩니다. 이 과정은 NVIDIA Titan X(파스칼) GPU 카드에서 수행되며, 모델이 수렴하는 데 약 5일이 걸립니다.

4

이 글은 새로운 비디오 프레임 보간 모델에 대한 연구를 요약한 것입니다. 이 연구에는 Middlebury, Vimeo90K, UCF101, HD 등 다양한 해상도의 다양한 동영상 데이터 세트를 사용하여 이 모델을 평가하는 내용이 포함되어 있습니다. 모델의 성능은 보간 오차(IE), 정규화 보간 오차(NIE), PSNR 및 SSIM과 같은 지표를 사용하여 측정됩니다.

이 모델은 깊이 인식 흐름 투영과 학습된 계층적 컨텍스트 특징이라는 두 가지 주요 구성 요소의 영향을 이해하기 위해 추가로 분석됩니다. 깊이 인식 흐름 투영의 다양한 변형을 테스트하여 최적화된 버전이 가장 좋은 결과를 도출합니다. 문맥적 특징, 특히 학습된 계층적 특징을 사용하면 모델의 성능이 크게 향상되는 것으로 나타났습니다.

그런 다음 이 모델을 MIND, DVF, SepConv, CtxSyn, ToFlow, 슈퍼 슬로모, MEMC-Net과 같은 다른 최신 프레임 보간 알고리즘과 비교합니다. 제안된 모델은 성능이 매우 뛰어나며 Middlebury 벤치마크에서 NIE 기준 1위, IE 기준 3위를 차지했습니다. 또한 더 적은 수의 모델 매개변수를 사용하면서도 UCF101, Vimeo90K, HD 및 Middlebury 데이터 세트에서 다른 방법보다 성능이 뛰어납니다.

하지만 이 모델에는 한계가 있습니다. 이 모델은 흐름 집계를 위한 오클루전을 감지하기 위해 뎁스 맵의 정확도에 크게 의존합니다. 뎁스 맵이 정확하게 추정되지 않는 상황에서는 물체 경계가 모호한 흐릿한 결과를 초래합니다. 이를 극복하기 위해 연구진은 두 입력 프레임에서 깊이를 추정하거나 광학 흐름과 깊이 사이의 일관성을 모델링할 것을 제안합니다.

5

이 작업은 새로운 깊이 인식 비디오 프레임 보간 알고리즘을 도입합니다. 이 방법은 깊이 정보를 사용하여 오클루전을 감지함으로써 멀리 있는 물체보다 가까운 물체의 샘플링을 촉진합니다. 또한 이 알고리즘은 학습된 계층적 특징과 깊이 맵을 중간 프레임을 생성하기 위한 컨텍스트로 활용합니다. 이 모델은 효율적이고 컴팩트합니다. 정량적, 정성적 테스트를 통해 이 방법이 다양한 데이터 세트에서 기존 프레임 보간 알고리즘보다 뛰어난 성능을 발휘한다는 사실이 입증되었습니다. 비디오 프레임 보간에 깊이 단서를 사용하는 이러한 발전은 향후 이 분야의 연구에 귀중한 인사이트를 제공합니다.

- 요약
    
    알고리즘 소개: 이 프로젝트는 비디오 시퀀스에서 기존 프레임 사이에 중간 프레임을 생성하는 프로세스인 비디오 프레임 보간을 위한 새로운 방법을 도입하고 있습니다. 이 방법의 참신함은 깊이 인식을 사용하여 비디오에서 물체가 얼마나 가깝거나 멀리 있는지를 고려한다는 데 있습니다.
    
    오클루전 감지: 알고리즘은 깊이 정보를 사용하여 비디오의 한 오브젝트가 다른 오브젝트를 시야에서 가리는 오클루전을 명시적으로 감지할 수 있습니다. 이는 중간 프레임에서 가려진 물체가 어디에 있는지 알기 어려울 수 있기 때문에 비디오 처리에서 까다로운 문제가 될 수 있습니다.
    
    깊이 인식 플로우 프로젝션 레이어: 연구진은 알고리즘에 깊이 인식 흐름 투영 레이어라는 새로운 구성 요소를 제안했습니다. 알고리즘의 이 부분은 새 프레임을 생성할 때 멀리 있는 물체보다 가까이 있는 물체를 우선시합니다.
    
    컨텍스트 정보 사용: 알고리즘은 심도를 사용하는 것 외에도 학습된 계층적 특징과 심도 맵이라는 것을 사용하여 중간 프레임을 생성하는 데 도움을 줍니다. 이러한 기능은 알고리즘이 비디오를 이해하고 정확한 새 프레임을 생성하는 데 더 많은 컨텍스트를 제공합니다.
    
    모델의 효율성과 소형화: 이 알고리즘은 수행하는 작업의 복잡성에도 불구하고 처리 효율이 높고 설계가 간결합니다.
    
    평가 및 테스트: 이 알고리즘에 대해 정량적(측정 가능한 수치) 및 정성적(보다 주관적이고 관찰에 기반한) 평가를 포함한 광범위한 테스트가 수행되었습니다. 이 알고리즘은 다양한 비디오 데이터 세트에서 우수한 성능을 보였습니다.
    
    향후 연구에 대한 시사점: 비디오 프레임 보간에 깊이 단서를 효과적으로 사용하는 이 방법은 향후 연구에 유용한 인사이트를 제공합니다. 비디오 향상 및 편집 도구에 대한 새로운 가능성을 열어줍니다.
    
- 비디오 프레임 보간
    
    비디오 프레임 보간은 비디오 시퀀스 내에서 누락된 프레임을 채워 기존 프레임 간의 전환을 부드럽게 만드는 기술입니다. 이 논문에서는 이 작업에 대한 다양한 접근 방식과 그 한계에 대해 논의합니다:
    
    G. Long, L. Kneip, J. M. Alvarez, H. Li, X. Zhang, and Q. Yu. Learning image matching by simply watching video. In ECCV, 2016 : 이 작업에 컨볼루션 신경망(CNN)을 사용한 가장 초기의 방법 중 하나입니다. 이들은 일반 CNN을 훈련시켜 중간 프레임을 직접 생성합니다. 그러나 일반 CNN은 자연 이미지와 비디오의 다중 모드 분포를 포착하는 데 어려움을 겪기 때문에 결과가 흐릿한 경우가 많습니다.
    
    Liu 외. Video frame synthesis using deep voxel flow. In ICCV, 2017: 이 팀은 공간과 시간을 가로지르는 3D 광학 흐름인 딥 복셀 흐름을 제안했습니다. 이들은 삼선형 샘플링을 기반으로 입력 프레임을 워핑합니다. 이 방법은 흐릿함을 줄이지만 큰 움직임이 포함된 장면에서는 여전히 어려움을 겪습니다. 부정확한 흐름은 심각한 왜곡과 시각적 아티팩트를 초래할 수 있습니다.
    
    Video frame interpolation via adaptive convolution. In CVPR, 2017 및 Video frame interpolation via adaptive separable convolution. In ICCV, 2017: 이 방법은 광학 흐름을 사용하는 대신 공간 적응형 보간 커널을 추정하여 큰 이웃의 픽셀을 합성합니다. 이러한 방법은 광학 흐름에 대한 의존도를 낮추지만 많은 메모리가 필요하고 계산 부하가 높습니다.
    
    MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement : 이 접근 방식은 흐름 기반 접근 방식과 커널 기반 접근 방식을 하나의 엔드투엔드 네트워크로 결합합니다. 입력 프레임은 먼저 광학 흐름에 의해 워핑된 다음 적응형 워핑 레이어 내에서 학습된 보간 커널을 통해 샘플링됩니다.
    
    기존 방식은 오클루전 마스크 추정, 컨텍스트 특징 추출, 대규모 로컬 보간 커널 학습 등의 방법으로 오클루전(장면에서 한 오브젝트가 다른 오브젝트 뒤에 숨어 있는 경우) 문제를 암시적으로 해결합니다.
    
    반면, 이 논문에서 소개하는 접근 방식은 플로우 투영 레이어의 깊이 정보를 사용하여 오클루전을 명시적으로 감지합니다. 또한 뎁스 맵과 학습된 계층적 특징을 컨텍스트 정보로 통합하여 출력 프레임을 생성합니다. 이 접근 방식은 보다 미묘하고 정확한 비디오 프레임 보간을 제공하는 것을 목표로 합니다
    
- 깊이 예측
    
    깊이 추정이란 이미지나 장면에서 물체가 얼마나 멀리 떨어져 있는지를 결정하는 과정으로, 장면의 3D 지오메트리를 이해하는 데 중요한 요소입니다. 이미지 분할 및 물체 감지와 같은 여러 인식 작업에 사용됩니다. 기존 방법에서는 스테레오 이미지(동일한 장면을 약간 다르게 본 두 개의 이미지)를 사용하여 디스패리티(두 이미지 내에서 동일한 물체의 위치 차이)를 계산하고 이를 통해 깊이를 구합니다.
    
    최근에는 단일 이미지에서 깊이를 추정하는 학습 기반 접근 방식[8, 9, 11, 18, 20, 31, 32, 37]이 개발되었습니다. 이 연구에서 제안한 모델은 네트워크, 특히 MegaDepth 데이터 세트[6]에서 학습된 Chen 등[19]의 모델을 사용하여 입력 프레임에서 깊이 맵을 예측합니다. 깊이 네트워크의 초기화는 오클루전 추론에 매우 중요한 것으로 밝혀졌습니다. 그런 다음 깊이 네트워크는 프레임 보간을 위해 특별히 다른 하위 모듈과 공동으로 미세 조정됩니다. 따라서 이 모델은 워핑과 보간에 사용되는 상대적 깊이를 학습합니다.
    
    - [8] D. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In CVPR, 2015
    - [9] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. In NIPS, 2014
    - [11] H. Fu, M. Gong, C. Wang, K. Batmanghelich, and D. Tao. Deep ordinal regression network for monocular depth estimation. In CVPR, 2018
    - [18] Y. Kuznietsov, J. Stuckler, and B. Leibe. Semi-supervised ¨ deep learning for monocular depth map prediction. In CVPR, 2017
    - [20] F. Liu, C. Shen, G. Lin, and I. D. Reid. Learning depth from single monocular images using deep convolutional neural fields. TPAMI, 2016
    - [31] A. Roy and S. Todorovic. Monocular depth estimation using neural regression forest. In CVPR, 2016
    - [32] A. Saxena, S. H. Chung, and A. Y. Ng. Learning depth from single monocular images. In NIPS, 2006
    - [37] P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. L. Yuille. Towards unified depth and semantic prediction from a single image. In CVPR, 2015
    - [6] W. Chen, Z. Fu, D. Yang, and J. Deng. Single-image depth perception in the wild. In NIPS, 2016
    - [19] Z. Li and N. Snavely. Megadepth: Learning single-view depth prediction from internet photos. In CVPR, 2018
    
    이 논문에서는 작업 간 제약 조건과 일관성을 활용하여 광학 흐름(물체 또는 카메라의 움직임으로 인해 연속된 프레임 사이에서 이미지 객체가 겉으로 보이는 움직임의 패턴)과 깊이를 공동으로 추정하는 다른 접근 방식에 대해서도 언급하고 있습니다. 제안된 모델도 광학 흐름과 깊이를 공동으로 추정하지만, 모델이 추정하는 흐름과 깊이 값은 프레임 보간을 위해 특별히 최적화되어 있으므로 픽셀 움직임과 장면 깊이의 실제 값과 유사하지 않을 수 있습니다.
    
    - [40] Z. Yin and J. Shi. Geonet: Unsupervised learning of dense depth, optical flow and camera pose. In CVPR, 2018
    - [42] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe. Unsupervised learning of depth and ego-motion from video. In CVPR, 2017
    - [43] Y. Zou, Z. Luo, and J.-B. Huang. DF-Net: Unsupervised Joint Learning of Depth and Flow using Cross-Task Consistency. In ECCV, 2018