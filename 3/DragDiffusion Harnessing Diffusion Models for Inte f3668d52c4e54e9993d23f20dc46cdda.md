# DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing

[https://yujun-shi.github.io/projects/dragdiffusion.html](https://yujun-shi.github.io/projects/dragdiffusion.html)

[https://arxiv.org/abs/2306.14435](https://arxiv.org/abs/2306.14435)

[https://github.com/Yujun-Shi/DragDiffusion](https://github.com/Yujun-Shi/DragDiffusion)

![DRAGDIFFUSION은 "드래그" 편집을 훨씬 더 다 versatile하게 만듭니다. DRAGGAN처럼 사용자는 이미지에서 이동할 점(빨강)을 선택하고, 그것들이 어디로 가야 할지(파란 점) 결정하며, 변경하고자 하는 영역(밝은 영역)을 선택할 수 있습니다. 우리의 방법은 다양한 객체, 다른 카테고리, 그리고 다양한 스타일을 가진 이미지를 정확하게 편집할 수 있습니다. 우리가 사용한 이미지들은 Unsplash, Pexels, 그리고 Pixabay에서 가져왔습니다.](DragDiffusion%20Harnessing%20Diffusion%20Models%20for%20Inte%20f3668d52c4e54e9993d23f20dc46cdda/Untitled.png)

DRAGDIFFUSION은 "드래그" 편집을 훨씬 더 다 versatile하게 만듭니다. DRAGGAN처럼 사용자는 이미지에서 이동할 점(빨강)을 선택하고, 그것들이 어디로 가야 할지(파란 점) 결정하며, 변경하고자 하는 영역(밝은 영역)을 선택할 수 있습니다. 우리의 방법은 다양한 객체, 다른 카테고리, 그리고 다양한 스타일을 가진 이미지를 정확하게 편집할 수 있습니다. 우리가 사용한 이미지들은 Unsplash, Pexels, 그리고 Pixabay에서 가져왔습니다.

### 1. Introduction

드래건과 같은 제너레이티브 모델을 사용한 이미지 편집은 최근 큰 인기를 끌고 있습니다. 드래건을 사용하면 이미지에서 한 쌍의 점을 선택하고 한 점에서 다른 점으로 콘텐츠를 '드래그'하여 인터랙티브하게 이미지를 편집할 수 있습니다. 또한 사용자는 이미지에서 편집할 수 있는 부분과 동일하게 유지해야 하는 부분을 선택할 수 있습니다. 하지만 드래그 앤 드롭의 기능은 기반 기술인 생성적 적대 신경망(GAN)의 특성으로 인해 제한적입니다.

이를 개선하기 위해 인터랙티브 포인트 기반 이미지 편집에 확산 모델을 사용하는 새로운 방법인 드래그디퓨전(DRAGDIFFUSION)이 개발되었습니다. 이를 통해 '드래그' 편집에 더 많은 범위를 제공합니다. 주로 텍스트 임베딩에 의존하는 이전의 확산 기반 이미지 편집 방법과 달리 DRAGDIFFUSION은 픽셀 수준에서 작동하므로 보다 정밀한 제어가 가능합니다. 이는 특정 단계에서 확산 잠재력을 조작하여 이미지 출력을 편집하는 방식으로 이루어지며, 확산 잠재력이 생성된 이미지의 공간적 레이아웃을 결정할 수 있다는 관찰에서 영감을 얻은 프로세스입니다.

드래그디퓨전은 인터랙티브 포인트 기반 편집을 위해 모션 감독과 포인트 추적이라는 두 단계 프로세스를 따릅니다. 먼저 모션 감독 손실을 최소화하기 위해 확산 잠재력을 최적화하여 핸들 포인트를 타깃으로 이동시킵니다. 이 프로세스에서는 확산 모델 UNet의 피처 맵을 사용합니다. 확산 잠재력이 업데이트되면 핸들 포인트의 위치도 변경되므로 핸들 포인트의 위치를 계속 업데이트하기 위해 포인트 추적 작업이 수행됩니다.

확산 모델에는 여러 단계의 프로세스가 필요하기 때문에 한 가지 잠재적인 우려는 t번째 단계의 잠재력만 사용하여 이미지를 정확하게 조작할 수 있는지 여부입니다. 그러나 실험에 따르면 t 번째 단계 잠복의 UNet 특징 맵을 사용하면 정밀한 공간 조작에 충분하다는 것이 밝혀졌습니다.

이 방법에서 발생할 수 있는 한 가지 문제는 편집 결과로 인해 객체의 정체성이나 이미지 스타일이 원치 않는 방향으로 바뀔 수 있다는 것입니다. 예를 들어, 편집 중에 고양이 머리가 강아지 머리로 바뀔 수 있습니다. 이 문제는 편집 전에 입력 이미지를 재구성하기 위해 UNet의 매개변수에서 LoRA를 미세 조정하여 완화할 수 있습니다.

광범위한 실험을 통해 드래그디퓨전은 여러 오브젝트 또는 다양한 스타일의 이미지 편집과 같은 까다로운 시나리오에 적용할 수 있으며, 인터랙티브 포인트 기반 편집 프레임워크에 다용도로 사용할 수 있는 범용 툴이라는 것이 입증되었습니다.

### 2. Methodology

제안된 방법론인 드래그디퓨전은 노이즈 제거 확산 확률 모델(DDPM)을 사용해 인터랙티브 포인트 기반 이미지 편집을 용이하게 합니다. 잠재 생성 모델의 일종인 DDPM은 초기 이미지의 확률 밀도를 일련의 잠재 변수를 포함하는 공동 분포의 한계로 나타냅니다. 이 시퀀스는 표준 정규 분포에서 시작하여 학습된 전환이 있는 마르코프 체인을 형성합니다. 이 맥락에서 초기 변수 Z0은 사용자가 제공한 이미지를 나타내고, 이후의 각 변수 Zt는 확산 프로세스에서 t 단계 이후의 이미지의 "노이즈" 버전을 나타냅니다.

![이 그림은 DRAGDIFFUSION이 어떻게 작동하는지 보여줍니다. 먼저, LoRA라는 모델을 사용하여 사용자의 이미지를 재생성합니다. 그런 다음 DDIM inversion이라는 방법을 적용하여 이미지의 특별한 버전(잠재 이미지)을 얻습니다. 사용자의 편집 선택사항은 우리가 이 잠재 이미지를 어떻게 조정하는지를 안내합니다. 조정 후, 우리는 DDIM을 사용하여 노이즈를 제거하고 최종 편집 이미지를 얻습니다.](DragDiffusion%20Harnessing%20Diffusion%20Models%20for%20Inte%20f3668d52c4e54e9993d23f20dc46cdda/Untitled%201.png)

이 그림은 DRAGDIFFUSION이 어떻게 작동하는지 보여줍니다. 먼저, LoRA라는 모델을 사용하여 사용자의 이미지를 재생성합니다. 그런 다음 DDIM inversion이라는 방법을 적용하여 이미지의 특별한 버전(잠재 이미지)을 얻습니다. 사용자의 편집 선택사항은 우리가 이 잠재 이미지를 어떻게 조정하는지를 안내합니다. 조정 후, 우리는 DDIM을 사용하여 노이즈를 제거하고 최종 편집 이미지를 얻습니다.

드래그 디퓨전 프로세스는 DDPM에서 낮은 순위 적응(LoRA)을 미세 조정하는 것으로 시작됩니다. 이렇게 하면 사용자가 제공한 이미지를 재구성하여 편집 과정에서 피사체의 정체성과 이미지의 스타일을 보존합니다. 그런 다음 입력 이미지에 노이즈 제거 확산 암시 모델(DDIM) 반전을 적용하여 특정 단계 t의 확산 잠복기를 얻습니다.

그런 다음 이 방법은 반복 모션 감독과 포인트 추적을 적용하여 t번째 단계에서 얻은 확산 잠복기를 최적화합니다. 목표는 핸들 포인트의 콘텐츠를 목표 위치로 '드래그'하는 것입니다. 이 편집 프로세스 중에는 이미지의 마스킹되지 않은 영역이 변경되지 않도록 하기 위해 정규화 용어가 사용됩니다.

프로세스의 마지막 단계에서는 DDIM을 사용하여 최적화된 t번째 단계 잠재 노이즈를 제거하여 최종 편집 후 결과를 제공합니다. 이 방법론에는 그림 2에 이 프로세스에 대한 그래픽 개요가 포함되어 있습니다.

이 방법론은 이미지 편집 프로세스를 수정하고 제어하기 위해 확산 모델과 관련된 반복적인 모션 감독 및 포인트 추적 프로세스를 사용합니다. 작동 방식에 대한 설명은 다음과 같습니다:

모션 감독:
이 방법은 {h_k_i = (x_k_i, y_k_i) : i = 1, . . . , n}, 해당 목표 점 {gi = (˜xi, y˜i) : i = 1, . . . , n}. 입력 이미지는 z0으로 표현되며, t번째 단계 DDIM 반전의 결과는 zt입니다.

모션 감시를 수행하기 위해 이 방법은 두 번째 UNet 블록의 특징 맵을 사용하며, 입력으로 zt가 주어집니다(F(zt)). 픽셀 위치 h_k_i의 특징 벡터는 F_h_k_i(zt)입니다. 이 방법은 또한 Ω(h_k_i, r1)을 h_k_i = (x_k_i, y_k_i)에 중심을 둔 정사각형 패치(변 길이 2r1 + 1)로 정의합니다.

그러면 모션 감시의 k 번째 반복에 대한 목적 함수는 다음과 같이 정의됩니다:

![Untitled](DragDiffusion%20Harnessing%20Diffusion%20Models%20for%20Inte%20f3668d52c4e54e9993d23f20dc46cdda/Untitled%202.png)

이 목적 함수는 모션 감독으로 잠재된 t 번째 스텝을 업데이트하는 데 사용되며, 여기서 di는 i 번째 핸들 포인트에서 i 번째 목표 포인트까지의 정규화된 방향을 나타내고 M은 사용자가 지정한 바이너리 마스크입니다.

![Untitled](DragDiffusion%20Harnessing%20Diffusion%20Models%20for%20Inte%20f3668d52c4e54e9993d23f20dc46cdda/Untitled%203.png)

여기서 η는 학습률입니다.

포인트 추적:
모션 감독 업데이트로 인해 핸들 포인트의 위치가 변경되면 확산 잠복을 최적화한 후 핸들 포인트를 업데이트하기 위해 포인트 트래킹이 필요합니다. 이는 원본 이미지와 업데이트된 이미지 모두의 UNet 특징을 사용하여 수행되며, 각 핸들 포인트 h_k_i는 제곱 Ω(h_k_i, r2) 내에서 가장 가까운 이웃 검색으로 업데이트됩니다:

업데이트 후 새 핸들 포인트는 다음 공식을 사용하여 추적됩니다:

![Untitled](DragDiffusion%20Harnessing%20Diffusion%20Models%20for%20Inte%20f3668d52c4e54e9993d23f20dc46cdda/Untitled%204.png)

이 방법은 UNet 기능이 캡처 포인트 대응을 얼마나 잘 포착하는지를 강조한 이전 연구에서 영감을 얻은 것으로, 포인트 추적의 정확성을 보장하는 효과적인 방법입니다.

이 방법의 구현은 Stable Diffusion 1.5 확산 모델에서 수행되었습니다. 디퓨저 인스턴스를 사용하여 모든 관심도 모듈의 쿼리, 키, 값 투영 행렬에 대해 저순위 자동 회귀(LoRA) 모델을 미세 조정했습니다. LoRA의 순위는 16으로 설정되었으며, 2 × 10^-4의 학습률로 AdamW 옵티마이저를 사용하여 미세 조정 프로세스를 수행했습니다. 계산 시간 단축과 고품질 이미지 편집 사이의 균형을 맞추기 위해 LoRA는 200단계로만 미세 조정되었습니다.

편집 단계에서 50단계는 DDIM을 위해 예약되었고, 40단계 확산 잠복기는 0.01의 학습률을 가진 Adam 옵티마이저를 사용하여 최적화되었습니다. DDIM 반전 또는 DDIM 노이즈 제거 프로세스에서는 분류기 없는 안내(CFG)를 적용하지 않았습니다. 이 결정은 CFG가 수치 오류를 증폭시키는 경향이 있어 DDIM 반전 프로세스에 적합하지 않다는 사실에 근거했습니다.

방정식 (2)의 하이퍼파라미터 r1은 1로, 방정식 (4)의 r2는 3으로 설정했습니다. 방정식 (2)의 정규화 파라미터 λ는 기본 설정으로 0.1로 설정했습니다. 그러나 마스킹되지 않은 영역이 원하는 것보다 더 많이 변경된 경우 사용자가 λ를 증가시킬 수 있습니다.

마지막으로, 텍스트 프롬프트의 경우 LoRA를 미세 조정하는 데 사용되는 프롬프트와 동일해야 합니다. 이렇게 하면 이미지 편집 프로세스의 일관성을 유지할 수 있습니다.

### 3. Qualitative Evaluation

이 섹션에서는 드래그디퓨전 방식에 대한 정성적 평가를 제시합니다. 그림 3과 그림 4에 일련의 이미지 편집 결과를 표시하여 드래그디퓨전 기법의 놀라운 다용도성을 보여줍니다.

![이 그림들은 DRAGDIFFUSION이 이미지의 일부(빨간색으로 표시)를 새로운 위치(파란색으로 표시)로 "드래그"할 수 있는 방법을 보여줍니다. 밝은 영역은 사용자가 편집할 수 있는 영역입니다. 우리의 방법은 여러 개체, 다른 카테고리, 다양한 스타일을 가진 다양한 이미지를 처리할 수 있습니다.](DragDiffusion%20Harnessing%20Diffusion%20Models%20for%20Inte%20f3668d52c4e54e9993d23f20dc46cdda/Untitled%205.png)

이 그림들은 DRAGDIFFUSION이 이미지의 일부(빨간색으로 표시)를 새로운 위치(파란색으로 표시)로 "드래그"할 수 있는 방법을 보여줍니다. 밝은 영역은 사용자가 편집할 수 있는 영역입니다. 우리의 방법은 여러 개체, 다른 카테고리, 다양한 스타일을 가진 다양한 이미지를 처리할 수 있습니다.

![Untitled](DragDiffusion%20Harnessing%20Diffusion%20Models%20for%20Inte%20f3668d52c4e54e9993d23f20dc46cdda/Untitled%206.png)

그러나 이 보고서의 완성일을 기준으로 드래그디퓨전 기법의 공식적인 구현은 아직 공개되지 않았습니다. 따라서 드래건과의 직접적인 비교는 이루어지지 않았으며 이 백서의 향후 버전을 위해 유보될 것입니다.

평가에 사용된 이미지는 Unsplash, Pexels, Pixabay를 비롯한 다양한 무료 사진 플랫폼에서 제공되었습니다. 이러한 소스는 드래그디퓨전 방식의 편집 기능을 다양하게 테스트할 수 있는 광범위한 이미지를 제공합니다.