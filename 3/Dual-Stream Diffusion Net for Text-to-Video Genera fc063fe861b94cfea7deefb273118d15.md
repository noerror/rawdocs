# Dual-Stream Diffusion Net for Text-to-Video Generation

*동영상 생성에 대한 새로운 접근법인 이중 스트림 확산 네트(DSDN)를 소개합니다. DSDN은 비디오의 내용 변동의 일관성을 향상시키기 위해 설계되었습니다. 두 개의 확산 스트림, 즉 비디오 내용 및 동작 분기는 개별적으로 실행되어 개인화된 비디오 변동과 내용을 생성할 수 있습니다. 또한, 이들은 교차 트랜스포머 상호 작용 모듈을 통해 내용과 동작 도메인 간에 잘 정렬됩니다. 이 기술은 생성된 비디오의 부드러움을 향상시키고, 프레임의 일관성과 다양성을 강화합니다. 실험적인 결과로, 이 방법은 더 연속적이고 깜박임이 적은 비디오를 생성한다는 것을 보여줍니다.*

[https://arxiv.org/abs/2308.08316](https://arxiv.org/abs/2308.08316)

[https://anonymous.4open.science/r/Private-C3E8](https://anonymous.4open.science/r/Private-C3E8)

- Aug 2023

### 1 Introduction

![Untitled](Dual-Stream%20Diffusion%20Net%20for%20Text-to-Video%20Genera%20fc063fe861b94cfea7deefb273118d15/Untitled.png)

텍스트를 이미지나 동영상과 같은 시각적 콘텐츠로 변환하는 것은 AI 세계에서 흥미로운 도전 과제입니다. 이는 엔터테인먼트, 광고, 교육 등 다양한 분야에서 활용될 수 있습니다. 최근에는 텍스트 설명을 이미지로 변환할 수 있는 모델도 개발되었습니다. 최근에는 더 복잡한 스토리를 표현할 수 있기 때문에 텍스트로 동영상을 제작하는 데 초점을 맞추고 있습니다.

하지만 텍스트를 동영상으로 변환하는 대부분의 최신 방법에는 일관성 문제가 있습니다. 동영상이 깜박거리거나 고르지 않게 보일 수 있습니다. 이 연구에서는 듀얼 스트림 확산 네트워크(DSDN)라는 새로운 방법을 소개합니다. DSDN은 매끄럽고 일관성 있는 동영상을 만드는 것을 목표로 합니다. 하나는 비디오의 움직임에 초점을 맞추고 다른 하나는 비디오의 콘텐츠에 초점을 맞추는 두 부분으로 구성되어 있습니다. 이 두 부분은 특별한 주의 메커니즘을 사용하여 함께 작동하여 모션과 콘텐츠가 잘 정렬되도록 합니다. 결과는? 동영상이 더 보기 좋고 매끄럽게 흐릅니다. 실험 결과 이 새로운 방법이 매우 효과적이라는 것이 입증되었습니다.

### 2 Related Work

텍스트-이미지 생성:
AI 커뮤니티는 텍스트 설명을 이미지로 변환하는 모델을 개발하는 데 열중하고 있습니다. 이 분야에서 중요한 돌파구는 이전 기술보다 성능이 뛰어나고 텍스트 설명에서 이미지를 생성할 수 있는 노이즈 제거 확산 확률 모델(DDPM)이었습니다. 그 밖에도 텍스트-이미지 쌍을 사용하는 GLIDE, CLIP 잠재 공간을 활용하는 DALLE-2, T5와 확산 모델을 결합한 Imagen, 확산 효율을 향상시키는 잠재 확산 모델 등 주목할 만한 여러 개발이 있었습니다.

텍스트-비디오 생성:
텍스트를 이미지로 변환하는 것은 상당한 발전을 이루었지만, 텍스트를 동영상으로 만드는 단계로 넘어가면서 새로운 장애물이 등장했습니다. 동영상에는 시간 관련 시퀀스가 있고 전체적으로 일관된 움직임을 유지해야 하기 때문입니다. 이를 달성하기 위한 초기 노력에는 GAN 기반 및 자동 회귀 방식이 포함되었습니다. DDPM 모델을 비디오에 맞게 조정하여 3D U-Net 설계를 만들었습니다. 그런 다음 향후 비디오 프레임의 배포를 모델링하기 위한 혁신적인 전략이 개발되었습니다. 특히 텍스트 입력을 기반으로 비디오를 생성하는 데 중점을 둔 CogVideo 및 Text2Video-Zero 프레임워크가 등장했습니다. 그러나 현재 많은 텍스트-비디오 모델은 글리치 또는 깜박임이 있는 비디오를 생성합니다. 잠재 스트림 확산 모델과 같이 동영상의 시각적 요소와 동적 요소를 모두 캡처하려는 시도가 있었습니다. 이러한 모델은 텍스트로부터 이미지를 생성하는 데는 탁월하지만 동영상 생성에는 완전히 활용되지 않았습니다.

이러한 이전의 노력을 바탕으로, 이번 연구에서는 텍스트 설명에서 생성된 비디오의 품질과 일관성을 향상시키는 것을 목표로 하는 이중 스트림 확산 네트워크를 제안합니다.

### 3 Method

DSDN 개요:

- DSDN은 프레임 단위로 인코딩된 입력 비디오를 수신하여 콘텐츠 기능으로 인코딩합니다.
- 이러한 콘텐츠 특징에서 시간적 역학을 활용하기 위해 모션 디코더를 도입하여 모션 정보를 추출합니다.
- 그런 다음 콘텐츠와 모션 잠재 피처는 모두 개인화된 비디오 생성을 위해 이중 스트림 확산 프로세스를 거칩니다.
- 그런 다음 이 두 가지 유형의 특징을 병합하기 위해 변형된 인터랙션 모듈이 제안되고, 이 모듈은 픽셀 공간에서 비디오를 생성하기 위해 디코더에 공급됩니다.

![DSDN 네트워크 프레임워크. 초기에는 확산 과정 중에 내용 및 동작 특징이 노이즈에 추가되며, 이어서 이중 스트림 확산 네트를 통한 노이즈 제거 단계가 이루어집니다. 마지막으로, 생성된 비디오](Dual-Stream%20Diffusion%20Net%20for%20Text-to-Video%20Genera%20fc063fe861b94cfea7deefb273118d15/Untitled%201.png)

DSDN 네트워크 프레임워크. 초기에는 확산 과정 중에 내용 및 동작 특징이 노이즈에 추가되며, 이어서 이중 스트림 확산 네트를 통한 노이즈 제거 단계가 이루어집니다. 마지막으로, 생성된 비디오

순방향 확산 프로세스:

- 이 아이디어는 노이즈 제거 확산 확률 모델(DDPM)을 사용하여 잠재 피처를 가우시안 포이어로 변환하는 것입니다.
- 이를 위해 저자는 프레임을 잠재 공간으로 투영하기 위해 사전 학습된 VQ-VAE를 활용합니다.
- 콘텐츠 피처는 모션 특징을 얻기 위해 모션 분해 프로세스를 거칩니다.
- 콘텐츠와 모션 특징 모두 마르코프 프로세스를 통해 노이즈 섭동을 경험합니다.

개인화된 콘텐츠 생성 스트림:

- 이 스트림은 텍스트-이미지 모델인 안정적 확산을 기본으로 활용하여 고유하고 개인화된 콘텐츠를 생성하는 데 중점을 둡니다.
- 콘텐츠 생성에 대한 세밀한 조정을 위해 LoRA라는 점진적 학습 모듈이 사용됩니다.
- 이 스트림은 자체 주의 및 교차 주의 메커니즘과 함께 수정된 U-Net 아키텍처와 같은 다른 구성 요소를 사용합니다.

개인화된 모션 생성 스트림:

- 이 스트림은 3D U-Net 모델을 사용하여 모션 일관성 있는 잠재 피처를 생성합니다.
- 3D U-Net을 사용하는 주된 이유는 전체 비디오의 전체적인 모션 변화를 캡처할 수 있기 때문입니다.
- 모션 노이즈 제거 프로세스는 생성된 콘텐츠 특징과 텍스트 프롬프트를 조건으로 고려합니다.

듀얼 스트림 변환 인터랙션:

- 크로스 트랜스포머 상호 작용은 생성된 콘텐츠와 모션 정보를 효과적으로 정렬하도록 설계되었습니다.
- 여기에는 각 컨볼루션 레이어 후에 콘텐츠와 모션 스트림 모두의 잠재적 특징을 통합하는 작업이 포함됩니다.
- 이를 통해 보다 부드러운 프레임 전환과 전반적인 콘텐츠 품질을 보장합니다.

![이중 스트림 변환 블록.](Dual-Stream%20Diffusion%20Net%20for%20Text-to-Video%20Genera%20fc063fe861b94cfea7deefb273118d15/Untitled%202.png)

이중 스트림 변환 블록.

![동작 분해기와 동작 결합기의 세부 사항.](Dual-Stream%20Diffusion%20Net%20for%20Text-to-Video%20Genera%20fc063fe861b94cfea7deefb273118d15/Untitled%203.png)

동작 분해기와 동작 결합기의 세부 사항.

모션 분해 및 결합:

- 콘텐츠에서 모션을 분리하고 계산 비용을 최적화하기 위해 모션 디코더와 모션 컴바이너가 도입되었습니다.
- 모션 디코더는 모든 순차 프레임 쌍에서 모션 특징을 추출하고 시간적 길이 호환성을 보장합니다.
- 그런 다음 모션 결합기가 콘텐츠 특징을 인접한 모션 특징과 융합합니다.
- 이렇게 결합된 피처는 최종 디코더에 공급되어 비디오를 생성합니다.

![Text2VideoZero (Khachatryan 외. 2023) (각 행의 프레임 1-4)와 우리의 방법 (각 행의 프레임 5-8) 사이의 질적 비교.](Dual-Stream%20Diffusion%20Net%20for%20Text-to-Video%20Genera%20fc063fe861b94cfea7deefb273118d15/Untitled%204.png)

Text2VideoZero (Khachatryan 외. 2023) (각 행의 프레임 1-4)와 우리의 방법 (각 행의 프레임 5-8) 사이의 질적 비교.

본질적으로, 설명한 방법은 텍스트 프롬프트에서 고품질 비디오를 생성하기 위해 콘텐츠와 모션을 분리하고 나중에 지능적으로 병합하는 정교한 이중 스트림 확산 프로세스를 사용합니다. 확산 프로세스의 세부적인 설계, 통합 방법, 콘텐츠와 모션 간의 신중한 균형이 혁신의 핵심입니다.

### 4 Experiments

구현 세부 사항:

- 각 비디오에 대해 해상도 512의 16프레임이 생성되었습니다.
- 이 방법은 WebVid-10M 및 HD-VILA100M 데이터 세트의 하위 집합을 활용했습니다.
- 동영상은 256×256 해상도로 크기 조정 및 크롭되었습니다.
- 콘텐츠 기본 단위에는 안정적인 확산 v1.5가 사용되었습니다.
- 콘텐츠 증분 단위는 2022년부터 시비타이 모델을 사용하여 초기화한 후 미세 조정했습니다.
- 모션 단위는 LDM에서 사전 학습된 가중치를 사용했습니다.
- 추론은 NVIDIA RTX 4090 GPU를 사용하여 비디오당 약 35초가 소요되었습니다.

기준선과의 비교:

- 기준선: CogVideo 및 Text2Video-Zero.
- 평가 지표로 사용된 것은 프레임 일관성을 위한 클립 이미지 임베딩과 텍스트 정렬을 위한 클립 점수였습니다.
- 이 방법은 프레임 일관성 및 텍스트 정렬 측면에서 두 가지 기준선을 모두 능가했습니다.
- 이 방법은 텍스트2비디오-제로에 비해 콘텐츠와 모션의 일관성이 더 우수했습니다.
- 예를 들면 걷는 팬더, 달리는 사람, 질주하는 말 등이 있습니다.
- 생성된 콘텐츠가 더 사실적이고 문맥적으로 정확했습니다.
- 다양성 생성의 경우, "고양이가 잔디 위를 걷고 있다"는 텍스트 입력이 다양한 비디오 출력을 생성했습니다.

![우리의 방법의 다양성에 관한 질적 결과. 프롬프트: 풀밭에서 걷는 고양이.](Dual-Stream%20Diffusion%20Net%20for%20Text-to-Video%20Genera%20fc063fe861b94cfea7deefb273118d15/Untitled%205.png)

우리의 방법의 다양성에 관한 질적 결과. 프롬프트: 풀밭에서 걷는 고양이.

절제 연구:

- 이 연구는 콘텐츠 증분 단위와 모션 단위의 중요성을 평가하는 것을 목표로 했습니다.
- 모션 단위:
    - 모션 유닛이 없는 동영상은 프레임 간 연속성이 부족했습니다.
    - 콘텐츠가 텍스트 설명과 잘 일치했습니다.
- 증분 단위:
    - 증분 단위가 없으면 콘텐츠 품질이 저하되었습니다.
    - 일관되지 않은 시점과 색상 영향 등의 문제가 있었습니다.
- 모션 단위 시각화:
    - 시각화를 통해 팔의 움직임이나 머리카락의 펄럭임과 같은 디테일을 포착하는 데 있어 모션 단위가 얼마나 중요한지 알 수 있었습니다.
    
    ![어블레이션 연구. 프롬프트: 나뭇잎 사이에서 춤추는 소녀, 곱슬머리.](Dual-Stream%20Diffusion%20Net%20for%20Text-to-Video%20Genera%20fc063fe861b94cfea7deefb273118d15/Untitled%206.png)
    
    어블레이션 연구. 프롬프트: 나뭇잎 사이에서 춤추는 소녀, 곱슬머리.
    

전반적으로, 제시된 방법은 기존 방법과 비교했을 때 사실감과 맥락 적합성이 높은 텍스트-비디오 콘텐츠를 생성하는 데 있어 우수한 기능을 보여주었습니다. 이번 실험은 모델 성능에서 콘텐츠 증분과 모션 단위의 중요성을 더욱 강조했습니다.

### 5 Conclusion

이 연구에서는 비디오 생성 중 콘텐츠 변형의 일관성을 향상시키기 위해 고유한 듀얼 스트림 확산 네트워크(DSDN)를 도입했습니다. 이 설계의 특징은 다음과 같습니다:

- 두 개의 확산 스트림: DSDN에는 비디오 콘텐츠와 모션에 대한 별도의 스트림이 있습니다. 이 두 가지 스트림은 독립적으로 작동하여 개인화된 비디오 콘텐츠를 생성할 뿐만 아니라 시너지 효과를 발휘하여 콘텐츠와 모션이 조화롭게 정렬되도록 합니다.
- 크로스 트랜스포머 인터랙션 모듈: 이 모듈은 비디오의 부드러운 전환을 보장하고 생성된 프레임의 일관성과 다양성을 높이기 위해 개발되었습니다. 이 접근 방식의 고유성은 모션이 개별 분기로 모델링되어 다른 비디오 확산 방법과 차별화된다는 데 있습니다.
- 모션 디컴포저 및 컴바이너: 비디오 모션과 관련된 작업을 지원하여 프로세스를 보다 간소화하고 효율적으로 만들기 위해 도입되었습니다.

정성적, 정량적 경험적 증거에 따르면 DSDN은 플리커가 최소화되고 부드럽고 연속적인 비디오가 특징인 우수한 비디오 생성 결과를 제공합니다. DSDN 기술의 발전은 텍스트나 기타 단서로부터 비디오를 생성하는 영역에서 더욱 발전하고 개선할 수 있는 잠재력을 보여줍니다.

- 특징
    1. **이중 스트림 구조**: DSDN는 비디오의 내용과 동작을 독립적으로 다룰 수 있는 두 개의 확산 스트림을 가집니다. 이 구조는 개인화된 비디오 변동과 내용을 독립적으로 생성할 수 있게 해줍니다.
    2. **교차 트랜스포머 상호 작용 모듈**: 이 모듈은 내용과 동작 도메인 간에 잘 정렬되도록 설계되었습니다. 이러한 상호작용은 생성된 비디오의 부드러움과 일관성을 향상시키는데 중요한 역할을 합니다.
    3. **동작 분기의 독립성**: DSDN의 동작 분기는 대부분의 기존 비디오 확산 방법과는 달리 독립적으로 모델링됩니다. 이로 인해 동작과 내용을 독립적으로 처리하면서도 둘 사이의 연관성을 유지할 수 있습니다.
    4. **동작 분해자와 결합자**: 이 논문은 비디오 동작에 대한 연산을 용이하게 하기 위해 동작 분해자와 결합자를 도입합니다.
    5. **실험적 검증**: DSDN 방식은 여러 실험을 통해 기존 방식과 비교해 검증되었습니다. 특히, 연속성 및 깜박임 감소 등의 측면에서 더 우수한 성능을 보였습니다.