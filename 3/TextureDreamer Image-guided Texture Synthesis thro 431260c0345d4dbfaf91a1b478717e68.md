# TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion

[https://arxiv.org/abs/2401.09416](https://arxiv.org/abs/2401.09416)

- Jan 2024

![희소 이미지에서 텍스처 전송. 적은 수의 이미지와 대상 메시가 주어지면 이 방법은 다양한 오브젝트에 대해 입력된 모양과 유사한 지오메트리 인식 텍스처를 합성합니다.](TextureDreamer%20Image-guided%20Texture%20Synthesis%20thro%20431260c0345d4dbfaf91a1b478717e68/Untitled.png)

희소 이미지에서 텍스처 전송. 적은 수의 이미지와 대상 메시가 주어지면 이 방법은 다양한 오브젝트에 대해 입력된 모양과 유사한 지오메트리 인식 텍스처를 합성합니다.

### 1 Introduction

고품질 3D 콘텐츠는 AR/VR, 로봇공학, 영화, 게임 등 다양한 분야에서 필수적입니다. 최근 3D 콘텐츠 제작이 크게 발전했지만, 고품질 텍스처 생성은 상대적으로 덜 주목 받았습니다. 텍스처는 사실적인 외관을 만드는데 중요한 요소입니다. 기존에는 전문가들이 수작업으로 텍스처를 제작했는데, 이는 비용이 많이 들고 비효율적인 과정입니다. 자동으로 다양한 시각적 특성을 텍스처에 적용하는 것이 유용할 것입니다.

TextureDreamer라는 새로운 프레임워크를 소개합니다. 이 프레임워크는 적은 수의 이미지에서 고품질, 재조명 가능한 텍스처를 생성할 수 있습니다. 객체의 몇 가지 뷰만 있어도, 다른 범주의 목표 기하학에 그 텍스처를 적용할 수 있습니다. 이는 기존 방법들이 해결하지 못한 도전적인 문제입니다. 우리의 프레임워크는 최신 확산 기반 생성 모델에서 영감을 받았으며, 이를 통해 텍스트 가이드 이미지 생성을 가능하게 합니다. 하지만, 텍스트만으로는 복잡한 패턴을 충분히 표현하기 어려웠습니다. 따라서 우리는 소수의 이미지에서 텍스처 정보를 추출하고, 이를 사전 훈련된 확산 모델에 적용하여 복잡한 텍스처를 정확하게 표현할 수 있습니다.

![텍스트 가이드 텍스처링의 한계. 이미지의 모든 디테일을 표현하지 못할 수 있는 텍스트 프롬프트를 생성하기 위해 캡션이 필요한 텍스트 가이드 텍스처링 방식에 비해 이미지 기반 가이드 텍스처링 방식이 더 효과적이고 표현력이 뛰어납니다. 이미지 캡션은 BLIP [33]을 통해 예측하고, 텍스트 가이드 텍스처링은 TEXTURE [53]를 통해 생성하며, 이미지 가이드 결과는 다음과 같습니다.](TextureDreamer%20Image-guided%20Texture%20Synthesis%20thro%20431260c0345d4dbfaf91a1b478717e68/Untitled%201.png)

텍스트 가이드 텍스처링의 한계. 이미지의 모든 디테일을 표현하지 못할 수 있는 텍스트 프롬프트를 생성하기 위해 캡션이 필요한 텍스트 가이드 텍스처링 방식에 비해 이미지 기반 가이드 텍스처링 방식이 더 효과적이고 표현력이 뛰어납니다. 이미지 캡션은 BLIP [33]을 통해 예측하고, 텍스트 가이드 텍스처링은 TEXTURE [53]를 통해 생성하며, 이미지 가이드 결과는 다음과 같습니다.

### 2 Related Work

텍스처 합성 및 재구성 분야에서는 이웃 기반 분포 샘플링, 반복 패턴 타일링, 다중 뷰 이미지를 객체 표면에 융합하는 전통적인 방법들이 있습니다. 이 방법들은 의미 있는 텍스처 생성에는 한계가 있거나 정확한 기하학적 재구성을 요구합니다. 학습 기반 방법들은 큰 규모의 3D 데이터셋에서 텍스처 생성을 학습하지만, 특정 범주에 국한됩니다. 최근에는 CLIP 모델을 사용한 텍스트 가이드 텍스처 생성 연구도 있으나, 품질이 낮은 경우가 많습니다. TextureDreamer는 이와 달리 희소한 이미지에서 의미 있고 고품질의 텍스처를 임의의 객체에 적용할 수 있습니다. 전통적으로 2D 이미지로 표현되던 텍스처는 UV 매핑을 통해 객체 표면에 투영되었지만, 최근의 신경 암시적 표현 발전을 활용하여, 우리의 방법은 텍스처를 신경 암시적 텍스처 필드로 표현합니다.

확산 모델은 최고의 생성 모델로 부상하며 탁월한 시각적 품질을 보여줍니다. 이들은 대규모 이미지-텍스트 쌍 데이터셋에서 훈련되었고, 텍스트 가이드 이미지 합성에서 뛰어난 능력을 보여줍니다. 최근 연구들은 이 모델들을 작은 데이터셋이나 소수의 이미지에 맞게 미세 조정하여 맞춤형 이미지 합성을 가능하게 했습니다. TextureDreamer는 이러한 진보를 활용하여 희소한 뷰에서 텍스처 정보를 추출하고, 기하학적으로 인식할 수 있는 방식으로 새로운 대상 객체에 전달합니다.

2D 확산 사전을 활용한 3D 콘텐츠 생성은 최근 큰 관심을 끌고 있습니다. 몇몇 방법은 3D 확산 모델을 직접 훈련하여 다양한 표현을 통한 3D 콘텐츠를 생성합니다. 다른 방법들은 사전 훈련된 2D 확산 모델을 사용하여 다양한 뷰에서 생성된 이미지를 융합하거나 3D 표현을 최적화합니다. 많은 방법들이 텍스트 가이드 3D 생성에 집중하는 동안, 이미지에서 3D 콘텐츠를 생성하기 위해 확산 모델을 활용하는 시도는 적습니다. 몇몇 동시대 작업들은 큰 규모의 3D 데이터셋에서 2D 확산 모델을 미세 조정하여 희소한 뷰 재구성에 초점을 맞춥니다. TextureDreamer는 이와 달리 소수의 이미지에서 텍스처를 추출하고, 일치하지 않는 기하학을 가진 대상 3D 형태에 적용하는 것을 목표로 합니다. Dreambooth3D와 TEXTure와 같은 연구는 희소한 뷰에서 정보를 추출하고 미세 조정된 확산 모델 가중치를 사용하여 개인화된 3D 객체나 텍스처를 생성합니다. TextureDreamer는 비슷한 방법을 사용하지만, 텍스처 생성에 활용하는 방식이 다르며, 일관성과 사진 리얼리즘 측면에서 개선을 이루었습니다.

### 3. Method

TextureDreamer는 주어진 메시에 대한 기하학을 고려한 텍스처를 합성하는 프레임워크입니다. 이 프레임워크는 객체의 3-5개 이미지를 기반으로 하며, 개인화된 기하학을 고려한 점수 증류 (PGSD)라는 핵심 기술 기여를 통해 고품질의 이미지 가이드 텍스처 전송을 가능하게 합니다.

3.1. 기본 개념
Dreambooth는 사전 훈련된 텍스트-이미지 확산 모델을 소수의 입력 이미지에 대해 미세 조정하는 간단하지만 효과적인 방법입니다. 이는 개체의 외관을 특정 텍스트 토큰과 함께 확산 모델 가중치에 저장합니다. Dreambooth는 입력 이미지에 대한 표준 확산 소음 감소 감독과 언어 드리프트 및 다양성 손실을 방지하기 위한 클래스별 보존 손실 함수를 사용합니다. TextureDreamer는 Dreambooth를 사용하여 입력 이미지에서 텍스처 정보를 추출하고, 이를 다른 기하학을 가진 3D 객체에 적용합니다.

![주어진 메시의 텍스처를 물체의 3~5개의 입력 이미지와 유사한 모양으로 합성하는 프레임워크인 TextureDreamer의 개요. 먼저 입력 이미지에 대한 드림부스[54] 미세 조정을 통해 개인화된 확산 모델 ψ를 얻습니다. 그런 다음 3D 메시 M에 대한 공간적으로 변화하는 양방향 반사율 분포(BRDF) 필드 fθ를 개인화된 기하학적 인식 점수 증류(PGSD)를 통해 최적화합니다(섹션 3.2에 자세히 설명되어 있음). 최적화가 완료되면 최적화된 BRDF 필드에서 알베도, 금속성 및 거칠기에 해당하는 고해상도 텍스처 맵을 추출할 수 있습니다.](TextureDreamer%20Image-guided%20Texture%20Synthesis%20thro%20431260c0345d4dbfaf91a1b478717e68/Untitled%202.png)

주어진 메시의 텍스처를 물체의 3~5개의 입력 이미지와 유사한 모양으로 합성하는 프레임워크인 TextureDreamer의 개요. 먼저 입력 이미지에 대한 드림부스[54] 미세 조정을 통해 개인화된 확산 모델 ψ를 얻습니다. 그런 다음 3D 메시 M에 대한 공간적으로 변화하는 양방향 반사율 분포(BRDF) 필드 fθ를 개인화된 기하학적 인식 점수 증류(PGSD)를 통해 최적화합니다(섹션 3.2에 자세히 설명되어 있음). 최적화가 완료되면 최적화된 BRDF 필드에서 알베도, 금속성 및 거칠기에 해당하는 고해상도 텍스처 맵을 추출할 수 있습니다.

ControlNet은 사전 훈련된 확산 모델에 공간 조절 제어를 추가하는 새로운 아키텍처를 제안합니다. 이는 작은 컨볼루션 네트워크를 모델에 삽입하여 다양한 유형의 2D 조건에 강인한 미세 조정 성능을 가능하게 합니다. 우리는 생성된 텍스처가 주어진 기하학과 일치하도록 하기 위해 ControlNet 모델을 사용합니다.

점수 증류 샘플링은 사전 훈련된 2D 확산 모델을 3D 콘텐츠 생성에 사용하는 많은 방법의 핵심 구성 요소입니다. 이는 3D 표현의 렌더링된 이미지를 사전 훈련된 확산 모델에 의해 모델링된 고차원 매니폴드로 밀어붙입니다. 하지만 이 방법은 과도한 부드러움과 포화된 외관을 만들어내는 문제가 있습니다. Wang 등은 이 문제를 해결하기 위해 표준 분류자 없는 지도를 사용하는 VSD를 제안했습니다. VSD는 3D 표현 전체를 무작위 변수로 취급하고 사전 훈련된 확산 모델에 의해 정의된 분포와의 KL 발산을 최소화합니다.

3.2. 개인화된 기하학을 고려한 점수 증류 (PGSD)
문제 설정. 우리의 방법은 다양한 뷰에서 캡처된 소수의 이미지와 목표 3D 메시를 입력으로 합니다. 우리의 프레임워크의 출력은 이미지 세트에서 메시로 전송된 재조명 가능한 텍스처입니다. 이 텍스처는 표준 미세면 반사 분포(BRDF) 모델로 매개변수화되며, 노멀 맵을 최적화하지 않음으로써 메시와 일관되지 않은 세부 사항을 방지합니다.

개인화된 텍스처 정보 추출. 우리는 Dreambooth를 사용하여 소수의 이미지에서 텍스처 정보를 추출합니다. 이는 개인화된 확산 모델을 입력 이미지에 미세 조정하여 질감 패턴을 보존하는 데 도움이 됩니다. 우리는 목표 객체의 배경을 마스킹하고, 입력 이미지의 짧은 가장자리를 512로 조정한 후 512x512 패치로 무작위로 자르는 것으로 재구성 손실을 적용합니다.

기하학을 고려한 점수 증류. Dreambooth를 사용하여 텍스처 정보를 추출한 후, 우리는 이 정보를 메시 M에 전송하기 위해 미세 조정된 Dreambooth 모델을 점수 증류 샘플링의 소음 제거 네트워크로 사용합니다. 우리는 기존 SDS 대신 VSD를 선택합니다. 하지만 단순히 SDS를 VSD로 대체하는 것만으로는 2D 확산 모델의 3D 지식 부족 문제를 해결할 수 없습니다. 따라서 우리는 기하학 정보를 메시 M에서 추출하여 개인화된 확산 모델에 주입합니다. 이는 생성된 텍스처의 3D 일관성을 크게 향상시킵니다.

### 4. Experiment

4.1. 실험 설정

데이터셋. 실험은 소파, 침대, 머그/볼, 플러시 장난감의 4가지 범주의 객체로 진행되었습니다. 각 범주에서 8개의 객체 인스턴스를 선정하고, 객체 주변의 3~5개 뷰를 샘플링하여 작은 이미지 세트를 만들었습니다. 총 32개의 이미지 세트가 생성되었습니다. 각 이미지에는 전경 마스크를 자동으로 얻기 위해 U2-Net을 적용하거나, 더 정확한 마스크를 얻기 위해 반자동 배경 제거 애플리케이션을 사용했습니다. 다양한 메시에 텍스처 전송을 수행했으며, 이는 동일 범주의 형태, 다른 범주의 형태, 또는 다른 속성의 기하학을 포함합니다. 텍스처 전송 프레임워크를 테스트하기 위해, 각 범주의 4가지 카테고리에서 캡처된 이미지 세트와 다른 3개의 메시를 선택했습니다. 이 3D 메시는 3D-FUTURE와 온라인 저장소에서 획득했습니다. 동일 범주의 객체뿐만 아니라 침대와 의자 간에 범주 간 텍스처 전송도 실행하여 방법의 일반화 능력을 테스트했습니다.

구현 세부사항. 우리는 PyTorch와 Threestudio를 기반으로 프레임워크를 구현했습니다. 사전 훈련된 확산 모델로는 잠재 확산과 ControlNet v1.1을 사용했습니다. 모든 실험에서 LP GSD의 분류기-자유 가이드 가중치를 1.0으로 설정했습니다. DreamFusion을 따라 입력 텍스트 프롬프트에 뷰-의존적 조건을 적용했습니다. BRDF 필드는 MLP를 사용하여 파라미터화되었고, 카메라 인코더는 카메라 외부를 잠재 벡터로 투영하는 두 개의 선형 층으로 구성되었습니다. 모든 실험에 대해 인코딩, MLP, 카메라 인코더의 학습률을 각각 0.01, 0.001, 0.0001로 설정했습니다.

![네 가지 카테고리(침대, 소파, 봉제 인형, 머그컵)의 이미지 세트에서 다양한 오브젝트로 이미지 가이드 전송 결과. 이 방법은 다양한 오브젝트 유형에 적용할 수 있으며 다양한 오브젝트 모양으로 텍스처를 전송할 수 있습니다.](TextureDreamer%20Image-guided%20Texture%20Synthesis%20thro%20431260c0345d4dbfaf91a1b478717e68/Untitled%203.png)

네 가지 카테고리(침대, 소파, 봉제 인형, 머그컵)의 이미지 세트에서 다양한 오브젝트로 이미지 가이드 전송 결과. 이 방법은 다양한 오브젝트 유형에 적용할 수 있으며 다양한 오브젝트 모양으로 텍스처를 전송할 수 있습니다.

4.2. 기준 방법
Latent-paint와 TEXTure는 2D 확산 사전을 사용한 최근의 텍스트 가이드 텍스처링 방법입니다. 이들은 이미지에서 메시를 텍스처링하는 능력을 보여줍니다. Latent-paint는 SDS를 사용하여 텍스처를 정제합니다. TEXTure는 사전 훈련된 확산 모델을 미세 조정한 후 메시 페인팅 알고리즘을 사용하여 텍스처를 합성합니다. 실험을 실행하기 위해 원래 구현을 따랐습니다.

4.3. 이미지 가이드 텍스처 전송

정성적 평가. 우리 방법은 다양한 객체 기하학에 텍스처 전송을 수행할 수 있습니다. 우리 방법은 입력과 유사한 패턴과 스타일을 가진 기하학적으로 인식되고 매끄러운 텍스처를 합성할 수 있습니다. 우리는 또한 서로 다른 범주 간 텍스처 전송을 수행할 수 있습니다. 합성된 텍스처는 알베도, 금속성, 거칠기 맵을 포함하므로 합성된 외관을 가진 대상 객체는 재조명될 수 있습니다. 우리 프레임워크는 다양한 무작위 시드를 사용하여 다양한 텍스처를 생성할 수 있습니다.

![카테고리 간 텍스처 전송 결과의 예시. 첫 번째 행에서는 봉제 인형의 외형을 컵과 의자로 옮겼습니다. 두 번째 행에서는 머그잔의 특수 패턴을 곰 인형과 의자로 옮겼습니다. 세 번째 행에서는 입력 소파의 텍스처가 컵과 곰으로 전송됩니다.](TextureDreamer%20Image-guided%20Texture%20Synthesis%20thro%20431260c0345d4dbfaf91a1b478717e68/Untitled%204.png)

카테고리 간 텍스처 전송 결과의 예시. 첫 번째 행에서는 봉제 인형의 외형을 컵과 의자로 옮겼습니다. 두 번째 행에서는 머그잔의 특수 패턴을 곰 인형과 의자로 옮겼습니다. 세 번째 행에서는 입력 소파의 텍스처가 컵과 곰으로 전송됩니다.

정량적 평가. 기하학과 사진 간의 형태 차이로 인해 텍스처 전송에 대한 정량적 비교는 간단하지 않습니다. 우리는 전송 충실도, 텍스처 사실주의 및 텍스처-기하학 호환성을 평가하기 위한 사용자 연구를 수행했습니다. 우리는 Amazon Turk를 통한 사용자 연구를 수행했으며, 결과는 우리 방법이 이미지 충실도, 텍스처 사실주의 및 형태-텍스처 일관성 측면에서 사용자의 상당한 선호도를 보여주었습니다. 또한, 우리는 CLIP 기능을 사용하여 참조 이미지와 합성된 텍스처의 렌더링 이미지 간의 유사성을 평가했습니다. 우리 방법은 가장 높은 CLIP 유사성을 보였습니다.

![재조명 결과의 예시. 텍스처는 원본 HDR 환경 맵(첫 번째 줄)과 신규 맵(두 번째 및 세 번째 줄)에 의해 리라이팅됩니다.](TextureDreamer%20Image-guided%20Texture%20Synthesis%20thro%20431260c0345d4dbfaf91a1b478717e68/Untitled%205.png)

재조명 결과의 예시. 텍스처는 원본 HDR 환경 맵(첫 번째 줄)과 신규 맵(두 번째 및 세 번째 줄)에 의해 리라이팅됩니다.

4.4. 소거 연구
우리는 기하학적으로 인식되는 ControlNet의 중요성에 대한 소거 연구를 수행했습니다. ControlNet 없이는 기하학-텍스처 불일치가 발생합니다. 점수 증류 손실의 중요성도 검증했습니다. SDS 손실만 사용하면 입력 충실도가 충분하지 않고 결과가 더 흐릿해질 수 있습니다. LoRA를 유지하면 합성된 텍스처에 무작위 패턴이 도입됩니다. 우리는 각 구성 요소의 중요성을 정량적으로 평가했습니다. 우리의 전체 방법은 ControlNet 없이 및 ControlNet(깊이)와 비교하여 가장 높은 유사성 점수를 달성했습니다. SDS 결과는 포화되거나 흐릿하며 입력에서 텍스처를 복구할 수 없습니다. LoRA를 일반 확산 모델에 유지하면 합성된 텍스처에 무작위 패턴이 도입됩니다.

![베이스라인 방법 간 비교. Latent-Paint [37] 및 TEXTure [53]와 비교했을 때, 우리의 방법은 타겟 메시 지오메트리와 호환되는 매끄럽고 지오메트리를 인식하는 텍스처를 합성할 수 있습니다.](TextureDreamer%20Image-guided%20Texture%20Synthesis%20thro%20431260c0345d4dbfaf91a1b478717e68/Untitled%206.png)

베이스라인 방법 간 비교. Latent-Paint [37] 및 TEXTure [53]와 비교했을 때, 우리의 방법은 타겟 메시 지오메트리와 호환되는 매끄럽고 지오메트리를 인식하는 텍스처를 합성할 수 있습니다.

![합성된 텍스처의 다양성.](TextureDreamer%20Image-guided%20Texture%20Synthesis%20thro%20431260c0345d4dbfaf91a1b478717e68/Untitled%207.png)

합성된 텍스처의 다양성.

![한계. 이 방법은 텍스처에 조명을 베이크인하고, 입력 시점이 충분하지 않을 경우 야누스 문제가 발생할 수 있으며, 입력에서 특수하고 반복되지 않는 패턴을 무시할 수 있습니다.](TextureDreamer%20Image-guided%20Texture%20Synthesis%20thro%20431260c0345d4dbfaf91a1b478717e68/Untitled%208.png)

한계. 이 방법은 텍스처에 조명을 베이크인하고, 입력 시점이 충분하지 않을 경우 야누스 문제가 발생할 수 있으며, 입력에서 특수하고 반복되지 않는 패턴을 무시할 수 있습니다.

![제거 연구. (첫 번째 행) 노멀 맵에 컨트롤넷을 적용한 결과 텍스처와 지오메트리의 일관성이 가장 우수합니다. 컨트롤넷을 사용하지 않거나 깊이 기반 컨트롤넷을 사용하면 텍스처와 지오메트리의 정렬이 잘못되어 결과가 좋지 않습니다. SDS 손실을 사용하면 텍스처가 흐릿해집니다. LoRA 모듈을 제거하지 않으면 개인화된 확산 모델에서 기존 텍스처가 제거되는 경향이 있습니다. 유니티의 전체 방식은 입력된 외관과 유사한 정확한 텍스처를 합성할 수 있습니다. (두 번째 줄) 일반 확산 모델 ϕ를 개인화 모델로 대체하거나 분류기 안내 스케일 7.5를 적용하면 합성된 텍스처에 일부 임의의 패턴이 나타날 수 있습니다. 카메라 인코더 ρ를 고정하면 전체 방법보다 결과가 더 나쁘거나 노이즈가 심할 수 있습니다.](TextureDreamer%20Image-guided%20Texture%20Synthesis%20thro%20431260c0345d4dbfaf91a1b478717e68/Untitled%209.png)

제거 연구. (첫 번째 행) 노멀 맵에 컨트롤넷을 적용한 결과 텍스처와 지오메트리의 일관성이 가장 우수합니다. 컨트롤넷을 사용하지 않거나 깊이 기반 컨트롤넷을 사용하면 텍스처와 지오메트리의 정렬이 잘못되어 결과가 좋지 않습니다. SDS 손실을 사용하면 텍스처가 흐릿해집니다. LoRA 모듈을 제거하지 않으면 개인화된 확산 모델에서 기존 텍스처가 제거되는 경향이 있습니다. 유니티의 전체 방식은 입력된 외관과 유사한 정확한 텍스처를 합성할 수 있습니다. (두 번째 줄) 일반 확산 모델 ϕ를 개인화 모델로 대체하거나 분류기 안내 스케일 7.5를 적용하면 합성된 텍스처에 일부 임의의 패턴이 나타날 수 있습니다. 카메라 인코더 ρ를 고정하면 전체 방법보다 결과가 더 나쁘거나 노이즈가 심할 수 있습니다.

### 5. Discussions

우리는 입력 이미지로부터 임의의 형태로 텍스처를 전송하는 프레임워크를 제안했습니다. 대부분의 경우 고품질 텍스처 전송이 가능하지만, 몇 가지 한계점이 있습니다. 예를 들어, 특별하거나 반복되지 않는 텍스처는 대상 모양에 전송하기 어려울 수 있습니다. 또한, 입력 이미지에 강한 반사광이 있을 경우, 우리의 방법은 조명을 텍스처에 구워 넣는 경향이 있습니다. 입력 이미지의 시점이 객체 전체를 커버하지 않을 경우, 양면 문제가 발생할 수 있습니다. 그럼에도 불구하고, 우리는 이 도전적인 문제에 대처하는 첫 단계가 될 수 있을 것이라고 믿습니다.