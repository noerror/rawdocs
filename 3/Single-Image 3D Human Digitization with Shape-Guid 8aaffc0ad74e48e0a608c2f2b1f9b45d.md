# Single-Image 3D Human Digitization with Shape-Guided Diffusion

[https://arxiv.org/abs/2311.09221](https://arxiv.org/abs/2311.09221)

![단일 이미지에서 3D 사람 디지털화. 유니티의 접근 방식은 단일 이미지를 입력으로 사용하여 지도 학습을 위한 3D 스캔에 의존하지 않고 사람의 3D 일관된 텍스처를 합성합니다. 우리의 핵심 아이디어는 일반적인 이미지 합성 작업을 위해 사전 학습된 고용량 2D 확산 모델을 사람 모양으로 활용하는 것입니다. Adobe Stock의 이미지.](Single-Image%203D%20Human%20Digitization%20with%20Shape-Guid%208aaffc0ad74e48e0a608c2f2b1f9b45d/Untitled.png)

단일 이미지에서 3D 사람 디지털화. 유니티의 접근 방식은 단일 이미지를 입력으로 사용하여 지도 학습을 위한 3D 스캔에 의존하지 않고 사람의 3D 일관된 텍스처를 합성합니다. 우리의 핵심 아이디어는 일반적인 이미지 합성 작업을 위해 사전 학습된 고용량 2D 확산 모델을 사람 모양으로 활용하는 것입니다. Adobe Stock의 이미지.

### 1 INTRODUCTION

사실적인 3D 인체 모델을 제작하는 것은 패션, 엔터테인먼트, 스포츠, AR/VR과 같은 산업에서 매우 중요합니다. 기존에는 이 프로세스에 여러 개의 이미지 또는 3D 스캐닝이 필요했는데, 이는 복잡하고 많은 사람들이 접근하기 어려울 수 있습니다. 이 백서에서는 사람의 뒷모습에 대한 가시적인 정보가 부족하고 단일 이미지 3D 재구성에 내재된 깊이 모호성으로 인해 어려운 작업인 단일 이미지로 사실적인 3D 인물을 만드는 새로운 방법을 소개합니다.

최근 데이터 기반 방법의 발전으로 3D 재구성이 크게 개선되었습니다. 이러한 방법은 메시, 복셀, 신경 필드와 같은 다양한 3D 표현을 사용합니다. 하지만 이러한 재구성의 결과물, 특히 가려진 영역의 모습은 여전히 완전히 사실적이지 않습니다. 이러한 한계는 부분적으로 다양한 3D 실사 데이터가 부족하기 때문인데, 특히 옷을 입은 사람의 경우 옷의 모양이 지오메트리보다 더 다양하기 때문입니다.

사람 외형 합성을 위한 또 다른 접근 방식은 대규모 이미지 데이터 세트를 사용하는 것입니다. 2D 사람 합성 방법은 단일 이미지에서 옷을 입은 사람을 재현하는 데는 인상적인 결과를 보여주었지만, 3D가 아닌 특성상 대규모 회전 시 외형의 일관성을 유지하는 데는 어려움을 겪습니다. 보다 일관된 뷰 합성을 제공하는 최근의 3D 제너레이티브 모델도 다양한 의상을 처리하고 사실감을 구현하는 데 한계가 있습니다.

![기존 방법의 한계. 단일 이미지에서 3D 인간을 생성하는 기존의 접근 방식은 사실감이 부족합니다. PIFu[Saito 외 2019]는 흐릿함, Impersonator++[Liu 외 2021b]는 정면 뷰의 콘텐츠를 복제하는 경향이 있어 프로젝션 아티팩트가 발생하고, TEXTure[Richardson 외 2023]는 입력 뷰의 외관을 보존하지 못하고 채도가 높은 색상을 생성하며, Magic123[Qian 외 2023]은 사실적인 형태와 외형을 합성하지 못합니다. Adobe Stock의 이미지.](Single-Image%203D%20Human%20Digitization%20with%20Shape-Guid%208aaffc0ad74e48e0a608c2f2b1f9b45d/Untitled%201.png)

기존 방법의 한계. 단일 이미지에서 3D 인간을 생성하는 기존의 접근 방식은 사실감이 부족합니다. PIFu[Saito 외 2019]는 흐릿함, Impersonator++[Liu 외 2021b]는 정면 뷰의 콘텐츠를 복제하는 경향이 있어 프로젝션 아티팩트가 발생하고, TEXTure[Richardson 외 2023]는 입력 뷰의 외관을 보존하지 못하고 채도가 높은 색상을 생성하며, Magic123[Qian 외 2023]은 사실적인 형태와 외형을 합성하지 못합니다. Adobe Stock의 이미지.

이 논문은 기존 방법의 차선책으로 훈련 데이터의 다양성이 제한되어 있기 때문이라고 주장합니다. 이 문제를 해결하기 위해 저자는 큐레이팅된 2D 데이터 세트에 의존하지 않고 단일 이미지에서 일관된 3D 텍스처의 사람을 생성하는 새로운 알고리즘을 제안합니다. 이 알고리즘의 핵심 혁신은 강력한 2D 생성 모델, 특히 잠재 확산 모델을 사용하여 방대한 이미지 코퍼스를 사람 모양으로 미리 학습시키는 것입니다. 이 접근 방식은 텍스트 입력에서 3D 객체를 생성하기 위해 2D 확산 모델을 사용하는 최근의 작업과는 다릅니다.

이 프로세스에는 기존 도구를 사용하여 인물의 3D 지오메트리를 재구성하고 일관된 360도 표현을 위해 인물의 뒷모습을 생성하는 과정이 포함됩니다. 그런 다음 3D 구조에 맞게 노멀 맵과 실루엣 맵에 따라 새로운 뷰를 인페인팅하여 멀티뷰 이미지를 합성합니다. 합성 프로세스에는 각도 차이와 누락된 픽셀과의 근접성에 따라 여러 뷰의 RGB 색상을 혼합하여 뷰 간 일관성을 보장하는 작업이 포함됩니다. 마지막 단계는 완전한 텍스처의 고해상도 3D 휴먼 메시를 생성하기 위한 멀티뷰 융합입니다.

실험을 통해 이 방법은 고품질 3D 스캔이나 방대한 데이터 세트 없이도 이전 방법에 비해 옷을 입은 사람을 더 세밀하고 충실하게 합성할 수 있음을 입증했습니다. 이 논문은 단일 이미지에서 3D 텍스처의 인간 디지털화를 위해 2D 확산 모델을 사용한 최초의 사례이며, 노멀 맵과 실루엣을 사용하여 3D 모델의 형태와 구조적 디테일을 보존하고, 합성된 멀티뷰 이미지를 융합하여 일관된 텍스처 재구성이 가능하다는 점에서 중요한 의미를 갖습니다.
노이즈 제거 확산 모델은 최근 3D 장면 표현을 학습하는 데 채택되어 이미지 합성에서 인상적인 결과를 보여주고 있습니다. 이러한 모델은 텍스트 입력에서 3D 객체를 생성하는 데 사용되었습니다. 그러나 현재의 작업은 입력 이미지에서 인물의 3D 일관된 텍스처를 생성하는 데 중점을 두어 테스트 시간 동안 미세 조정할 필요 없이 사실감을 구현한다는 점에서 차이가 있습니다. 다른 방법과 달리 이 접근 방식은 사람별 프리퍼가 필요하지 않으며 사실적이고 디테일한 텍스처의 사람 모델을 합성할 수 있습니다. 이 기법은 가시성, 시야각, 누락된 영역과의 거리에 따라 다양한 뷰의 RGB 색상을 혼합하고 노멀 및 실루엣 맵을 통합하여 더 나은 가이드를 제공하는 새로운 접근 방식을 사용하여 기존 방식을 개선합니다.

### 2 RELATED WORK

이 섹션에서는 사실적인 3D 인체 모델 합성과 관련된 기존 문헌을 검토하고 네 가지 주요 연구 분야를 중점적으로 살펴봅니다.

2.1 2D 인간 합성
생성적 적대 신경망(GAN)의 개발로 사람의 얼굴과 신체를 사실적으로 합성하는 기술이 크게 발전했습니다. 이러한 모델은 무조건 생성 모델에서 조건부 생성 모델로 발전하여 피사체의 신원을 유지하면서 포즈를 제어할 수 있게 되었습니다. 인간 리포즈, 가상 시착, 모션 전송과 같은 기술도 개발되었습니다. 그러나 이러한 방법은 종종 시점의 불일치로 인해 어려움을 겪습니다. 본 연구는 이러한 한계를 극복하기 위해 3D로 일관된 텍스처의 옷을 입은 사람을 생성하는 것을 목표로 합니다.

2.2 무조건적인 3D 인간 합성
최근 신경 분야와 역 렌더링의 발전으로 2D 이미지만으로 3D GAN을 훈련할 수 있게 되었습니다. 이러한 기술은 전신 인간 모델로 확장되었지만 복잡한 의복 텍스처를 처리하는 데 한계가 있어 사실적이고 일반화 가능한 3D 인간 디지털화에 어려움을 겪고 있습니다. 이 백서에서 제안하는 작업은 확산 모델에서 보다 표현력이 뛰어난 이미지 프리어를 통합하여 이러한 한계를 해결하고자 합니다.

2.3 단일 이미지에서 3D 인체 재구성하기
단일 이미지에서 3D 옷을 입은 사람을 재구성하는 것은 오랜 과제였습니다. 복셀, 메시 변위, 신경 필드와 같은 다양한 형상 표현이 연구되어 왔습니다. 일부 방법은 가려진 영역에 대한 텍스처 합성을 지원하지만, 제한된 3D 스캔 데이터로 인해 뒷면에 대한 사실적인 텍스처를 생성하지 못하는 경우가 많습니다. 3D 인체 표현을 학습하기 위해 차별적 렌더링과 NeRF가 적용되었지만, 이러한 방법에는 대규모로 수집하기 어려운 멀티뷰 이미지 또는 비디오가 필요합니다. 이 백서에서 설명하는 접근 방식은 이러한 광범위한 데이터 수집이 필요하지 않습니다.

2.4 3D 합성을 위한 확산 모델
노이즈 제거 확산 모델은 최근 3D 장면 표현을 학습하는 데 채택되어 이미지 합성에서 인상적인 결과를 보여주고 있습니다. 이러한 모델은 텍스트 입력에서 3D 객체를 생성하는 데 사용되었습니다. 그러나 현재의 작업은 입력 이미지에서 인물의 3D 일관된 텍스처를 생성하는 데 중점을 두어 테스트 시간 동안 미세 조정할 필요 없이 사실감을 구현한다는 점에서 차이가 있습니다. 다른 방법과 달리 이 접근 방식은 사람별 프리퍼가 필요하지 않으며 사실적이고 디테일한 텍스처의 사람 모델을 합성할 수 있습니다. 이 기법은 가시성, 시야각, 누락된 영역과의 거리에 따라 다양한 뷰의 RGB 색상을 혼합하고 노멀 및 실루엣 맵을 통합하여 더 나은 가이드를 제공하는 새로운 접근 방식을 사용하여 기존 방식을 개선합니다.

### 3 METHOD

이 섹션에서 설명하는 방법은 단일 이미지에서 인물의 포괄적인 고해상도 360도 뷰를 생성하는 것을 목표로 합니다. 이 프로세스에는 몇 가지 주요 단계가 포함됩니다:

3.1 뒷모습 합성
첫 번째 단계는 인물의 뒷모습을 합성하는 것입니다. 뒷모습은 옷의 비슷한 질감 등 앞모습과 의미적 상관관계를 공유하기 때문에 이 단계가 중요합니다. 이 방법은 고밀도 포즈 예측과 함께 최첨단 2D 사람 합성 기술을 사용하여 사실적인 뒷모습을 생성합니다. 이렇게 합성된 뒷모습은 인물의 다른 뷰를 만들기 위한 토대가 됩니다.

![형상 유도 확산을 사용한 사람 이미지 생성. 단일 이미지 𝐼1에서 사람의 360도 뷰를 생성하려면 먼저 사람의 다중 뷰 이미지를 합성합니다. 기성 방법을 사용하여 3D 지오메트리를 추론하고[2020], 인물의 초기 뒷모습[2021]을 지침으로 합성합니다. 입력 뷰 𝐼1과 합성된 초기 백뷰 𝐼𝑁를 서포트 세트 𝑉 에 추가합니다. 새 뷰 𝑉𝑐 를 생성하기 위해 서포트 세트 𝑉에서 보이는 모든 픽셀을 RGB 색상과 가시성, 시야각, 누락된 영역까지의 거리에 따라 가중치를 부여하여 블렌딩하여 집계합니다. 보이지 않는 외관을 환각화하고 뷰 𝐼𝑐를 합성하기 위해 모양 단서(노멀 𝑁𝑐 및 실루엣 𝑆𝑐 맵)에 따라 사전 학습된 인페인팅 확산 모델을 사용합니다. 생성된 뷰 𝐼𝑐를 서포트 세트에 포함하고 나머지 모든 뷰에 대해 이 과정을 반복합니다. Adobe Stock의 이미지.](Single-Image%203D%20Human%20Digitization%20with%20Shape-Guid%208aaffc0ad74e48e0a608c2f2b1f9b45d/Untitled%202.png)

형상 유도 확산을 사용한 사람 이미지 생성. 단일 이미지 𝐼1에서 사람의 360도 뷰를 생성하려면 먼저 사람의 다중 뷰 이미지를 합성합니다. 기성 방법을 사용하여 3D 지오메트리를 추론하고[2020], 인물의 초기 뒷모습[2021]을 지침으로 합성합니다. 입력 뷰 𝐼1과 합성된 초기 백뷰 𝐼𝑁를 서포트 세트 𝑉 에 추가합니다. 새 뷰 𝑉𝑐 를 생성하기 위해 서포트 세트 𝑉에서 보이는 모든 픽셀을 RGB 색상과 가시성, 시야각, 누락된 영역까지의 거리에 따라 가중치를 부여하여 블렌딩하여 집계합니다. 보이지 않는 외관을 환각화하고 뷰 𝐼𝑐를 합성하기 위해 모양 단서(노멀 𝑁𝑐 및 실루엣 𝑆𝑐 맵)에 따라 사전 학습된 인페인팅 확산 모델을 사용합니다. 생성된 뷰 𝐼𝑐를 서포트 세트에 포함하고 나머지 모든 뷰에 대해 이 과정을 반복합니다. Adobe Stock의 이미지.

3.2 멀티뷰 가시 텍스처 어그리게이션
다음으로 인물의 여러 뷰를 집계하여 대상 뷰를 생성하는 방법을 사용합니다. 이 프로세스에서는 가시성, 시야각, 거리 등의 요소에 따라 가중 평균을 사용하여 고해상도 디테일이 유지되도록 합니다. 이 방법은 각 뷰에 대한 블렌딩 가중치를 계산하여 가까운 뷰에 더 높은 가중치를 부여하고 누락된 영역 근처의 픽셀에 더 낮은 가중치를 부여하도록 합니다. 이 단계는 이전에 합성된 보기와 일치하는 블렌딩 이미지를 만드는 데 도움이 됩니다.

3.3 모양 가이드 확산 채우기
블렌딩된 이미지에서 보이지 않는 영역을 채우기 위해 2D 인페인팅 확산 모델이 사용됩니다. 그러나 인페인팅된 영역이 인물의 기본 지오메트리를 존중하도록 하기 위해 추가 구조 정보가 확산 모델에 통합됩니다. 여기에는 인페인팅 프로세스를 안내하기 위해 노멀 맵과 실루엣 맵을 모두 사용하여 인체의 모양과 메시의 구조적 디테일이 모두 보존되도록 하는 것이 포함됩니다.

![모양 안내 확산 인페인팅. 새 뷰에서 보이지 않는 모양을 합성하기 위해 사전 학습된 인페인팅 확산 모델을 사용합니다. 안내가 없으면 인페인팅된 영역은 모양(빨간색 실루엣)이나 3D 지오메트리의 구조적 세부 사항(a)을 보존하지 못하는 경우가 많습니다. 노멀 맵을 컨트롤넷 [2023]의 제어 신호로 사용하는 경우(b), 인페인팅된 영역은 메시의 구조적 디테일(예: 손가락)은 보존하지만 인체의 모양은 보존하지 않습니다. 실루엣 맵을 사용하면 인체의 형태는 보존되지만 메시의 구조적 디테일은 보존되지 않습니다(c). 노멀 맵과 실루엣 맵을 모두 사용하여 인페인팅 모델이 기본 3D 지오메트리를 존중하도록 안내할 것을 제안합니다(d). Adobe Stock 이미지](Single-Image%203D%20Human%20Digitization%20with%20Shape-Guid%208aaffc0ad74e48e0a608c2f2b1f9b45d/Untitled%203.png)

모양 안내 확산 인페인팅. 새 뷰에서 보이지 않는 모양을 합성하기 위해 사전 학습된 인페인팅 확산 모델을 사용합니다. 안내가 없으면 인페인팅된 영역은 모양(빨간색 실루엣)이나 3D 지오메트리의 구조적 세부 사항(a)을 보존하지 못하는 경우가 많습니다. 노멀 맵을 컨트롤넷 [2023]의 제어 신호로 사용하는 경우(b), 인페인팅된 영역은 메시의 구조적 디테일(예: 손가락)은 보존하지만 인체의 모양은 보존하지 않습니다. 실루엣 맵을 사용하면 인체의 형태는 보존되지만 메시의 구조적 디테일은 보존되지 않습니다(c). 노멀 맵과 실루엣 맵을 모두 사용하여 인페인팅 모델이 기본 3D 지오메트리를 존중하도록 안내할 것을 제안합니다(d). Adobe Stock 이미지

3.4 멀티뷰 융합
마지막 단계는 합성된 멀티뷰 이미지를 하나의 일관된 3D 텍스처 맵으로 융합하는 것입니다. 이 작업은 역 렌더링과 약간의 정렬 불량을 고려한 손실 함수로 UV 텍스처 맵을 최적화하여 수행됩니다. 그런 다음 최적화된 텍스처 맵을 사용하여 모든 시점에서 텍스처 메시를 렌더링함으로써 인물의 360도 뷰를 생성하는 프로세스를 완료할 수 있습니다.

![멀티뷰 융합. 합성된 멀티뷰 이미지 {𝐼1, ˆ𝐼2, ..., ˆ𝐼𝑁 }를 퓨전합니다. (그림 3 참조)를 융합하여 텍스처가 있는 3D 휴먼 메시를 얻습니다. 계산된 UV 파라미터화[2021]를 사용하여 지오메트리 𝐺를 고정하고 UV 텍스처 맵 𝑇을 최적화합니다. 각 반복에서 합성된 모든 뷰의 UV 텍스처 맵 𝑇을 뷰 세트 {𝑉 = 𝑉1,𝑉1, ...,𝑉𝑁 }에서 차별적으로 렌더링합니다. 렌더링된 뷰와 합성된 뷰 사이의 재구성 손실은 LPIPS 손실 [2018]과 L1 손실을 모두 사용하여 최소화합니다. 이렇게 하면 어떤 뷰에서든 렌더링할 수 있는 텍스처 메시가 생성됩니다. Adobe Stock의 이미지.](Single-Image%203D%20Human%20Digitization%20with%20Shape-Guid%208aaffc0ad74e48e0a608c2f2b1f9b45d/Untitled%204.png)

멀티뷰 융합. 합성된 멀티뷰 이미지 {𝐼1, ˆ𝐼2, ..., ˆ𝐼𝑁 }를 퓨전합니다. (그림 3 참조)를 융합하여 텍스처가 있는 3D 휴먼 메시를 얻습니다. 계산된 UV 파라미터화[2021]를 사용하여 지오메트리 𝐺를 고정하고 UV 텍스처 맵 𝑇을 최적화합니다. 각 반복에서 합성된 모든 뷰의 UV 텍스처 맵 𝑇을 뷰 세트 {𝑉 = 𝑉1,𝑉1, ...,𝑉𝑁 }에서 차별적으로 렌더링합니다. 렌더링된 뷰와 합성된 뷰 사이의 재구성 손실은 LPIPS 손실 [2018]과 L1 손실을 모두 사용하여 최소화합니다. 이렇게 하면 어떤 뷰에서든 렌더링할 수 있는 텍스처 메시가 생성됩니다. Adobe Stock의 이미지.

요약하면, 이 방법은 백뷰 합성, 멀티뷰 통합, 형상 유도 확산 인페인팅, 멀티뷰 융합의 고급 기술을 결합하여 단일 이미지에서 인물의 완전하고 상세한 360도 뷰를 효과적으로 합성합니다. 이 프로세스는 최종 결과물이 사실적이면서도 기하학적으로 일관성을 유지하도록 보장합니다.

### 4 EXPERIMENTAL RESULTS

이 백서에서는 단일 이미지에서 사람의 360도 뷰를 생성하는 방법의 효과를 평가하기 위해 수행한 실험 결과를 제시합니다. 실험과 그 결과는 여러 섹션에 걸쳐 자세히 설명되어 있습니다:

4.1 실험 설정

구현 세부 사항: 이 접근 방식은 RTX A6000 GPU에서 PyTorch를 사용하여 구현되었습니다. 주요 설정에는 인페인팅 확산 모델에 대한 15의 안내 척도와 뷰당 25개의 추론 단계가 포함됩니다. 모든 피사체에 대해 일반 텍스트 프롬프트가 사용되며, 시야각에 따라 달라집니다.
데이터 세트: 평가에는 THuman2.0 데이터 세트(30개 피사체)와 DeepFashion 데이터 세트가 사용됩니다. 또한 이 방법의 다양성을 보여주기 위해 Adobe Stock의 실제 이미지가 사용됩니다.
비교를 위한 기준선: 이 방법은 포즈 위드 스타일, PIFu, 임퍼소네이터++, 텍스처, Magic123, S3F, 엘리시티 등 기존의 여러 접근 방식과 비교됩니다.
4.2 정량적 비교

![제한 사항. 유니티의 접근 방식은 형상 재구성(비정상적인 발 모양(왼쪽))과 백뷰 합성(지오메트리 인식 부족으로 인해 정렬되지 않은 스커트 길이(가운데))에 대한 기존 방법의 한계를 그대로 계승합니다. 또한 얼굴과 의복 텍스처에 구워진 스페큘러를 보여주며, 이는 이상적으로 뷰에 따라 달라집니다(오른쪽). Adobe Stock 이미지.](Single-Image%203D%20Human%20Digitization%20with%20Shape-Guid%208aaffc0ad74e48e0a608c2f2b1f9b45d/Untitled%205.png)

제한 사항. 유니티의 접근 방식은 형상 재구성(비정상적인 발 모양(왼쪽))과 백뷰 합성(지오메트리 인식 부족으로 인해 정렬되지 않은 스커트 길이(가운데))에 대한 기존 방법의 한계를 그대로 계승합니다. 또한 얼굴과 의복 텍스처에 구워진 스페큘러를 보여주며, 이는 이상적으로 뷰에 따라 달라집니다(오른쪽). Adobe Stock 이미지.

![백뷰 합성의 필요성. 초기 백뷰가 있으면 다른 모든 뷰에서 특히 타겟 뷰가 입력 뷰에서 멀리 떨어져 있을 때 입력 이미지에 있는 인물의 모습을 보존할 수 있습니다. Adobe Stock의 이미지.](Single-Image%203D%20Human%20Digitization%20with%20Shape-Guid%208aaffc0ad74e48e0a608c2f2b1f9b45d/Untitled%206.png)

백뷰 합성의 필요성. 초기 백뷰가 있으면 다른 모든 뷰에서 특히 타겟 뷰가 입력 뷰에서 멀리 떨어져 있을 때 입력 이미지에 있는 인물의 모습을 보존할 수 있습니다. Adobe Stock의 이미지.

결과물의 품질은 PSNR, SSIM, FID, LPIPS, CLIP-score와 같은 다양한 지표를 사용하여 측정합니다. 이 방법은 이러한 메트릭 전반에 걸쳐 혼합된 결과를 보여 주며, 이는 기존 메트릭이 3D 텍스처의 인물을 일관되게 평가하지 못할 수 있음을 나타냅니다.
딥패션 데이터 세트의 ELICIT와 비교했을 때, 이 방법은 더 높은 CLIP 점수를 획득하여 더 나은 성능을 보여줍니다.

4.3 정성적 비교

Adobe Stock 이미지와 THuman2.0 데이터 세트를 시각적으로 비교한 결과, 이 방법이 입력 이미지를 정확하게 반영하는 고해상도의 사실적인 3D 인간 모델을 생성하는 것으로 나타났습니다.

4.4 절제 연구

안내 신호: 이 연구는 형상 유도 확산 인페인팅 프로세스에서 노멀 맵과 실루엣 맵을 모두 사용하는 것의 효과를 검증합니다. 이 조합을 통해 인물의 형태와 디테일을 더 잘 보존할 수 있습니다.
뒷모습 합성: 휴먼 리포즈 기법을 사용한 백뷰의 초기 합성은 특히 복잡한 텍스처의 경우 입력된 인물의 외형을 보존하는 데 매우 중요합니다.
4.5 한계와 향후 작업

이 방법은 베이스 지오메트리 재구성 및 백뷰 합성을 위한 외부 메서드에 의존하기 때문에 그 한계를 그대로 이어받습니다.
특히 스페큘러 하이라이트가 나타날 수 있는 사람의 피부와 관련된 뷰 종속성이 부족합니다.
이 방법은 사람의 포즈를 지원하지 않으며 피사체별 UV 텍스처 최적화가 필요합니다.
향후 연구에서는 옷을 입은 사람의 3D 형상 재구성과 범용 2D 확산 모델을 사용한 백뷰 합성을 개선하는 데 초점을 맞출 수 있습니다.
요약하면, 실험 결과는 단일 이미지에서 사실적인 3D 인간 모델을 생성할 수 있는 이 방법의 능력을 보여줍니다. 그러나 외부 방법에 대한 의존성을 줄이고 뷰 종속적인 기능을 강화하는 측면에서 한계와 향후 개선해야 할 부분이 있습니다.

### 5 CONCLUSIONS

이 백서는 단일 이미지에서 완전한 텍스처의 3D 휴먼 메시를 생성하는 데 있어 상당한 진전이 있었다는 점을 강조하며 마무리합니다. 결론의 핵심은 다음과 같습니다:

혁신적인 접근 방식: 이 연구는 가려진 뷰를 위해 고해상도와 사실적인 텍스처를 효과적으로 합성하는 새로운 방법을 소개합니다. 이는 3D 인체 디지털화 분야에서 주목할 만한 발전입니다.
형상 가이드 인페인팅: 이 방법의 중요한 요소는 형상 가이드 인페인팅을 사용하는 것입니다. 고용량 잠복 확산 모델을 기반으로 하는 이 기술을 사용하면 원본 이미지에서 볼 수 없는 영역을 정확하게 채울 수 있습니다.
강력한 멀티뷰 퓨전: 이 접근 방식에는 강력한 멀티뷰 퓨전 방식이 통합되어 있습니다. 이 방법은 서로 다른 합성된 뷰를 일관되고 상세한 3D 모델로 결합하는 데 유용합니다.
대규모 확산 모델 활용: 이 연구의 중요한 성과는 3D 휴먼 메시 생성을 위한 범용 대규모 확산 모델을 성공적으로 적용했다는 점입니다. 이는 큐레이팅된 사람 중심의 데이터 세트에 크게 의존하는 기존 방식에서 벗어난 것입니다.
잠재적 영향력: 연구진은 이번 연구가 3D 휴먼 디지털화의 새로운 방향을 제시했다고 생각합니다. 이 논문은 데이터 수집 노력에 대한 통합적인 접근 방식을 제안하여 3D 및 일반 2D/3D 합성 방법 모두에 잠재적으로 도움이 될 수 있습니다.
이 논문은 연구진이 개발한 방법이 3D 휴먼 모델링 분야에서 중요한 진전을 이루었으며, 기존의 데이터 세트에 의존하는 방식에 비해 더 다양하고 효과적인 접근 방식을 제공한다는 주장으로 결론을 내리고 있습니다.