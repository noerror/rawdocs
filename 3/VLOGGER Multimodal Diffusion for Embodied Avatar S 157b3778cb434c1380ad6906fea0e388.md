# VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis

[https://arxiv.org/abs/2403.08764](https://arxiv.org/abs/2403.08764)
[https://enriccorona.github.io/vlogger/](https://enriccorona.github.io/vlogger/)

- Mar 2024

### 1 Introduction

![VLOGGER는 오디오로부터 인간을 합성하는 새로운 프레임워크입니다. 첫 번째 열에 표시된 것과 같은 단일 입력 이미지와 샘플 오디오 입력이 주어지면, 우리의 방법은 말하고 생생하게 움직이는 사람의 광사실적이고 시간적으로 일관된 비디오를 생성합니다. 오른쪽 열에 표시된 합성된 이미지에서 볼 수 있듯이, 우리는 이전 방법들과 달리 머리 움직임, 시선, 눈 깜박임, 입 모양 움직임 뿐만 아니라 상체와 손짓까지 생성하여 오디오 주도 합성을 한 단계 더 발전시킵니다.](VLOGGER%20Multimodal%20Diffusion%20for%20Embodied%20Avatar%20S%20157b3778cb434c1380ad6906fea0e388/Untitled.png)

VLOGGER는 오디오로부터 인간을 합성하는 새로운 프레임워크입니다. 첫 번째 열에 표시된 것과 같은 단일 입력 이미지와 샘플 오디오 입력이 주어지면, 우리의 방법은 말하고 생생하게 움직이는 사람의 광사실적이고 시간적으로 일관된 비디오를 생성합니다. 오른쪽 열에 표시된 합성된 이미지에서 볼 수 있듯이, 우리는 이전 방법들과 달리 머리 움직임, 시선, 눈 깜박임, 입 모양 움직임 뿐만 아니라 상체와 손짓까지 생성하여 오디오 주도 합성을 한 단계 더 발전시킵니다.

1장에서는 VLOGGER라는 방법론을 소개하고 있습니다. VLOGGER는 텍스트나 오디오 입력을 기반으로 단 하나의 사람 이미지만을 사용하여, 말하고 움직이는 사람의 비디오를 자동으로 생성하는 기술입니다. 이 기술은 콘텐츠 생성, 엔터테인먼트, 게이밍 등 다양한 산업에서 인간 합성에 대한 높은 수요를 충족시키기 위해 고안되었습니다. 현재 인간의 현실적인 비디오를 생성하는 과정은 복잡하며 여러 가지 아티팩트가 발생할 수 있으며, 현실감 있는 결과물을 얻기 위해서는 상당한 수동 개입이 필요합니다. VLOGGER의 전체 자동화는 창의적인 과정을 간소화할 뿐만 아니라 향상된 온라인 커뮤니케이션, 교육, 개인화된 가상 어시스턴트 등 완전히 새로운 사용 사례를 가능하게 할 것입니다.

최근 챗봇의 성공을 바탕으로, 이러한 기술은 특히 중요해졌습니다. 사람들은 이러한 솔루션이 공감을 느끼기에 충분히 자연스럽지 않다고 느끼며, 여러 연구자들은 인간과 같은 외모와 행동의 실제성이 사용자로부터 공감적 반응을 유도하고 사회적 존재감을 형성하는 데 중요하다고 주장합니다. VLOGGER는 다차원 인터페이스를 통해 복잡한 얼굴 표정과 몸짓 수준을 높인 음성과 애니메이션 시각적 표현을 갖춘 대화형 에이전트를 제공함으로써, 인간 사용자와 자연스러운 대화를 지원하도록 설계되었습니다. 이는 발표, 교육, 이야기하기, 저대역폭 온라인 커뮤니케이션 및 텍스트 기반 HCI의 인터페이스로 사용될 수 있으며, 비디오 편집 작업에서의 잠재력도 추가적으로 시연됩니다.

![VLOGGER와 관련 작업과의 주요 특성 비교. 얼굴 재연[9,19,29,49,69,87,96]은 일반적으로 오디오나 텍스트를 사용한 주도를 고려하지 않습니다. 오디오-모션[14,18,57,65,68,84,90] 작업은 오디오를 3D 얼굴 모션으로 인코딩하는 컴포넌트를 공유하지만, 광사실성이 부족합니다. 입 모양 동기화[21, 54]는 다른 주제의 입력 비디오를 고려하지만, 입 모양 움직임만을 모델링합니다. 일반화 능력을 고려할 때, SadTalker[95]와 Styletalk[42]이 우리와 가장 가깝지만, 얼굴 이미지를 자르는 것이 필요하고, 몸 제어가 부족하며, 비디오를 편집할 수 없습니다.](VLOGGER%20Multimodal%20Diffusion%20for%20Embodied%20Avatar%20S%20157b3778cb434c1380ad6906fea0e388/Untitled%201.png)

VLOGGER와 관련 작업과의 주요 특성 비교. 얼굴 재연[9,19,29,49,69,87,96]은 일반적으로 오디오나 텍스트를 사용한 주도를 고려하지 않습니다. 오디오-모션[14,18,57,65,68,84,90] 작업은 오디오를 3D 얼굴 모션으로 인코딩하는 컴포넌트를 공유하지만, 광사실성이 부족합니다. 입 모양 동기화[21, 54]는 다른 주제의 입력 비디오를 고려하지만, 입 모양 움직임만을 모델링합니다. 일반화 능력을 고려할 때, SadTalker[95]와 Styletalk[42]이 우리와 가장 가깝지만, 얼굴 이미지를 자르는 것이 필요하고, 몸 제어가 부족하며, 비디오를 편집할 수 없습니다.

### 2 Related Work

![고수준 개요. VLOGGER는 통계적 3D 몸체 모델을 사용하여 비디오 생성 과정을 조건화합니다. 입력 이미지 Iref(왼쪽)가 주어지면, 예측된 형태 매개변수는 대상 정체성의 기하학적 특성을 인코딩합니다. 첫 번째로, 네트워크 M은 입력 발화의 멜 스펙트로그램 a를 가져와 N 프레임 동안의 3D 얼굴 표정 및 몸 포즈를 생성합니다. 우리는 움직이는 3D 몸체의 밀집 표현을 렌더링하여 비디오 생성 단계에서 2D 컨트롤 {Ci}1≤i≤N으로 작동합니다(컨트롤의 예시는 Sup. Mat.에서 제공). 대상의 참조 이미지와 함께 이것들은 시간적 확산 모델과 초해상도 모듈에 입력으로 주어지며, 이는 대상 정체성의 광사실적인 재연 {Gi}1≤i≤N을 생성하기 위해 훈련됩니다](VLOGGER%20Multimodal%20Diffusion%20for%20Embodied%20Avatar%20S%20157b3778cb434c1380ad6906fea0e388/Untitled%202.png)

고수준 개요. VLOGGER는 통계적 3D 몸체 모델을 사용하여 비디오 생성 과정을 조건화합니다. 입력 이미지 Iref(왼쪽)가 주어지면, 예측된 형태 매개변수는 대상 정체성의 기하학적 특성을 인코딩합니다. 첫 번째로, 네트워크 M은 입력 발화의 멜 스펙트로그램 a를 가져와 N 프레임 동안의 3D 얼굴 표정 및 몸 포즈를 생성합니다. 우리는 움직이는 3D 몸체의 밀집 표현을 렌더링하여 비디오 생성 단계에서 2D 컨트롤 {Ci}1≤i≤N으로 작동합니다(컨트롤의 예시는 Sup. Mat.에서 제공). 대상의 참조 이미지와 함께 이것들은 시간적 확산 모델과 초해상도 모듈에 입력으로 주어지며, 이는 대상 정체성의 광사실적인 재연 {Gi}1≤i≤N을 생성하기 위해 훈련됩니다

2장에서는 VLOGGER와 관련된 선행 연구들을 살펴보며, 주로 오디오 주도 페이스 생성과 관련된 작업들을 탐구합니다. 이 장은 크게 세 부분으로 나뉩니다: 오디오 주도 페이스 생성, 얼굴 재연, 그리고 비디오 생성에 관한 연구입니다.

**오디오 주도 페이스 생성**

이 분야의 연구들은 오디오 입력에 기반하여 얼굴의 움직임을 생성하는 다양한 접근 방식을 탐구합니다. 초기 작업들은 주로 입 모양의 동기화에 초점을 맞췄으며, 이후의 연구들에서는 머리 움직임, 시선, 깜빡임 등 보다 다양한 특징들을 포함시키기 시작했습니다. 이 분야의 주요 도전 과제는 오디오 신호와 표정 간의 복잡한 상호작용을 모델링하는 것입니다. 여기서 언급된 연구들은 3D 변형 가능한 얼굴 모델이나 전신 모델을 사용하여 오디오 세그먼트를 기반으로 다양한 표정과 포즈를 생성하는 방법을 탐구합니다. 이 접근법은 시간적 일관성을 유지하면서 다양한 통계적 머리 모델 또는 몸체 모델의 포즈와 형태 매개변수를 생성할 수 있는 잠재력을 가집니다.

**얼굴 재연**

얼굴 재연 분야의 연구는 소스 비디오의 움직임을 대상 인물에게 전달하는 것을 목표로 합니다. 이러한 방법들은 주로 중간 표현(예: 희소 또는 밀집 랜드마크, 의미적 마스크, 3D 밀집 표현 또는 변형된 특징)을 사용합니다. 이 방법들은 대상 인물이 말하는 다양한 프레임을 재훈련하고 애니메이션하는 데 필요한 상당한 양의 데이터를 요구합니다. 이 장에서는 비디오 기반 페이스 생성이 과거에 어떻게 탐구되었는지, 그리고 오디오 입력을 고려할 때 어떻게 이 중간 표현을 적용할 수 있는지를 살펴봅니다.

**비디오 생성**

비디오 생성에 관한 연구는 주로 텍스트-이미지 확산 모델의 성공을 바탕으로, 이를 비디오 도메인으로 확장하는 작업에 집중합니다. 이러한 작업들은 종종 몇 초 또는 해상도에 제한이 있으며, 인간을 명시적으로 다루지 않는 경우가 많습니다. VLOGGER의 접근 방식은 현재 최고 수준의 이미지 확산 모델을 시간적 영역으로 확장하여, 가변 길이의 비디오를 생성하는 동시에 공간적 및 시간적 제어를 가능하게 하는 새로운 방법을 제안합니다.

### 3 Method

3장에서는 VLOGGER, 즉 오디오 신호 또는 텍스트 기반으로 실제 사람처럼 말하고 움직이는 비디오를 생성하는 프레임워크의 구체적인 방법론에 대해 설명합니다. 이 방법론은 크게 두 단계의 과정으로 구성됩니다: 오디오 기반 모션 생성과 광사실적인 말하는 인간 생성입니다.

**3.1 오디오 기반 모션 생성**

이 단계에서는 오디오 입력으로부터 사람의 몸동작과 얼굴 표정을 예측하는 모델을 구현합니다. 오디오 신호는 멜 스펙트로그램으로 변환되며, 변환된 오디오 데이터는 트랜스포머 아키텍처를 기반으로 한 네트워크에 입력됩니다. 이 네트워크는 시간적 차원에 대해 여러 개의 멀티 헤드 어텐션 레이어를 사용하며, 각 프레임에서는 이전 프레임만을 참조하도록 합니다. 모델은 오디오 입력에 기반하여 얼굴 표정과 몸동작의 변화를 예측합니다. 이를 위해, 3D 몸 모델의 매개변수를 예측하며, 이는 몸의 형태와 표정, 포즈의 변화를 나타냅니다. 이 과정은 사람이 자연스럽게 움직이는 것을 모방하여, 예측된 모션을 시각적으로 표현할 수 있는 중간 컨트롤 데이터를 생성합니다.

**3.2 광사실적인 말하는 인간 생성**

다음 단계는 생성된 몸동작과 얼굴 표정 컨트롤을 바탕으로 광사실적인 비디오 프레임을 생성하는 과정입니다. 이를 위해, 참조 이미지와 함께 중간 컨트롤 데이터를 입력으로 사용하는 확산 모델을 활용합니다. 이 모델은 시간적으로 인식할 수 있는 확산 모델의 확장으로, 고해상도의 비디오 프레임을 생성할 수 있습니다. 생성된 비디오는 초기에는 낮은 해상도로 생성되며, 이후 슈퍼 해상도 확산 모델을 통해 고해상도로 상향 조정됩니다. 이 과정을 통해, 입력 오디오 또는 텍스트와 일치하는 사람의 얼굴과 몸동작을 담은 고품질 비디오를 생성할 수 있습니다.

**데이터셋과 학습**

VLOGGER의 훈련에는 MENTOR라는 새로운 대규모 데이터셋이 사용됩니다. 이 데이터셋은 다양한 인간의 얼굴과 몸동작을 포함하고 있으며, VLOGGER 모델의 학습에 활용됩니다. 훈련 과정에서는 다양한 손실 함수를 사용하여 모델이 실제와 같은 움직임을 생성할 수 있도록 합니다. 또한, 모델은 비디오의 각 프레임을 잘 생성할 수 있도록, 시간적 일관성을 유지하는 방법에 대해서도 학습합니다.

3장에서는 VLOGGER 프레임워크의 구체적인 방법론을 설명하며, 이는 오디오 또는 텍스트 입력으로부터 사람의 얼굴과 몸동작을 모방하는 비디오를 생성하는 두 단계 과정으로 구성됩니다. 이 과정은 특히 3D 모델과 확산 모델을 사용하여 고도로 정교한 비디오 생성을 가능하게 하며, 대규모 데이터셋에서의 훈련을 통해 실제 인간의 다양한 움직임과 표정을 모방할 수 있는 능력을 갖추게 됩니다.

### **4장 실험**

4장에서는 VLOGGER 시스템의 성능을 검증하기 위한 실험 과정과 결과를 상세하게 설명합니다. 이 장의 핵심은 VLOGGER가 다양한 데이터셋에서 이전 방법들과 비교하여 어떻게 성능을 향상시켰는지, 그리고 다양한 실험을 통해 얻은 통찰력을 공유하는 것입니다.

**데이터와 훈련**

VLOGGER는 MENTOR 데이터셋에서 훈련되었으며, 이 데이터셋은 2.2K 시간에 달하는 800K 이상의 고유 신원을 포함하는 대규모 데이터셋입니다. 이 데이터셋은 다양한 피부색, 몸 포즈, 시점, 발화, 그리고 몸 가시성을 포함하여 인간 커뮤니케이션의 복잡성을 학습하기 위한 다양성을 제공합니다. VLOGGER의 훈련 과정은 기본 해상도 128×128에서 시작하여, 256×256과 512×512 해상도로 순차적으로 상향 조정하는 캐스케이드 접근 방식을 사용합니다.

**정량적 결과**

VLOGGER는 HDTF 및 TalkingHead-1KH 데이터셋을 포함한 여러 공개 벤치마크에서 상태 기반 방법들을 대체하였습니다. 정량적 평가에서 VLOGGER는 이미지 품질, 입 모양 동기화, 시간적 일관성 및 정체성 보존 측면에서 뛰어난 성능을 보였습니다. 특히, 다양한 표정과 포즈를 생성할 수 있는 능력, 높은 이미지 품질, 그리고 생성된 비디오의 다양성 측면에서 우수한 결과를 보여주었습니다.

**정성적 결과**

VLOGGER는 다양한 인간 특성에 걸쳐 낮은 편향성을 보이며, 다른 방법들에 비해 더 폭넓은 시나리오에서 높은 해상도의 머리와 상체 움직임 비디오를 생성할 수 있습니다. 특히, 다양한 피부색, 연령, 성별에 걸쳐 일관된 성능을 유지하며, 보이지 않는 몸체 부분이나 손동작을 포함하는 비디오에서도 좋은 결과를 보여줍니다. 이는 MENTOR 데이터셋의 다양성과 VLOGGER의 일반화 능력 덕분에 가능한 결과입니다.

**다양성 분석과 개인화**

VLOGGER의 실험에서는 또한 생성된 비디오의 다양성을 분석하여, 같은 오디오 입력에 대해 다른 표정과 몸동작을 갖는 여러 비디오를 생성할 수 있는지를 보여줍니다. 이는 VLOGGER가 단순히 하나의 결과를 생성하는 것이 아니라, 다양한 가능성을 탐색할 수 있는 생성적 모델임을 시사합니다. 또한, 특정 인물의 정체성과 특성을 더 잘 포착하기 위해 추가 데이터로 모델을 세밀하게 조정하는 개인화 접근 방식도 소개합니다.

4장의 실험 결과는 VLOGGER가 오디오 또는 텍스트 기반 입력에서 사람의 움직임과 표정을 사실적으로 모방하는 비디오를 생성하는 데 있어 현존하는 최고의 방법들을 능가한다는 것을 보여줍니다. 이러한 결과는 다양한 데이터셋과 시나리오에서의 광범위한 평가를 통해 얻어진 것으로, VLOGGER의 강력한 일반화 능력과 다양성을 다룰 수 있는 능력을 강조합니다.

![입력 이미지(왼쪽)와 생성된 프레임을 보여주는 정성적 비교. 기준 모델은 일반적으로 전체 시퀀스에 걸쳐 표정을 유지하며, 머리를 자르는 것이 필요합니다[42,77,95]. 반면, VLOGGER는 얼굴을 고려할 때(세 번째 행)뿐만 아니라 보이는 상체(다섯 번째 행)에서도 변화를 생성합니다. 이 그림은 애니메이션된 얼굴을 보여주지만, 제스처와 함께 있는 예시는 Fig. 1과 Sup. Mat.에서 보여집니다.](VLOGGER%20Multimodal%20Diffusion%20for%20Embodied%20Avatar%20S%20157b3778cb434c1380ad6906fea0e388/Untitled%203.png)

입력 이미지(왼쪽)와 생성된 프레임을 보여주는 정성적 비교. 기준 모델은 일반적으로 전체 시퀀스에 걸쳐 표정을 유지하며, 머리를 자르는 것이 필요합니다[42,77,95]. 반면, VLOGGER는 얼굴을 고려할 때(세 번째 행)뿐만 아니라 보이는 상체(다섯 번째 행)에서도 변화를 생성합니다. 이 그림은 애니메이션된 얼굴을 보여주지만, 제스처와 함께 있는 예시는 Fig. 1과 Sup. Mat.에서 보여집니다.

![모델 다양성 전시. VLOGGER는 확률적이며 동일한 주제에 대해 다양한 비디오를 생성할 수 있습니다. 주제 이미지와 입력 발화가 주어지면, 2-5열은 각각 1-4초 후에 24개 생성된 비디오에서 얻은 픽셀 색상의 편차를 보여줍니다. 단지 1초 후에(두 번째 열) 모델은 이미 손 포즈와 얼굴 표정에서 큰 다양성을 보여주며, 모든 비디오는 좋은 시각적 품질을 가집니다.](VLOGGER%20Multimodal%20Diffusion%20for%20Embodied%20Avatar%20S%20157b3778cb434c1380ad6906fea0e388/Untitled%204.png)

모델 다양성 전시. VLOGGER는 확률적이며 동일한 주제에 대해 다양한 비디오를 생성할 수 있습니다. 주제 이미지와 입력 발화가 주어지면, 2-5열은 각각 1-4초 후에 24개 생성된 비디오에서 얻은 픽셀 색상의 편차를 보여줍니다. 단지 1초 후에(두 번째 열) 모델은 이미 손 포즈와 얼굴 표정에서 큰 다양성을 보여주며, 모든 비디오는 좋은 시각적 품질을 가집니다.

![비디오 편집 결과. 입력 비디오(첫 번째 행)가 주어지면, 입(두 번째 행), 눈(세 번째 행)을 변경하거나 전체 비디오 동안 눈을 뜨게 하는(네 번째 행) 새로운 얼굴 표정을 정의합니다. 시간적 인페인팅 마스크는 자동적으로 변화하는 몸체 부분에서 정의됩니다. Sup. Mat.에서 더 잘 볼 수 있습니다.](VLOGGER%20Multimodal%20Diffusion%20for%20Embodied%20Avatar%20S%20157b3778cb434c1380ad6906fea0e388/Untitled%205.png)

비디오 편집 결과. 입력 비디오(첫 번째 행)가 주어지면, 입(두 번째 행), 눈(세 번째 행)을 변경하거나 전체 비디오 동안 눈을 뜨게 하는(네 번째 행) 새로운 얼굴 표정을 정의합니다. 시간적 인페인팅 마스크는 자동적으로 변화하는 몸체 부분에서 정의됩니다. Sup. Mat.에서 더 잘 볼 수 있습니다.

![모델 개인화에 대한 정성적 결과. 사용자의 단일 비디오에서 모델[59]을 미세 조정하면 다양한 표정에 걸쳐 더 사실적인 합성을 지원합니다.](VLOGGER%20Multimodal%20Diffusion%20for%20Embodied%20Avatar%20S%20157b3778cb434c1380ad6906fea0e388/Untitled%206.png)

모델 개인화에 대한 정성적 결과. 사용자의 단일 비디오에서 모델[59]을 미세 조정하면 다양한 표정에 걸쳐 더 사실적인 합성을 지원합니다.

### **5장 결론**

5장에서는 VLOGGER 프로젝트의 결론 및 이 프로젝트가 가져올 수 있는 사회적 영향에 대해 논의합니다. VLOGGER는 오디오나 텍스트 입력을 바탕으로 실제 사람처럼 말하고 움직이는 비디오를 생성하는 혁신적인 방법론을 제시하며, 이는 기존의 확산 기반 모델을 확장하여 인간의 얼굴과 몸동작을 포함하는 고해상도 비디오 생성을 가능하게 합니다.

**결론의 주요 포인트**

- **성과**: VLOGGER는 단일 이미지와 오디오 또는 텍스트 입력을 사용하여 말하고 움직이는 사람의 비디오를 자동으로 생성할 수 있는 첫 번째 접근 방식입니다. 이는 복잡한 얼굴 표정과 몸동작을 포함하는 비디오를 자연스러운 대화 형태로 생성할 수 있음을 시사합니다. VLOGGER는 특히 교육, 나레이션, 저대역폭 온라인 커뮤니케이션, 비디오 편집과 같은 분야에서 활용 가능성이 높습니다.
- **데이터셋**: MENTOR 데이터셋의 도입은 이 분야의 연구를 한 단계 끌어올릴 수 있는 중요한 발전으로, 2.2K 시간 이상의 비디오와 800K 이상의 고유 신원을 포함하고 있습니다. 이 데이터셋은 인간의 다양성을 포괄적으로 반영하며, 다양한 연구와 응용 프로그램에 활용될 수 있습니다.
- **사회적 영향과 한계**: VLOGGER의 개발과 배포는 온라인 커뮤니케이션, 디지털 콘텐츠 생성, 가상 교육 등 다양한 분야에 긍정적인 영향을 미칠 수 있습니다. 하지만 이와 동시에 가짜 뉴스의 생성, 개인의 사생활 침해, 저작권 문제와 같은 부정적인 사용 가능성에 대해서도 신중한 고려가 요구됩니다. 연구자들은 이러한 기술의 개발과 사용에 있어 윤리적 지침과 사회적 기준을 수립하는 데 주력해야 합니다.
- **감사의 말**: 연구팀은 이 프로젝트에 기여한 모든 이들에게 감사의 말을 전하며, 특히 MENTOR 데이터셋의 수집과 분석에 참여한 연구원들의 노고를 치하합니다.

**전망**

VLOGGER 프로젝트는 비디오 생성 기술의 새로운 가능성을 열어주며, 더욱 자연스러운 인간-컴퓨터 상호작용을 위한 발판을 마련합니다. 이 기술의 발전은 창의적인 콘텐츠 생성뿐만 아니라 인간과 기계 간의 교류 방식을 혁신적으로 변화시킬 잠재력을 지니고 있습니다. 또한, 이러한 기술의 발전은 연구자들에게 새로운 연구 주제와 도전 과제를 제시하며, 이를 통해 인공지능과 기계 학습 분야가 더욱 발전할 수 있는 기회를 제공합니다.