# DiffSinger: Singing Voice Synthesis via Shallow Diffusion Mechanism

[https://arxiv.org/abs/2105.02446](https://arxiv.org/abs/2105.02446)

[https://github.com/MoonInTheRiver/DiffSinger](https://github.com/MoonInTheRiver/DiffSinger)

이 논문에서는 주어진 악보에서 고품질의 표현력 있는 노래 음성을 생성하도록 설계된 새로운 노래 음성 합성(SVS) 시스템인 'DiffSinger'를 소개합니다. 기존의 SVS 모델은 과도한 스무딩과 불안정한 훈련으로 인해 합성된 노래의 자연스러움이 사라지는 등의 문제가 있었습니다. 이러한 한계를 극복하기 위해 DiffSinger는 확산 확률 모델을 활용합니다.

DiffSinger의 핵심은 악보에 따라 노이즈를 점진적으로 멜-스펙트로그램으로 변환하는 매개변수화된 마르코프 체인입니다. 이 접근 방식은 훈련 시 더 안정적이며 변동 한계에 대한 암시적 최적화로 인해 더 사실적인 출력을 생성합니다.

음성 품질을 향상시키고 생성 프로세스의 속도를 높이기 위해 DiffSinger는 얕은 확산 메커니즘을 통합합니다. 이 메커니즘은 단순 손실에서 얻은 사전 지식을 효율적으로 활용합니다. 전체 확산 단계보다 낮은 단계 수인 '얕은 단계'에서 생성을 시작합니다. 이 '얕은 단계'는 실제 멜-스펙트로그램과 예측된 멜-스펙트로그램의 확산 궤적의 교집합에 의해 결정됩니다.

또한 이 논문에서는 교차를 찾아 얕은 단계를 적응적으로 결정하는 경계 예측 방법을 제안합니다. 중국 노래 데이터 세트에 대한 평가 결과 DiffSinger는 기존의 다른 SVS 시스템을 능가하는 것으로 나타났습니다. 또한 이 기술은 텍스트 음성 변환 작업(DiffSpeech)에서도 가능성을 보여 광범위한 적용 가능성을 입증했습니다.

저자들은 더 자세히 살펴볼 수 있도록 오디오 샘플과 작업 코드를 온라인으로 제공했습니다.

1 Introduction

"노래하는 목소리 합성"(SVS)은 악보에서 자연스럽고 표현력 있는 노래 목소리를 만들어내는 것을 목표로 합니다. 이 분야에 대한 연구자들과 엔터테인먼트 업계의 관심이 높아지고 있지만, 이전의 SVS용 음향 모델은 여러 가지 문제에 직면해 있었습니다. 이러한 모델은 주로 L1 또는 L2와 같은 단순 손실을 사용하여 음향 특징을 재구성하기 때문에 종종 흐릿하고 지나치게 평활화된 출력을 생성합니다. 일부 모델은 생성적 적대 신경망(GAN)을 사용하여 이 문제를 해결하려고 하지만, GAN은 불안정하고 때때로 실패할 수 있습니다.

DiffSinger는 이러한 문제를 해결하기 위해 제안된 솔루션입니다. 이미지 생성 및 신경 보코더 분야에서 성공을 거둔 생성 모델인 확산 확률 모델을 사용하는 SVS의 음향 모델입니다. 이 모델에서는 확산 프로세스가 데이터에 가우시안 노이즈를 점진적으로 추가하여 등방성 가우시안 분포로 변환하고, 신경망으로 구현된 역방향 프로세스가 가우시안 백색 잡음에서 원래 데이터를 복원합니다. DiffSinger는 이 확산 모델을 사용하여 악보를 기반으로 노이즈를 사운드 스펙트럼의 시각적 표현인 멜-스펙트로그램으로 변환합니다.

DiffSinger는 "변형 하한"(ELBO)의 최적화를 통해 효과적으로 훈련하고, 적대적인 피드백을 피하며, 원래 분포와 매우 일치하는 멜-스펙트로그램을 생성함으로써 이전 모델을 개선합니다. 음성 품질을 더욱 향상시키고 프로세스 속도를 높이기 위해 DiffSinger는 "얕은 확산 메커니즘"을 도입했습니다. 여기에는 원본 멜-스펙트로그램과 예측된 멜-스펙트로그램의 확산 궤적의 교차점을 기반으로 특정 '얕은 단계'(데이터를 가우시안 백색 잡음으로 바꿀 만큼 깊지 않은 확산 단계)에서 역 프로세스를 시작하는 것이 포함됩니다.

이 논문에서는 이 교차점을 적응적으로 찾아 최적의 얕은 단계를 결정하는 경계 예측 네트워크도 제안합니다. 이 방법은 가우시안 백색 잡음보다 더 나은 시작점을 제공하고 합성된 오디오의 품질을 개선하며 프로세스 속도를 높입니다.

마지막으로, SVS와 텍스트 음성 변환(TTS) 작업 간의 유사성 때문에 저자들은 DiffSinger의 변형인 "DiffSpeech"도 만들었습니다. 실험 결과 DiffSinger와 DiffSpeech 모두 현재 모델보다 더 나은 음성 품질과 더 빠른 추론을 제공하고 방법의 적응성을 입증하는 등 뛰어난 성능을 보였습니다.

2 Diffusion Model

확산 확률 모델은 확산 프로세스를 사용하여 원시 데이터를 가우스 분포로 점진적으로 변환한 다음 역방향 프로세스를 통해 가우스 백색 잡음에서 원래 데이터를 복원합니다.

![The directed graph for diffusion model.](DiffSinger%20Singing%20Voice%20Synthesis%20via%20Shallow%20Dif%20ff7e05f070764442bce022bd9094974e/Untitled.png)

The directed graph for diffusion model.

확산 프로세스는 고정된 매개변수를 가진 마르코프 체인으로 작동하여 일련의 단계를 거쳐 원본 데이터를 잠재적인 형태로 변환합니다. 각 단계마다 정의된 분산 일정에 따라 소량의 가우시안 노이즈가 추가됩니다. 적절한 일정과 충분한 수의 단계가 주어지면 출력 분포는 거의 등방성 가우시안입니다. 확산 프로세스의 특수한 특성 덕분에 원본 데이터에서 어느 단계에서든 일정한 시간에 상태를 계산할 수 있습니다.

반면에 역 프로세스는 학습 가능한 매개변수가 있는 마르코프 체인으로, 확산 프로세스를 역전시켜 가우시안 노이즈에서 원래 데이터를 복원합니다. 이 프로세스는 훈련 중에 매개변수가 학습되는 신경망을 사용하여 원본 데이터에 근사치를 구합니다.

훈련에는 확산 과정과 역방향 과정의 로그 확률 차이에 대한 기대값인 음의 로그 확률의 변동 한계를 최소화하는 작업이 포함됩니다. 이는 확률적 경사 하강을 사용하여 확률의 임의 항을 최적화함으로써 달성됩니다.

모델이 학습되면 등방성 가우스 분포의 샘플로 시작하여 역 프로세스를 실행하여 새로운 데이터 샘플을 생성하는 데 사용할 수 있습니다. 이 독특한 확산 및 역방향 프로세스를 통해 확산 확률 모델은 복잡한 분포를 처리하고 출력 품질을 개선할 수 있으므로 노래하는 음성 합성과 같은 생성 작업에 강력한 도구가 될 수 있습니다.

3 DiffSinger

DiffSinger는 확산 모델을 기반으로 구축된 노래하는 음성 합성 시스템입니다. 이 시스템은 확산 모델의 역방향 프로세스에서 악보를 조건으로 사용하여 악보와 해당 멜-스펙트로그램 사이의 조건부 분포를 모델링합니다. 이 시스템에는 나이브 버전과 새로운 얕은 확산 메커니즘과 경계 예측 네트워크가 포함된 개선된 버전이 있습니다.

![DiffSinger의 개요(점선 상자에 얕은 확산 메커니즘이 있음). 하위 그림 (a)에서 x는 악보, t는 스텝 수, M은 기준 진실 멜-스펙트로그램, Mf는 L1 손실로 훈련된 보조 디코더에 의해 생성된 흐릿한 멜-스펙트로그램 L1 손실로 훈련된 보조 디코더에 의해 생성된 흐릿한 멜-스펙트럼을 의미하며, Mt는 확산 과정의 t번째 단계에서의 M입니다. 하위 그림 (b)에서 MT는 다음을 의미합니다. T 번째 확산 단계에서의 M(가우시안 백색 잡음); k는 예측된 교차점 경계; MT(나이브 버전) 또는 Mfk( 버전) 또는 Mfk(얕은 확산 포함)를 추론 절차의 시작점으로 선택할 수 있는 스위치가 있습니다.](DiffSinger%20Singing%20Voice%20Synthesis%20via%20Shallow%20Dif%20ff7e05f070764442bce022bd9094974e/Untitled%201.png)

DiffSinger의 개요(점선 상자에 얕은 확산 메커니즘이 있음). 하위 그림 (a)에서 x는 악보, t는 스텝 수, M은 기준 진실 멜-스펙트로그램, Mf는 L1 손실로 훈련된 보조 디코더에 의해 생성된 흐릿한 멜-스펙트로그램 L1 손실로 훈련된 보조 디코더에 의해 생성된 흐릿한 멜-스펙트럼을 의미하며, Mt는 확산 과정의 t번째 단계에서의 M입니다. 하위 그림 (b)에서 MT는 다음을 의미합니다. T 번째 확산 단계에서의 M(가우시안 백색 잡음); k는 예측된 교차점 경계; MT(나이브 버전) 또는 Mfk( 버전) 또는 Mfk(얕은 확산 포함)를 추론 절차의 시작점으로 선택할 수 있는 스위치가 있습니다.

나이브 버전에서 DiffSinger는 확산 과정의 각 단계에서 멜-스펙트로그램을 사용하고 단계 수와 악보에 따라 무작위 노이즈를 예측합니다. 추론 절차는 가우시안 백색 잡음으로 시작하여 중간 샘플의 노이즈를 제거하기 위해 반복하여 최종적으로 악보에 해당하는 멜-스펙트로그램을 생성합니다.

개선된 버전은 가우시안 백색 잡음이 아닌 두 궤적의 교차점에서 역 프로세스를 시작하는 얕은 확산 메커니즘을 추가하여 계산 부하를 줄였습니다. 보조 디코더는 역 프로세스의 시작점으로 사용되는 멜-스펙트로그램의 지나치게 평활화된 버전을 생성합니다.

두 궤적의 교차점을 찾고 역 프로세스의 반복 횟수를 적응적으로 결정하기 위한 경계 예측 네트워크도 포함되어 있습니다. 경계 예측기는 확산 프로세스에서 생성된 멜-스펙트로그램과 보조 디코더에서 생성된 멜-스펙트로그램을 구별하도록 훈련됩니다.

이 시스템에는 악보를 변환하는 인코더, 멜-스펙트로그램의 초기 버전을 생성하는 보조 디코더, 리버스 프로세스의 각 단계를 처리하는 디노이저, 리버스 프로세스의 시작점을 식별하는 경계 예측기가 포함됩니다. 디노이저의 조건으로 확산 단계와 악보가 사용됩니다. 인코더는 가사 인코더, 길이 레귤레이터, 피치 인코더를 사용하여 악보를 디노이저의 조건에 사용되는 시퀀스로 변환합니다. 비인과적 WaveNet 아키텍처를 기반으로 하는 디노이저는 각 단계에서 멜-스펙트로그램을 받아 확산 프로세스에서 추가되는 랜덤 노이즈를 예측합니다.

4 Experiments

이 연구에서는 SVS(노래 음성 합성) 및 TTS(텍스트 음성 변환) 작업에 대한 실험 설정, 결과 및 분석을 제시합니다.

SVS 실험에서는 117개의 중국 팝송이 포함된 새로운 데이터 세트인 PopCS를 생성했습니다. 노래를 문장 단위로 분할하고 가사와 음소 수준의 정렬을 수행한 후 음정 정보를 추출했습니다. 그런 다음 데이터셋을 정리하고 다시 분할하여 1,651개의 노래 조각으로 만들었습니다. 중국어 가사를 음소로 변환하고 사용된 알고리즘에 대한 매개변수를 설정하는 등 다양한 기술적 세부 사항에 주의를 기울였습니다.

그런 다음 새로운 방법인 DiffSinger의 성능을 다른 시스템과 비교했습니다. 그 결과, DiffSinger는 학습 손실이 더 적고 최신 GAN 기반 방식을 사용하는 기준 시스템보다 성능이 뛰어나며 그 효과를 입증했습니다. 또한 DiffSinger 모델은 오디오 생성 속도가 45.1% 더 빨랐습니다.

![네 가지 시스템에서 멜-스펙트로그램을 시각화합니다: GT, DiffSinger, GAN-Singer 및 FFT-Singer.](DiffSinger%20Singing%20Voice%20Synthesis%20via%20Shallow%20Dif%20ff7e05f070764442bce022bd9094974e/Untitled%202.png)

네 가지 시스템에서 멜-스펙트로그램을 시각화합니다: GT, DiffSinger, GAN-Singer 및 FFT-Singer.

DiffSinger 모델의 다양한 구성 요소의 효과를 조사하기 위해 제거 연구를 수행했습니다. 얕은 확산 메커니즘을 제거하면 품질이 떨어지고 경계 예측 네트워크가 중요한 파라미터인 k의 적절한 값을 예측할 수 있다는 사실이 밝혀졌습니다.

TTS 실험에서 연구팀은 DiffSinger 모델을 조정하여 새로운 모델인 DiffSpeech를 만들었습니다. 이 모델은 Amazon Mechanical Turk를 사용해 평가한 결과, FastSpeech 2와 Glow-TTS보다 우수한 성능을 보였으며, DiffSinger에 사용된 방법이 TTS 작업에 잘 일반화되었음을 보여주었습니다. DiffSpeech의 얕은 확산 메커니즘은 또한 실시간 요소를 줄여 속도를 향상시켰습니다.

5 Related Work

이 섹션에서는 노래하는 음성 합성(SVS)의 진화와 노이즈 제거 확산 확률 모델 개발에 대해 살펴봅니다.

SVS는 처음에는 복잡하고 유연성이 부족한 연결 또는 HMM 기반 파라메트릭 방법을 사용했습니다. 그러나 딥러닝이 부상하면서 심층 신경망에 기반한 SVS 시스템이 개발되었습니다. 이러한 시스템은 문맥적 특징을 음향적 특징에 매핑하여 노래하는 목소리를 더욱 정교하고 효율적으로 합성할 수 있게 되었습니다. 피드 포워드 트랜스포머, 적대적 훈련, 다중 가수 SVS 시스템, 다중 스케일 적대적 훈련과 같은 기술이 사용되어 합성된 노래 목소리의 자연스러움, 다양성, 품질을 개선했습니다.

반면에 파라미터화된 마르코프 체인의 일종인 확산 확률 모델은 데이터 분포와 일치하는 샘플을 일정한 단계로 생성하는 데 사용되었습니다. 이 모델은 2015년에 처음 제안되었으며, 특정 매개변수화를 사용하여 고품질 이미지를 생성하는 데 발전이 있었습니다. 이 모델은 신경 보코더에 적용되어 멜-스펙트로그램을 기반으로 고충실도 파형을 생성합니다. 합성 품질과 효율성을 개선하기 위해 연속 노이즈 스케줄 및 더 빠른 샘플링 메커니즘과 같은 기술이 개발되었습니다.

최근에는 확산 모델이 SVS 분야로 확장되어 악보나 텍스트가 주어진 멜-스펙트로그램을 생성하기 위한 음향 모델로 사용되고 있습니다. 동시에 텍스트 음성 변환(TTS) 작업에도 적용되고 있습니다.

6 Conclusion

이 논문에서는 확산 확률 모델에 기반한 노래하는 음성 합성(SVS)을 위한 음향 모델인 DiffSinger를 소개합니다. 음성의 품질을 향상시키고 프로세스의 속도를 높이기 위해 고유한 얕은 확산 메커니즘이 개발되었습니다. 이 메커니즘은 확산 단계가 충분히 클 때 M과 Mf의 확산 궤적이 수렴하는 데서 영감을 얻었습니다. 깊은 확산 단계가 아닌 두 궤적의 교차점에서 역 프로세스를 시작하면 프로세스가 크게 간소화되어 오디오 품질이 향상되고 추론 속도가 빨라집니다.

PopCS 데이터 세트에서 수행한 실험 결과 DiffSinger는 이전 작업보다 성능이 뛰어나며 얕은 확산 메커니즘의 효과를 강조합니다. LJSpeech 데이터 세트에서 수행한 추가 실험에서는 텍스트 음성 변환(TTS) 작업에 대한 DiffSpeech의 효과를 입증했습니다. 향후 작업은 보코더 없이 직접 합성에 초점을 맞출 예정입니다.

- 요약
    
    소개: 이 백서는 악보를 노래하는 연주로 변환하는 기술인 노래하는 음성 합성(SVS)의 개념을 소개하는 것으로 시작합니다. 현재 모델은 저품질의 오디오를 생성하거나 실제 적용하기에는 너무 느리다는 등의 한계가 있습니다. 이러한 문제를 해결하기 위해 저자는 확산 확률 모델에 기반한 SVS용 음향 모델인 DiffSinger를 제안합니다. 또한 오디오 품질을 개선하고 프로세스 속도를 높이기 위해 새로운 얕은 확산 메커니즘을 제안합니다.
    
    실험 설정: 저자들은 평가를 위해 여성 보컬리스트가 부른 117곡(약 5.89시간)의 중국 만다린 팝송이 포함된 PopCS라는 새로운 데이터 세트를 생성합니다. 그런 다음 노래를 세분화하고 몬트리올 강제 정렬기라는 도구를 사용하여 음소 수준의 정렬을 생성합니다. 원시 오디오에서 피치 정보도 추출됩니다. 그런 다음 데이터는 모델 학습을 위해 사전 처리됩니다.
    
    구현 세부 사항: 중국어 가사를 음소로 변환하고 원시 파형에서 멜-스펙트로그램을 추출합니다. 모델의 인코더, 피치 인코더, 노이즈 제거기 및 경계 예측기에 대한 다양한 설정이 조정됩니다. 그런 다음 모델은 '워밍업' 단계와 '메인' 단계의 두 단계로 훈련됩니다. 추론하는 동안, 생성된 멜-스펙트로그램을 파형으로 변환하기 위해 사전 훈련된 모델인 Parallel WaveGAN이 사용됩니다.
    
    주요 결과 및 분석: 생성된 오디오의 품질은 평균 의견 점수(MOS) 방법론을 사용하여 평가됩니다. DiffSinger의 성능을 다른 시스템과 비교한 결과, 다른 시스템보다 우수한 성능을 보여 제안한 방법의 효과를 입증했습니다.
    
    절제 연구: 저자들은 제안된 방법의 효과를 입증하고 모델의 하이퍼파라미터를 미세 조정하기 위해 제거 연구를 수행합니다. 얕은 확산 메커니즘과 경계 예측 네트워크가 결과의 품질에 크게 기여한다는 결론을 내립니다.
    
    TTS에 대한 확장 실험: 저자들은 또한 LJSpeech 데이터 세트를 사용하여 텍스트 음성 변환(TTS) 작업에서 그들의 방법을 테스트합니다. 저자들은 DiffSinger를 수정하여 TTS용 모델인 DiffSpeech를 구축합니다. 그 결과, DiffSpeech가 다른 TTS 시스템보다 성능이 뛰어나다는 것을 보여줌으로써 이 방법의 일반화 가능성을 입증했습니다.
    
    관련 연구: 저자들은 SVS 및 확산 확률 모델 분야의 이전 연구에 대해 논의합니다. 저자들은 자신들의 연구가 이 분야와 어떻게 다르고 어떤 기여를 하는지 강조합니다.
    
    결론: 이 논문은 DiffSinger의 장점과 새로운 얕은 확산 메커니즘을 요약하여 결론을 내립니다. 또한 저자들은 보코더를 사용하지 않는 직접 합성과 같은 잠재적인 미래 작업도 강조합니다.