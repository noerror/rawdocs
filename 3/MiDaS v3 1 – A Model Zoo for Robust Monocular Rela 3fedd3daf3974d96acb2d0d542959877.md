# MiDaS v3.1 – A Model Zoo for Robust Monocular Relative Depth Estimation

[https://arxiv.org/abs/2307.14460](https://arxiv.org/abs/2307.14460)

[https://github.com/isl-org/MiDaS](https://github.com/isl-org/MiDaS)

- Jul 2023

### 1. Introduction

단안 깊이 추정이란 단일 이미지 또는 카메라 뷰에서 물체의 거리를 예측하는 프로세스입니다. 이 기술은 제너레이티브 AI, 3D 재구성 및 자율 주행과 같은 분야에 적용됩니다. 그러나 여러 이미지에 비해 데이터가 부족하기 때문에 하나의 이미지에서 깊이를 예측하는 것은 어렵습니다. MiDaS는 학습 기반 기술을 사용하여 보다 정확하게 깊이를 추정하는 깊이 추정 분야의 선도적인 방법입니다.

대부분의 깊이 추정 모델은 인코더-디코더 구조를 사용합니다. 기존의 컨볼루션 인코더와 함께 처음에는 자연어 처리용으로 설계된 트랜스포머 기반 인코더(예: ChatGPT)가 최근 컴퓨터 비전에서 사용되고 있습니다. 이러한 트랜스포머 기반 인코더는 이전의 컨볼루션 인코더보다 성능이 뛰어나기 때문에 깊이 추정을 위해 MiDaS에 통합되었습니다.

최신 릴리스인 MiDaS v3.1에는 최첨단 백본과 함께 다양한 새로운 깊이 추정 모델이 통합되어 있습니다. 이 연구의 초점은 이러한 백본을 MiDaS에 통합하는 방법을 논의하고, 다양한 v3.1 모델을 비교 및 분석하며, 향후 백본과 함께 MiDaS를 활용할 수 있는 방법에 대한 지침을 제공하는 것입니다.

### 2. Related Work

단안 깊이 추정에는 스케일 모호성을 비롯한 몇 가지 문제가 있습니다. 메트릭 심도를 예측하는 것을 목표로 하는 학습 기반 방법은 균일한 데이터 세트에 대한 지도 학습을 사용하여 이 문제를 해결하려고 시도해 왔습니다. 그러나 이는 종종 과적합으로 이어지고 다양한 환경에서 일반화할 수 있는 모델의 능력을 떨어뜨립니다.

다른 접근 방식인 상대적 깊이 추정(RDE)은 서로에 대해 정확하지만 계량적 의미를 갖지 않는 깊이를 예측합니다. MiDAS와 같은 RDE 모델은 다양한 깊이와 카메라 매개변수를 가진 데이터 세트의 불일치 레이블을 사용하여 학습되므로 일반화 가능성을 향상시키는 데 도움이 됩니다.

RDE 분야에서 핵심적인 역할을 하는 MiDaS는 혼합 데이터 세트를 사용하여 훈련하고, 격차 공간에서 예측하고, 스케일 및 시프트 불변량 손실을 통해 모호성을 처리합니다. 시간이 지남에 따라 더 많은 데이터 세트가 MiDaS에 통합되어 대규모 메타 데이터 세트가 생성되었습니다.

MiDaS의 아키텍처는 표준 인코더-디코더 구조를 따르며, 인코더는 이미지 분류 네트워크를 기반으로 합니다. 초기 MiDaS 모델은 ResNet 기반 아키텍처를 사용했으며, 이후 모델에서는 트랜스포머 기반 백본을 사용했습니다.

최신 버전인 MiDaS v3.1은 새로운 컨볼루션 및 트랜스포머 기반 백본이 MiDaS에 어떻게 통합될 수 있는지 보여줍니다. 또한 이러한 새로운 백본이 어떻게 깊이 추정 성능을 향상시킬 수 있는지도 보여줍니다.

### 3. Methodology

최신 버전의 MiDaS(v3.1)를 개발하는 방법론에 대해 설명합니다. 여기에서는 인코더 백본의 선택, MiDaS 아키텍처로의 통합, 훈련 설정에 대해 다룹니다. 팀은 깊이 추정 품질과 계산 요구 사항을 기준으로 백본을 선택했습니다.

MiDaS v3.1에는 5개의 새로운 유망한 인코더 유형이 선택되었습니다. 주로 높은 깊이 추정 품질 또는 실시간 애플리케이션에 대한 낮은 계산 요구 사항을 기준으로 선택되었습니다. 바로 그 제품들입니다:

- 다양한 변형을 가진 BEiT 트랜스포머: BEiT512-L, BEiT384-L 및 BEiT384-B입니다. 이 숫자는 이차 훈련 해상도를 나타내며, 문자 L과 B는 각각 '대형'과 '기본'을 나타냅니다.
- 스윈 트랜스포머는 스윈-L, 스윈V2-L, 스윈V2-B 변형이 있습니다. 저컴퓨팅 애플리케이션의 경우 SwinV2-T 기반 모델을 제공합니다.
- Next-ViT-L ImageNet-1K-6M 인코더를 기반으로 하는 Next-ViT 모델.
- 변형 LeViT-384가 포함된 LeViT 모델.

특정 백본이 포함된 모델도 검토되었지만 경쟁력이 낮아 출시되지 않았습니다. 여기에는 Next-ViT-L ImageNet-1K, ViT-L Hybrid, DeiT3, MobileViTv2와 같은 트랜스포머 백본이 포함됩니다. ConvNeXt 및 EfficientNet과 같은 컨볼루션 백본도 검토되었지만 최종적으로 포함되지 않았습니다.

MiDaS v3.0 및 v2.1과 같은 이전 MiDaS 릴리스의 백본도 MiDaS v3.1의 레거시 모델로 고려되어 포함되었습니다. 여기에는 바닐라 비전 트랜스포머 백본 ViT-L, ViT-B 하이브리드, ResNeXt-101 32x8d 및 모바일 친화적인 효율적인 net-lite3가 포함됩니다.

새로 사용된 백본은 BEiT512-L, BEiT384-L, BEiT384-B, Swin-L, SwinV2-L, SwinV2-B, SwinV2-T, Next-ViT-L ImageNet1K-6M 및 LeViT-224입니다.

- BEiT: BEiT 백본은 모델 생성을 위해 PyTorch 이미지 모델(timm) 리포지토리를 사용하여 구현됩니다. 구현은 간단하며, 팀 모델 생성 기능으로 생성된 BEiT 인코더당 단일 창 크기를 선택해야 하는 어려움만 있을 뿐입니다. 이 기능은 다양한 입력 해상도를 허용하도록 수정되었습니다.
- Swin: Swin 및 SwinV2 백본은 모두 BEiT 백본과 유사하게 구현됩니다. 그러나 가장 큰 차이점은 Swin과 SwinV2는 계층형 인코더로, 트랜스포머 블록의 구조가 변경된다는 점입니다. 결과적으로 계층 구조 레벨당 하나의 후크가 필요하므로 후크를 선택할 수 있는 옵션이 제한됩니다.
- Next-ViT: Next-ViT 역시 4단계로 구성된 계층형 트랜스포머입니다. 여기서는 계층 수준별 마지막 블록이 후크에 선택됩니다. 후크된 블록의 출력 텐서는 랭크 2가 아닌 랭크 3의 텐서이므로 깊이 디코더 단계에 연결하기 위해 이러한 텐서의 모양을 변경하는 데 추가 연산자가 필요하지 않습니다.

일반적으로 원래 이미지 분류를 위해 학습된 이러한 백본은 깊이 추정을 수행하기 위해 적절하게 수정하고 깊이 디코더에 연결해야 합니다. 백본마다 후크를 배치하고 원본 코드를 수정하는 등 백본을 적절히 통합하기 위해 서로 다른 기술이 필요합니다.

- 백본으로서의 LeViT: LeViT는 세 가지 계층 수준을 기반으로 하는 계층형 인코더입니다. 이 인코더는 해상도 224x224의 이미지를 처리하기 위해 추가 컨볼루션 스템을 사용하여 해상도를 14x14로 낮춥니다. 이러한 감소는 유사한 디컨볼루션 디코더를 깊이 디코더에 삽입하여 상쇄합니다. 이 섹션에서는 처리 단계당 채널 수를 관리하는 방법도 설명합니다.
- 기타 백본: 여기에는 ImageNet-22k에서 사전 학습되고 ImageNet-1K에서 미세 조정된 NextViT-L ImageNet-1K, ViT-L 하이브리드, 바닐라 DeiT3-L, DeiT3-L, MobileViTv2-0.5, MobileViTv2-2.0, ConvNeXt-L, ConvNeXt-XL 및 EfficientNetL2가 포함됩니다. 이러한 모델을 MiDaS에 연결하기 위해 취한 접근 방식도 설명되어 있습니다.
- 훈련 설정: 다중 목표 최적화 사용, 가중치 초기화, 사용된 훈련 데이터 세트 등 훈련 전략에 대해 설명합니다.
- 데이터 세트 믹스: 데이터 세트 믹스는 훈련에 사용되는 서로 다른 데이터 세트의 조합을 의미합니다. 3+10 조합과 5+12 조합의 두 가지 조합에 대해 설명합니다.
- 새로운 백본 사용에 대한 논의: MiDaS 아키텍처에 새로운 백본을 추가하는 전략에 대해 논의합니다. 여기에는 인코더 백본에서 적절한 후크 위치 선택, 텐서 모양 변경, 필요한 경우 계층 구조 부분 또는 깊이 디코더의 헤드 수정, 특정 네트워크 레이어에서 채널 수 조정 등이 포함됩니다.

요약하면, MiDAS는 뎁스 맵 예측을 위한 다양한 백본 모델을 수용할 수 있는 유연한 아키텍처이며, 이 문서에서는 이러한 모델을 통합하는 데 관련된 기술적 고려 사항을 설명합니다.

### 4. Experiments

모델 평가는 6가지 데이터 세트에 대해 수행되었습니다: DIW, ETH3D, Sintel, KITTI, NYU Depth v2, TUM. 각 데이터 세트에는 고유한 오차 계산 방법이 있었으며, 이는 원래 MiDaS 논문에서 설정되었습니다. 모델 간 비교는 MiDaS v3.0에서 가장 큰 모델인 ViT-L 384의 상대적 개선도를 통해 이루어졌습니다. 개선은 6개 데이터 세트 모두에 대한 평균 상대 제로샷 오차로 계산되었습니다.

이 연구에 따르면 MiDaS v3.1의 일부로 출시된 모델인 BEiT512-L이 정사각형 및 비제약 해상도 모두에서 가장 우수한 모델로 나타났습니다. MiDaS v3.1은 경량 모델을 포함하여 잠재적인 다운스트림 작업을 위한 더 많은 솔루션을 제공하기 위해 이전 버전보다 더 많은 모델을 도입했습니다.

깊이 추정 품질이 낮아 출시되지 않았던 몇 가지 모델도 분석되었습니다. 그 중 하나는 데이터 세트 구성 3+10으로 학습된 Swin-L입니다. 데이터 세트 수를 늘리면 품질 측정값이 크게 향상되는 것으로 나타났습니다. 1,300만 개의 매개변수를 가진 가장 작은 모델인 MobileViTv2-0.5는 품질이 너무 낮아 관련성이 떨어지는 것으로 나타났습니다.

이 연구에서는 트랜스포머 및 컨볼루션 인코더 백본이 있는 모델도 다루었지만, BEiT384-L의 품질을 능가하는 모델은 없었기 때문에 첫 번째 단계 이상으로 훈련하지 않았습니다. 컨볼루션 모델인 ConvNeXt-XL, ConvNeXt-L, EfficientNet-L2를 검토했지만 깊이 추정 품질에서 BEiT384-L을 이길 수 없어 폐기했습니다.

저자들은 모델과 다양한 수정이 성능에 미치는 영향을 더 깊이 이해하기 위해 일련의 절제 연구를 수행했습니다. 다음은 주요 연구 결과를 요약한 것입니다:

![FPS에 대한 개선도. 이 그림은 초당 프레임 수에 대한 MiDaS v3.0의 가장 큰 모델 DPTL 384(=ViT-L 384)에 대한 MiDaS v3.1의 모든 모델의 개선도를 보여줍니다. 프레임 속도는 RTX 3090 GPU에서 측정됩니다. 버블이 커버하는 영역은 해당 모델의 매개 변수의 수에 비례합니다. 모델 설명에서는 MiDaS 버전을 제공하며, 이는 MiDaS v3.1의 일부 모델이 이전 MiDaS 릴리스에서 이미 도입된 레거시 모델이기 때문입니다. 모델 이름의 첫 번째 3자리 숫자는 항상 정사각형 해상도인 훈련 해상도를 반영합니다. 두 개의 BEiT 모델의 경우, 추론 해상도가 훈련 해상도와 다르므로 모델 설명 끝에 추론 해상도를 제공합니다. 개선은 6개의 데이터셋에 대한 상대적인 제로샷 오류의 평균으로 Sec. 4.1에서 설명하였습니다.](MiDaS%20v3%201%20%E2%80%93%20A%20Model%20Zoo%20for%20Robust%20Monocular%20Rela%203fedd3daf3974d96acb2d0d542959877/Untitled.png)

FPS에 대한 개선도. 이 그림은 초당 프레임 수에 대한 MiDaS v3.0의 가장 큰 모델 DPTL 384(=ViT-L 384)에 대한 MiDaS v3.1의 모든 모델의 개선도를 보여줍니다. 프레임 속도는 RTX 3090 GPU에서 측정됩니다. 버블이 커버하는 영역은 해당 모델의 매개 변수의 수에 비례합니다. 모델 설명에서는 MiDaS 버전을 제공하며, 이는 MiDaS v3.1의 일부 모델이 이전 MiDaS 릴리스에서 이미 도입된 레거시 모델이기 때문입니다. 모델 이름의 첫 번째 3자리 숫자는 항상 정사각형 해상도인 훈련 해상도를 반영합니다. 두 개의 BEiT 모델의 경우, 추론 해상도가 훈련 해상도와 다르므로 모델 설명 끝에 추론 해상도를 제공합니다. 개선은 6개의 데이터셋에 대한 상대적인 제로샷 오류의 평균으로 Sec. 4.1에서 설명하였습니다.

- 레퍼런스 모델 비교: 성능의 차이를 평가하기 위해 다양한 훈련 조건에서 네 가지 참조 모델(BEiT384-L, Next-ViT-L-1K-6M, Swin-L 및 ViTL)을 분석했습니다. 그 결과, 다양한 구성에서 깊이 추정 품질이 대체로 일관되게 유지되는 것을 확인했습니다.
- ViT-L 반전: 연구팀은 바닐라 비전 트랜스포머 백본인 ViT-L의 '후크' 순서를 뒤집어 테스트했는데, 놀랍게도 깊이 추정 품질에 큰 영향을 미치지 않았습니다. 이는 디코더가 인코더의 트랜스포머 블록에 연결되는 순서가 출력에 큰 영향을 미치지 않는다는 것을 시사합니다.
- Swin-L 등거리: 이 실험은 스윈 트랜스포머의 후크 사이의 거리를 균일화하는 것을 목표로 했습니다. 그 결과 깊이 추정 품질이 약간 저하되었지만 두 개의 독립적인 훈련 실행 간의 차이가 미미한 것으로 나타나 이러한 변화가 크지 않음을 알 수 있습니다.
- BEiT384-L Wide: 인코더의 시작 부분에서 후크 갭을 제거한 결과 깊이 추정 품질에 거의 영향을 미치지 않는 것으로 나타났습니다. 제약이 없는 해상도에서 관찰된 약간의 개선은 정사각형 해상도의 약간의 감소로 상쇄되어 후크 갭을 유지하게 되었습니다.

![백본 비교. 이 표는 상단, 왼쪽의 예제 RGB 입력 이미지에 대한 MiDaS v3.1의 다양한 모델, 포함된 레거시 모델들의 역상대적 깊이 맵을 보여줍니다. 색상이 밝을수록 역상대적 깊이가 크며, 이는 카메라에 가까운 객체를 나타냅니다. 각 깊이 맵의 왼쪽 하단에는 모델의 이름이 표시되어 있습니다. 이는 MiDaS 버전, 백본 이름과 크기, 그리고 훈련 해상도를 포함합니다. 정사각형 해상도에서만 평가되는 모델들은 흰색 텍스트 끝의 사각형 기호로 표시됩니다. 하단 행의 두 번째 마지막 모델은 OpenVINO 모델입니다.](MiDaS%20v3%201%20%E2%80%93%20A%20Model%20Zoo%20for%20Robust%20Monocular%20Rela%203fedd3daf3974d96acb2d0d542959877/Untitled%201.png)

백본 비교. 이 표는 상단, 왼쪽의 예제 RGB 입력 이미지에 대한 MiDaS v3.1의 다양한 모델, 포함된 레거시 모델들의 역상대적 깊이 맵을 보여줍니다. 색상이 밝을수록 역상대적 깊이가 크며, 이는 카메라에 가까운 객체를 나타냅니다. 각 깊이 맵의 왼쪽 하단에는 모델의 이름이 표시되어 있습니다. 이는 MiDaS 버전, 백본 이름과 크기, 그리고 훈련 해상도를 포함합니다. 정사각형 해상도에서만 평가되는 모델들은 흰색 텍스트 끝의 사각형 기호로 표시됩니다. 하단 행의 두 번째 마지막 모델은 OpenVINO 모델입니다.

- 데이터 세트별 변경 사항: 저자들은 KITTI 데이터세트를 기본 고종횡비에서 학습할 때 관찰되는 높은 δ1 값을 해결하기 위해 세 가지 수정 사항을 살펴봤습니다. 그 결과, 원래 해상도(BEiT384-L 5+12+12K 및 BEiT384-L 5K+12K 모델)에서 KITTI를 훈련하면 전반적인 모델 품질이 약간 개선되고 δ1 값이 현저히 감소한다는 사실을 발견했습니다. 이 접근 방식을 모든 훈련 데이터 세트(모델 BEiT384-L 5A+12A)로 확장한 결과, 제약이 없는 해상도에서 δ1 점수가 가장 우수하고 상대적인 개선이 가장 높았지만 정사각형 해상도에서는 성능이 저하되어 과적합 가능성이 있음을 시사했습니다.

전반적으로 이러한 제거 연구는 모델에 대한 더 깊은 이해를 제공하고 향후 개선할 수 있는 잠재적인 영역을 강조했습니다. 관찰된 성능 차이는 훈련 과정과 데이터 세트별 변수가 모델의 효과에 영향을 미칠 수 있음을 시사합니다.

### 5. Applications

높은 상대적 깊이 추정 정확도, 견고성, 환경 전반에 걸친 일반화 가능성으로 인해 MiDaS v3.1 모델 제품군의 응용 분야는 다양합니다. 이 모델은 상대 및 메트릭 깊이 추정, 이미지 합성, 텍스트-RGBD 생성을 결합한 아키텍처를 비롯한 다양한 애플리케이션에 적합합니다.

깊이 출력이 메트릭 스케일로 정확해야 하는 실제 애플리케이션에서 메트릭 깊이 추정의 경우, 깊이 출력은 스케일과 시프트까지만 정확하기 때문에 MiDaS 모델만으로는 충분하지 않습니다. 이 문제를 해결하기 위해 두 가지 접근 방식이 모색되었습니다. 첫 번째는 단안 시각 관성 수심 추정에서 볼 수 있듯이, 시각 관성 오도메트리와 MiDaS와 같은 수심 모델을 통합하여 메트릭 스케일로 밀도 있는 수심 추정치를 생성하는 것입니다. 두 번째 접근 방식인 조이뎁스는 순전히 시각적 데이터 기반 방법을 사용하여 상대적 수심과 메트릭 수심 추정을 결합합니다.

또한 MiDaS는 이미지 간 생성을 위한 형태 보존 모델을 제공하기 위해 Stable Diffusion에 통합되었습니다. MiDaS의 깊이 출력은 입력 이미지의 의미적 모양을 유지하면서 다양한 예술적 스타일의 출력 샘플을 생성하기 위해 확산 모델을 조정하는 데 사용됩니다.

또한 텍스트-이미지 확산 공간에서는 텍스트 프롬프트에서 조인트 이미지와 깊이 데이터를 생성하는 3D용 잠복 확산 모델(LDM3D)이 개발되었습니다. 이 모델은 캡션, RGB 이미지, 깊이 맵이 포함된 튜플 데이터 세트에 대해 미세 조정된 사전 학습된 안정적 확산 모델을 사용하며, MiDaS를 사용하여 얻은 깊이 맵을 사용합니다. 이를 통해 LDM3D는 텍스트 프롬프트에서 사실적이고 몰입감 있는 360도 뷰를 생성할 수 있습니다. MiDaS v3.1 모델의 잠재적 통합으로 LDM3D 뎁스 출력과 후속 씬 뷰 생성의 품질이 더욱 향상될 수 있습니다.

### 6. Conclusion

결론에서는 새로운 MiDaS v3.1 릴리스의 발전과 기능을 요약합니다. 이 새로운 릴리스에서는 컨볼루션 백본에 대한 초기 탐색에도 불구하고 트랜스포머 기반 백본을 활용하는 데 상당한 진전을 이룬 강력한 깊이 추정 모델 세트를 제공합니다.

사용된 트랜스포머 백본에는 BEiT, Swin, SwinV2, Next-ViT, LeViT가 포함되며, BEiT와 SwinV2에는 여러 가지 변형이 제공됩니다. 특히 512x512 해상도의 BEiT512-L은 특히 비정사각형 해상도의 경우 이전 버전인 MiDaS v3.0보다 28% 더 정확도가 높은 것으로 나타났습니다.

또한 새 릴리스에서는 MiDaS의 훈련 데이터 세트가 기존 10개에서 12개로 확장되어 BTS 분할을 사용하는 KITTI 및 NYU Depth V2가 통합되었습니다. 또한 이러한 백본 유형이 MiDaS 아키텍처에 통합되는 방식에 대한 자세한 정보가 제공되어 향후 잠재적인 백본에 대한 일반적인 가이드로 사용할 수 있는 인사이트를 제공합니다.

전반적으로 MiDaS v3.1 릴리스는 깊이 추정 모델을 크게 개선하여 정확도의 한계를 뛰어넘고 잠재적인 적용 영역을 확장합니다.