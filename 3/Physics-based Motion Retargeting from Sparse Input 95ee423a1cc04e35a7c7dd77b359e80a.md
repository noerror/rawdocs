# Physics-based Motion Retargeting from Sparse Inputs

[https://arxiv.org/abs/2307.01938](https://arxiv.org/abs/2307.01938)

![우리의 방법은 헤드셋과 컨트롤러 포즈만을 입력으로 사용하여 실시간으로 다양한 캐릭터의 물리적으로 타당한 포즈를 생성합니다.](Physics-based%20Motion%20Retargeting%20from%20Sparse%20Input%2095ee423a1cc04e35a7c7dd77b359e80a/Untitled.png)

우리의 방법은 헤드셋과 컨트롤러 포즈만을 입력으로 사용하여 실시간으로 다양한 캐릭터의 물리적으로 타당한 포즈를 생성합니다.

### 1 INTRODUCTION

이 연구는 증강현실 및 가상현실(AR/VR) 환경에서 사용자가 자신을 인간이 아닌 캐릭터로 표현하는 방법에 대한 문제를 다룹니다. 현재 AR/VR 시스템에서 제공하는 사용자 포즈 정보가 제한적이고, 대상 캐릭터의 크기와 체형이 다양하며, 키네마틱 애니메이션에서 물리적 개연성이 부족하기 때문에 이 문제는 복잡합니다.

제안한 솔루션은 모방 기반 강화학습 방식입니다. 이 방법은 사용자의 희소 센서 입력을 사용하여 대상 캐릭터의 물리 기반 시뮬레이션을 구동합니다. 이 접근 방식은 공룡의 무거운 꼬리나 쥐의 짧은 다리와 같은 캐릭터의 물리적 속성을 설명합니다. 이 시스템의 훈련에는 사람의 모션 캡처 데이터만 필요하며 각 아바타에 대해 아티스트가 만든 애니메이션에 의존하지 않습니다.

대규모 모션 캡처 데이터 세트는 훈련 데이터에 보이지 않는 사용자의 움직임을 실시간으로 추적할 수 있는 일반 정책을 훈련하는 데 사용됩니다. 성공적인 리타겟팅을 위한 주요 기능으로는 발 접촉 보상, 주요 특징의 희소 매핑, 추가 스타일 제어를 제공하는 보상 조건 등이 있습니다.

이 작업은 이러한 기능을 희박한 센서 데이터에서 실시간으로 사람이 아닌 캐릭터의 고품질 모션을 생성할 수 있는 프레임워크에 결합하여 이전에는 볼 수 없었던 독특한 방식으로 구현했습니다. 이 설계의 효과는 다양한 절제 연구를 통해 검증되었습니다.

### 2 RELATED WORK

관련 문헌은 인체 모션 트래킹, 모션 리타겟팅, 물리 기반 캐릭터 시뮬레이션의 세 가지 영역으로 분류할 수 있습니다.

인체 모션 트래킹:
전신 인체 모션 트래킹에는 다양한 기법이 사용되며, 광학 마커 기반 시스템이 가장 정확합니다. 마커리스 및 비전 기반 방식은 SMPL과 같은 신체 모델을 포즈 사전으로 사용하고 포즈 생성을 위해 카메라를 사용합니다. 관성 측정 장치(IMU)와 같은 웨어러블 센서도 또 다른 대안입니다. 하지만 AR/VR 디바이스는 주로 헤드 마운트 디바이스(HMD)와 두 개의 핸드 컨트롤러를 통해 드문드문 센서 데이터를 제공합니다. 이는 특히 하체 및 다리 동작 추적에 한계가 있습니다. 트랜스포머, VAE, 생성 모델을 사용하여 이 문제를 해결하기 위한 기술이 제안되었지만, 이러한 기술은 물리적 특성을 적용하지 못해 모션 아티팩트가 발생하는 경우가 많습니다. 최근의 일부 접근 방식은 강화 학습과 물리학을 사용하여 전신 아바타를 구동하는 정책을 학습함으로써 보다 유망한 방향을 제시합니다.

![우리 시스템의 개요입니다. 정책 𝜋는 퀘스트 센서 입력 𝑜𝑡,𝑢𝑠𝑒𝑟과 시뮬레이션된 캐릭터의 현재 상태 𝑜𝑡,𝑠𝑖𝑚을 관측하고 토크 𝑎𝑡를 계산하여 물리 시뮬레이터에 적용합니다. 훈련 중에는, 우리는 인간 모션 캡처 데이터 𝑠𝑡,𝑔𝑡를 사용하여 시뮬레이션된 캐릭터의 대략적인 포즈 𝑠𝑡,𝑘𝑖𝑛을 추정합니다("키네틱 리타게팅"). 보상은 시뮬레이션된 캐릭터 𝑠𝑡,𝑠𝑖𝑚이 이 대략적인 키네틱 포즈 𝑠𝑡,𝑘𝑖𝑛을 최대한 잘 모방하도록 장려하며, 동시에 시뮬레이터가 부과하는 모든 물리적 제약을 준수하도록 합니다. 정책이 훈련된 후에는 전체 몸 데이터나 키네틱 리타게팅이 더 이상 필요하지 않고, 시뮬레이션된 캐릭터는 HMD와 컨트롤러의 희박한 센서만으로 운영될 수 있습니다.](Physics-based%20Motion%20Retargeting%20from%20Sparse%20Input%2095ee423a1cc04e35a7c7dd77b359e80a/Untitled%201.png)

우리 시스템의 개요입니다. 정책 𝜋는 퀘스트 센서 입력 𝑜𝑡,𝑢𝑠𝑒𝑟과 시뮬레이션된 캐릭터의 현재 상태 𝑜𝑡,𝑠𝑖𝑚을 관측하고 토크 𝑎𝑡를 계산하여 물리 시뮬레이터에 적용합니다. 훈련 중에는, 우리는 인간 모션 캡처 데이터 𝑠𝑡,𝑔𝑡를 사용하여 시뮬레이션된 캐릭터의 대략적인 포즈 𝑠𝑡,𝑘𝑖𝑛을 추정합니다("키네틱 리타게팅"). 보상은 시뮬레이션된 캐릭터 𝑠𝑡,𝑠𝑖𝑚이 이 대략적인 키네틱 포즈 𝑠𝑡,𝑘𝑖𝑛을 최대한 잘 모방하도록 장려하며, 동시에 시뮬레이터가 부과하는 모든 물리적 제약을 준수하도록 합니다. 정책이 훈련된 후에는 전체 몸 데이터나 키네틱 리타게팅이 더 이상 필요하지 않고, 시뮬레이션된 캐릭터는 HMD와 컨트롤러의 희박한 센서만으로 운영될 수 있습니다.

모션 리타겟팅:
모션 리타겟팅은 소스 캐릭터에서 다른 타겟 캐릭터로 움직임을 리매핑하는 작업을 포함합니다. 캐릭터의 형태와 골격이 상당히 다른 경우 특히 복잡해집니다. 포즈 대응을 설정하는 키네마틱 리타기팅 방법부터 출력 모션이 물리 기반 시뮬레이션인 물리 기반 리타기팅 방법까지 다양한 솔루션이 있습니다. 최근 연구에서는 접촉과 같은 제약 조건을 유지하면서 사람과 유사한 스켈레톤의 모션을 리타기팅하는 효율적인 방법을 제안했으며, 일부는 이를 실시간 인간 대 실제 로봇 퍼펫으로 확장하기도 했습니다.

물리 기반 캐릭터 시뮬레이션:
물리 기반 캐릭터의 컨트롤러에 대한 광범위한 작업이 진행되어 왔습니다. 초기에는 반복적인 최적화와 간단한 제어 법칙을 사용하여 강력한 밸런스 피드백을 제공했습니다. 신경망 정책과 심층 강화 학습의 등장으로 새로운 기술을 처음부터 학습하거나 아티스트가 제공한 모션 또는 모션 캡처 클립을 모방하는 것이 쉬워졌습니다. 최근에는 모션 시퀀싱의 유연성, 원하는 모션과 캐릭터 형태에 따라 제어 정책을 학습하는 기능 등이 발전했습니다. 하지만 희박하고 모호할 수 있는 입력 데이터에서 다양한 크기와 비율을 가진 사람이 아닌 물리 기반 캐릭터로 리타게팅하는 것은 여전히 주요 과제로 남아 있습니다.

### 3 METHOD

이 섹션에서는 그림 2에 설명된 대로 시스템의 방법론을 설명합니다. 이 전략은 강화 학습을 활용하여 물리 시뮬레이터의 토크를 생성하는 정책을 학습하는 것입니다. 휴먼 모션 캡처 데이터를 사용하여 정책에 대한 HMD 및 컨트롤러 데이터를 합성하고 훈련 중에 보상 훈련 신호를 구성합니다.

![우리는 세 가지 다른 캐릭터에 대해 우리의 리타게팅 솔루션을 보여줍니다 (왼쪽에서 오른쪽으로): Oppy라는 이름의 쥐 같은 생물, Jesse라는 이름의 인간, 그리고 우리가 Dino라고 부르는 공룡.](Physics-based%20Motion%20Retargeting%20from%20Sparse%20Input%2095ee423a1cc04e35a7c7dd77b359e80a/Untitled%202.png)

우리는 세 가지 다른 캐릭터에 대해 우리의 리타게팅 솔루션을 보여줍니다 (왼쪽에서 오른쪽으로): Oppy라는 이름의 쥐 같은 생물, Jesse라는 이름의 인간, 그리고 우리가 Dino라고 부르는 공룡.

3.1 강화 학습
유니티는 심층 강화 학습(RL)을 활용하여 각 캐릭터에 대한 리타겟팅 정책을 학습합니다. RL에서 제어 정책은 각 시간 단계 t에서 동작을 수행하여 환경 상태 st에 반응하고, 수행한 동작에 따라 정책은 보상 신호 rt = r(st, at)를 받습니다. 딥러닝에서 제어 정책 𝜋𝜃(a|s)는 신경망입니다. 딥 RL의 목표는 방정식 (1)에 정의된 대로 기대 수익을 극대화하는 네트워크 파라미터 𝜃를 발견하는 것입니다. 할인 계수 𝛾 ∈ [0, 1)는 미래 상태를 얼마나 중요하게 여기는지에 영향을 줍니다. 이 최적화 문제는 정책 그라데이션 액터-크리틱 알고리즘인 근사 정책 최적화(PPO) 알고리즘을 사용하여 해결합니다.

3.2 캐릭터
독특한 특징을 가진 세 가지 캐릭터를 대상으로 리타겟팅 솔루션을 시연합니다: 짧은 하체, 큰 머리, 큰 귀, 꼬리를 가진 생쥐 오피, 길고 무거운 꼬리와 머리, 짧은 팔을 가진 키 큰 공룡 디노, 모캡 데이터와 유사한 골격 구조를 가진 인간과 유사한 만화 캐릭터 제시.

3.3 관찰
관찰은 시뮬레이션된 캐릭터 데이터 ot,sim과 사용자의 스파스 센서 데이터 ot,user의 두 부분으로 구성됩니다. 시뮬레이션된 캐릭터의 상태는 시뮬레이션에서 완전히 관찰할 수 있으므로 정책은 센서 신호가 희소하더라도 시뮬레이션된 캐릭터의 전체 상태에 의존할 수 있습니다. 이 관찰에는 관절 각도, 관절 각도 속도, 데카르트 위치 및 캐릭터의 방향이 포함됩니다. 방향은 회전 행렬의 처음 두 열로 구성됩니다.

센서 데이터는 실제 디바이스에서 가져오거나 훈련 데이터에서 합성적으로 생성한 것으로, HMD, 왼쪽 컨트롤러, 오른쪽 컨트롤러의 위치와 방향을 포함합니다. 속도를 추론하기 위해 정책은 두 개의 연속적인 센서 관측을 사용합니다.

3.4 합성 훈련 데이터
훈련 중에는 보상 rt를 계산하는 각 캐릭터의 운동학적 포즈와 짝을 이루는 관찰을 위한 HMD 및 컨트롤러 데이터가 필요합니다. 유니티는 모션캡 머리와 손목 관절을 오프셋하여 HMD와 컨트롤러 데이터를 합성적으로 생성하고, 피험자가 AR/VR 기기를 착용한 것처럼 HMD, 왼쪽 및 오른쪽 컨트롤러의 위치와 방향을 에뮬레이션합니다.

![훈련 데이터는 키네틱 리타게팅을 통해 생성됩니다. 왼쪽 캐릭터는 인간 모션 캡처 데이터입니다. 중간 캐릭터는 선택된 관절 각도를 매칭하여 대략적인 키네틱 리타게팅을 보여줍니다. 이 포즈에는 다리 길이가 다른 것으로 인한 발이 미끄러지는 등의 아티팩트, 자체 충돌, 바닥 충돌, 꼬리와 귀의 움직임이 없습니다. 오른쪽 캐릭터는 모든 물리적 제약을 준수하는 가장 가까운 시뮬레이션 포즈입니다. 인간을 완벽하게 따르지 않는 머리를 주목하십시오. 머리는 무겁고 반응하는 데 더 많은 시간이 필요하며, 미래의 정보에 접근할 수 없으며 과거와 현재의 정보만을 사용할 수 있습니다.](Physics-based%20Motion%20Retargeting%20from%20Sparse%20Input%2095ee423a1cc04e35a7c7dd77b359e80a/Untitled%203.png)

훈련 데이터는 키네틱 리타게팅을 통해 생성됩니다. 왼쪽 캐릭터는 인간 모션 캡처 데이터입니다. 중간 캐릭터는 선택된 관절 각도를 매칭하여 대략적인 키네틱 리타게팅을 보여줍니다. 이 포즈에는 다리 길이가 다른 것으로 인한 발이 미끄러지는 등의 아티팩트, 자체 충돌, 바닥 충돌, 꼬리와 귀의 움직임이 없습니다. 오른쪽 캐릭터는 모든 물리적 제약을 준수하는 가장 가까운 시뮬레이션 포즈입니다. 인간을 완벽하게 따르지 않는 머리를 주목하십시오. 머리는 무겁고 반응하는 데 더 많은 시간이 필요하며, 미래의 정보에 접근할 수 없으며 과거와 현재의 정보만을 사용할 수 있습니다.

유니티의 시스템은 특정 캐릭터마다 아티스트가 생성한 애니메이션을 훈련 데이터로 사용할 필요가 없습니다. 대신 기존의 휴먼 모션 캡처 데이터를 재사용하고 시뮬레이션된 캐릭터의 형태에 맞춰 스킨을 대략적인 키네마틱 리타기팅을 수행합니다. 이렇게 하면 크리처의 모션에 대한 대략적인 추정치를 얻을 수 있습니다. 이 모션에는 많은 아티팩트가 존재하지만 시뮬레이션된 캐릭터를 훈련하기 위한 보상 신호로 사용할 수 있습니다. 그런 다음 시뮬레이션에 의해 부과된 물리적 제약이 나머지 아티팩트를 제거합니다.

강화 학습을 통해 시뮬레이션 환경에서 캐릭터 동작을 생성하는 시스템에 대한 자세한 설명으로 보입니다. 이 시스템은 물리 법칙을 준수하면서 사람의 동작을 최대한 가깝게 모방하는 시뮬레이션 캐릭터를 생성하도록 설계되었습니다.

보상: 시스템의 보상 기능은 모방, 접촉, 동작 정규화의 세 가지 구성 요소로 이루어져 있습니다. 각 구성 요소에는 각기 다른 조건이 있으며, 각 조건은 중요도에 따라 가중치가 부여됩니다. 모방 보상은 관절 각도와 속도, 위치 좌표, 방향 등의 요소를 고려하여 시뮬레이션된 캐릭터를 운동학적으로 리타기팅된 기준 데이터 포즈에 일치시킵니다. 접촉 보상은 시뮬레이션된 캐릭터와 사람의 발 접촉이 일치하는지 확인합니다. 동작 보상은 캐릭터가 소비하는 총 에너지를 최소화하는 것을 목표로 하는 정규화 용어입니다.

![캐릭터 크기가 사용자와 일치하면 캐릭터와 사용자 사이의 관절 각도와 발 접촉이 더 유사하게 됩니다(왼쪽). 시뮬레이션된 캐릭터가 매우 다른 형태를 가지고 있을 경우 (예: 여기서는 훨씬 작음), 키네틱 리타게팅된 포즈의 정확성이 떨어지며, 시뮬레이션된 캐릭터가 물리적으로 유효한 움직임을 생성하기 위해 대부분 무시됩니다. 여기서 캐릭터는 인간의 한 걸음을 일치시키기 위해 많은 걸음을 걸어야 합니다.](Physics-based%20Motion%20Retargeting%20from%20Sparse%20Input%2095ee423a1cc04e35a7c7dd77b359e80a/Untitled%204.png)

캐릭터 크기가 사용자와 일치하면 캐릭터와 사용자 사이의 관절 각도와 발 접촉이 더 유사하게 됩니다(왼쪽). 시뮬레이션된 캐릭터가 매우 다른 형태를 가지고 있을 경우 (예: 여기서는 훨씬 작음), 키네틱 리타게팅된 포즈의 정확성이 떨어지며, 시뮬레이션된 캐릭터가 물리적으로 유효한 움직임을 생성하기 위해 대부분 무시됩니다. 여기서 캐릭터는 인간의 한 걸음을 일치시키기 위해 많은 걸음을 걸어야 합니다.

종료: 캐릭터가 넘어져 상체가 땅에 닿는 등 회복 불가능한 상태가 되거나 캐릭터 루트 위치가 모션 캡처 데이터의 스케일링된 루트로부터 30cm 이상 벗어나는 두 가지 조건에서 환경이 리셋됩니다. 모션 궤적의 초기 부분에 대한 편향을 완화하기 위해 캐릭터는 500스텝마다 재설정됩니다.

제어 정책 학습: 시뮬레이션된 각 캐릭터에 대한 제어 정책은 토크 값을 출력한 다음 각 조인트의 최소 및 최대 토크 값에 따라 재조정됩니다. 이 정책은 PPO(근거리 정책 최적화) 및 PyTorch 자동 차별화 소프트웨어를 사용하여 학습되며, 물리학은 NVIDIA PhysX Isaac Gym 물리 시뮬레이터를 사용하여 시뮬레이션됩니다.

전반적으로 이 시스템은 시뮬레이션된 캐릭터의 사실적인 동작을 생성하는 정교하고 유연한 접근 방식을 제공하며 비디오 게임, 영화 및 기타 형태의 애니메이션에 잠재적으로 응용할 수 있습니다.

### 4 RESULTS

이 섹션에서는 강화 학습 모델을 사용하여 수행한 실험의 결과를 설명합니다. 훈련은 12코어 CPU와 NVIDIA RTX 2070 GPU가 탑재된 머신에서 진행되었습니다. 모델은 24시간 동안 훈련되었으며, 이는 약 60억 개의 환경 단계에 해당합니다. 이 모델은 사내 데이터 세트와 Ubisoft La Forge Animation(LaFAN1) 데이터 세트의 두 가지 모션 캡처 데이터 세트로 테스트되었습니다.

![오른쪽 Dino는 방향 보상을 가지고 있으며, 왼쪽 Dino는 그렇지 않습니다. 사용자가 머리를 돌리면, 오른쪽 Dino는 더 가까이 따릅니다.](Physics-based%20Motion%20Retargeting%20from%20Sparse%20Input%2095ee423a1cc04e35a7c7dd77b359e80a/Untitled%205.png)

오른쪽 Dino는 방향 보상을 가지고 있으며, 왼쪽 Dino는 그렇지 않습니다. 사용자가 머리를 돌리면, 오른쪽 Dino는 더 가까이 따릅니다.

![희박한 감각 입력으로 실시간으로 제어되는 세 캐릭터의 프레임 시퀀스를 보여줍니다. 하반신의 움직임은 사용자의 것과 완벽하게 일치하며 발 접촉은 정확하게 추정됩니다. 더 많은 결과를 보려면 동반하는 비디오를 참조하십시오.](Physics-based%20Motion%20Retargeting%20from%20Sparse%20Input%2095ee423a1cc04e35a7c7dd77b359e80a/Untitled%206.png)

희박한 감각 입력으로 실시간으로 제어되는 세 캐릭터의 프레임 시퀀스를 보여줍니다. 하반신의 움직임은 사용자의 것과 완벽하게 일치하며 발 접촉은 정확하게 추정됩니다. 더 많은 결과를 보려면 동반하는 비디오를 참조하십시오.

주요 내용은 다음과 같습니다:

헤드셋과 컨트롤러를 사용한 실시간 리타기팅: 연구진은 이 모델을 사용하여 헤드셋과 컨트롤러 데이터만으로 다양한 캐릭터를 실시간으로 제어할 수 있었습니다. 이 모델은 상체의 단 세 지점에서 사용자의 하체 포즈를 정확하게 예측하여 다양한 형태를 가진 캐릭터로 변환할 수 있었습니다. 또한 가상 캐릭터가 물리적 동작을 존중하고 흔들림, 발 미끄러짐, 관통 등의 문제가 발생하지 않는다는 사실도 발견했습니다. 또한 이 모델은 훈련 세트에 포함되지 않은 사용자와 다양한 키의 사용자에게도 일반화할 수 있었습니다.

![왼쪽 Dino의 꼬리는 2개의 활성 관절과 나머지 6개의 수동 관절을 가지고 있습니다; 중앙 Dino의 꼬리는 2개의 활성 관절과 나머지 6개가 고정되어 있습니다; 오른쪽 Dino의 꼬리는 완전히 수동입니다.](Physics-based%20Motion%20Retargeting%20from%20Sparse%20Input%2095ee423a1cc04e35a7c7dd77b359e80a/Untitled%207.png)

왼쪽 Dino의 꼬리는 2개의 활성 관절과 나머지 6개의 수동 관절을 가지고 있습니다; 중앙 Dino의 꼬리는 2개의 활성 관절과 나머지 6개가 고정되어 있습니다; 오른쪽 Dino의 꼬리는 완전히 수동입니다.

헤드셋만 사용한 리타겟팅: 두 개의 컨트롤러 없이 헤드마운트 디바이스(HMD)만 사용할 수 있는 시나리오에서도 모델은 원포인트 입력으로 전신 포즈를 예측하고 가상 캐릭터를 제어할 수 있을 만큼 강력했습니다. 컨트롤러를 사용할 때보다 품질은 떨어졌지만, 보이지 않는 사용자로부터 실시간 사용자 데이터를 리타기팅할 수 있었습니다.

보상 구성 요소 제거: 연구진은 좋은 모션을 얻기 위해 몇 가지 보상 구성 요소가 필수적이라는 사실을 발견했습니다. 특히 접촉 보상 요소는 캐릭터의 걸음걸이 스타일을 형성하는 데 중요한 역할을 했으며, 방향 보상은 머리와 뿌리 방향을 모방하는 데 필수적이었으며, 이를 통해 사용자의 머리와 전반적인 움직임을 더욱 충실하게 추적할 수 있었습니다.

전반적으로 이러한 결과는 입력이 제한된 시나리오에서도 사람의 움직임 데이터를 기반으로 가상 캐릭터를 실시간으로 효율적으로 제어할 수 있는 모델의 능력을 보여줍니다. 이는 시스템의 견고함과 실용적인 유용성을 보여줍니다.

### 5 DISCUSSION

물리 기반 제어: 물리 엔진은 포즈 정보가 없는 모델 부분의 모션을 구동하며, 기본 스켈레톤 설명이 선행 역할을 합니다. 디노 캐릭터 모델의 꼬리에는 고정, 수동 작동 또는 정책에 따라 능동적으로 제어되는 등 다양한 스타일 옵션을 선택할 수 있습니다.

스타일 제어: 이 모델은 다양한 파라미터 세트에 강력하며 다양한 스타일로 합리적인 모션 컨트롤러를 출력할 수 있습니다. 접촉 보상과 캐릭터의 크기를 변경하면 걸음걸이 스타일을 수정할 수 있습니다. 키네마틱 리타기팅된 모션의 주요 관절을 조정하면 스타일을 추가로 수정할 수 있습니다.

비대칭 관찰의 중요성: 훈련 중에 값 함수는 정책에 비해 더 풍부한 관찰을 받습니다. 미래와 전신 포즈가 없는 값 함수로 정책을 학습하면 달리기와 같은 복잡한 동작에서 실패하고 보이지 않는 사용자 데이터로 일반화할 수 없는 덜 강력한 정책이 만들어지는 것으로 밝혀졌습니다.

오픈소스 데이터 세트의 품질: 사내 데이터 세트와 오픈 소스 데이터 세트 모두 강력하고 유능한 모델을 생성했지만, 데이터 세트가 더 크고 다양할수록 최종 모션의 품질이 더 높았습니다. 사용된 데이터 세트에 관계없이 모델은 보이지 않는 사용자에게도 일반화할 수 있었고 헤드셋만으로 실시간으로 작동할 수 있었습니다.

전반적으로 이 논의에서는 VR에서 모션을 리타겟팅하기 위한 강화 학습 모델의 다양성과 견고성, 파라미터와 스타일 변화에 대한 반응성, 훈련 중 비대칭 관찰의 이점, 데이터 세트 품질이 최종 모션에 미치는 영향에 대해 설명합니다.

### 6 CONCLUSIONS

이 글에서는 특히 캐릭터의 크기와 형태가 크게 다르고 AR/VR 디바이스의 희박한 모션 데이터에 기반한 실시간 리매핑이 필요한 경우 사용자의 모션을 시뮬레이션된 캐릭터에 리타기팅하는 방법을 요약합니다. 이들은 효과적인 리타겟팅을 위해 비대칭 액터-크리틱 강화 학습(RL) 정책에 따라 구동되는 물리 기반 시뮬레이션을 사용했습니다. 도입된 일반적인 보상 설명은 감독 정도를 조정할 수 있으며 다양한 캐릭터 형태에 맞게 조정할 수 있습니다.

이 논문은 또한 사용 가능한 추적 정보의 양, 접촉 보상의 영향, 꼬리 및 귀와 같은 보조 동작과 관련된 선택 등 다양한 매개변수와 디자인 선택의 영향에 대한 인사이트를 제공했습니다.

그러나 이러한 작업에는 한계가 있습니다. 컨트롤러는 특히 사용자가 빠르고 역동적인 동작을 수행하거나 상체와 하체가 서로 연관되지 않은 동작을 수행하는 경우와 같이 까다로운 동작 시퀀스를 추적하지 못합니다. 이러한 시나리오에서 키네마틱 기반 컨트롤러는 여전히 모션을 생성할 수 있지만 품질이 높지는 않으며 모션이 쉬워지면 보정할 수 있습니다.

향후에는 파이프라인을 두 단계로 나누어 먼저 네트워크가 전신 포즈를 예측한 다음 고주파 컨트롤러가 토크를 출력하는 접근 방식이 포함될 수 있습니다. 이렇게 하면 키네마틱 기반 시스템의 장점을 일부 되찾을 수 있습니다. 현재 프레임워크는 사용자가 더 복잡한 형태의 자기 표현을 할 수 있게 해주지만, 다양한 타깃 스켈레톤과 관련해서는 여전히 해결해야 할 복잡성이 있습니다.

캐릭터의 복잡성을 높이기 위한 잠재적인 방법으로는 정책에 스켈레톤 정보를 제공하거나, 그래프 신경망을 사용하여 유연한 정책을 학습하거나, 소스 스켈레톤과 타겟 스켈레톤을 매핑하기 위해 보조 네트워크를 훈련하는 방법이 있습니다.

### Appendix

A. 보상 세부 사항: 다양한 모방 보상 구성 요소에 대한 거리 메트릭은 시뮬레이션된 캐릭터 상태 값과 (모션 캡처 데이터 세트의) 기준 포즈 사이의 유클리드 거리의 가중치 합으로 정의됩니다. 가중치는 캐릭터와 보상 구성 요소에 따라 다릅니다. 사람의 모양과 크기를 더 닮은 캐릭터의 경우 모방 보상에 더 많이 의존합니다. 방향 거리 메트릭도 쿼터니언 메서드를 사용하여 계산됩니다.

B. 프록시멀 정책 최적화: 이 섹션에서는 정책 최적화에 사용되는 방법에 대해 설명합니다. 경험 튜플은 상태, 액션, 다음 상태 및 보상으로 정의됩니다. 궤적은 이러한 경험 튜플의 일련의 집합입니다. 가치 함수 네트워크는 각 상태에 대해 예상되는 미래 수익을 계산하며, 재귀적 특성으로 인해 지도 학습을 사용하여 최적화할 수 있습니다. PPO 방법은 가치 함수를 사용하여 이점을 계산한 다음 정책을 학습하는 데 사용합니다.

C. 학습 매개변수: (이 요약에서는 제공되지 않음)

D. 토크 제한: 각 캐릭터의 관절에 대해 토크 제한 스케일이 설정됩니다. 값이 명시적으로 제공되지 않으면 스케일이 적용되지 않습니다. 오피의 꼬리와 귀, 다이노 꼬리의 마지막 6개 관절과 같은 캐릭터의 특정 요소는 수동적으로 작동하므로 토크 값을 사용할 수 없습니다.