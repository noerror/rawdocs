# Image Style Transfer Using Convolutional Neural Networks

[https://rn-unison.github.io/articulos/style_transfer.pdf](https://rn-unison.github.io/articulos/style_transfer.pdf)

1. Introduction

"컨볼루션 신경망을 이용한 이미지 스타일 전송"이라는 제목의 이 논문 요약 섹션에서는 대상 이미지의 의미적 내용을 보존하면서 소스 이미지의 텍스처를 합성하는 문제인 텍스처 전송의 기존 방법에 대해 설명합니다. 저자는 이러한 방법이 인상적인 결과를 얻을 수 있지만 대상 이미지의 저수준 이미지 특징만을 텍스처 전송에 사용하기 때문에 근본적인 한계가 있다고 지적합니다.

이상적으로 스타일 전송 알고리즘은 대상 이미지에서 높은 수준의 시맨틱 콘텐츠(예: 오브젝트 또는 전체 장면)를 추출하고 이 정보를 사용하여 텍스처 전송을 안내하여 콘텐츠를 소스 이미지의 스타일로 렌더링할 수 있어야 합니다. 이를 위해서는 시맨틱 이미지 콘텐츠와 스타일의 변형을 별도로 모델링하는 알고리즘이 필요합니다.

자연 이미지에서 콘텐츠와 스타일을 분리하는 것은 어려운 문제입니다. 그러나 저자들은 최근 심층 컨볼루션 신경망(CNN)의 발전으로 이미지에서 높은 수준의 의미 정보를 추출하는 방법을 학습할 수 있는 강력한 컴퓨터 비전 시스템이 등장했다고 지적합니다. 물체 인식과 같은 특정 작업에 대해 훈련된 CNN은 질감 인식, 예술적 스타일 분류 등 다양한 데이터 세트와 작업에 걸쳐 전송할 수 있는 일반적인 특징 표현으로 높은 수준의 이미지 콘텐츠를 추출할 수 있습니다.

![컨볼루션 신경망(CNN)의 이미지 표현. 주어진 입력 이미지는 CNN의 각 처리 단계에서 필터링된 이미지 집합으로 의 집합으로 표현됩니다. 처리 계층 구조에 따라 다양한 필터의 수가 증가하지만 필터링된 이미지의 크기는 이미지의 크기는 일부 다운샘플링 메커니즘(예: 최대 풀링)에 의해 감소하여 네트워크의 계층당 총 단위 수가 감소합니다. 네트워크. 콘텐츠 재구성. 입력을 재구성하여 CNN의 여러 처리 단계에 있는 정보를 시각화할 수 있습니다. 이미지를 재구성하여 CNN의 여러 처리 단계에서 정보를 시각화할 수 있습니다. 'conv1 2'(a) 레이어에서 입력 이미지를 재구성합니다, 'conv2 2'(b), 'conv3 2'(c), 'conv4 2'(d), 'conv5 2'(e)로부터 원본 VGG-Network의 레이어를 재구성합니다. 하위 레이어에서 재구성한 결과 거의 완벽합니다(a-c). 네트워크의 상위 레이어에서는 이미지의 높은 수준의 콘텐츠는 보존되는 반면 세부적인 픽셀 정보는 손실됩니다. (d,e). 스타일 재구성. 원래 CNN 활성화에 더해 입력 이미지의 텍스처 정보를 캡처하는 특징 공간을 사용합니다. 특징 공간을 사용합니다. 스타일 표현은 CNN의 여러 레이어에서 서로 다른 특징 간의 상관관계를 계산합니다. 입력 이미지의 스타일을 CNN 레이어의 서로 다른 하위 집합에 구축된 스타일 표현에서 입력 이미지의 스타일을 재구성합니다('conv1 1'(a), 'conv1 1' 및 'conv2 1' (b), 'conv1 1', 'conv2 1', 'conv3 1' (c), 'conv1 1', 'conv2 1', 'conv3 1', 'conv4 1' (d), 'conv1 1', 'conv2 1', 'conv3 1', 'conv4 1' 및 'conv5 1'(e). 이렇게 하면 주어진 이미지의 스타일과 일치하는 이미지가 점진적으로 생성되는 동시에 장면의 정보를 버립니다.](Image%20Style%20Transfer%20Using%20Convolutional%20Neural%20Ne%2032b476ad38134533a27829e01c370467/Untitled.png)

컨볼루션 신경망(CNN)의 이미지 표현. 주어진 입력 이미지는 CNN의 각 처리 단계에서 필터링된 이미지 집합으로 의 집합으로 표현됩니다. 처리 계층 구조에 따라 다양한 필터의 수가 증가하지만 필터링된 이미지의 크기는 이미지의 크기는 일부 다운샘플링 메커니즘(예: 최대 풀링)에 의해 감소하여 네트워크의 계층당 총 단위 수가 감소합니다. 네트워크. 콘텐츠 재구성. 입력을 재구성하여 CNN의 여러 처리 단계에 있는 정보를 시각화할 수 있습니다. 이미지를 재구성하여 CNN의 여러 처리 단계에서 정보를 시각화할 수 있습니다. 'conv1 2'(a) 레이어에서 입력 이미지를 재구성합니다, 'conv2 2'(b), 'conv3 2'(c), 'conv4 2'(d), 'conv5 2'(e)로부터 원본 VGG-Network의 레이어를 재구성합니다. 하위 레이어에서 재구성한 결과 거의 완벽합니다(a-c). 네트워크의 상위 레이어에서는 이미지의 높은 수준의 콘텐츠는 보존되는 반면 세부적인 픽셀 정보는 손실됩니다. (d,e). 스타일 재구성. 원래 CNN 활성화에 더해 입력 이미지의 텍스처 정보를 캡처하는 특징 공간을 사용합니다. 특징 공간을 사용합니다. 스타일 표현은 CNN의 여러 레이어에서 서로 다른 특징 간의 상관관계를 계산합니다. 입력 이미지의 스타일을 CNN 레이어의 서로 다른 하위 집합에 구축된 스타일 표현에서 입력 이미지의 스타일을 재구성합니다('conv1 1'(a), 'conv1 1' 및 'conv2 1' (b), 'conv1 1', 'conv2 1', 'conv3 1' (c), 'conv1 1', 'conv2 1', 'conv3 1', 'conv4 1' (d), 'conv1 1', 'conv2 1', 'conv3 1', 'conv4 1' 및 'conv5 1'(e). 이렇게 하면 주어진 이미지의 스타일과 일치하는 이미지가 점진적으로 생성되는 동시에 장면의 정보를 버립니다.

저자들은 "예술적 스타일의 신경 알고리즘"이라는 새로운 접근 방식을 제안합니다. 이 방법은 고성능 CNN이 학습한 일반적인 특징 표현을 사용하여 이미지의 내용과 스타일을 독립적으로 처리하고 조작합니다. 기본적으로 딥러닝 특징 표현을 사용하여 텍스처 합성을 안내하는 텍스처 전송 알고리즘입니다. 스타일 전송 방법은 단일 신경망 내에서 최적화 문제로 공식화됩니다. 새로운 이미지를 생성하기 위해 예제 이미지의 특징 표현과 일치하도록 사전 이미지 검색이 수행됩니다. 이 접근 방식은 CNN을 기반으로 하는 파라메트릭 텍스처 모델과 이미지 표현을 반전시키는 방법을 결합합니다.

2. Deep image representations

이 섹션에서는 저자들이 이미지 스타일 전송 작업에 사용한 모델에 대해 자세히 설명합니다. 저자들은 VGG 네트워크, 특히 16개의 컨볼루션 레이어와 5개의 풀링 레이어로 구성된 19층 VGG 네트워크의 정규화된 버전을 활용했습니다. 연구진은 최대 풀링을 평균 풀링으로 대체하면 이미지 합성에서 더 매력적인 결과를 얻을 수 있다는 사실을 발견했습니다.

![Untitled](Image%20Style%20Transfer%20Using%20Convolutional%20Neural%20Ne%2032b476ad38134533a27829e01c370467/Untitled%201.png)

저자들은 '콘텐츠 표현'과 '스타일 표현'의 개념을 정의합니다.

콘텐츠 표현은 네트워크의 상위 계층에서 캡처한 높은 수준의 이미지 콘텐츠입니다. 컨볼루션 신경망(CNN)은 객체 인식에 대한 학습을 거치면서 객체 정보를 점진적으로 명시적으로 만드는 이미지 표현을 개발합니다. 즉, 네트워크의 계층이 올라갈수록 입력 이미지가 이미지의 실제 내용에 더 집중하고 정확한 모양에 덜 민감한 표현으로 변환됩니다. 따라서 이러한 상위 레이어는 객체와 객체의 배열 측면에서 이미지의 콘텐츠를 캡처하지만 재구성의 정확한 픽셀 값에 제약을 두지 않습니다.

반면 스타일 표현은 텍스처 정보를 캡처하도록 설계된 특징 공간을 사용하며, 네트워크의 모든 계층에 있는 필터 응답 위에 구축됩니다. 저자들은 이 특징 공간이 그램 행렬에 의해 주어진 서로 다른 필터 응답 간의 상관관계로 구성된다고 설명합니다. 이 스타일 표현은 입력 이미지의 텍스처 정보를 캡처하지만 전역 배열은 고려하지 않습니다.

그런 다음 저자는 '스타일 전송'이라는 개념을 소개합니다. 예술 작품의 스타일을 사진으로 옮기기 위해 사진의 콘텐츠 표현과 예술 작품의 스타일 표현을 동시에 일치시키는 새로운 이미지가 합성됩니다. 이는 콘텐츠 손실과 스타일 손실을 모두 포함하는 손실 함수를 최소화하고 각 요소에 가중치를 부여하여 수행됩니다.

마지막으로, 연구진은 이전의 다른 연구와 달리 합성 결과에서 어떤 형태의 정규화나 이미지 선행도 사용하지 않았다는 점에 주목합니다. 그럼에도 불구하고 네트워크의 하위 레이어에서 텍스처 특징을 스타일 이미지에 대한 특정 이미지 선행으로 볼 수 있다고 제안합니다. 네트워크 아키텍처와 사용된 최적화 알고리즘의 차이로 인해 이미지 합성 결과의 차이가 예상됩니다.

3. Results

이 논문에서는 이미지의 콘텐츠와 스타일 표현을 조작하고 분리하기 위해 컨볼루션 신경망(CNN), 특히 VGG 네트워크를 사용하여 시각적으로 의미 있는 새로운 이미지를 생성하는 방법에 대해 설명합니다. 이 방법에는 한 이미지의 콘텐츠와 다른 이미지의 스타일을 결합하여 새로운 이미지를 만드는 스타일 전송이라는 기술이 사용됩니다.

이 프로세스는 콘텐츠와 스타일 표현을 일치시키기 위해 CNN에서 레이어를 선택하는 것으로 시작됩니다. 레이어 선택은 스타일이 일치하는 스케일과 이미지의 시각적 경험에 영향을 미칩니다. 스타일 표현을 네트워크의 상위 레이어까지 일치시키면 로컬 이미지 구조가 대규모로 보존되어 시각적 경험이 더 부드러워집니다.

한 이미지의 콘텐츠와 다른 이미지의 스타일을 결합한 이미지를 합성할 때 일반적으로 두 가지 제약 조건에 완벽하게 일치하는 이미지는 존재하지 않습니다. 하지만 이미지 합성 시 손실 함수를 사용하면 콘텐츠와 스타일 중 어느 쪽의 강조점을 재구성할지 부드럽게 조절할 수 있습니다.

이 논문에서는 독일 튀빙겐의 강변을 찍은 사진의 내용과 여러 유명 예술 작품의 스타일을 결합한 이미지를 생성하여 이를 보여줍니다. 콘텐츠와 스타일의 강조 비율을 조정하여 시각적으로 매력적인 이미지를 만들 수 있습니다.

화이트 노이즈, 콘텐츠 이미지 또는 스타일 이미지로 시작하는 등 다양한 초기화 방법을 탐색했습니다. 이러한 초기화는 최종 이미지를 초기화의 공간 구조에 편향되게 만들지만 합성 절차의 결과에는 큰 영향을 미치지 않는 것으로 보입니다.

이 논문에서는 한 사진의 스타일을 다른 사진으로 옮기는 포토리얼리스틱 스타일 전송에 대해서도 살펴봅니다. 예를 들어, 밤에 찍은 뉴욕 사진의 스타일을 낮에 찍은 런던 이미지로 옮기는 것입니다. 그 결과 완전히 사실적이지는 않지만 스타일 이미지의 색상과 조명을 상당 부분 포착할 수 있습니다.

4. Discussion

이 백서에서는 컨볼루션 신경망(CNN)을 사용하여 이미지의 원본 콘텐츠를 보존하면서 한 이미지의 스타일을 다른 이미지로 옮기는 방법을 보여줍니다. 유망한 결과에도 불구하고 합성된 이미지의 해상도 및 이미지에 낮은 수준의 노이즈가 존재하는 등 몇 가지 기술적 한계가 있습니다.

생성된 이미지의 해상도는 CNN의 복잡성과 최적화 문제에 정비례합니다. 따라서 이미지 해상도는 약 512×512픽셀로 제한되며, 합성 프로세스는 특정 유형의 그래픽 처리 장치(GPU)에서 최대 1시간까지 소요될 수 있습니다. 현재로서는 실시간 적용이 불가능하지만, 향후 딥 러닝이 발전하면 이 방법의 성능이 향상될 수 있습니다.

합성된 이미지에는 때때로 이미지의 사실성에 영향을 미치는 낮은 수준의 노이즈가 포함되어 있습니다. 이 문제는 콘텐츠와 스타일 이미지가 모두 사진일 때 특히 두드러집니다. 그러나 이 노이즈는 네트워크의 유닛 필터와 유사하게 나타나기 때문에 최적화 절차 후에 이미지를 개선하는 효율적인 노이즈 제거 기술을 개발할 수 있습니다.

이미지의 스타일을 정확히 정의하는 것이 무엇인지 명확하지 않기 때문에 이미지의 콘텐츠에서 스타일을 분리하는 개념은 잘 정의된 문제가 아닙니다. 그림의 붓터치, 색상 팔레트, 지배적인 모양, 구도, 피사체 선택 등 다양한 요소가 스타일을 정의할 수 있습니다. 따라서 이미지 콘텐츠와 스타일을 완전히 분리할 수 있는지, 분리할 수 있다면 어떻게 분리할 수 있는지는 불확실합니다.

이러한 어려움에도 불구하고 이 연구는 물체 인식을 수행하도록 훈련된 신경 시스템이 이미지 콘텐츠와 스타일을 어느 정도 분리할 수 있는 이미지 표현을 자동으로 학습할 수 있다는 흥미로운 사실을 발견했습니다. 이는 우리가 예술을 창작하고 즐기는 능력을 뒷받침하는 스타일에서 콘텐츠를 분리하는 능력이 주로 시각 시스템의 강력한 추론 능력의 결과일 수 있음을 시사할 수 있습니다.

- 요약
    
    신경 스타일 전송의 맥락에서, 콘텐츠 이미지와 스타일 이미지 모두에 대해 사전 학습된 컨볼루션 신경망(CNN, 이 경우 VGG 네트워크)의 특정 레이어에서 특징 맵(활성화 맵이라고도 함)을 추출하는 일이 일어나고 있습니다.
    
    콘텐츠 이미지는 스타일을 적용하려는 이미지이고, 스타일 이미지는 적용하려는 스타일의 소스입니다.
    
    콘텐츠 이미지의 피처 맵은 출력 이미지의 '콘텐츠' 또는 전체 구조를 보존하는 데 사용됩니다. 스타일 이미지의 피처 맵은 '스타일' 또는 텍스처, 색상, 패턴을 캡처하여 출력 이미지에 적용하는 데 사용됩니다.
    
    하지만 피처 맵을 직접 합성하는 대신 콘텐츠 이미지(및 스타일 이미지)의 피처 맵을 무작위 노이즈로 시작하여 반복적으로 업데이트되는 생성된 이미지의 피처 맵과 비교합니다.
    
    - 콘텐츠 손실: 콘텐츠 이미지의 경우, 생성된 이미지의 피처 맵이 콘텐츠 이미지의 피처 맵과 얼마나 다른지(하나 이상의 레이어에서) 측정합니다. 이를 통해 생성된 이미지가 콘텐츠 이미지의 콘텐츠와 얼마나 일치하는지 알 수 있습니다.
    - 스타일 손실: 스타일 이미지의 경우 조금 더 복잡합니다. 단순히 피처 맵을 직접 비교하지 않습니다. 대신 스타일 이미지와 생성된 이미지(하나 이상의 레이어에서)의 피처 맵의 그램 행렬을 계산합니다. 그램 행렬은 서로 다른 피처 맵 간의 상관 관계를 측정하는 척도이며 텍스처 또는 스타일 정보를 캡처하는 역할을 합니다. 그런 다음 이러한 그램 행렬이 얼마나 다른지 측정하여 생성된 이미지가 스타일 이미지의 스타일과 얼마나 일치하는지 알려줍니다.
    
    그런 다음 콘텐츠 손실과 스타일 손실의 가중치 조합을 사용하여 총 손실을 계산합니다. 그런 다음 역전파 및 그라데이션 하강을 사용하여 생성된 이미지의 픽셀 값을 반복적으로 조정하여 이 총 손실을 최소화합니다.
    
    이 프로세스의 결과로 콘텐츠 이미지의 콘텐츠는 유지하되 스타일 이미지의 스타일을 가진 이미지가 생성됩니다.
    
    간단히 말해, 피처 맵을 합성하는 것이 아니라 콘텐츠 이미지의 피처 맵과 스타일 이미지의 그램 매트릭스(스타일을 캡처하는)가 일치하도록 이미지를 최적화하는 것입니다.
    
- VGGNet
    
    흔히 VGGNet이라고도 하는 VGG(Visual Geometry Group) 네트워크는 옥스퍼드 대학교의 시각 기하학 그룹에서 설계하고 제안한 심층 컨볼루션 신경망(CNN)입니다. 이 네트워크는 단순성과 성능으로 인해 딥 러닝 및 이미지 분류 분야에서 획기적인 발전을 이루었습니다.
    
    VGG 네트워크는 2014년 ImageNet 대규모 시각 인식 챌린지(ILSVRC)에서 가장 우수한 성적을 거둔 네트워크 중 하나였습니다. VGG 네트워크의 주요 기여는 컨볼루션 신경망이 깊이를 가질 수 있다는 아이디어로, 이는 표현을 학습하는 데 매우 유용합니다.
    
    VGG 네트워크에는 두 가지 주요 버전이 있습니다: VGG16과 VGG19입니다. 숫자는 네트워크에서 학습 가능한 가중치가 있는 레이어 수(즉, 컨볼루션 레이어와 완전히 연결된 레이어)를 나타냅니다. 
    
    VGGNet의 단순성과 뛰어난 성능으로 인해 이미지 인식 작업에 널리 사용되고 있습니다. 그럼에도 불구하고 VGGNet은 ResNet이나 EfficientNet과 같은 다른 모델에 비해 훈련 속도가 상당히 느리고 저장 용량도 상당히 큰 것으로 알려져 있습니다.
    
- Feature map
    
    활성화 맵이라고도 하는 피처 맵은 컨볼루션 신경망(CNN)에서 컨볼루션 레이어 또는 다른 유형의 레이어의 출력입니다.
    
    입력(예: 이미지)이 CNN의 컨볼루션 레이어를 통과하면 이 레이어는 필터(또는 커널) 집합으로 정의된 일련의 수학적 연산을 수행합니다. 각 필터는 입력에서 특정 유형의 특징을 감지하도록 설계되었습니다. 이 작업의 출력은 피처 맵 세트입니다.
    
    이미지 처리의 맥락에서 피처는 이미지의 가장자리, 색상, 텍스처 또는 기타 패턴일 수 있습니다. 각 특징 맵은 입력 이미지에서 해당 특징이 감지된 위치를 나타냅니다. 특징 맵은 본질적으로 이미지 전체에서 특징의 강도 또는 존재 여부를 나타내는 공간 맵입니다.
    
    예를 들어 64개의 필터가 있는 컨볼루션 레이어에서 출력은 64개의 특징 맵이 됩니다. 이 64개의 특징 맵은 각각 입력 이미지에서 서로 다른 특징을 감지했음을 나타냅니다.
    
    입력이 CNN의 레이어를 거치면서 특징 맵은 점점 더 복잡한 특징을 나타냅니다. 초기 레이어는 가장자리와 색상과 같은 단순한 특징을 나타내는 반면, 더 깊은 레이어는 물체의 일부 또는 전체 물체와 같이 이러한 단순한 특징의 복잡한 조합을 나타낼 수 있습니다.
    
- 피처맵과 이미지 생성
    
    컨볼루션 신경망(CNN)의 피처 맵(활성화 맵이라고도 함)은 이미지 전체에 걸쳐 다양한 필터의 반응을 나타내며 이미지의 다양한 수준의 세부 사항, 패턴 및 구조를 캡처합니다. 이러한 맵은 일반적으로 자체적으로 시각적으로 해석할 수 없으며 객체 감지, 분할, 이 경우 스타일 전송과 같은 다양한 작업을 수행하는 데 사용됩니다.
    
    그러나 CNN에서 주어진 이미지의 특징 맵과 일치하도록 이미지를 최적화할 수 있습니다. 이는 기본적으로 신경 스타일 전송에서 일어나는 일로, 콘텐츠 이미지의 특징 맵(콘텐츠 손실의 경우)과 스타일 이미지의 그램 매트릭스(스타일 손실의 경우)에 일치하도록 이미지를 반복적으로 업데이트하는 것입니다.
    
    DeepDream이라는 또 다른 접근 방식에서는 CNN의 레이어를 선택하고 네트워크를 통해 해당 레이어까지 이미지를 순방향 전파한 다음, 선택한 레이어가 감지하는 특징을 강화하여 꿈과 같은 이미지를 만들려는 의도로 데이터를 입력으로 역전파하는 방식이 있습니다.
    
    "특징 반전" 또는 "네트워크 반전"이라는 개념도 있는데, 주어진 특징 맵 세트와 유사한 특징 맵을 생성하는 이미지를 찾는 것을 목표로 합니다. 이를 통해 CNN이 학습한 내용에 대한 통찰력을 얻을 수 있습니다.
    
    따라서 특징 맵에서 직접 이미지를 생성할 수는 없지만 이미지 생성 또는 수정을 안내하는 일종의 목표 또는 청사진으로 사용할 수 있습니다.
    
- 스타일 전송 절차
    
    스타일 전송의 경우, 무작위 이미지(또는 때로는 콘텐츠 이미지)로 시작한 다음 그라데이션 하강을 사용하여 이 이미지를 반복적으로 수정하여 생성된 이미지와 콘텐츠 이미지의 특징 표현(콘텐츠 손실의 경우)과 생성된 이미지와 스타일 이미지의 스타일 표현(스타일 손실의 경우) 간의 차이를 최소화합니다.
    
    1. 임의의 이미지(또는 때로는 콘텐츠 이미지)로 시작합니다: 이 이미지가 반복적으로 수정할 초기 이미지입니다. 무작위 이미지로 시작하면 일반적으로 더 스타일화된 최종 이미지가 생성되는 반면 콘텐츠 이미지로 시작하면 콘텐츠를 더 잘 알아볼 수 있습니다.
    2. 이 이미지를 반복적으로 수정하려면 그라디언트 하강을 사용합니다: 그라디언트 하강은 함수를 최소화하는 데 사용되는 최적화 알고리즘입니다. 여기서는 초기 이미지의 픽셀 값을 반복적으로 조정하여 콘텐츠 손실과 스타일 손실의 값을 줄이는 데 사용됩니다.
    3. 생성된 이미지의 기능 표현과 콘텐츠 이미지 간의 차이를 최소화합니다(콘텐츠 손실의 경우): '콘텐츠 손실'은 생성된 이미지의 콘텐츠가 콘텐츠 이미지의 콘텐츠와 얼마나 다른지를 측정합니다. 여기서 '콘텐츠'는 이미지에 포함된 개체와 그 배열을 의미합니다. 이 손실은 네트워크의 하나 이상의 레이어에서 생성된 이미지와 콘텐츠 이미지의 특징 표현(즉, 특징 맵의 값)을 비교하여 계산됩니다. 목표는 이러한 레이어에서 생성된 이미지의 특징 표현이 콘텐츠 이미지의 특징 표현과 유사하도록 생성된 이미지를 수정하는 것입니다.
    4. 생성된 이미지의 스타일 표현과 스타일 이미지 간의 차이를 최소화합니다(스타일 손실): '스타일 손실'은 생성된 이미지의 스타일이 스타일 이미지의 스타일과 얼마나 다른지를 측정합니다. 여기서 '스타일'은 이미지의 텍스처, 색상 및 시각적 패턴을 의미합니다. 이 손실은 네트워크의 하나 이상의 레이어에서 생성된 이미지와 스타일 이미지의 그램 매트릭스(특징 맵 간의 상관관계를 캡처)를 비교하여 계산됩니다. 목표는 생성된 이미지를 수정하여 이러한 레이어에서 스타일 이미지의 스타일 표현이 스타일 이미지와 유사하도록 하는 것입니다.
    
    이 두 가지 손실을 합산하여(일반적으로 가중치 합산) 최적화 프로세스 중에 실제로 최소화되는 총 손실을 구성합니다.
    
    총 손실이 더 이상 감소하지 않거나 일정 횟수의 반복이 끝나면 프로세스가 완료됩니다. 결과 이미지는 콘텐츠 이미지의 콘텐츠를 유지하지만 스타일 이미지의 스타일로 표시됩니다.