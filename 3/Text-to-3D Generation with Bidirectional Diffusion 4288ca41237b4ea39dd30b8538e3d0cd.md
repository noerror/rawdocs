# Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors

[https://arxiv.org/abs/2312.04963](https://arxiv.org/abs/2312.04963)

[https://bidiff.github.io/](https://bidiff.github.io/)

- Dec 2023-

### 1 Introduction

텍스트-3D 생성의 배경과 과제: 텍스트-3D 생성의 최근 발전은 2D 기초 모델을 3D 공간으로 변환하는 데 중점을 두고 있습니다. 널리 사용되는 솔루션은 2D 확산 모델에서 2D 스코어 증류 샘플링(SDS) 손실을 사용하여 3D 생성을 감독합니다. 이러한 방법은 고품질 텍스처를 생성할 수 있지만 3D 제약 조건이 없기 때문에 다면 야누스 문제와 같은 기하학적 모호성이 발생하는 경우가 많습니다. 또한 이러한 최적화 방법은 시간이 많이 소요되어 하나의 오브젝트를 생성하는 데 몇 시간이 걸립니다. 이러한 방법을 개선하려는 시도가 있었지만, 여전히 기하학적 일관성을 보장하는 데 어려움을 겪고 있습니다.

현재 3D 데이터세트와 방법의 한계: 또 다른 접근 방식은 3D 데이터세트에서 직접 3D 구조를 학습하는 것입니다. 하지만 많은 3D 데이터세트에는 수작업으로 제작된 오브젝트가 포함되어 있거나 실제 오브젝트와 비교할 수 있는 고품질 텍스처가 부족합니다. 또한 이러한 데이터 세트는 2D 데이터 세트에 비해 크기가 작고 확장하기가 더 어렵습니다. 따라서 3D 디퓨전 모델은 2D 모델보다 더 나은 3D 일관성을 제공함에도 불구하고 디테일한 텍스처와 복잡한 지오메트리를 생성하지 못하는 경우가 많습니다.

양방향 확산(BiDiff)을 소개합니다: 2D와 3D 디퓨전 모델의 강점을 모두 활용하기 위해 양방향 디퓨전(BiDiff)을 제안합니다. 이 방법은 하이브리드 표현을 사용하여 통합 프레임워크 내에서 2D 및 3D 확산 모델을 통합합니다. 3D 특징 학습에는 부호화된 거리 필드(SDF)를, 2D 특징 학습에는 멀티뷰 이미지를 사용하며, 이러한 표현을 상호 변환할 수 있는 기능을 갖추고 있습니다. BiDiff는 사전 학습된 3D 및 2D 확산 모델로 시작하여 효과적인 3D 생성을 위해 2D와 3D를 공동으로 미세 조정하여 미리 캡처합니다.

양방향 안내 및 장점: 두 확산 프로세스의 생성 방향을 일치시키기 위해 BiDiff는 양방향 가이던스를 도입했습니다. 이를 통해 각 확산 단계에서 한 모델의 중간 결과가 다른 모델에 안내 신호를 제공합니다. 이러한 상호 안내는 두 확산 프로세스가 같은 방향으로 학습하도록 규칙화합니다. 2D 텍스처와 3D 지오메트리의 생성을 개별적으로 제어할 수 있고, 더 다양한 오브젝트를 생성할 수 있으며, 이전 방식에 비해 생성 시간이 크게 단축되는 등의 이점이 있습니다.

향상된 최적화 및 모델 훈련: BiDiff는 기존 최적화 기반 방법의 강력한 초기화로도 출력을 사용할 수 있습니다. 이 단계를 통해 최적화 시간을 단축하고 기하학적 부정확성을 해결하면서 3D 오브젝트의 품질을 향상시킬 수 있습니다. 셰이프넷과 오브제버스 40K와 같은 데이터세트로 훈련된 이 프레임워크는 강력한 일반화 가능성을 갖춘 고품질의 텍스처 3D 오브젝트를 생성할 수 있는 능력을 보여줍니다.

BiDiff 모델의 기여: 유니티의 접근 방식은 고품질의 다양한 3D 오브젝트를 생성할 수 있는 2D-3D 공동 확산 모델 개발, 사전 훈련된 모델을 모두 활용하는 새로운 훈련 파이프라인, 텍스처와 지오메트리를 독립적으로 제어할 수 있는 최초의 확산 기반 모델, 빠르고 충실도가 높은 결과를 위해 3D 모델을 최적화하고 개선하기 위한 BiDiff의 출력 사용 등을 주요 공헌 사항으로 꼽을 수 있습니다.

### 2. Related Work

3D 제너레이티브 방법의 발전과 과제 개요

초기 3D 생성 방법과 그 한계: 3D 생성 방법의 초기 단계에서는 3D 복셀, 포인트 클라우드, 메시, 암시적 함수 등 다양한 3D 표현이 채택되었습니다. 이러한 방법은 소규모 3D 데이터 세트에 대한 직접 학습을 기반으로 했습니다. 하지만 이러한 접근 방식은 종종 작은 기하학적 구조를 놓치거나 다양성이 부족한 오브젝트를 생성하는 결과를 초래했습니다. 최근에는 대규모 또는 고품질의 3D 데이터 세트가 등장했지만, 2D 이미지 생성 훈련에 사용되는 데이터 세트에 비하면 여전히 부족합니다.

3D 생성을 위한 2D 생성 모델 활용: 강력한 텍스트-이미지 합성 모델의 등장으로 새로운 패러다임이 등장했습니다. 이 접근 방식은 사전 학습된 텍스트-이미지 합성 모델(예: CLIP)과 2D 확산 생성 모델을 포함한 2D 생성 모델을 활용하여 기본 3D 표현의 최적화를 유도합니다. 이러한 모델은 혁신적이기는 하지만 크로스뷰 3D 일관성을 보장하는 데 어려움을 겪었고, 높은 컴퓨팅 비용과 과포화 문제와 같은 문제에 직면했습니다. 이후 텍스트 코드, 뎁스 맵, 직접 3D 분포 모델링을 사용하여 이러한 모델을 개선하여 시각적 아티팩트를 줄였지만 여전히 고품질 3D 결과를 보장하지는 못했습니다.

3D 데이터세트에서 직접 3D 프리어 학습: 또 다른 접근 방식은 3D 데이터세트에서 직접 3D 전구를 학습하는 것입니다. 최신 제너레이티브 모델의 일반적인 근간인 확산 모델은 암시적 공간(예: 포인트 클라우드 특징, NeRF 파라미터 또는 SDF 공간)을 사용하여 3D 프리어를 학습하도록 조정되었습니다. 이러한 방법은 빠른 추론과 일관된 3D 결과를 제공하지만, 3D 데이터 세트의 품질이 낮고 다양성이 제한적이기 때문에 시각적으로 열등한 결과를 초래하는 경우가 많습니다.

2D와 3D 프리어 결합의 과제: 최근 일부 방법에서는 사전 학습된 개별 확산 모델에서 2D와 3D 프리어를 결합하려는 시도가 있었습니다. 하지만 두 가지 생성 프로세스 간에 불일치가 발생하는 경우가 많습니다. 이는 2D와 3D 모델을 원활하게 통합하는 것이 어렵다는 점을 강조하며, 이는 3D 생성 기법 발전의 핵심 과제로 남아 있습니다.

요약하자면, 3D 생성 기법은 기능이 제한적인 초기 모델에서 2D와 3D 데이터 소스를 모두 활용하는 보다 정교한 접근 방식으로 발전해 왔습니다. 이러한 발전에도 불구하고 크로스뷰 일관성 보장, 컴퓨팅 리소스 관리, 고품질의 다양한 결과물 확보와 같은 과제는 여전히 남아 있습니다. 2D와 3D 선행 작업의 통합은 향후 개발과 혁신을 위한 중요한 영역으로 남아 있습니다.

### 3. Method

BiDiff의 방법론: 새로운 양방향 확산 모델

2D와 3D 모델 통합의 과제: 이전 연구에서는 3D 오브젝트 생성에서 2D 텍스처와 3D 지오메트리의 중요성을 강조했습니다. 하지만 3D 구조 및 2D 텍스처 선행 모델을 하나의 일관된 프레임워크로 통합하는 것은 복잡합니다. 훈련 및 추론 과정에서 두 가지 생성 모델이 서로 상충되는 생성 방향으로 이어질 수 있습니다.

BiDiff 접근 방식: 이러한 문제를 해결하기 위해 BiDiff 모델은 사전 학습된 3D 확산 모델을 양방향 안내를 사용하여 2D 모델과 통합합니다. 이 모델은 2D와 3D 정보를 모두 포함하는 새로운 하이브리드 표현을 특징으로 합니다. 이 아키텍처는 부호화된 거리 필드(SDF) 공간에 대한 3D 확산 모델과 이미지 도메인 내의 2D 멀티뷰 확산 모델이라는 두 가지 다른 확산 모델로 구성됩니다. 이 설정은 3D 및 2D 특징의 공동 학습을 보장하여 3D 지오메트리와 2D 멀티뷰 이미지 간의 생성 불일치를 극복합니다.

일관성을 위한 양방향 안내: BiDiff는 양방향 가이던스를 사용하여 3D와 2D 출력 간의 일관성을 유지합니다. 2D 확산 프로세스는 3D 생성 프로세스를 안내하여 2D-3D 일관성을 보장합니다. 마찬가지로 3D 디퓨전 프로세스는 2D 생성 프로세스를 안내하여 3D 노이즈 제거 필드에서 멀티뷰 이미지를 렌더링하여 일관된 2D 이미지를 생성합니다.

차별화된 3D 및 2D 디퓨전 모델: 3D 확산 모델은 3D 기초 모델의 지오메트리 선행 요소를 통합하여 새로운 2D-3D 가이드를 통해 신경 표면 필드를 생성합니다. 3D 방사 필드에 의해 안내되는 2D 확산 모델은 멀티뷰 이미지를 생성하며, 이미지 품질을 향상시키기 위해 독립적으로 고정된 2D 파운데이션 모델을 기반으로 구축됩니다.

지오메트리와 텍스처의 개별 제어: BiDiff의 주요 장점은 2D 텍스처 생성과 3D 지오메트리 생성을 독립적으로 제어할 수 있다는 점입니다. 이는 3D 및 2D 선행의 강도를 독립적으로 수동으로 제어할 수 있는 선행 향상 전략을 통해 구현됩니다. 이 기능을 통해 사용자는 모델의 전체적인 일관성을 유지하면서 텍스처와 지오메트리를 개별적으로 변경할 수 있는 유연성을 확보할 수 있습니다.

BiDiff 초기화를 통한 최적화: BiDiff는 최적화 기반 방법을 위한 강력한 초기화로 사용되어 3D 모델의 품질을 더욱 향상시킬 수 있습니다. 이 단계는 효율적이며 다중 면 이상과 같은 잘못된 지오메트리를 방지하는 데 도움이 됩니다. BiDiff에서 생성된 레이디언스 필드는 세부적인 이미지 렌더링을 위해 더 높은 해상도로 변환되어 보다 효과적인 최적화 프로세스를 촉진합니다.

요약하자면, BiDiff는 새로운 양방향 확산 방식을 사용하여 3D 및 2D 생성 모델을 통합할 때 발생하는 주요 과제를 해결합니다. 3D와 2D 표현 간의 일관성을 보장하고 텍스처와 지오메트리를 개별적으로 제어할 수 있으며 효과적인 최적화 전략을 통해 3D 모델의 품질을 향상시킵니다. 이 방법은 3D 오브젝트 생성 분야에서 상당한 발전을 이루었습니다.

### 4. Experiment

실험 설정: BiDiff 프레임워크는 2D 기초 모델로는 DeepFloyd-IF-XL을, 3D 선행 모델로는 ShapE를 사용하여 ShapeNet-Chair 및 Objaverse LVIS 40k 데이터 세트에 대해 훈련되었습니다. 그리드 크기 128의 SparseNeuS 신경 표면 필드 프레젠테이션이 채택되었습니다. 3D에서 2D로의 안내를 위해 해상도 64×64의 멀티뷰 이미지 8개를 렌더링했습니다. 훈련은 배치 크기가 4인 NVIDIA A100 GPU 4개에서 수행되었습니다. 샘플링하는 동안 3D 및 2D 선행의 안내 척도는 각각 3.0과 7.5로 설정되었습니다.

텍스트-3D 결과: 셰이프넷-체어 데이터 세트에서 프레임워크는 지오메트리의 세밀한 디테일을 캡처하고 텍스트 프롬프트를 통해 텍스처의 다양성을 보여주었습니다. 더 큰 Objaverse-40K 데이터 세트로 확장한 결과, 이 모델은 텍스트 프롬프트에 밀접하게 부합하고 다양한 지오메트리를 생성할 수 있어 3D 전용 솔루션보다 뛰어난 성능을 보여주었습니다.

다른 모델과의 비교:

최적화 방법: BiDiff는 속도와 품질 측면에서 기존 SDS 기반 방식보다 뛰어난 성능을 보였습니다. 다른 방법에서는 몇 시간이 걸렸던 것에 비해 BiDiff는 단 40초 만에 적절한 지오메트리를 가진 사실적인 오브젝트를 생성했습니다. 또한 BiDiff는 최적화 기반 방법의 성능과 효율성을 향상시키는 강력한 선행 기술 역할을 했습니다.
멀티뷰 메서드: 3D 구조에 대한 진정한 이해가 부족한 0-1-3과 달리 BiDiff는 3D와 2D 프리오어를 모두 통합하여 보다 정확한 3D 지오메트리와 일관된 멀티뷰 이미지를 생성합니다.
절제 연구:

3D 선행의 영향: 3D 선구자를 제거하면 훈련 세트에서 일반적인 객체만 생성되어 이러한 선구자의 중요성을 알 수 있습니다.
2D 전위의 영향: 2D 프리어가 없으면 텍스처가 합성 훈련 데이터와 더 많이 일치하는 반면, 2D 프리어를 사용하면 더 사실적인 텍스처를 구현할 수 있었습니다.
사전 향상 전략: 이 전략은 다양하고 유연한 3D 생성을 달성하는 데 결정적인 역할을 했습니다.
SDS의 노이즈 레벨: 최적화 과정에서 노이즈 레벨을 조정하여 생성된 오브젝트의 텍스처 유사성을 효과적으로 제어할 수 있었습니다.

### 5.Conclusion

BiDiff 프레임워크는 3D 및 2D 확산 프로세스를 성공적으로 통합하여 두 영역의 강력한 전제를 활용하여 지오메트리와 텍스처에 대한 일반화 가능한 이해를 달성합니다. 이는 3D 오브젝트 생성 분야에서 상당한 발전을 이루었으며, 생성된 결과물의 속도, 다양성 및 품질을 제공합니다.