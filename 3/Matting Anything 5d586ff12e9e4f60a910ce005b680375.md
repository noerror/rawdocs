# Matting Anything

[https://arxiv.org/pdf/2306.05399v1.pdf](https://arxiv.org/pdf/2306.05399v1.pdf)

[https://github.com/SHI-Labs/Matting-Anything](https://github.com/SHI-Labs/Matting-Anything)

이 논문에서는 이미지에 있는 모든 요소의 알파 매트(또는 투명도 레이어)를 추정하기 위한 새로운 프레임워크인 매팅 애니씽 모델(MAM)을 소개합니다. 이 모델은 의미론적, 인스턴스, 참조 등 다양한 유형의 이미지 매팅을 단일 모델로 처리할 수 있다는 점에서 독특합니다.

MAM은 세그먼트 애니씽 모델(SAM)이라는 다른 모델을 기반으로 하며, 가벼운 M2M(Mask-to-Matte) 모듈을 사용합니다. M2M 모듈은 훈련 가능한 파라미터가 270만 개에 불과해 효율적이며 시간이 지남에 따라 알파 매트 추정치를 개선합니다.

또한 MAM은 물체의 경계(트랩) 정의와 같은 세부적인 지침을 상자, 점 또는 텍스트 프롬프트 표시와 같은 간단한 지침으로 줄여 사용자 인터랙션 프로세스를 간소화합니다.

이 모델은 다양한 이미지 매팅 벤치마크에서 테스트되었으며 최첨단 모델과 비교되었습니다. 그 결과 비슷한 성능을 보였지만 매개변수가 더 적어 더 효율적이었습니다. 결론적으로, MAM은 이미지 매팅 작업에 대한 보다 다양하고 실용적인 접근 방식을 제시합니다. 저자들은 코드와 모델을 GitHub에 공개했습니다.

![매팅 애니씽 모델(MAM)은 단일 모델로 다양한 유형의 이미지 매팅 시나리오를 처리할 수 있는 다목적 프레임워크를 제공합니다. 다양한 유형의 이미지 매팅 시나리오를 처리할 수 있는 다용도 프레임워크를 제공합니다. 전경에 있는 모든 인스턴스의 단일 알파 매트를 출력하는 (A) 시맨틱 매팅, 전경에 있는 모든 인스턴스의 단일 알파 매트를 출력하는 (B) 인스턴스 매팅에 대한 이전의 전문 모델과 비교 (B) 모든 인간 인스턴스의 알파 매트를 반환하는 인스턴스 매팅 (C) 매팅 애니씽 모델은 모든 대상 인스턴스의 알파 매트를 알파 매트를 추정할 수 있으며, SAM을 통합하여 대화형 사용을 위한 상자, 점 또는 텍스트 설명과 같은 사용자 프롬프트가 있는 모든 대상 인스턴스의 알파 매트를 추정할 수 있습니다[18]. 여러 벤치마크에서 전문화된 매팅 모델과 비슷한 성능을 보이며, 통합된 이미지 매팅 모델로서 더 적은 매개변수로도 우수한 일반화 능력을 보여줍니다.](Matting%20Anything%205d586ff12e9e4f60a910ce005b680375/Untitled.png)

매팅 애니씽 모델(MAM)은 단일 모델로 다양한 유형의 이미지 매팅 시나리오를 처리할 수 있는 다목적 프레임워크를 제공합니다. 다양한 유형의 이미지 매팅 시나리오를 처리할 수 있는 다용도 프레임워크를 제공합니다. 전경에 있는 모든 인스턴스의 단일 알파 매트를 출력하는 (A) 시맨틱 매팅, 전경에 있는 모든 인스턴스의 단일 알파 매트를 출력하는 (B) 인스턴스 매팅에 대한 이전의 전문 모델과 비교 (B) 모든 인간 인스턴스의 알파 매트를 반환하는 인스턴스 매팅 (C) 매팅 애니씽 모델은 모든 대상 인스턴스의 알파 매트를 알파 매트를 추정할 수 있으며, SAM을 통합하여 대화형 사용을 위한 상자, 점 또는 텍스트 설명과 같은 사용자 프롬프트가 있는 모든 대상 인스턴스의 알파 매트를 추정할 수 있습니다[18]. 여러 벤치마크에서 전문화된 매팅 모델과 비슷한 성능을 보이며, 통합된 이미지 매팅 모델로서 더 적은 매개변수로도 우수한 일반화 능력을 보여줍니다.

### 1 Introduction

이미지 매팅은 이미지의 알파 매트 또는 투명도 레이어를 추정하는 데 중점을 둔 컴퓨터 비전 작업입니다. 이미지 매팅의 주요 초점은 시맨틱 수준에서 사람과 물체였지만, 최근에는 인스턴스 매팅(특정 인스턴스의 알파 매트를 예측)과 참조 이미지 매팅(자연어 설명을 기반으로 알파 매트를 추출)으로 확장되고 있습니다.

이미지 매팅을 위한 과거의 딥러닝 방법은 특정 작업을 위해 설계되었으며 다양한 매팅 작업을 처리할 수 있을 만큼 유연하지 않아 보다 다양한 모델을 개발하는 데 한계가 있었습니다. 이러한 모델은 정확한 예측을 위해 사용자 가이드 트림 트랩(일종의 보조 입력)에 의존하는 경우가 많습니다. 일부 방법은 마스크나 배경 이미지를 사용하여 트림 트랩의 필요성을 없애려고 시도했지만, 이러한 방법은 사용자 상호 작용을 기반으로 인스턴스의 알파 매트를 추정할 수 없습니다.

이러한 한계를 극복하기 위해 매팅 애니띵 모델(MAM)이 개발되었습니다. 이 모델은 간단한 사용자 안내를 통해 모든 인스턴스의 알파 매트를 추정할 수 있으며 이전 방법보다 더 유연합니다. 이 모델은 인터랙티브 사용을 위해 모든 인스턴스의 세그먼트 마스크를 생성하는 세그먼트 애니띵 모델(SAM)을 사용합니다. MAM은 SAM의 특징과 마스크 출력을 가져와 가벼운 마스크 투 매트(M2M) 모듈을 추가하여 인스턴스의 알파 매트를 예측합니다. 이 모듈은 5개의 서로 다른 데이터 세트에 대해 학습되었으며, 반복적인 개선 프로세스를 통해 최종 알파 매트를 얻습니다.

MAM의 성능은 6개의 이미지 매트 벤치마크에서 테스트되었으며, 다양한 지표에서 모든 벤치마크에서 최신 모델과 비슷한 성능을 보였습니다. 이러한 결과는 다양한 이미지 매팅 작업을 인터랙티브하고 효율적으로 처리하는 MAM의 효율성을 입증합니다.

### 2. Related Works

2.1. 이미지 매팅
이미지 매팅은 이미지 I에서 알파 매트(α)를 추정하는 과정으로, 여기서 I는 전경 이미지(F)와 배경 이미지(B)의 조합입니다. 기존 방법에서는 전경, 배경 및 전환 영역에 레이블을 지정하는 사용자 가이드 트림 트랩을 사용합니다. 그런 다음 샘플링 기반 솔루션은 낮은 수준의 특징을 사용하여 이러한 영역을 구분합니다.

최근의 딥러닝 방법은 신경망을 사용하여 엔드 투 엔드 방식으로 알파 매트를 추정하며, 트림 트랩은 보조 입력으로 사용됩니다. 배경 이미지, 마스크 안내 또는 분할 데이터를 사용하는 트림 트랩이 없는 방법도 있습니다. 이미지 I에 여러 인스턴스가 포함된 경우 이러한 인스턴스가 방정식에 반영되어 알파 매트를 적절히 조정합니다.

InstMatt와 같은 일부 방법은 대상 및 참조 마스크를 사용하여 인스턴스 인식 알파 매트 예측을 안내합니다. 다른 기법은 자연어 설명을 사용하여 대상 인스턴스의 알파 매트를 추정합니다. 실시간 추론을 위해 비디오 매트 방법도 검토되고 있지만 프레임당 예측 품질은 이미지 매트 방법과 동등하지 않습니다. 그러나 이러한 방법은 일반적으로 특정 시나리오를 위해 설계되어 다양한 이미지 매팅 작업을 처리할 수 있는 유연성이 제한됩니다.

2.2. 이미지 세분화
이미지 매팅과 관련된 연구 분야인 이미지 분할은 이미지에서 서로 다른 인스턴스의 이진 마스크를 예측하는 것을 목표로 합니다. 시맨틱 분할, 인스턴스 분할, 파놉틱 분할과 같은 많은 이미지 분할 방법이 특정 작업을 위해 설계되었습니다.

최근에는 통합 이미지 분할을 위한 트랜스포머 기반 프레임워크를 탐색하는 작업이 시작되었습니다. 언어 기반 세분화 프레임워크는 인스턴스 인식 마스크를 세분화하기 위해 텍스트 감독을 찾습니다. 원포머와 같은 모델은 공동 훈련 전략과 함께 단일 트랜스포머 모델을 사용하여 다양한 유형에 걸쳐 보편적인 세분화를 수행함으로써 전문화된 모델보다 뛰어난 성능을 발휘합니다.

최근에는 세그먼트 애니씽 모델(SAM)이 도입되었습니다. 이 모델을 사용하면 대화형 사용을 위해 이미지의 모든 인스턴스를 세그먼트화하도록 사용자에게 유연하게 프롬프트할 수 있습니다. Grounded-SAM은 DINO와 SAM을 통합하여 텍스트 프롬프트 지원을 추가합니다. 이러한 기본 모델은 다양한 애플리케이션을 위한 다용도 프레임워크를 개발할 수 있는 기회를 제공합니다.

### 3. Matting Anything

이 섹션에서는 고정된 세그먼트 애니씽 모델(SAM)과 훈련 가능한 마스크 투 매트(M2M) 모듈이라는 두 가지 주요 구성 요소로 구성된 MAM(매팅 애니씽 모델) 아키텍처에 대해 간략하게 설명합니다.

3.1. 세그먼트 애니씽 모델

세그먼트 애니씽 모델(SAM)은 세그먼테이션을 위한 최근의 기초 모델입니다. 이미지 I가 주어지면 SAM은 ViT 기반 이미지 인코더를 사용하여 심층 특징 맵 F를 얻은 다음, 다양한 N개의 입력 프롬프트를 인코딩하여 특징 맵과 함께 마스크 디코더로 보냅니다. 마스크 디코더는 입력 프롬프트로 표시된 마스크 후보 집합 mi를 반환합니다. SAM은 대화형으로 사용할 수 있고 다운스트림 작업에 쉽게 적용할 수 있도록 설계되었습니다.

![매팅 애니씽 모델 아키텍처. MAM 아키텍처는 사전 학습된 SAM과 M2M 모듈로 구성됩니다. 주어진 입력 이미지 I가 주어지면 SAM은 박스 또는 포인트 사용자 프롬프트에 따라 대상 인스턴스에 대한 마스크 예측을 생성합니다. M2M 모듈은 이미지, 마스크, 특징 맵을 포함한 연결된 입력을 받아 멀티 스케일 예측 αos8, αos4, αos1을 생성합니다. 반복적 개선 프로세스 섹션 3에 자세히 설명된 반복적 개선 프로세스는 다중 스케일 출력의 정보를 통합하여 최종 세심한 알파 매트 α의 정밀도를 점진적으로 개선합니다. 정보를 통합하여 최종 알파 매트 α의 정밀도를 점진적으로 향상시킵니다.](Matting%20Anything%205d586ff12e9e4f60a910ce005b680375/Untitled%201.png)

매팅 애니씽 모델 아키텍처. MAM 아키텍처는 사전 학습된 SAM과 M2M 모듈로 구성됩니다. 주어진 입력 이미지 I가 주어지면 SAM은 박스 또는 포인트 사용자 프롬프트에 따라 대상 인스턴스에 대한 마스크 예측을 생성합니다. M2M 모듈은 이미지, 마스크, 특징 맵을 포함한 연결된 입력을 받아 멀티 스케일 예측 αos8, αos4, αos1을 생성합니다. 반복적 개선 프로세스 섹션 3에 자세히 설명된 반복적 개선 프로세스는 다중 스케일 출력의 정보를 통합하여 최종 세심한 알파 매트 α의 정밀도를 점진적으로 개선합니다. 정보를 통합하여 최종 알파 매트 α의 정밀도를 점진적으로 향상시킵니다.

3.2. 마스크 투 매트

마스크-투-매트(M2M) 모듈은 SAM의 인스턴스 인식 마스크 예측을 인스턴스 인식 알파 매트 예측으로 효율적으로 변환하도록 설계되었습니다. SAM에서 생성된 피처 맵과 마스크 예측은 M2M의 보조 입력으로 사용됩니다. 예측의 정확도를 높이기 위해 알파 매트 예측을 위해 멀티스케일 브랜치를 채택하고 반복적인 개선 일정을 통해 이러한 예측을 병합합니다.

멀티 스케일 예측: 사전 학습된 SAM 모델은 신속한 지침을 통해 대상 인스턴스에 대한 피처 맵 F와 마스크 예측 m을 생성합니다. 재조정된 이미지, 마스크, 특징 맵을 연결하여 M2M 모듈에 대한 입력을 구성합니다. M2M은 다양한 해상도에서 알파 매트 예측을 생성하기 위해 여러 가지 정제 블록을 사용합니다. 이 접근 방식을 통해 MAM은 다양한 크기의 오브젝트를 처리하고 세분화된 알파 매트를 제공할 수 있습니다.

반복적 개선: 예측의 정확도를 높이기 위해 반복적 개선 프로세스가 사용됩니다. 훈련 중에 이미지의 여러 영역을 강조하는 가중치 맵을 계산합니다. 이러한 가중치 맵은 예측의 각 척도에 대한 손실을 계산하는 데 사용되며, 이미지의 다른 영역을 다른 척도로 강조합니다. 추론하는 동안 예측을 SAM의 마스크 예측 m과 병합하여 최종 알파 매트 예측을 얻습니다.

3.3. 무엇이든 매팅 모델

M2M 모듈을 개발한 후 이를 SAM과 통합하여 매팅 애니씽 모델(MAM)을 생성합니다. 이 포괄적이고 통합된 프레임워크는 특징 추출부터 알파 매트 예측에 이르는 전체 매팅 프로세스를 처리합니다.

다중 데이터 세트 훈련: 견고성과 다양성을 보장하기 위해 다양한 이미지 매팅 데이터 세트의 다양한 인스턴스와 배경 이미지를 포괄하는 다중 데이터 세트 훈련 방식을 채택합니다. 훈련 중에 전경 인스턴스와 해당 기준점 알파 매트 및 배경 이미지를 결합하여 합성 이미지를 생성합니다. 합성 이미지와 관심 인스턴스를 캡슐화하는 바운딩 박스는 SAM으로 전송되며, SAM은 인스턴스의 마스크 예측을 반환합니다. 그런 다음 이미지, 마스크, 특징 맵이 M2M 모듈로 전송되어 멀티 스케일 알파 매트 예측을 반환합니다. 손실은 멀티 스케일 예측과 기준 진실 사이에서 계산됩니다.

다중 벤치마크 추론: 추론 단계에서 여러 이미지 매트 벤치마크에서 MAM을 평가했습니다. 입력 이미지가 주어지면 SAM은 인스턴스의 대략적인 묘사를 캡처하여 초기 마스크 예측을 생성했습니다. 그런 다음 M2M은 다중 스케일 예측을 제공하여 알파 매트 예측을 개선했습니다. 마스크 예측의 해당 영역을 각각의 멀티 스케일 예측으로 대체하여 예측을 반복적으로 업데이트했습니다. 이러한 반복적인 개선 작업을 통해 알파 매트 예측을 반복적으로 개선하고 최종 예측의 정밀도를 높일 수 있었습니다.

### 4. Experiments

설명한 평가 및 실험 결과에 따르면 다중 모드, 다중 스케일 매팅 방법을 사용하는 MAM 모델이 다양한 이미지 매팅 작업에 매우 효과적이라는 것을 알 수 있습니다. 이러한 작업은 시맨틱 이미지 매팅, 인스턴스 이미지 매팅, 참조 이미지 매팅을 포함한 여러 벤치마크에서 테스트되었습니다.

MAM은 여러 이미지 매팅 데이터 세트의 다양한 전경 인스턴스와 실제 및 합성 데이터 세트의 배경 이미지를 사용하여 학습되었습니다. 평가에는 5가지 메트릭이 사용되었으며, 사용된 메트릭에 따라 값이 낮거나 높을수록 더 나은 성능을 나타냅니다. 훈련 및 추론 중 설정도 설명했습니다.

주요 결과에 따르면, MAM은 모든 벤치마크에서 SAM(또 다른 이미지 매팅 모델)에 비해 상당한 개선을 보였습니다. 또한 MAM 모델은 각 매팅 작업에 특화된 최첨단 모델과 비슷하거나 더 나은 성능을 달성했습니다. MAM의 성능은 특히 HIM2K, RWP635, RefMatte-RW100 벤치마크에서 두드러지게 나타났습니다.

또한 MAM의 개별 구성 요소를 평가하기 위해 상세한 절제 연구를 수행했습니다. 연구 결과 멀티스케일 접근 방식, 반복적 개선, 다중 데이터 세트 트레이닝이 MAM의 우수한 성능의 핵심 요소로 밝혀졌습니다.

하지만 MAM은 SAM의 정확한 피처 맵과 마스크 예측에 의존한다는 한계가 있습니다. SAM이 부정확한 마스크 예측을 생성하면 MAM 모델이 이러한 오류를 수정하는 데 어려움을 겪을 수 있으며, 이는 부정확한 최종 예측으로 이어질 수 있습니다. 이러한 한계는 향후 연구 및 개발에서 해결해야 할 과제입니다.

![Untitled](Matting%20Anything%205d586ff12e9e4f60a910ce005b680375/Untitled%202.png)

전반적으로 실험 결과, MAM은 다양한 작업과 벤치마크에서 고품질의 성능을 달성할 수 있는 이미지 매팅을 위한 효과적이고 다재다능한 솔루션으로 나타났습니다.

### 5. Conclusion

이 논문에서는 이미지 매팅 작업을 위한 새로운 솔루션인 매팅 애니씽 모델(MAM)을 소개합니다. MAM은 세그먼트 애니씽 모델(SAM)을 가이던스 모듈로 활용하고 경량 마스크 투 매트(M2M) 모듈을 사용하여 마스크 출력을 대상 인스턴스의 알파 매트로 구체화합니다.

M2M 모듈은 다양한 이미지 매팅 작업을 처리하도록 특별히 설계되었습니다. 점, 상자 또는 텍스트와 같은 다양한 사용자 프롬프트와 함께 작동할 수 있습니다. 시맨틱 이미지 매팅, 인스턴스 이미지 매팅, 참조 이미지 매팅까지 확장할 수 있습니다.

MAM의 성능은 6가지 이미지 매팅 벤치마크에 걸쳐 평가되었습니다. 그 결과, MAM은 모든 벤치마크에서 특화된 최첨단 방법과 비슷한 성능을 달성할 수 있을 뿐만 아니라 다양한 평가 지표에서도 우수한 성능을 보였습니다.

결론적으로, 제안된 MAM은 인터랙티브하고 통합된 이미지 매팅을 위한 보다 다양하고 효율적인 솔루션을 제공합니다. 여러 이미지 매팅 작업 전반에 걸쳐 유연성과 적응성을 제공하므로 이 분야에서 흥미로운 진전을 이룰 수 있습니다.