# Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation

[https://arxiv.org/abs/2212.11565](https://arxiv.org/abs/2212.11565)

[https://github.com/showlab/Tune-A-Video](https://github.com/showlab/Tune-A-Video)

![하나의 텍스트-비디오 쌍과 사전 학습된 T2I 모델을 사용한 T2V 생성을 위한 새로운 방법, Tune-A-Video.](Tune-A-Video%20One-Shot%20Tuning%20of%20Image%20Diffusion%20Mo%20c32ea2951c4c4935bc8f50cd7122890c/Untitled.png)

하나의 텍스트-비디오 쌍과 사전 학습된 T2I 모델을 사용한 T2V 생성을 위한 새로운 방법, Tune-A-Video.

### 1. Introduction

이 연구에서는 텍스트 프롬프트에서 비디오를 생성하는 새로운 방법인 '튠어 비디오'를 소개하는데, 이 방법은 원샷 비디오 튜닝이라는 기술을 사용합니다. 이 방법을 사용하면 다른 텍스트-비디오(T2V) 생성 모델의 전제 조건인 대규모 비디오 데이터 세트에 대한 비용과 시간이 많이 소요되는 훈련의 필요성을 줄일 수 있습니다.

![사전 학습된 T2I 모델에 대한 관찰: 1) 동사 항목을 정확하게 나타내는 정지 이미지를 생성할 수 있습니다. 2) 공간적 자기주의를 공간-시간적 자기주의로 확장하면 프레임 간 일관된 콘텐츠를 생성합니다.](Tune-A-Video%20One-Shot%20Tuning%20of%20Image%20Diffusion%20Mo%20c32ea2951c4c4935bc8f50cd7122890c/Untitled%201.png)

사전 학습된 T2I 모델에 대한 관찰: 1) 동사 항목을 정확하게 나타내는 정지 이미지를 생성할 수 있습니다. 2) 공간적 자기주의를 공간-시간적 자기주의로 확장하면 프레임 간 일관된 콘텐츠를 생성합니다.

튠어비디오는 텍스트-이미지(T2I) 모델의 관찰을 기반으로 하며, 동작 동사와 프레임 전체에 걸쳐 일관된 객체 등 텍스트 설명에 따라 이미지를 효과적으로 생성할 수 있습니다. 이러한 모델의 공간적 자체주의를 여러 이미지로 확장하면 모션이 연속적이지는 않지만 프레임 전체에 걸쳐 일관된 콘텐츠를 생성할 수 있습니다.

튠어 비디오에서는 T2I 모델을 시공간적 차원에 적용하여 희박한 시공간적 주의 메커니즘과 효율적인 튜닝 전략을 도입합니다. 그러나 이 방법은 높은 계산 비용을 초래할 수 있으며 프레임에 연속적인 움직임이 부족할 수 있습니다. 따라서 이 연구에서는 부드럽고 일관된 비디오를 제작하기 위해 DDIM 반전으로 알려진 구조 안내 방법을 사용합니다. 특히 이 접근 방식은 기존의 개인화 및 조건부 사전 학습 T2I 모델과 호환됩니다.

이 논문은 튠어비디오가 다양한 텍스트 기반 비디오 생성 애플리케이션에서 상당히 우수한 성능을 발휘한다는 것을 보여줍니다. 광범위한 실험을 통해 최첨단 모델과 비교했을 때 우수한 결과를 보여줌으로써 T2V 생성에 대한 보다 효율적이고 경제적인 접근 방식을 제공합니다.

### 2. Related Work

이 연구는 확산 모델, 텍스트 프롬프트에서 이미지/비디오 생성, 실제 이미지/비디오의 텍스트 기반 편집, 단일 비디오로 학습된 생성 모델 등 다양한 분야의 교차점에 있습니다.

텍스트-이미지(T2I) 확산 모델은 널리 연구되어 왔으며, 최근에는 이미지 품질과 텍스트-이미지 정렬을 개선하기 위해 확산 기법을 채택한 여러 모델이 등장했습니다. 예를 들어, GLIDE는 확산 모델에서 분류기 없는 안내를 사용하고 Imagen은 고화질 비디오 생성을 위해 캐스케이드 확산 모델을 사용합니다. 제안된 방법은 이러한 노력을 기반으로 2D 모델을 잠재 공간의 시공간 영역으로 확장합니다.

텍스트-비디오(T2V) 생성 모델은 대규모 텍스트-비디오 데이터 세트의 부족과 시간적 일관성 모델링의 복잡성으로 인해 T2I 모델보다 뒤쳐져 있습니다. 이 분야에서는 2D VQ-VAE를 최초로 사용하고 T2V 생성을 위해 드물게 관심을 기울인 GODIVA와 멀티태스크 학습 접근 방식을 통해 이를 확장한 NUWA 등 많은 진전이 있었습니다. 그러나 이러한 모든 모델은 훈련을 위해 광범위한 비디오 데이터에 크게 의존하는 반면, 이 연구에서는 단일 텍스트-비디오 쌍을 사용하여 기존의 T2I 확산 모델을 훈련하는 새로운 프레임워크를 소개합니다.

이 논문에서는 텍스트 기반 비디오 편집에 대해서도 논의하며, 기존 모델에는 시간 인식 기능이 부족하여 프레임 간 불일치가 발생한다는 점을 지적합니다. 또한 현재 모델은 계산이 까다롭고 공개적으로 액세스할 수 없습니다.

마지막으로, 단일 비디오에서 콘텐츠를 생성하는 측면에서 기존 GAN 기반 방법은 계산 비용이 많이 들고 확장성이 다소 떨어지는 경향이 있습니다. 일부 진전에도 불구하고 이러한 모델은 입력된 비디오의 일부를 복사하는 것이 당연한 작업만 처리할 수 있으며, 새로운 의미적 맥락을 가진 비디오를 생성하는 데 어려움을 겪고 있습니다. 이번 연구는 이러한 한계를 해결하고자 합니다.

### 3. Method

제공된 텍스트는 텍스트 입력에서 새로운 동영상 콘텐츠를 생성할 수 있는 모델을 만드는 데 사용되는 연구 방법에 대한 설명인 것으로 보입니다. 이 방법은 이미지 합성과 같은 작업에 사용되는 생성 모델인 노이즈 제거 확산 확률 모델(DDPM)과 잠복 확산 모델(LDM)을 활용하며, 이를 동영상 생성을 위한 시간적 영역으로 확장하여 적용합니다.

![Tune-A-Video의 고수준 개요. 캡션이 달린 비디오가 주어지면, 우리는 사전 학습된 T2I 모델(예: Stable Diffusion)을 T2V 모델링을 위해 미세 조정합니다. 추론 단계에서는 입력 비디오의 시간적 일관성을 유지하면서 텍스트 프롬프트의 편집을 나타내는 새로운 비디오를 생성합니다.](Tune-A-Video%20One-Shot%20Tuning%20of%20Image%20Diffusion%20Mo%20c32ea2951c4c4935bc8f50cd7122890c/Untitled%202.png)

Tune-A-Video의 고수준 개요. 캡션이 달린 비디오가 주어지면, 우리는 사전 학습된 T2I 모델(예: Stable Diffusion)을 T2V 모델링을 위해 미세 조정합니다. 추론 단계에서는 입력 비디오의 시간적 일관성을 유지하면서 텍스트 프롬프트의 편집을 나타내는 새로운 비디오를 생성합니다.

이 방법의 단계는 여러 섹션으로 나뉩니다:

3.1 사전 준비: 이 섹션에서는 노이즈 제거 확산 확률 모델(DDPM)과 잠복 확산 모델(LDM)에 대한 이론적 개요를 제공합니다. 이러한 모델은 마르코프 체인에서 데이터를 재구성하도록 학습되며, DDPM은 데이터를 생성하기 위해 마르코프 체인의 역과정을 활용합니다. LDM은 자동 인코더의 잠재 공간에서 작동하는 DDPM의 변형 모델입니다.

![Tune-A-Video 파이프라인: 텍스트-비디오 쌍(예: "남자가 스키를 타고 있다")을 입력으로 받아, 우리의 방법은 사전 학습된 T2I 확산 모델을 T2V 생성을 위해 활용합니다. 미세 조정 과정에서는, 표준 확산 훈련 손실을 사용하여 attention 블록 내의 projection matrices를 업데이트합니다. 추론 단계에서는, 입력 비디오에서 역산된 잠재 노이즈에서 새로운 비디오를 샘플링하며, 수정된 프롬프트(예: "스파이더맨이 해변에서 서핑을 하고 있고, 만화 스타일이다")를 안내합니다.](Tune-A-Video%20One-Shot%20Tuning%20of%20Image%20Diffusion%20Mo%20c32ea2951c4c4935bc8f50cd7122890c/Untitled%203.png)

Tune-A-Video 파이프라인: 텍스트-비디오 쌍(예: "남자가 스키를 타고 있다")을 입력으로 받아, 우리의 방법은 사전 학습된 T2I 확산 모델을 T2V 생성을 위해 활용합니다. 미세 조정 과정에서는, 표준 확산 훈련 손실을 사용하여 attention 블록 내의 projection matrices를 업데이트합니다. 추론 단계에서는, 입력 비디오에서 역산된 잠재 노이즈에서 새로운 비디오를 샘플링하며, 수정된 프롬프트(예: "스파이더맨이 해변에서 서핑을 하고 있고, 만화 스타일이다")를 안내합니다.

3.2 네트워크 인플레이션: 연구원들은 2D 잠복 확산 모델(LDM)을 시공간 영역으로 확장하여 동영상을 생성합니다. 이는 추가 시간적 차원을 수용하기 위해 DDPM과 LDM에 일반적으로 사용되는 U-Net 아키텍처를 수정함으로써 달성됩니다. 이 확장에는 생성된 비디오 프레임에서 시간적 일관성을 유지하도록 설계된 시공간적 주의 메커니즘의 적용이 포함됩니다.

![우리의 ST-Attn 설명: 프레임 vi의 잠재 특징, 이전 프레임 vi-1 및 v1이 쿼리 Q, 키 K 및 값 V로 투영됩니다. 출력은 쿼리와 키 특징 사이의 유사성에 의해 가중된 값들의 가중 평균입니다. 우리는 업데이트된 매개변수 WQ를 강조합니다.](Tune-A-Video%20One-Shot%20Tuning%20of%20Image%20Diffusion%20Mo%20c32ea2951c4c4935bc8f50cd7122890c/Untitled%204.png)

우리의 ST-Attn 설명: 프레임 vi의 잠재 특징, 이전 프레임 vi-1 및 v1이 쿼리 Q, 키 K 및 값 V로 투영됩니다. 출력은 쿼리와 키 특징 사이의 유사성에 의해 가중된 값들의 가중 평균입니다. 우리는 업데이트된 매개변수 WQ를 강조합니다.

3.3 미세 조정 및 추론: 여기서는 시간적 모델링을 개선하고 텍스트-비디오 정렬을 개선하기 위해 특정 레이어에 초점을 맞춰 모델에 적용된 미세 조정 프로세스에 대해 설명합니다. 또한 연구진은 생성된 비디오에서 소스 비디오의 구조적 움직임을 유지하기 위해 노이즈 제거 확산 암시적 모델(DDIM)을 활용하는 구조 안내 방법을 제안합니다.

요약하면, 이 방법은 텍스트 설명을 생성된 비디오로 변환하는 방법을 설명하며, 시간적 일관성이 높고 구조적 움직임이 보존될 수 있습니다.

![우리의 방법에 대한 샘플 결과.](Tune-A-Video%20One-Shot%20Tuning%20of%20Image%20Diffusion%20Mo%20c32ea2951c4c4935bc8f50cd7122890c/Untitled%205.png)

우리의 방법에 대한 샘플 결과.

### 4. Applications of Tune-A-Video

원고는 텍스트 기반 비디오 생성 및 편집에서 제안된 "Tune-A-Video" 방법론의 적용 시나리오를 제시합니다. 주요 사용 사례는 다음과 같습니다:

개체 편집: "Tune-A-Video" 방법을 사용하면 텍스트 프롬프트를 변경하는 것만으로 비디오의 개체를 수정할 수 있습니다. 여기에는 기존 개체 교체, 새 개체 추가 또는 개체 제거가 포함됩니다. 예를 들어, "남자"를 "스파이더맨" 또는 "원더우먼"으로 바꾸거나, "토끼"를 "고양이" 또는 "강아지"로 바꾸거나, "수박"을 "치즈버거"로 바꿀 수 있으며, 해당 텍스트 프롬프트를 수정하는 것만으로 변경할 수 있습니다.

배경 변경: 개체 편집 외에도 '비디오 튜닝'을 사용하면 개체의 움직임을 유지하면서 비디오의 배경을 변경할 수 있습니다. 텍스트 프롬프트에서 위치/시간 설명을 사용하여 새 배경을 설명할 수 있습니다.

스타일 전송: "튠어 비디오"에서 사전 학습된 텍스트-이미지(T2I) 모델을 사용하면 실제 비디오를 만화 또는 반 고흐 스타일과 같은 다양한 스타일로 변환할 수 있습니다. 원하는 스타일은 텍스트 프롬프트에 통합됩니다.

개인화 및 제어 가능한 생성: '튠어 비디오'는 드림부스와 같은 개인화된 T2I 모델과 통합하여 특정 스타일이나 주제의 비디오를 만들 수 있습니다. 또한 T2I 어댑터 및 컨트롤넷과 같은 조건부 T2I 모델과 함께 사용하여 생성된 비디오를 다양하게 제어할 수 있습니다. 이러한 폭넓은 호환성을 통해 사용자는 더욱 개인화되고 창의적인 비디오 콘텐츠를 제작할 수 있습니다.

이러한 애플리케이션은 텍스트 기반 비디오 생성 및 편집을 위한 '튠어 비디오' 접근 방식의 다양성을 보여줍니다.

### 5. Experiments

실험 섹션에서는 구현 프로세스, 기준 모델과의 비교 및 절제 연구에 대한 자세한 정보를 제공했습니다. 주요 요점은 다음과 같습니다:

![Untitled](Tune-A-Video%20One-Shot%20Tuning%20of%20Image%20Diffusion%20Mo%20c32ea2951c4c4935bc8f50cd7122890c/Untitled%206.png)

5.1. 구현 세부 사항: 개발은 잠재 확산 모델을 기반으로 하며 사전 학습된 가중치를 사용했습니다. 512 x 512 해상도의 입력 비디오에서 32 프레임을 사용했으며, 학습률 3 × 10^-5, 배치 크기 1로 500단계에 걸쳐 모델을 미세 조정했습니다. 추론 시에는 분류기 없는 가이드를 갖춘 DDIM 샘플러를 사용했습니다. NVIDIA A100 GPU에서는 미세 조정에 약 10분, 샘플링에 약 1분이 소요되었습니다.

5.2. 기준선 비교: 평가는 상용 캡션 모델과 수동으로 디자인된 140개의 프롬프트를 사용하여 DAVIS 데이터 세트의 42개 대표 비디오에 대해 수행되었습니다. 비교는 세 가지 기준선에 대해 이루어졌습니다: T2V 모델인 CogVideo, 이미지 편집 모델인 플러그 앤 플레이, 텍스트 가이드 비디오 편집 방식인 Text2LIVE. 연구 결과, 이러한 모델에는 특정 강점이 있는 반면 프레임 일관성이나 편집 능력 부족과 같은 중대한 약점도 있었습니다. 반면, 여러분의 방식은 편집된 프롬프트와 잘 일치하는 시간적 일관성이 있는 동영상을 성공적으로 생성했습니다.

정량적 결과: 평가에는 자동 지표(프레임 일관성을 위한 클립 이미지 임베딩 및 텍스트 충실도를 위한 평균 클립 점수)와 사용자 연구가 모두 사용되었습니다. 연구 결과, 프레임 일관성과 텍스트 충실도 모두에서 사용자 방식이 기준선보다 우수한 것으로 나타났습니다.

5.3. 절제 연구: 이 연구는 Tune-A-Video에서 시공간 주의(ST-Attn) 메커니즘, DDIM 반전 및 미세 조정의 중요성을 평가하기 위해 수행되었습니다. 그 결과 이러한 모든 구성 요소가 방법의 성공에 결정적인 역할을 하는 것으로 나타났습니다. ST-Attn이 없으면 콘텐츠에 상당한 불일치가 발생하고 반전이 없으면 모델이 입력 비디오의 모션을 재현하는 데 실패했습니다. 미세 조정이 없으면 프레임 사이의 움직임이 부드럽지 않아 비디오가 깜박이는 현상이 발생했습니다.

![한계: 입력 비디오에 여러 개체가 포함되어 있고 가림현상이 나타날 때 우리의 방법은 불쾌한 결과를 낼 수 있습니다. 예를 들어, 아래쪽의 두 판다가 서로 섞여 있습니다.](Tune-A-Video%20One-Shot%20Tuning%20of%20Image%20Diffusion%20Mo%20c32ea2951c4c4935bc8f50cd7122890c/Untitled%207.png)

한계: 입력 비디오에 여러 개체가 포함되어 있고 가림현상이 나타날 때 우리의 방법은 불쾌한 결과를 낼 수 있습니다. 예를 들어, 아래쪽의 두 판다가 서로 섞여 있습니다.

### 6. Limitations and Future Work

이 방법의 가장 큰 한계는 입력 비디오에 여러 개체가 포함되어 있고 오클루전이 나타나는 경우입니다. 이는 여러 객체와 객체 상호 작용을 처리하는 데 있어 T2I 모델의 본질적인 한계 때문입니다. 잠재적인 해결책은 깊이와 같은 추가 조건부 정보를 사용하여 여러 객체와 객체의 상호 작용을 구분하는 데 도움이 될 수 있습니다. 이 측면은 향후 연구에서 살펴볼 필요가 있습니다.

### 7. Conclusion

원샷 비디오 튜닝이라는 T2V 생성을 위한 새로운 작업을 소개했습니다. 이 작업에는 단일 텍스트-비디오 쌍과 사전 학습된 T2I 모델만을 사용하여 T2V 생성기를 훈련하는 작업이 포함됩니다. 텍스트 기반 비디오 생성 및 편집을 위한 프레임워크인 Tune-A-Video를 발표하셨습니다. 시간적으로 일관된 비디오를 생성하기 위해 효율적인 튜닝 전략과 구조적 반전을 제안했습니다. 실험을 통해 이 방법이 다양한 사용 사례와 애플리케이션에 적용 가능하며 놀라운 결과를 얻을 수 있음을 보여주었습니다.

### Appendix

논문의 부록에 사용된 데이터 세트, 수행한 사용자 연구 및 추가 결과에 대한 자세한 내용을 제공합니다.

A. 데이터 세트 세부 정보: 동물, 차량, 사람 등 다양한 카테고리를 포괄하는 DAVIS 데이터 세트의 42개 동영상을 사용했습니다. 선택한 비디오 항목은 표 2에 나열되어 있습니다. 비디오 영상을 생성하기 위해 자동 캡션에 BLIP-2를 사용했습니다. 그런 다음 각 비디오에 대해 3개의 편집 프롬프트를 수동으로 생성하여 총 140개의 편집 프롬프트를 생성했습니다. 이러한 프롬프트에는 섹션 4에서 설명한 대로 개체 편집, 배경 변경 및 스타일 전송이 포함되었습니다.

B. 사용자 연구 세부 정보: 편집된 140개의 프롬프트 데이터 세트에 대한 사용자 연구를 수행하여 두 가지 기준선과 방법을 비교했습니다: 플러그 앤 플레이와 코그비디오. 사용자 연구 참가자는 주로 학생과 대학 동료였습니다. 5명의 평가자에게 두 가지 방법으로 제작한 두 개의 동영상을 비교하여 각 편집된 프롬프트를 평가해 달라고 요청했습니다. 그런 다음 두 가지 질문에 답하도록 요청했습니다: 어떤 동영상의 일관성이 더 높은지(또는 동영상으로서 더 매끄럽게 보이는지), 어떤 동영상이 텍스트 설명과 더 잘 일치하는지.

C. 추가 결과: 방법의 추가 비디오 예시, 기준선과의 추가 비교 및 절제 연구의 더 많은 결과는 각각 그림 10, 11, 12, 13에 나와 있습니다.