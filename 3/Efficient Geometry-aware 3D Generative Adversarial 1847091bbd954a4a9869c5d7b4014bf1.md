# Efficient Geometry-aware 3D Generative Adversarial Networks

[https://nvlabs.github.io/eg3d/](https://nvlabs.github.io/eg3d/)

[https://arxiv.org/abs/2112.07945](https://arxiv.org/abs/2112.07945)

[https://nvlabs.github.io/eg3d/media/eg3d.pdf](https://nvlabs.github.io/eg3d/media/eg3d.pdf)

- Dec 2021

### 1. Introduction

생성적 적대 신경망(GAN)은 크게 발전하여 최근에는 사실적인 2D 이미지를 생성할 수 있는 모델도 등장했습니다. 그러나 GAN의 3D 모델링은 이와 같은 수준의 발전을 이루지 못했습니다. 일반적으로 이미지 품질, 해상도, 3D 재구성 품질이 2D GAN에 비해 뒤떨어집니다. 이러한 지연은 이전 3D 제너레이터와 뉴럴 렌더링 시스템의 계산 비효율성 때문인 경우가 많습니다.

새로 도입된 제너레이터 아키텍처는 2D 이미지에서 3D 렌더링의 효율성을 개선하는 것을 목표로 합니다. 이는 모델의 품질 저하 없이 속도와 메모리를 개선하는 하이브리드 명시적-묵시적 3D 표현을 사용하여 달성할 수 있습니다. 또한 이 방법은 이전에 렌더링 해상도와 품질을 저해했던 계산상의 제약 조건을 우회할 수 있습니다. 이중 판별 전략은 뉴럴 렌더링과 최종 출력 간의 일관성을 보장하고 뷰의 불일치를 수정하는 데 사용됩니다.

또한 이 아키텍처는 포즈 기반 컨디셔닝을 제너레이터에 도입합니다. 이를 통해 얼굴 표정과 같은 포즈 관련 특징을 분리하여 출력의 일관성을 보장하는 동시에 데이터 내 포즈 관련 특징의 고유한 공동 분포를 고려합니다.

이 프레임워크의 주요 이점은 피처 생성과 뉴럴 렌더링을 분리한다는 점입니다. 이를 통해 StyleGAN2와 같은 고성능 2D CNN 기반 피처 생성기를 활용하면서도 일관된 3D 뉴럴 볼륨 렌더링을 제공할 수 있습니다.

이 작업의 주요 기여 사항은 다음과 같습니다:

- 효율적이고 표현력이 뛰어나며 고해상도 지오메트리 인식 이미지 합성이 가능한 트라이플레인 기반 3D GAN 프레임워크 도입.
- 멀티뷰 일관성을 촉진하고 포즈와 관련된 속성 분포를 충실하게 모델링하는 3D GAN 훈련 전략 개발.
- 무조건적인 3D 인식 이미지 합성 및 2D 이미지로부터 고품질 3D 지오메트리 학습을 위한 최첨단 결과물을 시연합니다.

### 2. Related work

새로운 신경 장면 표현은 신경 렌더링을 통해 2D 멀티뷰 이미지를 사용하여 최적화할 수 있는 차별화 가능한 3D 인식 모델을 사용합니다. 하지만 메모리 효율성, 계산 비용, 장면 복잡성 사이의 절충점에 직면해 있습니다. 복셀 그리드와 같은 명시적 표현은 빠르지만 상당한 메모리 오버헤드가 발생할 수 있습니다. 암시적 표현은 메모리 효율이 더 높고 복잡한 장면을 처리할 수 있지만 더 큰 네트워크가 필요하고 처리 속도가 느립니다.

![신경 암시적 표현은 위치 인코딩(PE)이 있는 완전 연결 레이어(FC)를 사용하여 장면을 표현합니다. 쿼리 속도가 느릴 수 있습니다(a). 명시적 복셀 그리드 또는 하이브리드 변형 작은 암시적 디코더를 사용하는 경우 쿼리 속도는 빠르지만 해상도 확장성이 확장성이 떨어집니다(b). 우리의 명시적-암시적 하이브리드 트라이플레인 표현(c)은 빠르고 해상도에 따라 효율적으로 확장되므로 동일한 용량으로 더 큰 디테일을 구현할 수 있습니다.](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled.png)

신경 암시적 표현은 위치 인코딩(PE)이 있는 완전 연결 레이어(FC)를 사용하여 장면을 표현합니다. 쿼리 속도가 느릴 수 있습니다(a). 명시적 복셀 그리드 또는 하이브리드 변형 작은 암시적 디코더를 사용하는 경우 쿼리 속도는 빠르지만 해상도 확장성이 확장성이 떨어집니다(b). 우리의 명시적-암시적 하이브리드 트라이플레인 표현(c)은 빠르고 해상도에 따라 효율적으로 확장되므로 동일한 용량으로 더 큰 디테일을 구현할 수 있습니다.

명시적-암시적 표현의 하이브리드 방식을 사용하여 두 가지 장점을 결합한 새로운 접근 방식이 제안되었습니다. 이 모델은 메모리 효율적인 트라이플레인 표현을 사용하여 축 정렬 평면에 피처를 명시적으로 저장하고, 이를 경량 암시적 피처 디코더로 집계하여 효율적인 볼륨 렌더링을 구현합니다. 이것이 3D GAN에서 고품질 이미지를 구현하는 핵심입니다.

생성적 적대 신경망(GAN)은 사실적인 2D 이미지를 생성하는 데 상당한 진전을 이루었습니다. 이러한 기능을 3D 설정으로 확장하기 위한 노력이 진행되고 있지만 결과는 엇갈리고 있습니다. 메시 기반 접근 방식은 고충실도 이미지를 생성하기에는 표현력이 부족하고, 복셀 기반 GAN은 고해상도 3D GAN 학습에 너무 많은 메모리와 연산 리소스를 필요로 합니다. 3D 장면 생성을 위한 암시적 표현 네트워크는 속도가 느리고 생성된 이미지의 품질과 해상도에 제한이 있습니다.

이 연구에서는 3D 기반 유도 바이어스를 사용하여 고해상도, 뷰 일관성 있는 이미지와 고품질 3D 형상을 성공적으로 생성하는 효율적인 3D GAN 아키텍처를 소개합니다. 2D CNN 기반 특징 생성기를 활용하고 보다 효율적인 신경 볼륨 렌더링을 가능하게 하는 3면 표현을 도입합니다. 또한 뉴럴 렌더링 후 2D CNN 기반 업샘플링을 사용하지만 이중 판별을 사용하여 뷰 불일치를 방지합니다.

이 새로운 접근 방식은 주로 3D로 작동하기 때문에 StyleNeRF 및 CIPS-3D와 같은 다른 3D 인식 GAN에 비해 보기 일관성이 뛰어나고 고품질 3D 형상을 생성할 수 있는 능력을 보여줍니다. 또한 FFHQ 및 AFHQ 데이터 세트에서 우수한 프리셰트 시작 거리(FID) 이미지 점수를 보고합니다.

### 3. Tri-plane hybrid 3D representation

고해상도 GAN을 훈련하려면 효율적이고 표현력이 뛰어난 3D 표현이 필요합니다. 이러한 기준을 충족하는 새로운 하이브리드 명시적-묵시적 삼면 표현이 제안되었습니다.

![멀티뷰 패밀리 씬의 합성된 뷰입니다, 완전 암시적 Mip-NeRF 표현(왼쪽), 고밀도 복셀 그리드(가운데), 삼면 표현(가운데) 비교. 복셀 그리드(가운데), 삼면 표현(오른쪽)을 비교합니다. 심지어 복셀이나 트라이플레인 모두 뷰 종속 효과를 모델링하지 않지만, 높은 품질을 구현합니다.](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%201.png)

멀티뷰 패밀리 씬의 합성된 뷰입니다, 완전 암시적 Mip-NeRF 표현(왼쪽), 고밀도 복셀 그리드(가운데), 삼면 표현(가운데) 비교. 복셀 그리드(가운데), 삼면 표현(오른쪽)을 비교합니다. 심지어 복셀이나 트라이플레인 모두 뷰 종속 효과를 모델링하지 않지만, 높은 품질을 구현합니다.

3면 표현에서는 명시적 특징이 세 개의 직교 특징 평면을 따라 정렬되며, 각 평면의 해상도는 N × N × C입니다(여기서 N은 공간 해상도이고 C는 채널 수입니다). 3D 위치는 세 개의 특징 평면 각각에 투영하고, 이중 선형 보간을 통해 해당 특징 벡터를 검색하고, 세 개의 특징 벡터를 합산하여 쿼리합니다. 추가적인 경량 디코더 네트워크는 집계된 3D 피처를 색상과 밀도로 해석한 다음 볼륨 렌더링을 사용하여 RGB 이미지로 렌더링합니다.

이 하이브리드 표현의 가장 큰 장점은 효율성으로, 디코더를 작게 유지하고 대부분의 표현력을 명시적 피처로 이동시킴으로써 표현력을 잃지 않으면서도 완전 암시적 아키텍처에 비해 뉴럴 렌더링의 계산 비용을 절감할 수 있습니다.

![Mip-NeRF에 비한 상대 속도 향상과 메모리 사용량 비교. 제안된 tri-plane 표현법은 완전히 암묵적인 Mip-NeRF 네트워크에 비해 3-8배 빠르며, 그 메모리의 일부만 필요로 합니다. 이 예에서, 보텍스 그리드와 tri-plane 표현법 모두 MLP 기반의 디코더를 사용하며, 이는 표시되어 있습니다. 보텍스의 개수는 tri-plane 표현법의 총 파라미터와 일치하도록 선택되었으므로, 해상도는 상대적으로 낮고 메모리 사용량은 Mip-NeRF보다 적습니다. SSO 실험(Fig. 3)에서는, 이 실험에 속도보다 표현력을 최적화하기 위해 tri-plane 표현법에 대해 GAN 실험에서 보다 큰 디코더를 사용했습니다. 이 내용은 섹션 4에서 논의됩니다.](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%202.png)

Mip-NeRF에 비한 상대 속도 향상과 메모리 사용량 비교. 제안된 tri-plane 표현법은 완전히 암묵적인 Mip-NeRF 네트워크에 비해 3-8배 빠르며, 그 메모리의 일부만 필요로 합니다. 이 예에서, 보텍스 그리드와 tri-plane 표현법 모두 MLP 기반의 디코더를 사용하며, 이는 표시되어 있습니다. 보텍스의 개수는 tri-plane 표현법의 총 파라미터와 일치하도록 선택되었으므로, 해상도는 상대적으로 낮고 메모리 사용량은 Mip-NeRF보다 적습니다. SSO 실험(Fig. 3)에서는, 이 실험에 속도보다 표현력을 최적화하기 위해 tri-plane 표현법에 대해 GAN 실험에서 보다 큰 디코더를 사용했습니다. 이 내용은 섹션 4에서 논의됩니다.

트라이플레인 표현은 탱크와 사원 데이터 세트의 한 장면의 360도 뷰에 맞게 평면의 특징과 디코더의 가중치를 최적화한 새로운 뷰 합성 설정에서 평가되었습니다. 그 결과 3면 표현이 복잡한 장면을 표현할 수 있으며 피크 신호 대 잡음비(PSNR) 및 구조적 유사성 지수 측정(SSIM) 측면에서 밀도가 높은 피처 볼륨 및 완전 암시적 표현보다 성능이 뛰어나다는 것이 입증되었습니다. 또한 트라이플레인 표현은 고밀도 복셀처럼 O(N^3)가 아닌 O(N^2)로 확장되므로 동일한 용량과 메모리로 더 높은 해상도의 피처를 사용하고 더 많은 디테일을 캡처할 수 있습니다.

3면 표현의 또 다른 주요 장점은 기성 2D CNN 기반 생성기로 특징 면을 생성할 수 있어 GAN 프레임워크를 사용하여 3D 표현 전반에 걸쳐 일반화할 수 있다는 점입니다.

### 4. 3D GAN framework

제안된 3면 표현은 명시적인 3D 또는 멀티뷰 감독 없이도 2D 사진에서 기하학적 구조를 인식하는 이미지 합성을 위해 3D 생성적 적대 신경망(GAN) 프레임워크에 통합되었습니다. 카메라 내재적 요소와 외재적 요소는 기성품 포즈 감지기를 사용하여 각 훈련 이미지와 연결되었습니다.

![우리의 3D GAN 프레임워크는 여러 부분으로 구성되어 있습니다: 포즈 조건부 StyleGAN2 기반 특징 생성기 및 매핑 네트워크, 경량 특징 디코더를 사용한 삼면 3D 표현, 신경 볼륨 렌더러, 초고해상도 모듈, 이중 판별 기능을 갖춘 포즈 조건부 StyleGAN2 판별기입니다. 이 아키텍처는 특징 생성 및 신경 렌더링을 우아하게 분리하여 3D 장면 일반화를 위한 강력한 StyleGAN2 제너레이터를 사용할 수 있습니다. 또한 가벼운 3D 3면 표현은 표현력이 뛰어나고 효율적이어서 고품질의 3D 인식 뷰 합성을 실시간으로 구현할 수 있습니다.](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%203.png)

우리의 3D GAN 프레임워크는 여러 부분으로 구성되어 있습니다: 포즈 조건부 StyleGAN2 기반 특징 생성기 및 매핑 네트워크, 경량 특징 디코더를 사용한 삼면 3D 표현, 신경 볼륨 렌더러, 초고해상도 모듈, 이중 판별 기능을 갖춘 포즈 조건부 StyleGAN2 판별기입니다. 이 아키텍처는 특징 생성 및 신경 렌더링을 우아하게 분리하여 3D 장면 일반화를 위한 강력한 StyleGAN2 제너레이터를 사용할 수 있습니다. 또한 가벼운 3D 3면 표현은 표현력이 뛰어나고 효율적이어서 고품질의 3D 인식 뷰 합성을 실시간으로 구현할 수 있습니다.

네트워크 아키텍처는 몇 가지 핵심 요소로 구성됩니다. 먼저, CNN 제너레이터 백본과 렌더링은 무작위 잠재 코드와 카메라 파라미터를 기반으로 StyleGAN2 제너레이터를 사용하여 3면 특징을 생성합니다. 특징 이미지는 3개의 32채널 평면을 형성하도록 재구성됩니다. 시스템은 이러한 각 트라이플레인의 특징을 취합하고 주어진 카메라 포즈에서 32채널 특징 이미지를 예측합니다.

![듀얼 디스크리에이션은 원시 신경 렌더링 IRGB와 초고해상도 출력 I + RGB가 일관성을 유지합니다, 고해상도 및 멀티뷰 일관성 있는 렌더링이 가능합니다.](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%204.png)

듀얼 디스크리에이션은 원시 신경 렌더링 IRGB와 초고해상도 출력 I + RGB가 일관성을 유지합니다, 고해상도 및 멀티뷰 일관성 있는 렌더링이 가능합니다.

둘째, '슈퍼 해상도' 모듈을 사용하여 신경 렌더링된 이미지를 업샘플링하고 개선합니다. 3면 표현은 계산적으로 효율적이지만 인터랙티브 프레임 속도를 유지하면서 고해상도로 훈련하거나 렌더링하는 것은 불가능합니다. 따라서 볼륨 렌더링은 적당한 해상도에서 수행되며 이미지 공간 컨볼루션을 사용하여 신경 렌더링을 최종 이미지 크기로 업샘플링합니다.

셋째, 렌더링은 StyleGAN2 판별기에서 수정된 2D 컨볼루션 판별기에 의해 비판됩니다. 다중 뷰 불일치 문제를 피하기 위해 "이중 판별"이라는 기술이 도입되었습니다. 신경 렌더링된 특징 이미지의 처음 세 개의 특징 채널은 저해상도 RGB 이미지로 처리되어 이 이미지와 초고해상도 이미지 간의 일관성을 보장합니다.

또한 판별기는 생성된 이미지가 렌더링되는 카메라 포즈도 고려하므로 보다 정확한 3D 선행 이미지를 얻을 수 있습니다. 데이터 세트의 포즈에 따른 편향을 처리하고 렌더링 과정에서 모델이 2D 광고판으로 변질되는 것을 방지하기 위해 제너레이터 포즈 컨디셔닝과 같은 다른 기술도 도입되었습니다.

이러한 기술을 결합하여 2D 사진에서 지오메트리 인식 이미지 합성을 위한 효율적이고 표현력이 풍부한 3D GAN 프레임워크를 구축하는 동시에 계산 효율성을 유지하고 멀티뷰 불일치 문제와 같은 일반적인 문제를 피할 수 있습니다.

### 5. Experiments and results

이 섹션에서는 3D 생성적 적대 신경망(GAN) 방법의 실험 설정과 결과에 중점을 둡니다. 이 방법은 두 가지 데이터 세트에서 테스트되었습니다: 사람 얼굴 이미지가 포함된 FFHQ와 고양이 얼굴 이미지가 포함된 AFHQv2 Cats입니다. 두 데이터 세트 모두에 수평 플립을 적용하고 포즈 추정기를 사용하여 카메라 외형을 추출했습니다.

![FFHQ [28] 및 AFHQv2 고양이 [7]로 훈련된 모델에 의해 합성된 5122에서 선별한 예시들](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%205.png)

FFHQ [28] 및 AFHQv2 고양이 [7]로 훈련된 모델에 의해 합성된 5122에서 선별한 예시들

제안한 방법은 기존의 세 가지 3D 인식 이미지 합성 방법인 πGAN, GIRAFFE, Lifting StyleGAN과 비교했습니다. 이 방법은 뷰 일관성이 있고 세부적인 3D 형상을 정확하게 캡처하는 고품질 이미지를 합성하는 것으로 나타났습니다. 또한 이 방법은 프리셋 시작 거리(FID), 깊이와 포즈에 대한 평균 제곱 오차(MSE), 얼굴 동일성 일관성 등 여러 정량적 측정에서 다른 세 가지 방법보다 뛰어난 성능을 보였습니다.

![GIRAFFE, π-GAN, Lifting StyleGAN, 우리의 것과 함께 FFHQ에서의 2562에서의 질적 비교. 형태들은 marching cubes를 사용하여 밀도 필드에서 추출한 iso-표면입니다. 우리는 GIRAFFE의 기본적인 3D 표현을 조사하였고, 그것의 이미지 공간 근사치에 대한 과도한 의존이 3D 기하학의 학습에 상당한 해를 끼친다는 것을 발견하였습니다.](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%206.png)

GIRAFFE, π-GAN, Lifting StyleGAN, 우리의 것과 함께 FFHQ에서의 2562에서의 질적 비교. 형태들은 marching cubes를 사용하여 밀도 필드에서 추출한 iso-표면입니다. 우리는 GIRAFFE의 기본적인 3D 표현을 조사하였고, 그것의 이미지 공간 근사치에 대한 과도한 의존이 3D 기하학의 학습에 상당한 해를 끼친다는 것을 발견하였습니다.

다양한 뷰에서 일관성을 유지하는 데 도움이 되는 제안된 방법의 특징인 이중 차별의 효과를 평가하기 위해 제거 연구를 수행했습니다. 연구 결과 이중 판별이 합성된 이미지의 불일치를 크게 줄여주는 것으로 나타났습니다.

또한 제안된 방법이 효율적인 실시간 렌더링을 가능하게 하여 실시간 시각화와 같은 애플리케이션에 적합하다는 것을 보여주었습니다. 이 방법은 리프팅 스타일간이나 지라프만큼 빠르지는 않지만 우수한 이미지 품질, 지오메트리 품질, 뷰 일관성으로 인해 약간 증가하는 컴퓨팅 비용을 정당화할 수 있습니다.

마지막으로 이 방법의 몇 가지 잠재적인 적용 사례에 대해 설명합니다. 3D 표현은 StyleGAN2 백본으로 설계되었기 때문에 스타일 믹싱과 같은 시맨틱 이미지 조작이 가능합니다. 또한 학습된 3D 선행은 고품질의 단일 뷰 3D 재구성을 가능하게 하므로 향후 연구에 유익한 영역이 될 수 있습니다.

![FFHQ 5122와 함께한 스타일-믹싱 [27-29]](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%207.png)

FFHQ 5122와 함께한 스타일-믹싱 [27-29]

![우리는 목표 이미지에 맞추고 기본적인 3D 형태를 복구하기 위해 PTI [57]를 사용합니다. 목표 (왼쪽); 재구성된 이미지 (가운데); 재구성된 형태 (오른쪽). FFHQ 5122에서 훈련된 모델에서 가져옴.](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%208.png)

우리는 목표 이미지에 맞추고 기본적인 3D 형태를 복구하기 위해 PTI [57]를 사용합니다. 목표 (왼쪽); 재구성된 이미지 (가운데); 재구성된 형태 (오른쪽). FFHQ 5122에서 훈련된 모델에서 가져옴.

### 6. Discussion

한계: 제안된 방법으로 생성된 형상은 이전의 3D 인식 GAN에 비해 상당한 개선을 보였지만, 여전히 아티팩트가 포함될 수 있으며 개별 치아와 같은 세밀한 디테일이 부족할 수 있습니다. 이를 개선하기 위해 더 강력한 지오메트리 사전 처리 또는 방사 필드의 밀도 구성 요소의 정규화를 구현할 수 있습니다. 또한 이 모델은 데이터 세트의 카메라 포즈 분포에 대한 지식이 필요하며, 이는 제한 요소가 될 수 있습니다. 향후 연구에서는 포즈 분포를 즉석에서 학습하는 방법을 모색할 수 있습니다. 이 방법은 외모와 포즈를 분리하는 데 진전을 이루었지만 아직 완전히 달성하지는 못했습니다.

잠재적인 미래 방향: 이미지 간 변환 모델이나 트랜스포머 기반 모델과 같은 다양한 2D 백본 모델을 제안된 프레임워크로 테스트할 수 있습니다. 이를 통해 조건부 합성에서 새로운 애플리케이션을 구현할 수 있습니다.

윤리적 고려 사항: 저자들은 모델의 3D 재구성 및 스타일 혼합 애플리케이션이 실제 인물의 편집된 이미지를 생성하는 데 오용될 수 있으며, 이로 인해 잘못된 정보가 확산되거나 평판이 손상될 수 있음을 인지하고 있습니다. 그들은 이러한 오용을 강력히 권장하지 않습니다. 또한 사용한 데이터 세트의 암묵적인 편견으로 인해 얼굴 결과의 다양성이 부족할 수 있다는 가능성도 인정합니다.

결론: 제안된 방법은 효율적인 명시적-암시적 신경 표현과 포즈 인식 컨볼루션 생성기 및 이중 판별기를 결합합니다. 이 접근 방식은 사실적인 3D 인식 이미지 합성 및 고품질 비지도 형상 생성을 크게 발전시킵니다. 3D 모델의 신속한 프로토타이핑, 보다 제어 가능한 이미지 합성, 시간적 데이터로부터 형상을 재구성하는 새로운 기법 등이 잠재적으로 응용될 수 있습니다.

### Supplemental Material

본 논문의 부록에서 저자는 추가 실험, 시각적 결과, 구현, 모델 아키텍처, 학습 프로세스 및 하이퍼파라미터에 대한 세부 정보를 제공합니다. 또한 사용된 데이터 세트 및 기준선과 같은 실험의 세부 사항을 자세히 살펴보고 반전과 같은 특정 실험에 대한 설명도 제공합니다. 향후 작업의 잠재적 영역을 검토하여 결론을 내립니다.

또한 FFHQ라는 데이터 세트에서 포즈와 표정 사이의 상관관계를 분석합니다. 연구진은 카메라를 마주보고 있는 사람이 미소를 지을 가능성이 더 높다는 사실을 발견했으며, 이러한 현상은 사람들이 의도적으로 사진을 찍힐 때 미소를 짓는 경향이 있기 때문이라고 설명합니다. 이러한 상관관계는 카메라가 움직일 때 합성된 얼굴의 표정이 바뀌는 '표정 왜곡'으로 이어질 수 있습니다. 이를 해결하기 위해 연구진은 이중 판별 및 제너레이터 포즈 컨디셔닝 방법을 제안합니다.

또한 이미지에서 3D 모델을 재구성하는 소프트웨어인 COLMAP을 사용하여 멀티뷰 일관성을 검증합니다. 합성된 비디오 시퀀스에서 3D 포인트 클라우드를 생성하고 3D GAN(생성적 적대 신경망)이 여러 시점에 걸쳐 일관된 렌더링을 생성하는 것을 확인합니다.

![COLMAP [32, 33]을 이용해 합성된 비디오의 128 프레임(상단)을 복구하여 타원형 궤적을 따릅니다. 결과적으로 밀도가 높고 잘 정의된 포인트 클라우드(하단)는 매우 다양한 시점의 일관성 있는 렌더링을 나타냅니다.](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%209.png)

COLMAP [32, 33]을 이용해 합성된 비디오의 128 프레임(상단)을 복구하여 타원형 궤적을 따릅니다. 결과적으로 밀도가 높고 잘 정의된 포인트 클라우드(하단)는 매우 다양한 시점의 일관성 있는 렌더링을 나타냅니다.

마지막으로 제너레이터 포즈 컨디셔닝을 위한 정규화 기법을 도입합니다. 이 기술이 없으면 모델은 원래 카메라 위치에서만 보기 좋은 3D 렌더링을 생성하는 '퇴화된' 솔루션을 학습하게 됩니다. 이를 방지하기 위해 제너레이터에 주어진 포즈 정보를 데이터 세트와 다른 포즈로 무작위로 교체하여 모델이 여러 각도에서 보기 좋은 얼굴을 생성하도록 유도하는 무작위 요소를 도입했습니다. 훈련 시작 시 100%의 스와핑 확률로 시작하여 100만 개의 이미지를 처리한 후 선형적으로 50%로 감소합니다. 이 시점 이후에는 나머지 훈련 기간 동안 스와핑 확률을 50%로 유지합니다.

![생성기 포즈 조절을 간단히 적용하면, 생성기는 항상 렌더링 카메라의 위치를 인식하기 때문에 열화된 해결책이 나옵니다. 이런 접근법은 "의도된" 시점에서 (즉, 생성기가 조절된 카메라 포즈) 합리적인 렌더링을 만들지만, 조절 정보를 고정하고 카메라를 이동하면 모델이 카메라의 알려진 위치를 향해 "빌보드"를 만들도록 배웠다는 것이 분명해집니다.](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%2010.png)

생성기 포즈 조절을 간단히 적용하면, 생성기는 항상 렌더링 카메라의 위치를 인식하기 때문에 열화된 해결책이 나옵니다. 이런 접근법은 "의도된" 시점에서 (즉, 생성기가 조절된 카메라 포즈) 합리적인 렌더링을 만들지만, 조절 정보를 고정하고 카메라를 이동하면 모델이 카메라의 알려진 위치를 향해 "빌보드"를 만들도록 배웠다는 것이 분명해집니다.

저자는 부정확한 카메라 포즈에 대한 방법의 견고성을 평가하기 위해 연구를 진행합니다. 저자들은 각 이미지에 대략적인 카메라 포즈로 레이블이 지정된 데이터 세트를 사용하여 모델을 학습시킵니다. 이러한 라벨링은 특정 데이터 세트에서는 쉽게 수행할 수 있지만 다른 데이터 세트에서는 어려울 수 있습니다. 저자들은 이 방법의 견고성을 테스트하기 위해 포즈 정밀도가 각각 다른 5개의 모델을 추가로 학습시켰습니다(포즈 컨디셔닝이 없는 기준 모델 포함). 그 결과 매우 부정확한 카메라 포즈도 3D 형상을 렌더링하는 데 충분한 것으로 나타났습니다. 연구진은 약한 감독만 있으면 된다는 사실을 발견했으며, 이는 향후 연구를 위한 잠재적인 영역을 열어주었습니다.

![공급된 카메라 포즈의 정확도에 대한 견고성을 평가하기 위해, 판별자 포즈 조절 없는 기준과 카메라 외부의 잡음으로 인해 손상된 판별자 포즈 조절 모델을 비교합니다. 판별자 포즈 조절 없이 모델은 헤드를 평면에 평평하게 그린 텍스처로 열화된 해결책을 배웁니다. 심지어 매우 부정확한 외부요소(예: 세 표준 편차의 잡음으로 손상된 카메라 포즈)도 이 열화된 해결책을 해결하고 정확한 3D 형상을 복구할 수 있습니다.](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%2011.png)

공급된 카메라 포즈의 정확도에 대한 견고성을 평가하기 위해, 판별자 포즈 조절 없는 기준과 카메라 외부의 잡음으로 인해 손상된 판별자 포즈 조절 모델을 비교합니다. 판별자 포즈 조절 없이 모델은 헤드를 평면에 평평하게 그린 텍스처로 열화된 해결책을 배웁니다. 심지어 매우 부정확한 외부요소(예: 세 표준 편차의 잡음으로 손상된 카메라 포즈)도 이 열화된 해결책을 해결하고 정확한 3D 형상을 복구할 수 있습니다.

또한 가파른 카메라 각도에서 뷰를 생성하는 다른 알고리즘과도 비교했습니다. 데이터 세트가 주로 정면 이미지로 구성되어 있다는 점을 고려할 때, 가파른 각도로 추정할 수 있는 기능은 바람직한 품질입니다. 이 점에서 이들의 방법은 다른 방법, 특히 이미지 합성을 위해 이미지 공간 컨볼루션에 크게 의존하는 방법보다 희귀한 뷰에 대해 더 합리적인 외삽을 제공함으로써 더 나은 성능을 보였습니다.

![우리는 카메라의 시점 각도가 가파를 경우의 메서드들을 비교합니다. 카메라 피치 또는 요의 백분위수가 표시됩니다. 요 각도가 96 백분위수인 경우, 96%의 훈련 포즈가 덜 가파르며, 즉 주어진 포즈를 넘는 훈련 포즈는 4%입니다.](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%2012.png)

우리는 카메라의 시점 각도가 가파를 경우의 메서드들을 비교합니다. 카메라 피치 또는 요의 백분위수가 표시됩니다. 요 각도가 96 백분위수인 경우, 96%의 훈련 포즈가 덜 가파르며, 즉 주어진 포즈를 넘는 훈련 포즈는 4%입니다.

마지막으로, 모든 데이터 세트에 대한 커널 시작 거리와 ShapeNet Cars의 이미지 품질 평가와 같은 추가 메트릭을 제공하는 확장된 표에 추가적인 정량적 결과를 제시합니다. 이 방법은 카메라 포즈가 균일하게 분포된 자동차 데이터 세트에서도 강력한 성능을 보여줬으며, 이는 이 방법이 정면 데이터 세트에만 국한되지 않음을 나타냅니다.

저자들은 도형의 '스타일 혼합'을 탐구하고 모델의 고해상도 레이어가 눈 영역이나 머리카락 패턴과 같은 도형의 미세한 디테일에 영향을 미칠 수 있음을 보여줍니다.

또한 피벗 튜닝 반전(PTI)이라는 기법을 사용하여 단일 테스트 이미지의 3D 재구성을 추가로 보여줍니다. 이 기술은 사진을 3D 아바타로 변환하는 것과 같은 새로운 응용 분야를 열 수 있다고 제안합니다.

셰이프넷 자동차라는 데이터 세트로 학습된 모델에 대해 무작위 카메라 포즈를 취한 렌더링을 선보입니다. 이는 정면 데이터세트에 국한된 다른 모델과 달리 전체 360° 카메라 방위각과 180° 카메라 고도를 처리할 수 있는 모델의 기능을 보여줍니다.

선별된 예제와 선별되지 않은 예제를 모두 포함하여 AFHQv2 Cats로 훈련된 모델의 시각적 결과를 제시합니다.

FFHQ라는 데이터 세트에서 훈련된 모델이 생성한 얼굴의 큐레이션되지 않은 예시를 제공합니다. 이들은 "잘라내기"라는 기법을 사용하여 고품질의 결과물을 유지했습니다.

이 예시에서는 잠상 코드 간의 부드러운 보간을 보여줌으로써 3D GAN 모델이 StyleGAN2 백본과 마찬가지로 컬러 렌더링과 기본 모양 모두에서 부드러운 전환을 허용하는 잘 동작하는 잠상 공간을 유지한다는 것을 보여줍니다.

![스타일-믹싱 [16-18]은 FFHQ 5122에서 훈련된 모델의 형태를 보여줍니다. 이는 메인 원고의 그림 8과 일치하며, 동일한 씨앗의 컬러 렌더링을 보여줍니다. 결과는 혼합된 예시가 대부분의 구조를 "대충" 입력(즉, 레이어 0-6의 변조)으로부터 물려받지만, "세밀" 입력(즉, 레이어 7-13의 변조)이 형태의 더 섬세한 세부 사항(예: 눈 부분, 머리 패턴)에 영향을 미치고, 렌더링된 이미지에서 전반적인 색상을 크게 제어할 수 있다는 것을 보여줍니다.](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%2013.png)

스타일-믹싱 [16-18]은 FFHQ 5122에서 훈련된 모델의 형태를 보여줍니다. 이는 메인 원고의 그림 8과 일치하며, 동일한 씨앗의 컬러 렌더링을 보여줍니다. 결과는 혼합된 예시가 대부분의 구조를 "대충" 입력(즉, 레이어 0-6의 변조)으로부터 물려받지만, "세밀" 입력(즉, 레이어 7-13의 변조)이 형태의 더 섬세한 세부 사항(예: 눈 부분, 머리 패턴)에 영향을 미치고, 렌더링된 이미지에서 전반적인 색상을 크게 제어할 수 있다는 것을 보여줍니다.

이 모델은 StyleGAN2의 공식 PyTorch 구현을 기반으로 구현되었습니다. 균등화된 학습 속도, 미니배치 표준편차 레이어, 제너레이터 가중치의 지수 이동 평균, R1 정규화를 사용한 비포화 로지스틱 손실 등 주요 훈련 파라미터가 StyleGAN2에서 그대로 이어집니다.

컴퓨팅 리소스를 절약하기 위해 2단계 훈련이 사용됩니다. 대부분의 훈련은 64x64의 뉴럴 렌더링 해상도에서 수행된 후 점차 128x128까지 단계적으로 증가합니다. 그러나 최종 이미지 해상도는 훈련 내내 고정된 상태로 유지됩니다(예: 256x256 또는 512x512).

"백본"(StyleGAN2 생성기)은 최종 이미지 해상도에 관계없이 모든 실험에서 256x256의 해상도로 작동합니다. 이는 96채널 출력 특징 이미지를 생성하며, 이 이미지는 각각 256x256x32 모양의 세 개의 평면으로 재구성됩니다.

FFHQ 및 ShapeNet Cars와 같은 대규모 데이터 세트의 경우 모델을 처음부터 훈련합니다. AFHQv2와 같은 작은 데이터 세트의 경우, 더 큰 데이터 세트에서 학습된 체크포인트로부터 미세 조정합니다.

이 디코더는 64개의 숨겨진 유닛으로 구성된 단일 숨겨진 레이어로 구성된 다층 퍼셉트론(MLP)으로 구현됩니다. 이 디코더는 소프트플러스 활성화 기능을 사용합니다.

특징의 신경 볼륨 렌더링이라는 방법을 사용하며, 2패스 중요도 샘플링을 사용합니다. 얇은 표면이 특징인 동영상을 렌더링할 때 추론하는 동안 광선당 샘플 수를 조정하여 깜박임을 줄입니다.

초고해상도: 초고해상도 모델에는 노이즈 입력 없이 StyleGAN2의 변조 컨볼루션 두 블록이 포함됩니다. 이 블록은 각각 채널 깊이가 128과 64인 컨볼루션으로 구성됩니다.

판별자: 판별기는 표준 3채널 입력 이미지와 달리 6채널 입력 이미지를 수용하도록 입력 레이어를 조정하여 이중 판별을 허용하는 수정된 StyleGAN2입니다. 판별기는 들어오는 이미지의 카메라 매개변수에 따라 조정됩니다.

혼합 정밀도: 저자는 훈련 속도를 개선하기 위해 혼합 정밀도를 활용했습니다. 판별기의 가장 높은 해상도 블록 4개와 초고해상도 모듈의 두 블록 모두에 FP16 정밀도를 사용했습니다. 그러나 생성기 백본은 FP16 정밀도를 사용하지 않았습니다.

R1 정규화: R1 정규화는 γ = 0.1을 사용하는 셰이프넷 자동차를 제외한 모든 데이터 세트 및 해상도에 γ = 1로 사용됩니다.

밀도 정규화: 원치 않는 이음새와 모양 아티팩트를 줄이기 위해 추정 밀도 필드에 추가 정규화를 도입했습니다. 이 정규화는 밀도 필드의 부드러움을 향상시킵니다.

훈련: 모델은 배치 크기 32, 판별자 학습률 0.002, 생성기 학습률 0.0025로 훈련되었습니다. 훈련은 2단계로 진행되었으며, 25M 이미지의 경우 64x64 해상도로, 추가 250M 이미지의 경우 128x128 해상도로 진행되었습니다. 훈련 시간은 사용된 뉴럴 렌더링 해상도에 따라 1000개 이미지당 24초에서 46초 사이로 다양했습니다.

추론 시간 깊이 샘플: 추론 시간에는 얇은 물체가 등장하는 동영상을 렌더링할 때 원치 않는 깜박임을 줄이기 위해 광선당 샘플 수를 늘렸습니다. 거친 샘플과 미세 샘플의 수를 두 배로 늘려 광선당 깊이 샘플의 총 수를 192개로 늘렸습니다.

AFHQv2: 15]의 방법론에 따라 FFHQ에서 훈련된 모델을 미세 조정하여 고양이에서 최적의 성능을 달성했습니다. 64x64의 뉴럴 렌더링 해상도로 620만 개의 이미지를 학습하고, 추가로 260만 개의 이미지를 학습하면서 뉴럴 렌더링 해상도를 128x128로 미세 조정했습니다. 또한 512x512 해상도에서만 적응형 판별자 증강(ADA)을 활용했습니다.

이 섹션에서는 저자가 기준선, 데이터 세트 세부 정보 및 학습 방법론을 포함하여 실험의 세부 사항을 설명합니다.

기준선: 저자들은 자신의 모델을 π-GAN, GIRAFFE, 리프팅 StyleGAN, StyleGAN2를 포함한 여러 기준 모델과 비교했습니다. 이러한 각 모델은 유사한 데이터 세트에 권장되는 매개 변수를 사용하여 수렴할 때까지 공식 코드를 사용하여 학습되었습니다.

데이터 세트 세부 정보:

FFHQ: 이 데이터 세트는 Flickr에서 가져온 사람들의 자르지 않은 원본 PNG 이미지의 "야생" 컬렉션입니다. 작성자는 상용 얼굴 감지 및 포즈 추출 파이프라인을 사용하여 얼굴 영역을 식별하고 이미지에 포즈 라벨을 붙였습니다. 이미지는 원본 FFHQ 데이터세트와 거의 같은 크기로 잘라냈습니다. 이 데이터 세트는 모든 이미지에서 표준 인물 렌즈와 동일한 초점 거리로 고정된 카메라 내재성을 가정합니다. 얼굴 인식에 저항하는 몇 개의 이미지가 제거되어 최종 데이터 세트에는 69957개의 이미지가 포함됩니다.

![이중 차별에서, 우리는 고해상도의 최종 이미지와 시점 일관성이 있는(하지만 저해상도인) 신경 렌더링 사이의 일관성을 유지하기 위해 최종 이미지와 원시 신경 렌더링의 6채널 이어붙임에 대해 차별화합니다. 이 다이어그램은 실제 이미지와 가짜 이미지에 대해 6채널 판별자 입력 텐서를 얻는 방법을 보여줍니다. 우리의 생성기는 5122 최종 렌더링(I+RGB)과 원시 신경 렌더링(IRGB) 1282를 모두 만듭니다. 원시 렌더링, IRGB는 32채널 렌더링된 특징, IF의 처음 세 채널입니다. 우리는 원시 이미지를 5122로 업샘플링하고 최종 이미지와 이어붙여 (512 × 512 × 6) 판별자 입력 텐서를 만듭니다. 실제 이미지의 경우, 우리는 데이터 세트에서 5122 실제 이미지를 추출하고 IRGB와 같은 크기로 다운샘플링하여 IRGB의 아날로그를 얻습니다. 그런 다음 이 이미지를 다시 5122로 업샘플링하고 원래 이미지와 이어붙여 (512 × 512 × 6) 판별자 입력 텐서를 만듭니다. 다운샘플 후 업샘플 작업은 원래 이미지를 흐리게 만드는 효과가 있습니다.](Efficient%20Geometry-aware%203D%20Generative%20Adversarial%201847091bbd954a4a9869c5d7b4014bf1/Untitled%2014.png)

이중 차별에서, 우리는 고해상도의 최종 이미지와 시점 일관성이 있는(하지만 저해상도인) 신경 렌더링 사이의 일관성을 유지하기 위해 최종 이미지와 원시 신경 렌더링의 6채널 이어붙임에 대해 차별화합니다. 이 다이어그램은 실제 이미지와 가짜 이미지에 대해 6채널 판별자 입력 텐서를 얻는 방법을 보여줍니다. 우리의 생성기는 5122 최종 렌더링(I+RGB)과 원시 신경 렌더링(IRGB) 1282를 모두 만듭니다. 원시 렌더링, IRGB는 32채널 렌더링된 특징, IF의 처음 세 채널입니다. 우리는 원시 이미지를 5122로 업샘플링하고 최종 이미지와 이어붙여 (512 × 512 × 6) 판별자 입력 텐서를 만듭니다. 실제 이미지의 경우, 우리는 데이터 세트에서 5122 실제 이미지를 추출하고 IRGB와 같은 크기로 다운샘플링하여 IRGB의 아날로그를 얻습니다. 그런 다음 이 이미지를 다시 5122로 업샘플링하고 원래 이미지와 이어붙여 (512 × 512 × 6) 판별자 입력 텐서를 만듭니다. 다운샘플 후 업샘플 작업은 원래 이미지를 흐리게 만드는 효과가 있습니다.

AFHQv2: 이 데이터 세트는 원본 AFHQ 데이터 세트의 고품질 버전으로 고양이, 개, 야생동물 등 동물 얼굴의 클로즈업 이미지를 제공합니다. 저자들은 실험에 약 5000개의 이미지가 포함된 '고양이' 분할을 사용했습니다. 이 데이터 세트는 모든 이미지에 고정된 카메라 내재값을 가정하며, FFHQ의 내재값과 동일합니다.

셰이프넷 자동차: 이 데이터 세트는 추가 검증에 사용되었습니다. 여기에는 128x128 해상도의 합성 자동차 렌더링이 포함되어 있으며, 각 렌더링에는 카메라 파라미터가 레이블로 지정되어 있습니다. 이 데이터세트에는 2457개의 고유한 자동차가 포함되어 있으며, 각 자동차는 전체 구체에서 무작위로 샘플링한 50개의 뷰에서 렌더링됩니다.

훈련: StyleGAN2 구성 F와 [15]의 512x512 구성을 모두 훈련하여 R1 정규화 강도를 스윕했습니다. γ = 1에서 10M 이미지에 대해 훈련한 후 StyleGAN2 구성 F를 사용하여 AFHQv2에 대한 최상의 결과를 얻었습니다.

단일 장면 과적합: 저자들은 탱크와 사원 데이터 세트의 가족 장면에서 복셀 기반 하이브리드 표현과 Mip-NeRF에 대한 3면 3D 표현의 효과를 테스트합니다. 복셀 및 3면 하이브리드 표현은 모두 푸리에 피처 임베딩이 포함된 2계층 128개의 숨겨진 유닛 디코더와 결합됩니다. 복셀 및 큐브 표현은 200만 회 반복, Mip-NeRF는 100만 회 반복에 대해 훈련됩니다.

피벗 튜닝 반전: 저자는 기성품 얼굴 감지 기능을 사용하여 테스트 이미지에서 적절한 크기의 크롭과 카메라 외형을 추출합니다. 이미지의 크기는 5122로 조정됩니다. 저자는 피벗 튜닝 반전을 사용하여 500회 반복을 위해 잠재 코드를 최적화한 다음, 추가로 500회 반복을 위해 제너레이터 가중치를 미세 조정합니다.

평가 지표:

FID 및 KID: 저자는 생성된 50만 개의 이미지와 모든 훈련 이미지 사이에서 프리쳇 시작 거리 및 커널 시작 거리 이미지 품질 메트릭을 계산합니다.

지오메트리: 데이터 세트 포즈 분포와 일치하는 무작위 포즈에서 1024개의 이미지와 뎁스 맵을 생성하여 지오메트리를 평가합니다.

멀티뷰 일관성: ArcFace 코사인 유사도를 측정하여 평가합니다. 1024개의 무작위 얼굴이 생성되고 훈련 데이터 세트 포즈 분포에서 무작위로 선택된 포즈에서 각 얼굴의 두 가지 뷰가 렌더링됩니다.

포즈 정확도: 사전 훈련된 얼굴 재구성 모델의 도움으로 평가됩니다. 1024개의 생성된 이미지에서 피치, 요, 롤을 감지하고 기준선 포즈에 대한 L2 손실을 계산하여 각 모델의 포즈 드리프트를 결정합니다.

런타임: 저자는 400프레임 시퀀스에 대한 평균 프레임 속도를 계산하여 각 모델의 런타임을 평가합니다.

FACS 추정: 얼굴 표정을 평가하기 위해 저자는 얼굴 동작 코딩 시스템 계수 측면에서 얼굴의 하위 영역의 세부적인 움직임을 측정하는 독점적인 얼굴 추적기를 사용합니다.

지오메트리 시각화: 도형을 시각화하기 위해 저자는 볼륨을 샘플링하여 5123큐브의 밀도 값을 얻고 행진 큐브를 사용하여 씬의 표면을 메시로 추출합니다.

모양 아티팩트: 이 방법은 아티팩트에서 완전히 자유롭지 않습니다. 예를 들어, 움푹 들어간 눈구멍은 카메라를 따라 눈이 움직이는 듯한 착각을 불러일으키는데, 이를 '속이 빈 얼굴 착시 현상'이라고 합니다. 마찬가지로 입꼬리 근처에 깊은 주름이 있으면 시야에 일관성이 없는 효과를 만들 수 있습니다. 저자들은 향후 연구에서 안구가 볼록하다는 가정과 같은 더 강력한 데이터 세트 선행 조건을 통합하여 이러한 아티팩트를 해결할 수 있다고 제안합니다.

안경: 이 방법은 이전 방법에 비해 더 세밀한 안경을 생성할 수 있지만, 안경의 측면이 빈 공간이 아닌 불투명한 '고글'을 생성하는 경향이 있습니다. 향후 렌즈 굴절을 정확하게 모델링할 수 있는 방법이 개발되면 안경과 투명 요소가 포함된 기타 물체를 더욱 충실하게 재구성할 수 있을 것입니다.

심 가시성: 이 방법으로 생성된 일부 모양과 렌더링에는 얼굴과 머리의 나머지 부분 사이에 이음새가 보입니다. 저자들은 이미지에서 강력한 지오메트리 복구에 유망한 결과를 보여준 최근의 하이브리드 SDF 렌더링 솔루션이 아티팩트를 줄이면서 향상된 모양을 생성할 수 있다고 제안합니다.

배경 처리: 현재 모델에서 저자들은 장면의 배경을 명시적으로 처리하지 않으므로 제너레이터가 전경 오브젝트에 융합된 텍스처 표면으로 배경을 표현하는 방법을 학습합니다. 향후 작업에서는 전경 오브젝트를 분리할 수 있도록 별도의 3D 표현으로 배경을 모델링할 수 있다고 제안합니다.

- 요약
    
    한계:
    
    - 이 모델은 이전 3D 인식 GAN보다 향상된 성능을 보여주지만 여전히 아티팩트를 포함할 수 있으며 개별 치아와 같은 세밀한 디테일이 부족합니다. 이는 추가 개선이 필요한 부분을 강조합니다.
    - 이 모델을 사용하려면 데이터 세트의 카메라 포즈 분포에 대한 지식이 필요합니다. 저자들은 이 분포를 즉석에서 학습하는 방법을 연구하기 위한 추가 연구를 제안합니다.
    - 외모와 포즈를 더 분리할 수 있는 여지가 있습니다. 지오메트리로 설명할 수 있는 모호성은 아직 해결되지 않은 상태로 남아 있습니다.
    
    향후 작업:
    
    - 학습된 모양의 품질을 개선하기 위해 저자는 더 강력한 지오메트리를 미리 구현하거나 방사 필드의 밀도 성분을 정규화할 것을 제안합니다.
    - 이미지 간 변환 모델이나 트랜스포머 기반 모델과 같은 다양한 2D 백본 모델을 테스트하면 조건부 합성에 새로운 응용 분야를 열 수 있습니다.
    
    윤리적 고려 사항:
    
    - 저자들은 자신들의 기술이 실제 인물의 오해의 소지가 있거나 잘못된 이미지를 생성하는 데 오용될 수 있음을 인정합니다. 저자는 이러한 오용을 권장하지 않습니다.
    - 또한 저자들은 사용한 데이터 세트에 암묵적인 편견이 있을 수 있으며, 이로 인해 생성된 얼굴의 다양성이 부족할 수 있다고 지적합니다.
    
    결론:
    
    - 이러한 한계와 윤리적 우려에도 불구하고 저자들의 방법은 사실적인 3D 인식 이미지 합성 및 고품질 비지도 형상 생성 분야에서 중요한 진전을 이루었습니다.
    - 이 방법은 효율적인 명시적-암시적 신경 표현, 포즈 인식 컨볼루션 생성기, 이중 판별기를 결합합니다.
    - 향후에는 신속한 3D 모델 프로토타이핑, 보다 제어 가능한 이미지 합성, 시간적 데이터로부터 형상을 재구성하는 새로운 기법 등이 적용될 수 있습니다.
    
    이 단계는 저자들의 연구에 대한 논의와 결론을 요약한 것입니다.