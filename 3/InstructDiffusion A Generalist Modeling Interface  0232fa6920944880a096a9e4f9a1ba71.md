# InstructDiffusion: A Generalist Modeling Interface for Vision Tasks

[https://arxiv.org/abs/2309.03895](https://arxiv.org/abs/2309.03895)

[https://gengzigang.github.io/instructdiffusion.github.io/](https://gengzigang.github.io/instructdiffusion.github.io/)

- Sep 2023

### 1 Introduction

![비전 작업을 위한 일반 모델링 인터페이스인 InstructDiffusion을 소개합니다. 입력 이미지와 인간의 지시를 받아, 우리의 통합 모델은 이미지 편집, 세분화, 키포인트 추정, 검출 및 저수준 비전과 같은 작업을 효과적으로 수행합니다.](InstructDiffusion%20A%20Generalist%20Modeling%20Interface%20%200232fa6920944880a096a9e4f9a1ba71/Untitled.png)

비전 작업을 위한 일반 모델링 인터페이스인 InstructDiffusion을 소개합니다. 입력 이미지와 인간의 지시를 받아, 우리의 통합 모델은 이미지 편집, 세분화, 키포인트 추정, 검출 및 저수준 비전과 같은 작업을 효과적으로 수행합니다.

배경:

- 인공 지능, 특히 자연어 처리 또는 NLP라고 하는 언어 이해 및 처리 분야에서는 상당한 발전이 있었습니다.
- 단일 접근 방식을 사용하여 다양한 언어 관련 작업을 수행할 수 있는 자연어 처리 모델 중 하나가 GPT입니다.
- 이제 연구자들은 컴퓨터 비전(컴퓨터가 이미지를 '보고' 이해하는 분야)에 대해서도 동일한 작업을 시도하고 있습니다.

컴퓨터 비전의 과제:

- 컴퓨터 비전에는 물체 식별, 모양 윤곽선 그리기, 이미지 생성, 이미지의 특정 지점 찾기 등 다양한 작업이 있습니다.
- 각 작업은 서로 다른 유형의 결과를 생성합니다. 예를 들어, 어떤 작업은 좌표를 제공하는 반면, 어떤 작업은 이미지나 카테고리를 제공합니다.
- 이러한 다양성 때문에 컴퓨터 비전을 위한 획일적인 솔루션이 존재하기 어렵습니다.
- 게다가 컴퓨터 비전에는 다양한 문제에 대한 다양한 전문화된 방법이 있습니다. 예를 들어, 어떤 모델은 이미지 생성에 탁월한 반면 다른 모델은 물체 인식에 탁월합니다.
- 컴퓨터 비전이 처리하는 정보(입력과 출력 모두)는 데이터 스트림처럼 연속적인 경우가 많습니다. 이를 불연속적인 덩어리로 바꾸면 오류가 발생할 수 있습니다.

제안된 솔루션:

- 연구진은 모든 컴퓨터 비전 문제를 '이미지 생성' 문제로 취급하는 새로운 방법을 고안했습니다.
인간이 자연스럽게 시각적 정보를 처리하는 방식과 일치하는 간단한 이미지 편집 지침을 제공합니다.

예시:

- 이미지에서 물체를 식별하기 위해 이 방법은 해당 물체의 색상을 변경할 수 있습니다.
- 이미지의 특정 지점을 정확히 찾아내는 경우, 해당 지점에 컬러 점을 배치할 수 있습니다.
- 이 새로운 접근 방식은 연속적인 데이터를 청크로 변환할 필요가 없으므로 특정 오류를 피할 수 있습니다.

백서의 초점:

- 이 논문에서는 주로 표준 이미지, 흑백 윤곽선, 정확한 위치의 세 가지 유형의 결과를 살펴봅니다.
- 이는 일반적인 컴퓨터 비전 작업의 대부분을 다룹니다.
- 연구진은 이러한 결과를 통합된 방식으로 표현하는 새로운 방법을 개발했습니다.
- 또한 새로운 데이터를 수집하여 모델을 훈련하고 그 효과를 테스트했습니다.

결과 및 중요성:

- 새로운 접근 방식은 다양한 작업에 적합합니다.
- 흥미롭게도 여러 작업에 대해 학습하면 모델이 더 잘 일반화될 수 있습니다. 즉, 특별히 학습하지 않은 작업이나 데이터에 대해 더 잘 수행할 수 있습니다.
- 이 모델은 직접 학습되지 않은 작업도 수행할 수 있어 컴퓨터 비전을 위한 보다 일반화되고 적응력 있는 형태의 인공 지능에 가까워질 수 있습니다.

요컨대, 컴퓨터가 언어를 이해하는 데는 큰 진전이 있었지만, 이미지를 '보고' 이해하게 하는 데는 나름의 어려움이 있습니다. 이 백서에서는 컴퓨터 비전 작업에 대한 보다 통합되고 일반화된 접근 방식을 향한 중요한 발걸음이 될 수 있는 새로운 방법을 소개합니다.

### 2. Related Work

범용 모델:

- AI 연구자들은 항상 모든 작업을 수행할 수 있는 모델을 만들고 싶어했습니다. 많은 연구에서 여러 작업을 하나의 솔루션으로 결합하려고 시도했습니다.

비전과 언어의 결합:

- 웹에 존재하는 방대한 양의 이미지-텍스트 쌍 덕분에 연구자들은 이미지와 텍스트를 모두 이해할 수 있는 모델을 만들려고 노력해 왔습니다.
- CLIP 및 ALIGN과 같은 초기 모델은 이미지와 텍스트를 인상적으로 연결했습니다. 이후 모델들은 이 아이디어를 확장했지만 처음부터 언어를 생성하는 데 어려움을 겪어 사용이 제한적이었습니다.
- GPT 시리즈와 같은 큰 텍스트 기반 모델이 등장하면서 이러한 모델에 비주얼을 '보고' 이해할 수 있는 기능을 부여하는 것에 대한 관심이 높아졌습니다.
- 일부 모델은 시각적 작업을 텍스트 예측 문제로 전환하여 시각적 이해와 언어 이해를 혼합하려고 시도했습니다.
- LLaVA와 같은 다른 모델은 이미지-텍스트 쌍을 지시를 따르는 과제로 전환했습니다. 우리의 연구는 시각적 과제를 지침으로 전환한다는 점에서 다른데, 이러한 과제에는 항상 명확한 지침이 없기 때문에 어렵습니다.

통합 비전 모델:

- 컴퓨터 비전 연구자들은 각각의 작업에 대해 특별히 훈련받지 않고도 많은 시각적 작업을 수행할 수 있는 모델을 꿈꿉니다. 여러 작업을 함께 수행하는 것, 즉 멀티태스크 학습이 트렌드가 되고 있습니다.
- 큰 문제는 각 작업마다 서로 다른 유형의 결과를 얻을 수 있다는 것입니다. 이러한 결과를 결합하는 방법에는 크게 두 가지가 있습니다:
- 언어와 유사한 생성: 언어 모델에서 영감을 받아 시각적 결과를 텍스트 또는 토큰의 시퀀스로 변환하는 방법도 있습니다.
- 이미지 유사 생성: 모든 것을 텍스트로 변환하는 대신 이미지 자체를 사용하는 모델도 있습니다.
예를 들어, Painter는 이미지 편집 문제와 같은 작업을 처리하는 반면, PromptDiffusion은 시각 자료와 텍스트를 모두 사용하여 여러 작업을 학습합니다.
- 저희의 연구는 이미지 닮은꼴 생성에 초점을 맞추고 있지만 조금 다릅니다. 일반적인 방법 대신 작업에 대한 명확한 지침을 사용합니다. 또한 이미지 생성의 중요한 부분인 이미지 편집에 중점을 둡니다.

본질적으로 많은 연구가 언어, 시각 또는 두 가지 모두에 대한 보편적인 모델을 만들려고 시도했지만 여전히 혁신의 여지가 있습니다. 이 연구는 다양한 시각적 작업에 대해 모델에 명시적인 지침을 제공함으로써 독특한 접근 방식을 취합니다.

### 3. Method

메서드 - InstructDiffusion 개요:

![우리의 방법의 훈련 파이프라인. 간결하게 설명하기 위해, 키포인트 검출을 예로 듭니다.](InstructDiffusion%20A%20Generalist%20Modeling%20Interface%20%200232fa6920944880a096a9e4f9a1ba71/Untitled%201.png)

우리의 방법의 훈련 파이프라인. 간결하게 설명하기 위해, 키포인트 검출을 예로 듭니다.

InstructDiffusion은 다양한 비전 작업을 위해 설계된 새로운 모델입니다.
언어 출력에 중점을 두는 Flamingo 및 BLIP2와 같은 모델과 달리, InstructDiffusion은 비전 작업을 이미지 조작 프로세스로 간주합니다.
이 모델의 출력에는 3채널 RGB 이미지, 바이너리 마스크 및 키포인트가 포함되며 이미지 향상, 키포인트 감지 및 분할과 같은 작업에 적합합니다.

3.1. 비전 작업을 위한 통합 명령어:

- 이 모델은 "지침 이미지 편집"이라는 방법을 사용하여 훈련 데이터를 표현합니다.
- 이 방법은 작업에 대한 매우 상세한 지침을 제공합니다. 예를 들어 키포인트 감지의 경우 "왼쪽 어깨를 빨간색으로 둘러싸"라는 명령이 있을 수 있습니다.
- 세분화의 경우 "개에게 파란색 반투명 마스크를 적용"이 그 예입니다.

3.2. 훈련 데이터 구축:

- 연구원들은 다양한 작업이 통합 접근 방식을 통해 이점을 얻을 수 있는지에 중점을 둡니다.
- 연구진은 다양한 비전 작업에 COCO 및 MPII와 같은 기존 데이터 세트를 사용합니다.
- 이미지 편집의 경우, 159,000개의 이미지 편집 쌍이 포함된 새로운 데이터 세트인 "이미지 편집 인 더 와일드(IEIW)"를 제안합니다.
- 이 데이터 중 일부는 웹, 특히 포토샵 요청 웹사이트로부터 가져옵니다.
- 이미지 품질 평가 도구를 사용하여 학습 데이터의 품질을 보장합니다.

3.3. 통합 프레임워크:

- 이 프레임워크는 확산 모델을 기반으로 합니다.
- 세 가지 훈련 단계가 있습니다:
    - 사전 훈련 적응: 여기에는 새 작업에 맞게 안정적 확산 모델을 미세 조정하는 작업이 포함됩니다.
    - 작업별 훈련: 다양한 데이터 세트를 사용하여 특정 작업에 맞게 모델을 미세 조정하여 작업 간의 균형을 유지합니다.
    - 휴먼 얼라인먼트: 편집된 이미지에 대한 사람의 피드백을 사용하여 모델을 더욱 세밀하게 조정합니다.

### 4. Experiments

실험 설정

- 모델은 {지시, 소스 이미지, 목표 이미지} 세트에 대해 학습됩니다.
- 작업: 키포인트 감지, 의미적 분할, 참조 분할, 이미지 향상(디블러링, 노이즈 제거, 워터마크 제거), 이미지 편집.
- 훈련 데이터 세트:
    - 키포인트 감지: COCO, CrowdPose, MPII, AIC.
    - 세분화: 시맨틱 세그먼테이션을 위한 COCO-Stuff, 참조 세그먼테이션을 위한 gRefCOCO 및 RefCOCO.
    - 이미지 향상: 노이즈 제거를 위한 GoPro 및 REDS, 노이즈 제거를 위한 SIDD, 워터마크 제거를 위한 CLWD.
    - 이미지 편집: 7개의 데이터 세트(InstructPix2Pix 및 MagicBrush 포함).
- 모델의 기술적 세부 사항: 48개의 NVIDIA V100 GPU에서 200회에 걸쳐 3072개의 배치 크기로 훈련된 Stable Diffusion v1.5 초기화를 사용합니다.

키포인트 감지:

- COCO, HumanArt 및 AP-10K 동물 데이터 세트에서 평가되었습니다.
- HumanArt 데이터 세트는 COCO와는 다른 데이터를 가지고 있으며, AP-10K는 동물 키포인트에 대한 모델의 능력을 보여줍니다.
- 이 모델은 강력한 일반화 능력을 보여주었고 보이지 않는 키포인트에 대해 뛰어난 성능을 보였지만 정확도에서는 전문 모델을 능가하지 못했습니다.
    
    ![우리 모델로 생성된 키포인트 검출 결과. 지시 사항은 다음과 같습니다: (a) 자동차 로고를 파란색 원으로 표시하세요. (b) 흰색 호랑이의 코에 파란색 원을 그리고 흰색 호랑이의 왼쪽 어깨 주변에 빨간색 원을 그리세요. (c) 고래의 오른쪽 눈 주변에 노란색 원을 만드세요. (d) 가장 왼쪽에 있는 사람의 오른쪽 손목 주변에 파란색으로 원을 그리고, 가장 오른쪽에 있는 사람의 왼쪽 손목 위에 노란색 원을 그리세요.](InstructDiffusion%20A%20Generalist%20Modeling%20Interface%20%200232fa6920944880a096a9e4f9a1ba71/Untitled%202.png)
    
    우리 모델로 생성된 키포인트 검출 결과. 지시 사항은 다음과 같습니다: (a) 자동차 로고를 파란색 원으로 표시하세요. (b) 흰색 호랑이의 코에 파란색 원을 그리고 흰색 호랑이의 왼쪽 어깨 주변에 빨간색 원을 그리세요. (c) 고래의 오른쪽 눈 주변에 노란색 원을 만드세요. (d) 가장 왼쪽에 있는 사람의 오른쪽 손목 주변에 파란색으로 원을 그리고, 가장 오른쪽에 있는 사람의 왼쪽 손목 위에 노란색 원을 그리세요.
    

세분화:

- 개방형 어휘 능력으로 평가된 모델의 능력. RefCOCO+, G-Ref, 파스칼 VOC와 같은 데이터 세트에서 평가되었습니다.
- 특히 RefClef 및 VOC 데이터 세트에서 제너럴리스트 모델보다 우수한 성능을 보였습니다.
- COCO-Stuff 데이터 세트에서 전문 모델을 능가했으며 오픈 세트 시나리오에서도 비슷한 성능을 달성했습니다.
    
    ![우리 모델로 생성된 세분화 결과. 지시 사항은 다음과 같습니다: (a) 거울 안의 고양이의 픽셀을 파란색으로 표시하고 나머지는 변경하지 마세요. (b) 중성구의 픽셀을 노란색으로 채우고 나머지 픽셀의 색상을 그대로 유지하세요. (c) 다른 픽셀에 영향을 주지 않고 동방 명주 타워의 픽셀을 빨간색으로 변경하세요. (d) 그림자의 픽셀을 파란색으로 칠하고 다른 픽셀의 현재 모양을 유지하세요.](InstructDiffusion%20A%20Generalist%20Modeling%20Interface%20%200232fa6920944880a096a9e4f9a1ba71/Untitled%203.png)
    
    우리 모델로 생성된 세분화 결과. 지시 사항은 다음과 같습니다: (a) 거울 안의 고양이의 픽셀을 파란색으로 표시하고 나머지는 변경하지 마세요. (b) 중성구의 픽셀을 노란색으로 채우고 나머지 픽셀의 색상을 그대로 유지하세요. (c) 다른 픽셀에 영향을 주지 않고 동방 명주 타워의 픽셀을 빨간색으로 변경하세요. (d) 그림자의 픽셀을 파란색으로 칠하고 다른 픽셀의 현재 모양을 유지하세요.
    

이미지 향상:

- GoPro(디블러링), SIDD(노이즈 제거), CLWD(워터마크 제거) 데이터 세트에서 평가했습니다.
- 출력 이미지와 기준 이미지 간의 차이를 측정하기 위해 PSNR 메트릭을 사용했습니다.
- 전문 모델은 이미지 향상을 위한 일반화에 한계가 있는 반면, 제너럴리스트 모델인 페인터는 노이즈 제거에는 뛰어났지만 상황 내 학습에는 어려움을 겪었습니다.
- 이 모델의 성능은 VAE 모델에 의해 제약을 받아 일부 정보 손실로 이어졌습니다.
    
    ![InstructDiffusion은 이미지 블러 제거, 노이즈 제거 및 워터마크 제거를 포함한 저수준 비전 작업에도 적용 가능합니다.](InstructDiffusion%20A%20Generalist%20Modeling%20Interface%20%200232fa6920944880a096a9e4f9a1ba71/Untitled%204.png)
    
    InstructDiffusion은 이미지 블러 제거, 노이즈 제거 및 워터마크 제거를 포함한 저수준 비전 작업에도 적용 가능합니다.
    

이미지 편집:

- 이미지 편집 방법을 테스트하기 위해 1,000개의 샘플로 벤치마크를 만들었습니다.
- 각 샘플에는 소스 이미지, 캡션, 편집 지침, 편집된 이미지의 대상 캡션이 포함되어 있습니다.
- 편집 시나리오에는 교체, 제거, 추가의 세 가지 주요 편집 시나리오가 있습니다.
- 편집을 평가하는 데는 클립 유사도(CLIP-Sim)와 미적 예측 점수(AP)라는 두 가지 메트릭이 사용됩니다.
- 이 방법은 Instruct-Pix2Pix 및 MagicBrush와 같은 다른 모델과 비교했을 때 유리합니다.
- 이미지 편집 품질을 평가할 때는 두 가지 지표뿐만 아니라 여러 요소를 고려하는 것이 중요합니다.
- 이 모델은 트럭을 기차로 바꾸거나 고양이의 스타일을 바꾸는 등 정밀한 편집을 수행할 수 있습니다.
    
    ![다양한 지시를 기반으로 한 이미지 편집 간의 비교. 왼쪽에서 오른쪽: 입력, Prompt-to-prompt [21], MagicBrush [96], EDICT [72], Null-text Inversion [48], 그리고 우리의 결과.](InstructDiffusion%20A%20Generalist%20Modeling%20Interface%20%200232fa6920944880a096a9e4f9a1ba71/Untitled%205.png)
    
    다양한 지시를 기반으로 한 이미지 편집 간의 비교. 왼쪽에서 오른쪽: 입력, Prompt-to-prompt [21], MagicBrush [96], EDICT [72], Null-text Inversion [48], 그리고 우리의 결과.
    
    ![우리 모델로 생성된 이미지 편집 결과.](InstructDiffusion%20A%20Generalist%20Modeling%20Interface%20%200232fa6920944880a096a9e4f9a1ba71/Untitled%206.png)
    
    우리 모델로 생성된 이미지 편집 결과.
    

매우 상세한 지침:

- 자세한 지침은 모델이 작업을 더 잘 이해할 수 있으므로 단순한 지침보다 선호됩니다.
- 단순한 지침은 특히 새로운 유형의 데이터를 사용할 때 결과가 좋지 않을 수 있습니다.
- 자세한 지침은 다양한 시나리오에서 더 많은 유연성을 제공합니다.

멀티태스크 학습:

- 멀티태스크 학습은 여러 작업을 동시에 학습하여 일반화를 향상시킬 수 있습니다.
- 이러한 방식으로 훈련된 모델은 특히 오픈 도메인 테스트에서 특정 작업에 대해 훈련된 모델보다 더 나은 성능을 발휘합니다.
- 이러한 멀티태스킹의 이점은 이미지 편집으로 확장되어 모델이 사물을 더 효과적으로 식별할 수 있습니다.
    
    ![멀티 태스크 훈련에 대한 세부 분석. RefClef, ADE-847, PC-459, ADE-150와 같은 네 가지 보이지 않는 데이터셋에서 우리의 모델을 평가합니다. 이것은 공동 훈련이 개방형 시나리오를 다루는 능력을 크게 향상시킨다는 것을 보여줍니다.](InstructDiffusion%20A%20Generalist%20Modeling%20Interface%20%200232fa6920944880a096a9e4f9a1ba71/Untitled%207.png)
    
    멀티 태스크 훈련에 대한 세부 분석. RefClef, ADE-847, PC-459, ADE-150와 같은 네 가지 보이지 않는 데이터셋에서 우리의 모델을 평가합니다. 이것은 공동 훈련이 개방형 시나리오를 다루는 능력을 크게 향상시킨다는 것을 보여줍니다.
    
    ![편집에 대한 멀티 태스크 훈련의 효과. "호랑이에게 모자를 쓰게 하세요"라는 지시가 주어지면, 다른 작업과의 공동 훈련은 단일 작업 훈련에 비해 더 정확한 편집 결과를 낼 수 있습니다.](InstructDiffusion%20A%20Generalist%20Modeling%20Interface%20%200232fa6920944880a096a9e4f9a1ba71/Untitled%208.png)
    
    편집에 대한 멀티 태스크 훈련의 효과. "호랑이에게 모자를 쓰게 하세요"라는 지시가 주어지면, 다른 작업과의 공동 훈련은 단일 작업 훈련에 비해 더 정확한 편집 결과를 낼 수 있습니다.
    

인간 정렬:

- 사람이 정렬한 데이터 세트를 사용하여 모델을 미세 조정합니다.
- 이러한 미세 조정은 모델의 이미지-텍스트 정렬 점수를 향상시키며, 데이터 세트의 크기를 고려할 때 인상적입니다.
    
    ![인간 정렬의 효과. 선택된 인간 정렬 데이터에 대한 추가적인 미세 조정은 CLIP을 향상시킵니다.](InstructDiffusion%20A%20Generalist%20Modeling%20Interface%20%200232fa6920944880a096a9e4f9a1ba71/Untitled%209.png)
    
    인간 정렬의 효과. 선택된 인간 정렬 데이터에 대한 추가적인 미세 조정은 CLIP을 향상시킵니다.
    

일반화 기능:

- 이 모델은 다양한 작업에서 상세한 지침을 따라 인공 지능(AGI) 능력을 보여줍니다.
- 이미지 감지, 분류, 얼굴 정렬과 같은 세분화된 작업 등 특별히 학습되지 않은 작업도 처리할 수 있습니다.
- 얼굴 정렬의 경우 동물의 얼굴에도 적응할 수 있어 다재다능함이 돋보입니다.
    
    ![검출, 분류 및 얼굴 정렬을 포함한 보이지 않는 작업에 InstructDiffusion 적용.](InstructDiffusion%20A%20Generalist%20Modeling%20Interface%20%200232fa6920944880a096a9e4f9a1ba71/Untitled%2010.png)
    
    검출, 분류 및 얼굴 정렬을 포함한 보이지 않는 작업에 InstructDiffusion 적용.
    

### 5. Discussion and conclusion

이 논문에서는 컴퓨터 비전 작업을 사람의 지시에 따라 정렬하는 새로운 방법인 '인스트럭트디퓨전(InstructDiffusion)'을 소개합니다. 이 방법은 모든 컴퓨터 비전 작업을 이미지 생성으로 간주하여 컬러 이미지, 흑백 마스크, 키포인트의 세 가지 주요 출력을 목표로 합니다. 이 기술은 그 자체로도 잘 작동하지만 한 번에 여러 작업을 훈련하면 훨씬 더 효과적입니다. 흥미로운 점은 InstructDiffusion이 직접 학습하지 않은 작업도 처리할 수 있어 새로운 과제에 대해 기존 방법보다 더 나은 성능을 보인다는 점입니다. 이 작업은 비전 관련 작업을 위한 다용도 도구를 만드는 데 있어 큰 도약이며, 더 스마트한 컴퓨터 비전 시스템을 만드는 데 있어 더 큰 진전을 이룰 수 있는 길을 열어주고 있습니다. 앞으로 연구팀은 이 방법이 다양한 작업을 이해하는 방법을 개선하고 라벨이 없는 데이터를 사용하여 더 고급 훈련 기법을 사용할 계획입니다.