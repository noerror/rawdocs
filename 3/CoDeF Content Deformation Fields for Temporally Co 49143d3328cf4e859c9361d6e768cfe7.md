# CoDeF: Content Deformation Fields for Temporally Consistent Video Processing

[https://arxiv.org/abs/2308.07926](https://arxiv.org/abs/2308.07926)

- Aug 2023

![CoDeF의 다양한 응용 프로그램, 포함하여 (a) 텍스트 안내 비디오 대 비디오 변환 (왼쪽 절반: 번역된 프레임, 오른쪽 절반: 입력 프레임), (b) 비디오 객체 추적, 및 (c) 비디오 키 포인트 추적. 제안된 비디오 표현 유형을 사용하여 이미지 알고리즘을 비디오 처리를 위해 직접 향상시키며 비디오에 대한 어떠한 조정도 필요하지 않습니다.](CoDeF%20Content%20Deformation%20Fields%20for%20Temporally%20Co%2049143d3328cf4e859c9361d6e768cfe7/Untitled.png)

CoDeF의 다양한 응용 프로그램, 포함하여 (a) 텍스트 안내 비디오 대 비디오 변환 (왼쪽 절반: 번역된 프레임, 오른쪽 절반: 입력 프레임), (b) 비디오 객체 추적, 및 (c) 비디오 키 포인트 추적. 제안된 비디오 표현 유형을 사용하여 이미지 알고리즘을 비디오 처리를 위해 직접 향상시키며 비디오에 대한 어떠한 조정도 필요하지 않습니다.

### 1. Introduction

이미지 처리 영역에서 제너레이티브 모델은 놀라운 발전을 이끌어냈습니다. 하지만 비디오 프로세싱은 이와 같은 발전을 이루지 못했습니다. 동영상에서 일관된 타이밍을 보장하고 이미지 데이터 세트에 비해 낮은 품질의 동영상 데이터 세트를 처리해야 하는 과제가 있습니다. 더 나은 처리를 위해 비디오를 이미지로 처리하려는 시도가 있었지만, 이러한 방법에는 사소한 비디오 세부 정보가 누락되거나 왜곡된 시각적 요소가 나타나는 등의 한계가 있습니다.

이 연구는 동영상을 표현하는 새로운 방법을 소개합니다. 이 방법은 2D 및 3D 해시 기반 시스템을 결합하여 물의 흐름과 같은 복잡한 움직임을 포착하여 비디오 재구성을 개선합니다. 연구진은 훈련 과정에서 기술을 개선하여 비디오 표현이 자연스러워지도록 합니다. 그 결과, 비디오 재구성의 품질이 크게 향상되어 이전 방법보다 훨씬 적은 시간이 소요됩니다.

이를 바탕으로 이미지 처리 기술을 비디오에 적용하여 이미지 번역, 이미지 향상, 분할과 같은 작업을 비디오 콘텐츠에 더 효과적으로 수행할 수 있음을 보여주었습니다. 새로운 접근 방식은 비디오 출력물의 품질과 일관성을 개선하여 비디오 처리 기술의 혁신적인 변화를 암시합니다.

### 2. Related Work

암시적 신경 표현: 이미지, 비디오, 3D/4D 표현을 표현하는 데 큰 잠재력을 보여주었습니다. 이미지 향상 및 3D/4D 재구성과 같은 애플리케이션에 사용됩니다. 효율적인 훈련을 위해 다양한 가속 기법이 채택되었습니다. 이 연구는 이러한 기법에서 파생된 '표준 이미지'를 사용하여 비디오를 재구성하는 것을 목표로 합니다.

일관된 비디오 편집: 이 분야에는 크게 두 가지 접근 방식이 있습니다.

- 전파 기반 방법(Propagation-based methods)은 시작 프레임을 편집한 후 해당 편집 내용을 동영상 전체에 퍼뜨리는 방식입니다. 효율적이지만, 특히 복잡한 움직임이 있는 동영상에서는 오류가 발생하기 쉽습니다.
- 레이어 표현 기반 기술(Layered representation-based techniques)은 동영상을 레이어로 분할하여 편집하는 동안 더 많은 제어가 가능합니다. 이 연구의 접근 방식은 동영상에 최적화된 표현을 사용하는 Text2Live와 유사합니다. 그러나 이 연구는 더 통찰력 있는 표준 표현을 제공하고 더 나은 품질의 비디오 처리를 생성합니다.

생성 모델을 통한 비디오 처리: 확산 모델의 발전으로 텍스트-이미지 생성의 품질이 크게 향상되었으며, GLIDE 및 Dall-E 2와 같은 모델이 그 선두에 서 있습니다. 생성된 이미지를 정밀하게 제어하기 위해 다양한 기술과 모델이 제안되었습니다. 일부 연구는 비디오 처리를 위해 텍스트-이미지 모델을 사용하는 데만 초점을 맞추지만, 예측할 수 없는 생성의 특성으로 인해 일관성이 없을 수 있습니다.

텍스트-비디오 생성: 이 분야는 입력 텍스트로부터 비디오 프레임을 생성하도록 모델을 학습시키는 새로운 분야입니다. 여러 아키텍처가 텍스트에서 의미적으로 정확한 비디오 프레임을 생성할 수 있지만, 높은 계산 요구 사항으로 인해 정확한 제어에 어려움을 겪거나 해상도가 낮을 수 있습니다.

### 3. Method

비디오가 주어졌을 때 V와 프레임 {I1,I2,...,IN}이 주어졌을 때, 이미지 처리 알고리즘 X를 각 프레임에 개별적으로 적용하면 프레임 간 불일치가 발생할 수 있습니다. 알고리즘 향상 시간 모듈이 포함된 X는 충분한 학습 데이터 부족으로 인해 일관성과 잠재적인 성능 저하의 문제가 있습니다. 이 문제를 해결하기 위해 이 연구에서는 평평한 캐논을 사용하여 비디오 V를 평평한 표준 이미지로 표현할 것을 제안합니다. Ic와 변형 필드 D. 이 표현은 이미지 알고리즘과 비디오 작업을 연결하여 다음을 보장하는 것을 목표로 합니다:

- 정확한 비디오 재구성.
- 표준 이미지의 의미적 정확성.
- 시간적 일관성을 위한 변형 필드의 부드러움.

3.1. 콘텐츠 변형 필드: 이 연구는 동적 NeRF에서 영감을 얻어 표준 필드와 변형 필드라는 두 가지 구성 요소로 표현을 도입했습니다. 이는 2D 및 3D 해시 테이블을 사용하여 구성되며, 두 개의 작은 MLP를 통합하여 강화됩니다.

표준 필드 C는 비디오의 모든 텍스처를 연속적으로 표현하는 역할을 합니다. V. 그것은 함수입니다 F 2D 위치 매핑 x를 색으로 매핑하는 함수 c. 다중 해상도 해시 인코딩은 학습 속도를 향상시키고 네트워크가 고주파 디테일을 파악할 수 있도록 하는 데 사용됩니다.

변형 필드 D는 각 프레임에 대한 관측 위치와 표준 위치 간의 관계를 정의합니다. 그러나 3D 공간에서의 기존 구현은 훈련 효율성과 대표 능력의 문제로 인해 비디오 표현에 직접 사용할 수 없습니다. 이 연구에서는 변형 필드를 소형 MLP가 지원하는 3D 해시 테이블로 표현할 것을 제안합니다.

변형 필드를 위한 3D 해시 인코딩: 비디오의 모든 지점은 위치로 시각화됩니다. 3 x3D로 시각화됩니다. 이 3D 공간은 3D 해시 인코딩 기술을 사용하여 표현되며, 다중 해상도 특징 그리드로 구성됩니다. 이 그리드는 학습 가능한 피처로 채워진 다양한 해상도의 그리드로 구성됩니다. 해상도는 가장 거친 수준과 가장 미세한 수준 사이에서 기하학적으로 확장됩니다. 쿼리된 지점에 대한 특징을 도출하려면 3 x3D의 특징을 도출하기 위해 그리드의 해상도에 따라 입력 좌표가 조정되고 인접한 점으로부터 특징을 보간합니다.

최종 출력 색상 값은 프레임의 좌표 좌표의 최종 출력 색상 값 t의 최종 출력 색상 값은 표준 및 변형 필드를 사용하여 계산되며, 입력 프레임의 실제 색상을 사용하여 감독할 수 있습니다.

![제안된 비디오 표현인 CoDeF는 임의의 비디오를 2D 콘텐츠 표준 필드와 3D 시간 변형 필드로 분해합니다. 각 필드는 효율적인 MLP를 사용하여 다중 해상도 2D 또는 3D 해시 테이블로 구현됩니다. 이러한 새로운 유형의 표현은 이미지 알고리즘을 비디오 처리를 위해 자연스럽게 올릴 수 있게 해주며, 표준 이미지 (즉, 캐노니컬 콘텐츠 필드에서 렌더링된 이미지)에서 확립된 알고리즘을 직접 적용하고 결과를 시간 축을 따라 임시 변형 필드를 통해 전파하는 방식입니다.](CoDeF%20Content%20Deformation%20Fields%20for%20Temporally%20Co%2049143d3328cf4e859c9361d6e768cfe7/Untitled%201.png)

제안된 비디오 표현인 CoDeF는 임의의 비디오를 2D 콘텐츠 표준 필드와 3D 시간 변형 필드로 분해합니다. 각 필드는 효율적인 MLP를 사용하여 다중 해상도 2D 또는 3D 해시 테이블로 구현됩니다. 이러한 새로운 유형의 표현은 이미지 알고리즘을 비디오 처리를 위해 자연스럽게 올릴 수 있게 해주며, 표준 이미지 (즉, 캐노니컬 콘텐츠 필드에서 렌더링된 이미지)에서 확립된 알고리즘을 직접 적용하고 결과를 시간 축을 따라 임시 변형 필드를 통해 전파하는 방식입니다.

3D 해시 변형은 동영상 모델링에는 능숙하지만, 시간적 진행을 매끄럽게 유지하는 데는 어려움을 겪습니다. 이로 인해 동영상의 의미가 흐트러져 기존의 이미지 알고리즘을 동영상에 적용하기가 어렵습니다.

제안된 솔루션:

- 변형에 대한 어닐드 3D 해시 인코딩: 이 접근 방식은 복잡한 변형 피팅을 더 세밀한 해상도로 향상시킵니다. 하지만 왜곡이 발생할 수도 있습니다. 이 문제를 해결하기 위해 이 모델은 동적 NeRF에서 영감을 얻은 어닐링 전략을 사용합니다. 이 접근 방식은 다양한 해상도에 걸쳐 특징의 가중치를 점진적으로 제어하여 보다 부드러운 전환을 보장합니다.
- 흐름 유도 일관성 손실: 연속된 프레임 사이에 흐르는 포인트가 일관성을 유지하도록 하는 것이 목표입니다. RAFT 알고리즘을 사용하여 순방향 및 역방향 흐름을 감지하고, 흐름 일관성을 유지하기 위해 손실 함수를 공식화합니다. 이 손실은 시간적 진행이 일관되고 매끄럽게 유지되도록 하는 데 매우 중요합니다.
- RAFT
    
    RAFT (Recurrent All-Pairs Field Transforms)는 최근에 개발된, 성능이 뛰어난 optical flow 예측 알고리즘이다.  RAFT의 주요 특징 및 구조는 다음과 같다:
    
    1. **All-Pairs Field Transforms**: RAFT는 이미지의 모든 픽셀 쌍 간의 상호작용을 고려하며, 이를 통해 적절한 correlation volume을 만든다. 이는 optical flow의 코렐레이션을 더 잘 캡처하고, 이웃 픽셀간의 상호작용을 반영한다.
    2. **Recurrent Update Mechanism**: RAFT는 예측된 optical flow를 반복적으로 업데이트한다. 초기 예측을 기반으로 상호작용을 반복적으로 반영하여 flow 예측을 점진적으로 개선한다.
    3. **Symmetric Cost Volume**: 이 알고리즘은 symmetric cost volume을 사용하여 두 연속된 이미지 사이의 차이를 캡처한다.
    4. **Gridding**: RAFT는 이미지를 여러 격자로 분할하고, 각 격자 내에서의 correlation을 사용하여 optical flow를 계산한다.
    5. **Training Strategy**: RAFT는 손실 함수와 함께 학습된 특정 전략을 사용하여 모델을 훈련시킨다. 이는 optical flow의 정확성을 높이며, 더 나은 일반화 성능을 제공한다.
    
    RAFT는 다양한 벤치마크에서 높은 성능을 보였으며, 기존 optical flow 예측 방법들에 비해 더 우수한 결과를 제공한다는 평가를 받았다.
    
- 그룹화된 콘텐츠 변형 필드: 동영상에 여러 개의 겹치는 오브젝트로 인한 복잡성을 처리하기 위해 시맨틱 분할을 기반으로 콘텐츠를 여러 개의 변형 필드로 나누는 아이디어입니다. 이는 특히 오브젝트가 서로 겹치거나 가려지는 영역에서 보다 정밀하게 재구성하는 데 도움이 됩니다.

훈련 목표: 비디오 프레임의 특정 좌표에서 예측된 색상 값과 실제 색상 값의 차이를 최소화하여 모델을 학습시킵니다. 이 손실은 흐름 유도 일관성 손실로 인한 정규화와 결합되어 정확하고 시간적으로 일관된 표현을 보장합니다.

3.3. 일관된 비디오 처리에 적용

최적화 프로세스가 끝나면 모든 변형을 0으로 설정하여 표준 이미지가 도출됩니다. 이 이미지의 크기는 비디오의 움직임에 따라 유연하게 조정할 수 있어 모든 콘텐츠가 캡슐화되도록 보장합니다. 이 표준 이미지는 다른 비디오 처리 작업의 기초가 됩니다:

SOTA 알고리즘에서의 사용:

- ControlNet: 특정 프롬프트 또는 가이드를 기반으로 비디오 간 번역을 용이하게 합니다.
- 세그먼트-애니씽(SAM): 비디오 내에서 객체 추적을 활성화합니다.
- R-ESRGAN: 동영상 해상도 향상을 위해 적용됩니다.

수동 비디오 편집: 표준 이미지의 주목할 만한 장점 중 하나는 동영상 편집 프로세스를 간소화한다는 것입니다. 비디오를 프레임 단위로 편집하는 대신 표준 이미지를 편집하기만 하면 변경 사항이 비디오 전체에 전파됩니다.

본질적으로, 제안된 디자인은 정교한 알고리즘과 기술을 활용하여 동영상의 재구성이 정확하고 시간적으로 일관성을 유지하도록 보장합니다. 어닐링된 해시 인코딩, 흐름에 따른 일관성, 그룹화된 변형 필드가 모두 결합되어 비디오를 효과적으로 표현할 뿐만 아니라 추가 비디오 처리 애플리케이션의 기반이 되는 강력한 시스템을 형성합니다.

### 4. Experiments

1 실험 설정:

- 이 연구는 강성, 비강성 및 복잡한 시나리오를 포함한 다양한 변형에 대해 제안된 방법의 견고성과 적응성을 강조하고자 합니다.
- 실험의 기본 설정은 어닐 시작 및 종료 단계를 각각 4000 및 8000으로 설정하고 총 반복 횟수를 10,000으로 제한합니다.
- 단일 NVIDIA A6000 GPU를 사용하는 경우 평균 훈련 시간은 100개의 비디오 프레임에 대해 약 5분입니다.

2 평가:

재구성 품질:

- 제안된 모델은 비강체 움직임에 대한 견고성 측면에서 뉴럴 이미지 아틀라스보다 성능이 뛰어나며 미묘한 움직임을 효율적으로 재구성할 수 있습니다.
    
    ![비디오 재구성과 관련하여 레이어화된 신경 아틀라스 [16]와 우리의 CoDeF 사이의 질적 비교, 이것은 비디오 표현의 용량을 반영하며 또한 충실한 비디오 처리에서 기본적인 역할을 합니다. 확대하면 세부 사항이 더 잘 보입니다.](CoDeF%20Content%20Deformation%20Fields%20for%20Temporally%20Co%2049143d3328cf4e859c9361d6e768cfe7/Untitled%202.png)
    
    비디오 재구성과 관련하여 레이어화된 신경 아틀라스 [16]와 우리의 CoDeF 사이의 질적 비교, 이것은 비디오 표현의 용량을 반영하며 또한 충실한 비디오 처리에서 기본적인 역할을 합니다. 확대하면 세부 사항이 더 잘 보입니다.
    
- 정량적으로, 제안된 방법을 사용하면 비디오 재구성 PSNR이 4.4dB 향상됩니다.
- 또한 제안 모델은 아틀라스에 비해 훈련 효율성 측면에서도 뛰어납니다.

다운스트림 비디오 처리: 이 표현은 다음과 같은 다양한 애플리케이션에 활용될 수 있습니다:

- 비디오 간 번역: 표준 이미지에 이미지 번역을 적용하면 비디오 대 비디오 번역이 가능합니다. 제안된 모델은 높은 품질과 시간적 일관성을 모두 보장합니다.
    
    ![Text2Live [1], Tune-A-Video [64], FateZero [36] 및 우리의 CoDeF를 통해 직접적으로 ControlNet [69]을 향상시키는 다양한 방법에 대한 텍스트 안내 비디오 대 비디오 변환 작업의 질적 비교. 시간 일관성과 합성 품질의 상세한 평가를 위해 독자들에게 프로젝트 페이지의 비디오를 보는 것을 강력히 권장합니다.](CoDeF%20Content%20Deformation%20Fields%20for%20Temporally%20Co%2049143d3328cf4e859c9361d6e768cfe7/Untitled%203.png)
    
    Text2Live [1], Tune-A-Video [64], FateZero [36] 및 우리의 CoDeF를 통해 직접적으로 ControlNet [69]을 향상시키는 다양한 방법에 대한 텍스트 안내 비디오 대 비디오 변환 작업의 질적 비교. 시간 일관성과 합성 품질의 상세한 평가를 위해 독자들에게 프로젝트 페이지의 비디오를 보는 것을 강력히 권장합니다.
    
- 비디오 키포인트 추적: 이 방법은 표준 공간 내 한 프레임에서 특정 키포인트의 위치를 정확히 찾아내고 모든 프레임에서 해당 키포인트의 위치를 식별할 수 있습니다.
    
    ![비디오를 CoDeF로 재구성한 후 임시 변형 필드에서 직접 추출된 프레임 간의 점 대응 시각화.](CoDeF%20Content%20Deformation%20Fields%20for%20Temporally%20Co%2049143d3328cf4e859c9361d6e768cfe7/Untitled%204.png)
    
    비디오를 CoDeF로 재구성한 후 임시 변형 필드에서 직접 추출된 프레임 간의 점 대응 시각화.
    
- 비디오 객체 추적: 이 모델은 표준 이미지에서 세그먼테이션 알고리즘을 사용하여 비디오 시퀀스에 대한 마스크를 일관되게 생성합니다.
    
    ![우리의 CoDeF를 통해 이미지 분할 알고리즘 [18]을 향상시켜 얻은 비디오 객체 추적 결과.](CoDeF%20Content%20Deformation%20Fields%20for%20Temporally%20Co%2049143d3328cf4e859c9361d6e768cfe7/Untitled%205.png)
    
    우리의 CoDeF를 통해 이미지 분할 알고리즘 [18]을 향상시켜 얻은 비디오 객체 추적 결과.
    
- 비디오 초해상도: 표준 이미지에 이미지 초고해상도를 적용하여 깜박임 없이 고품질 비디오를 제작할 수 있습니다.
    
    ![우리의 CoDeF를 통해 이미지 초고해상도 알고리즘 [61]을 향상시켜 얻은 비디오 초고해상도 결과.](CoDeF%20Content%20Deformation%20Fields%20for%20Temporally%20Co%2049143d3328cf4e859c9361d6e768cfe7/Untitled%206.png)
    
    우리의 CoDeF를 통해 이미지 초고해상도 알고리즘 [61]을 향상시켜 얻은 비디오 초고해상도 결과.
    
- 사용자 인터랙티브 비디오 편집: 사용자는 보다 정밀한 편집을 위해 표준 이미지의 콘텐츠를 수동으로 조정할 수 있습니다.
    
    ![사용자 대화식 비디오 편집은 우리의 CoDeF를 사용하여 시간 축을 따라 결과를 전파하면서 오직 한 이미지만 편집하여 달성되었습니다. 시간 일관성을 더욱 감상하기 위해 독자들에게 프로젝트 페이지의 비디오를 보는 것을 강력히 권장합니다.](CoDeF%20Content%20Deformation%20Fields%20for%20Temporally%20Co%2049143d3328cf4e859c9361d6e768cfe7/Untitled%207.png)
    
    사용자 대화식 비디오 편집은 우리의 CoDeF를 사용하여 시간 축을 따라 결과를 전파하면서 오직 한 이미지만 편집하여 달성되었습니다. 시간 일관성을 더욱 감상하기 위해 독자들에게 프로젝트 페이지의 비디오를 보는 것을 강력히 권장합니다.
    

3. 절제 연구:

- 제안된 모듈의 중요도를 테스트했습니다.
- 3D 해시 인코딩을 포지션 인코딩으로 대체한 결과 재구성 PSNR이 3.1dB 감소했습니다.
- 어닐링된 해시가 없으면 표준 이미지가 부자연스럽게 보이며, 이는 여러 개의 손과 같은 아티팩트로 입증됩니다.
    
    ![점차적으로 해시의 효과에 대한 소거 연구. 규범적 이미지에서의 비자연스러움은 하류 작업의 성능에 해를 끼칠 것입니다.](CoDeF%20Content%20Deformation%20Fields%20for%20Temporally%20Co%2049143d3328cf4e859c9361d6e768cfe7/Untitled%208.png)
    
    점차적으로 해시의 효과에 대한 소거 연구. 규범적 이미지에서의 비자연스러움은 하류 작업의 성능에 해를 끼칠 것입니다.
    
- 흐름 손실을 생략하면 매끄러운 영역에서 뚜렷한 깜박임이 발생합니다.
- 실험 결과는 다양한 비디오 처리 애플리케이션에서 제안된 방법의 견고성, 효율성 및 적용 가능성을 입증합니다.

### 5. Conclusion and Discussion

주요 연구 결과: 이 연구에서는 시간적으로 일관된 비디오 처리를 목표로 콘텐츠 변형 필드로서의 비디오 표현을 탐구했습니다. 제안된 방법은 충실도와 시간적 일관성에서 유망한 결과를 보여주었습니다.

과제와 향후 작업:

- 장면별 최적화: 한 가지 중요한 장애물은 장면별 최적화의 필요성입니다. 향후 피드포워드 암시적 필드 기술의 발전으로 이 문제를 해결할 수 있을 것입니다.
- 극심한 시점의 변화: 시점이 급격하게 바뀌는 상황에서는 어려움이 따릅니다. 3D 사전 지식을 도입하면 이 문제를 관리하기 위한 추가 데이터와 제약 조건을 제공할 수 있습니다.
- 큰 비강체 변형: 상당한 비강체 변형을 처리하는 것은 여전히 어려운 문제입니다. 한 가지 가능한 해결책은 여러 개의 표준 이미지를 활용하여 복잡한 변형을 보다 정확하게 캡처하고 묘사하는 것입니다.