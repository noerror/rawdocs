# DragonDiffusion: Enabling Drag-style Manipulation on Diffusion Models

[https://arxiv.org/abs/2307.02421](https://arxiv.org/abs/2307.02421)

[https://github.com/MC-E/DragonDiffusion](https://github.com/MC-E/DragonDiffusion)

![우리는 DragonDiffusion을 제안합니다. 이는 객체 이동, 객체 크기 조정, 객체 외형 교체, 콘텐츠 드래그 등 다양한 실제 이미지 편집 작업을 수행할 수 있는 효율적인 분류자 지침과 같은 방법입니다. 우리의 방법은 사전 훈련된 Stable Diffusion을 기반으로 하며, 모델을 미세 조정하거나 훈련할 필요가 없습니다.](DragonDiffusion%20Enabling%20Drag-style%20Manipulation%20o%20f33d9d8ad36244f69d9b0e27c9eb5c96/Untitled.png)

우리는 DragonDiffusion을 제안합니다. 이는 객체 이동, 객체 크기 조정, 객체 외형 교체, 콘텐츠 드래그 등 다양한 실제 이미지 편집 작업을 수행할 수 있는 효율적인 분류자 지침과 같은 방법입니다. 우리의 방법은 사전 훈련된 Stable Diffusion을 기반으로 하며, 모델을 미세 조정하거나 훈련할 필요가 없습니다.

### 1. Introduction

이 논문에서는 이미지 생성 및 편집을 위한 생성 모델, 특히 대규모 텍스트-이미지(T2I) 확산 모델의 사용에 중점을 둡니다. 텍스트 프롬프트에서 이미지를 생성하는 것은 이미 가능하지만, 특정 사용자 요구 사항에 따라 이미지를 편집하는 것은 더 어렵습니다. 생성적 적대 신경망(GAN)을 기반으로 하는 방법이 이미지 편집에 성공적으로 사용되어 왔지만, 이 논문에서는 확산 모델이 더 높은 안정성과 우수한 생성 품질을 제공할 수 있다고 제안합니다.

이 접근 방식을 구현할 때 가장 큰 문제는 편집하기 쉬운 잠재 공간이 없다는 것입니다. 기존의 여러 확산 기반 방법은 중간 텍스트 및 이미지 특징을 사용하여 편집 프로세스를 안내하지만, 특히 여러 개체가 있는 복잡한 시나리오에서는 이러한 대응이 취약한 경우가 많습니다.

이 논문에서는 확산 과정에서 중간 이미지 특징 간의 강력한 대응을 활용하는 솔루션을 제안합니다. 이 방법에는 안내 특징과 생성 특징이라는 두 가지 특징 세트가 포함됩니다. 안내 기능은 원본 이미지와의 콘텐츠 일관성을 유지하면서 생성 기능을 제한하고 편집하는 데 도움을 주는 타겟 역할을 합니다.

연구진이 개발한 드래곤디퓨전(DragonDiffusion)이라는 솔루션은 모든 콘텐츠 편집 및 보존 신호를 이미지 자체에서 직접 도출하는 이미지 편집 방법을 제공합니다. 이 방법은 모델 미세 조정이나 학습이 필요하지 않으며 이동, 크기 조정, 물체 모양 바꾸기, 콘텐츠 드래그와 같은 다양한 세분화된 이미지 편집 작업에 사용할 수 있습니다. 이 논문에서는 다양한 레이어에서의 특징 역할에 대한 연구와 의미론적 및 기하학적 대응을 모두 고려한 다중 스케일 특징 매칭 체계의 개발을 강조합니다.

### 2. Related Work

이 섹션에서는 확산 모델, 확산 모델의 분류기 안내 및 이미지 편집과 관련된 연구 및 모델에 중점을 둡니다.

![우리의 모델 디자인 설명. 우리의 제안 방법은 지침 가지와 생성 가지, 즉 두 가지 가지로 구성됩니다. 지침 가지는 중간 기능의 대응을 통해 생성 가지에 편집 및 일관성 지침을 제공합니다. 우리의 DragonDiffusion은 Stable Diffusion [27]을 기반으로 하며, 모델을 미세 조정하거나 훈련할 필요가 없습니다.](DragonDiffusion%20Enabling%20Drag-style%20Manipulation%20o%20f33d9d8ad36244f69d9b0e27c9eb5c96/Untitled%201.png)

우리의 모델 디자인 설명. 우리의 제안 방법은 지침 가지와 생성 가지, 즉 두 가지 가지로 구성됩니다. 지침 가지는 중간 기능의 대응을 통해 생성 가지에 편집 및 일관성 지침을 제공합니다. 우리의 DragonDiffusion은 Stable Diffusion [27]을 기반으로 하며, 모델을 미세 조정하거나 훈련할 필요가 없습니다.

2.1. 확산 모델
확산 모델은 열역학을 기반으로 설계된 모델로 이미지 합성 분야에서 큰 성공을 거두었습니다. 이 모델에는 확산 과정과 역방향 과정이 포함됩니다. 확산 프로세스는 무작위 가우시안 노이즈를 추가하여 자연 이미지를 가우시안 분포로 변환하고, 역방향 프로세스는 최종 노이즈 반복에서 원본 이미지를 복구합니다. GLID 및 SD와 같은 텍스트 조건 확산 모델도 개발되어 교차 주의 전략을 사용하여 디노이저에 텍스트 조건을 도입합니다.

2.2. 확산 모델의 분류기 안내
연속적인 관점에서 확산 모델은 랑그빈 역학에 따라 해당 분포에서 샘플링하는 점수 함수로 볼 수 있습니다. 조건부 확산 프로세스는 공동 점수 함수를 사용하여 보다 풍부한 분포에서 샘플링하는 것으로 볼 수 있습니다. 공동 점수 함수는 원래의 무조건 확산 노이즈 제거기와 에너지 함수라고도 하는 분류기 안내의 두 부분으로 나눌 수 있습니다. 에너지 함수는 원하는 출력에 따라 선택할 수 있습니다. 분류기 유도는 스케치 가이드 생성, 마스크 가이드 생성, 범용 가이드 생성, 이미지 편집 등 여러 제어 가능한 이미지 생성 작업에 사용되었습니다.

2.3. 이미지 편집
전통적인 이미지 편집 방법은 이미지 도메인 간의 변환을 대상으로 합니다. 다양한 편집 방식에서 StyleGAN을 사용하여 이미지를 잠재 공간으로 반전시킨 다음 잠재 벡터를 조작하여 특정 콘텐츠를 편집했습니다. DragGAN은 보다 정교한 콘텐츠 편집을 위해 포인트 투 포인트 드래그 방식을 도입했습니다. Diffusion과 같은 보다 안정적인 생성 모델은 확산 기반 이미지 편집 방법에 영감을 주었습니다. 이러한 방법의 대부분은 텍스트를 편집 신호로 사용합니다. 여러 가지 방법으로 이미지 편집이 가능했지만 이미지와 텍스트 간의 대응이 거칠게 이루어졌습니다. 확산 모델을 사용하여 세밀하고 일반화된 이미지 편집을 수행하는 방법에 대한 문제는 여전히 미해결 과제입니다.

### 3. Method

드래곤디퓨전 기법은 최근의 최첨단 텍스트 투 이미지(T2I) 확산 모델인 안정적 확산 모델을 사용하여 실제 이미지의 세밀한 이미지 편집을 달성하기 위해 제안된 기법입니다. 핵심 목표는 나머지는 보존하면서 원하는 이미지 콘텐츠만 수정하는 것입니다. 이는 안내 브랜치와 생성 브랜치라는 두 가지 브랜치 프로세스를 구현하여 달성할 수 있습니다. 두 가지 프로세스 모두 확산 프로세스의 중간 표현을 사용합니다.

![원래 이미지를 재구성하기 위해 다른 계층의 특성을 가이드로 사용하는 방법을 설명. 이 실험에서, 우리는 zT를 무작위 가우스 잡음으로 설정하고, mgen, mgud를 0 행렬로, mshare를 1 행렬로 설정합니다.](DragonDiffusion%20Enabling%20Drag-style%20Manipulation%20o%20f33d9d8ad36244f69d9b0e27c9eb5c96/Untitled%202.png)

원래 이미지를 재구성하기 위해 다른 계층의 특성을 가이드로 사용하는 방법을 설명. 이 실험에서, 우리는 zT를 무작위 가우스 잡음으로 설정하고, mgen, mgud를 0 행렬로, mshare를 1 행렬로 설정합니다.

저자는 교차 브랜치 자체 주의 설계를 통해 달성한 이 두 가지 브랜치의 중간 기능 간의 강력한 대응을 사용하여 콘텐츠 정보를 전송하고 일관성을 보장합니다.

![대조 손실과 인페인팅 손실이 객체 이동 작업에서 역할을하는 것을 시각화. 우리가 설계한 대조 손실은 다중 객체 현상을 제거할 수 있으며, 인페인팅 손실은 누락된 영역에서 더 자연스러운 콘텐츠를 생성할 수 있습니다.](DragonDiffusion%20Enabling%20Drag-style%20Manipulation%20o%20f33d9d8ad36244f69d9b0e27c9eb5c96/Untitled%203.png)

대조 손실과 인페인팅 손실이 객체 이동 작업에서 역할을하는 것을 시각화. 우리가 설계한 대조 손실은 다중 객체 현상을 제거할 수 있으며, 인페인팅 손실은 누락된 영역에서 더 자연스러운 콘텐츠를 생성할 수 있습니다.

편집 프로세스는 분류기 안내 기반 설계에 의해 제어되며, 이 설계는 중간 표현을 수정하기 위해 점수 함수를 통해 편집 신호를 그라데이션으로 변환합니다. 이 점수 함수는 중간 특징을 사용하여 서로 다른 이미지 간에 점 대 점 일치를 만듭니다.

점수 함수는 확산 모델의 중간 특징이 강한 대응성을 가지고 있어 서로 다른 이미지 간에 점 대 점 매칭이 가능하다는 통찰력을 기반으로 구성됩니다. 이 함수는 특정 위치의 콘텐츠 측면에서 원본 이미지와 편집된 이미지 간의 유사성을 측정하는 데 사용됩니다. 점수 함수는 [0,1] 범위로 정규화된 코사인 거리를 이 유사도의 측정값으로 사용합니다. 목표는 점수 함수에 따라 유사성을 최대화하는 것입니다.

또한 이 모델은 원본 이미지와 편집되지 않은 영역의 일관성을 유지하려고 노력합니다. 편집된 이미지에서 편집되지 않은 영역과 원본 이미지 간의 코사인 유사도가 계산됩니다.

모델의 손실 함수는 편집과 콘텐츠 보존 측면을 모두 결합합니다. 모델은 손실 함수의 이 두 부분의 균형을 맞추기 위해 두 개의 가중치를 사용합니다. 손실 함수의 기울기는 모델의 공동 점수 함수에 사용됩니다.

다중 스케일 안내 측면에서 모델은 서로 다른 스케일의 네 블록이 포함된 Unet 디노이저의 디코더를 사용합니다. 디코더의 두 번째 및 세 번째 레이어의 기능은 원본 이미지를 재구성하는 데 특히 유용합니다. 두 번째 레이어는 더 많은 의미 정보를 전달하고 세 번째 레이어는 더 많은 기하학적 정보를 전달합니다.

모델은 이 두 계층의 지침을 결합하여 낮은 수준의 시각적 특징과 높은 수준의 시각적 특징 사이의 균형을 유지합니다. 따라서 F_gen_t 및 F_gud_t 특징 집합에는 레이어 2와 레이어 3의 두 가지 특징 집합이 포함됩니다.

![크로스-브랜치 셀프-주의와 없이 객체 이동을 시각화.](DragonDiffusion%20Enabling%20Drag-style%20Manipulation%20o%20f33d9d8ad36244f69d9b0e27c9eb5c96/Untitled%204.png)

크로스-브랜치 셀프-주의와 없이 객체 이동을 시각화.

오브젝트 이동의 경우 모델은 특정 마스크를 사용하여 동일한 오브젝트를 다른 공간 위치에서 찾습니다. 그러나 이동 후에도 오브젝트의 잔여물이 원래 위치에 남아있는 문제가 발생할 수 있습니다. 이 문제를 해결하기 위해 모델의 손실 함수에 대비 손실 함수가 추가됩니다. 또한 인페인팅 손실 함수는 인페인팅 영역의 특징을 제한하도록 설계되어 이동한 오브젝트가 남긴 간격을 올바르게 채울 수 있도록 도와줍니다.

오브젝트 크기 조정의 경우 모델은 보간을 사용하여 마스크와 피처를 목표 크기로 변환합니다. 모델은 보간을 포함하는 마스크의 로컬 크기 조정 프로세스를 통해 생성 브랜치가 동일한 크기의 대상 오브젝트를 생성하도록 안내한 다음 중앙 자르기 또는 확장을 통해 원래 크기로 복원합니다.

외형 대체 작업에서 모델은 같은 카테고리의 객체 간에 외형을 대체하는 것을 목표로 합니다. 해당 영역의 특징 평균을 사용하여 객체 모양을 표현합니다. 안내 분기에는 원본 이미지와 모양 참조 이미지라는 두 가지 안내 이미지의 확산이 포함됩니다.

포인트 드래그 작업에는 이미지의 특정 지점을 통해 이미지 콘텐츠를 드래그하는 작업이 포함됩니다. 이 작업의 마스크는 목적지와 시작점, 그리고 이들을 둘러싼 작은 범위 내의 인접한 점을 나타냅니다. 그라데이션 안내는 다른 특정 디자인 없이 식 5에서 제공됩니다.

이 외에도 생성된 결과와 원본 이미지 간의 일관성을 유지하기 위해 모델에는 두 가지 전략이 사용됩니다: DDIM 반전 및 교차 브랜치 자체 주의 설계. 교차 분기 자체 주의 설계는 생성 분기에 있는 노이즈 제거기의 자체 주의 모듈에 있는 키와 값을 안내 분기의 해당 키와 값으로 대체하는 방식으로 작동합니다. 이 설계의 효과는 그림 5에서 확인할 수 있으며, 생성된 결과와 원본 이미지 사이의 거리가 상당히 가까워진 것을 볼 수 있습니다.

또한 편집 결과와 원본 이미지 간의 높은 일관성을 유지하기 위해 교차 지점 자체주의 설계가 사용됩니다. 이 작업은 디노이저 인코더의 피처 대응이 약하기 때문에 디노이저 디코더에서만 발생합니다.

전반적으로 드래곤디퓨전은 확산 모델에서 중간 특징의 대응을 활용하여 이미지 편집 작업을 위한 정교한 방법을 제공합니다. 모델을 미세 조정하거나 재교육할 필요 없이 정밀한 이미지 조작을 제공할 수 있습니다.

### 4. Experiments

드래곤디퓨전은 다양한 이미지 편집 작업을 수행할 수 있는 혁신적인 도구입니다. 여기에는 이미지 내 오브젝트 이동 및 크기 조정, 오브젝트의 모양을 다른 이미지의 유사한 오브젝트로 바꾸기, 이미지 내에서 콘텐츠 드래그 등이 포함됩니다.

![우리의 객체 이동 및 크기 조정 애플리케이션을 시각화. 우리의 DragonDiffusion은 실제 이미지에서 효과적으로 객체를 이동시키는 데 유능하며 동시에 원래 객체의 영역도 잘 인페인트될 수 있음을 볼 수 있습니다. 객체 이동 과정에서 우리는 객체를 선택적으로 확대하거나 축소할 수 있습니다.](DragonDiffusion%20Enabling%20Drag-style%20Manipulation%20o%20f33d9d8ad36244f69d9b0e27c9eb5c96/Untitled%205.png)

우리의 객체 이동 및 크기 조정 애플리케이션을 시각화. 우리의 DragonDiffusion은 실제 이미지에서 효과적으로 객체를 이동시키는 데 유능하며 동시에 원래 객체의 영역도 잘 인페인트될 수 있음을 볼 수 있습니다. 객체 이동 과정에서 우리는 객체를 선택적으로 확대하거나 축소할 수 있습니다.

그림 6에서는 오브젝트를 원활하게 이동하고 크기를 조정하여 기존 이미지 콘텐츠와 잘 어우러지도록 하는 방법을 보여줍니다. 그림 7은 원본 오브젝트의 윤곽을 그대로 유지하면서 다른 이미지의 유사한 오브젝트로 오브젝트의 모양을 대체하는 방법을 보여줍니다. 마지막으로 그림 8은 하나 또는 여러 개의 포인트를 사용하여 이미지 내에서 콘텐츠를 이동할 수 있는 콘텐츠 드래그 기능을 보여줍니다. 중요한 점은 이 도구로 변경한 내용이 편집 방향과 일치하고 콘텐츠가 이미지의 나머지 부분과 조화를 이룬다는 점입니다.

![객체 외형 교체 시각화. 우리의 방법은 참조 이미지에서 같은 범주 내의 객체의 외형 특징을 추출하고, 그 후에 편집된 이미지의 객체 외형을 그에 따라 교체할 수 있습니다.](DragonDiffusion%20Enabling%20Drag-style%20Manipulation%20o%20f33d9d8ad36244f69d9b0e27c9eb5c96/Untitled%206.png)

객체 외형 교체 시각화. 우리의 방법은 참조 이미지에서 같은 범주 내의 객체의 외형 특징을 추출하고, 그 후에 편집된 이미지의 객체 외형을 그에 따라 교체할 수 있습니다.

![콘텐츠 드래그 시각화. 우리의 방법은 하나 또는 여러 점을 사용하여 이미지 콘텐츠를 드래그하는 것을 허용합니다. 연속적인 드래그 결과는 우리의 DragonDiffusion의 약속된 편집 능력과 안정성을 보여줍니다.](DragonDiffusion%20Enabling%20Drag-style%20Manipulation%20o%20f33d9d8ad36244f69d9b0e27c9eb5c96/Untitled%207.png)

콘텐츠 드래그 시각화. 우리의 방법은 하나 또는 여러 점을 사용하여 이미지 콘텐츠를 드래그하는 것을 허용합니다. 연속적인 드래그 결과는 우리의 DragonDiffusion의 약속된 편집 능력과 안정성을 보여줍니다.

### 5. Conclusion

드래곤디퓨전은 이미지 편집 영역에서 우수한 결과를 얻었지만, 개선하고 더 탐구할 수 있는 많은 잠재적 여지가 있습니다.

첫째, 드래곤디퓨전의 강점은 특징 대응에 있습니다. 저희는 이 현상을 더욱 연구하여 이미지 특징 간의 관계를 더욱 향상시킬 수 있는 방법을 모색하고자 합니다. 이를 통해 편집 과정을 더욱 세밀하게 제어할 수 있을 것입니다.

둘째, 현재 저희의 방법은 다양한 편집 응용 프로그램에서 잘 작동하지만, DragonDiffusion을 적용할 수 있는 다른 편집 작업이 있을 수 있습니다. 이러한 작업을 식별하고 모델의 기능을 확장하는 것이 중요한 초점이 될 것입니다.

셋째, 현재 모델은 미세 조정이나 추가 모듈이 필요하지 않으므로 효율성과 사용 편의성 측면에서 상당한 이점이 있습니다. 그러나 특정 작업에서 DragonDiffusion의 성능을 향상시킬 수 있는 옵션 모듈을 탐색하는 것이 이점이 있을 수 있습니다. 이러한 개선 사항이 언제 유용하고 어떻게 구현하는 것이 가장 좋은지 이해하는 것은 향후 연구의 중요한 영역입니다.

마지막으로, DragonDiffusion은 생성된 이미지와 실제 이미지 모두에서 잘 작동하지만 다양한 유형의 이미지를 처리하는 데는 개선의 여지가 있을 수 있습니다. 예를 들어 특정 스타일의 이미지, 특정 조명 조건에서 촬영한 사진 또는 특정 색상 프로필을 가진 이미지의 성능을 개선할 수 있습니다.

결론적으로 드래곤디퓨전은 이미지 편집에 있어 강력한 도구임이 입증되었습니다. 그러나 모든 기술과 마찬가지로 항상 성장과 발전의 여지가 있습니다. 앞으로도 드래곤디퓨전을 지속적으로 개선하고 이 기술로 또 다른 가능성을 발견할 수 있기를 기대합니다.