# MusicLM: Generating Music From Text

[https://google-research.github.io/seanet/musiclm/examples/](https://google-research.github.io/seanet/musiclm/examples/)

[https://arxiv.org/abs/2301.11325](https://arxiv.org/abs/2301.11325)

1. Introduction

이 글에서는 조건부 신경망 오디오 생성 분야에 대해 설명합니다. 여기에는 입력 조건이나 가이드에 따라 음성과 음악을 포함한 소리를 생성할 수 있는 컴퓨터 모델을 만드는 것이 포함됩니다.

먼저 텍스트 음성 변환(모델에 텍스트를 입력하면 해당 텍스트를 말할 수 있는 음성을 생성하는 작업) 또는 미디 시퀀스에서 오디오 합성(모델에 미디 파일을 입력하면 해당 오디오를 생성하는 작업)과 같은 작업에서 많은 진전이 이루어졌다고 말합니다. 이러한 작업은 시간이 지남에 따라 입력 조건(예: 텍스트 또는 MIDI)을 출력 오디오와 일치시킬 수 있는 모델을 사용하여 수행되는 경우가 많습니다.

그러나 최근 들어 높은 수준의 시퀀스 전체 캡션을 기반으로 오디오를 생성하려는 시도에 대한 관심이 높아지고 있습니다. 이는 간단한 설명이나 제목을 제공하고 모델에 해당 사운드를 생성하도록 요청하는 것과 유사합니다. 예를 들어 모델에 "바람이 불면서 휘파람 소리"와 같은 캡션을 지정하면 모델이 해당 소리를 생성하려고 시도합니다. 하지만 현재 이러한 모델은 대부분 간단한 시나리오에 국한되어 있으며, 하나의 캡션으로 음악 클립과 같은 복잡한 여러 부분으로 구성된 사운드를 만드는 것은 여전히 어려운 문제입니다.

최근 오디오 생성 문제를 해결하기 위해 AudioLM이라는 모델이 개발되었습니다. 이 모델은 오디오 생성을 언어 모델링 작업처럼 취급하며, 언어 모델에서 단어 시퀀스를 생성하는 것처럼 오디오 단위 시퀀스를 생성하여 사운드를 생성합니다. 이 모델은 어떤 종류의 사운드를 생성할지에 대한 구체적인 지침 없이도 몇 초에 걸쳐 고품질의 일관된 오디오를 생성할 수 있습니다.

하지만 오디오와 일치하는 텍스트가 모두 포함된 데이터 세트가 많지 않기 때문에 이러한 모델에 적합한 학습 데이터를 찾는 데 어려움이 있습니다. 오디오는 복잡할 수 있고 시간적 측면이 있어 텍스트로 포착하기 어렵기 때문에 오디오에 대한 텍스트 설명을 생성하는 것은 어려울 수 있습니다.

연구진은 AudioLM을 확장하여 텍스트를 음악 생성을 위한 가이드로 사용할 수 있는 새로운 모델인 MusicLM을 제안합니다. 연구진은 훈련 데이터 부족 문제를 극복하기 위해 MuLan이라는 모델을 사용하여 임베딩 공간에서 가까운 오디오와 텍스트의 임베딩(숫자 표현)을 생성합니다. 이를 통해 대량의 오디오 전용 데이터를 사용하여 모델을 훈련할 수 있으며, MuLan 임베딩이 가이드 역할을 합니다.

MusicLM 모델은 레이블이 지정되지 않은 대규모 데이터 세트를 학습시켰을 때 "기억에 남는 색소폰 솔로와 솔로 가수가 등장하는 매혹적인 재즈 곡" 또는 "낮은 베이스와 강한 킥이 특징인 90년대 베를린 테크노"와 같은 텍스트 설명을 기반으로 길고 복잡한 음악을 생성할 수 있었습니다.

연구진은 모델을 평가하기 위해 전문 음악가가 작성한 설명 캡션과 짝을 이루는 550만 개의 음악 예시가 포함된 새로운 데이터 세트인 MusicCaps도 도입했습니다.

그 결과 생성된 음악의 품질과 입력된 캡션과 얼마나 잘 일치하는지 모두에서 MusicLM이 이전 모델보다 뛰어난 성능을 발휘한다는 사실을 발견했습니다. 또한 오디오 멜로디 형태의 추가 입력을 받아 텍스트 프롬프트에 설명된 스타일로 음악 클립을 생성하기 위한 가이드로 사용하도록 모델을 확장했습니다.

그러나 크리에이티브 콘텐츠의 오용 가능성 등 음악 제작과 관련된 위험이 있을 수 있다는 점을 인정합니다. 그들은 모델이 학습 데이터를 단순히 암기하고 역류하는 것이 아닌지 확인하기 위해 연구를 수행했으며, 생성된 음악이 학습 세트의 음악과 크게 다르다는 것을 발견했습니다.

이 작업의 주요 공헌을 요약하면 다음과 같습니다:

텍스트 설명에서 고품질의 일관된 음악을 생성할 수 있는 모델인 MusicLM의 도입.
음악 생성을 안내하기 위해 오디오 멜로디와 같은 추가 입력을 수용하도록 MusicLM을 확장.
텍스트-음악 생성 모델을 평가하기 위한 음악-텍스트 쌍의 새로운 데이터 세트인 MusicCaps의 생성.

2. Background and Related Work

이 섹션에서는 MusicLM과 관련이 있고 설계와 기능에 영향을 준 모델, 기법 및 기술에 대해 설명합니다.

먼저, 이산 토큰을 사용하는 자동 회귀 모델의 중요성을 강조합니다. 이 모델은 자연어 처리(NLP) 분야와 이미지 또는 비디오 생성에서 그 효과를 입증했습니다. 자동 회귀 모델은 일반적으로 정량화되거나 컴팩트한 불연속 단위 또는 토큰으로 세분화된 데이터에서 작동합니다. 벡터 양자화 가변 자동 인코더(VQ-VAE)라는 특정 접근 방식은 낮은 비트레이트에서 오디오, 이미지, 비디오의 고품질 재구성을 제공하는 데 있어 인상적인 결과를 보여주었습니다.

SoundStream은 높은 재구성 품질을 유지하면서 낮은 비트레이트에서 오디오를 압축하는 범용 뉴럴 오디오 코덱입니다. 이 코덱은 상당한 계산 비용 없이 더 높은 비트 전송률과 품질로 확장할 수 있는 방법인 잔여 벡터 양자화(RVQ)를 통해 이를 달성합니다.

그런 다음 이 문서에서는 장기적으로 일관된 고품질 오디오를 생성하기 위한 몇 가지 성공적인 방법(예: Jukebox 및 PerceiverAR)에 대해 설명합니다. AudioLM 모델은 토큰화 및 생성에 계층적 접근 방식을 사용하여 일관성과 고품질 합성의 균형을 맞출 수 있다는 점에서 두드러집니다.

MusicLM은 AudioLM을 기반으로 구축되었지만 설명 텍스트를 기반으로 생성 프로세스를 조건화하고, 멜로디와 같은 다른 신호로 이 조건화를 확장하며, 다양한 긴 음악 시퀀스를 모델링한다는 세 가지 중요한 기여를 도입했습니다.

이 글에서는 텍스트 임베딩을 사용하여 대상 오디오의 특징을 예측하는 DiffSound 및 AudioGen과 같은 모델을 통해 조건부 오디오 생성이 최근 연구에서 어떻게 탐구되어 왔는지에 대해 설명합니다. Mubert와 Riffusion과 같은 모델은 텍스트에 기반한 음악 생성에 초점을 맞추고 있으며, MusicLM은 이러한 모델을 개선한 것입니다.

이 백서에서는 생성 프로세스에 기호적 표현(예: MIDI)을 사용하는 방법과 아키텍처 개선과 고품질의 대규모 페어링 훈련 데이터의 가용성으로 인해 상당한 진전을 보인 텍스트 조건부 이미지 생성 모델에 대해서도 설명합니다.

음악과 텍스트를 위한 공동 임베딩 모델(예: MuLan)의 사용에 대해서도 논의합니다. 이러한 모델은 서로 다른 양식(예: 음악과 텍스트)을 공유 임베딩 공간에 매핑하여 검색 또는 제로샷 음악 태깅에 유용합니다. MusicLM 모델은 이러한 임베딩을 활용합니다.

3. Method

MusicLM은 텍스트에 맞춰 음악을 생성하는 강력한 모델입니다. 이 모델은 오디오와 텍스트의 표현 및 토큰화를 위해 세 가지 주요 구성 요소를 사용합니다. 이러한 구성 요소는 SoundStream, w2v-BERT, MuLan입니다.

SoundStream은 잔여 벡터 양자화(RVQ)를 사용하여 연속 오디오 신호를 음향 토큰이라고 하는 일련의 개별 토큰으로 변환하는 자동 회귀 모델입니다. 이 모델은 고음질 합성을 제공하며, 보폭이 480인 24kHz 오디오에서 작동하여 50Hz 임베딩을 제공하므로 6kbps 비트레이트에서 초당 600개의 오디오 토큰을 생성합니다.

w2v-BERT는 장기적인 코히어런트 생성을 용이하게 하는 마스킹 언어 모델입니다. 7번째 레이어에서 임베딩을 추출한 다음 K-평균을 사용하여 1024개의 클러스터로 정량화하여 초당 25개의 오디오 시맨틱 토큰을 생성합니다.

MuLan은 음악 생성 프로세스의 컨디셔닝 신호 역할을 하는 오디오 및 텍스트 임베딩을 제공합니다. MuLan의 오디오 임베딩 네트워크는 대상 오디오 시퀀스의 표현을 추출하는 데 사용되며, 이러한 임베딩은 정량화되어 12개의 MuLan 오디오 토큰(MA)을 생성합니다. 추론하는 동안 모델은 텍스트 프롬프트에서 MuLan 텍스트 임베딩을 추출하고, 이를 정량화한 후 12개의 토큰(MT)을 생성합니다. MuLan 모델을 사용하면 MusicLM을 확장할 수 있고 노이즈가 많은 텍스트 설명에 더욱 강력하게 대응할 수 있습니다.

MusicLM의 계층적 모델링에서 첫 번째 단계는 시맨틱 모델링으로, MuLan 오디오 토큰에서 시맨틱 토큰으로의 매핑을 학습합니다. 두 번째 단계는 음향 모델링 단계로, MuLan 오디오 토큰과 시맨틱 토큰을 기반으로 음향 토큰을 예측합니다. 음향 모델링 단계는 다시 거친 모델링 단계와 세밀한 모델링 단계로 나뉘는데, 거친 단계에서는 SoundStream RVQ의 처음 4개 레벨을 모델링하고 나머지 8개 레벨은 세밀한 단계로 모델링합니다. 이러한 계층적 접근 방식을 통해 모델은 텍스트에 따라 조정된 고품질의 일관된 음악을 생성할 수 있습니다.

4. Experimental Setup

이 연구에서는 MusicLM을 구축하는 과정에서 AudioLM의 의미적 단계와 음향적 단계를 모두 모델링하기 위해 디코더 전용 트랜스포머를 사용했습니다. 아키텍처는 24개의 레이어, 16개의 주의 헤드, 임베딩 차원 1024, 차원 4096의 피드 포워드 레이어, 드롭아웃 0.1, 상대 위치 임베딩으로 구성된 모든 모델에서 공유됩니다. 이 모델을 통해 스테이지당 4억 3천만 개의 파라미터가 생성되었습니다.

MusicLM의 학습 프로세스는 FMA(Free Music Archive) 데이터 세트의 오디오 전용 데이터를 활용하며, 추가 학습은 500만 개의 오디오 클립으로 구성된 데이터 세트에서 수행되며, 이는 24kHz에서 280만 시간 분량의 음악에 해당합니다. 추론 프로세스는 MuLan이 학습한 공동 임베딩 공간을 사용하여 오디오 토큰을 텍스트 토큰으로 대체합니다. 샘플링은 생성된 음악의 다양성과 시간적 일관성의 균형을 맞추기 위해 각 단계마다 다른 온도에서 수행됩니다.

MusicLM을 평가하기 위해 이 연구에서는 전문 음악가가 작성한 영어로 된 해당 텍스트 설명과 짝을 이루는 5.5만 개의 음악 클립이 포함된 음악 캡션 데이터 세트인 MusicCaps를 준비했습니다. 이 데이터 세트는 음악 클립에만 초점을 맞추고 있으며, 각 클립은 자유 텍스트 캡션과 음악 측면 목록과 짝을 이룹니다.

이 연구에서는 다음과 같은 MusicLM을 평가하기 위한 몇 가지 메트릭을 소개합니다:

프리셋 오디오 거리(FAD): 공개적으로 사용 가능한 두 가지 오디오 임베딩 모델인 트릴과 VGGish를 기반으로 오디오 품질을 측정합니다. FAD 점수가 낮을수록 오디오 품질이 우수함을 나타냅니다.

KL 다이버전스(KLD): 생성된 음악과 레퍼런스 음악의 클래스 예측을 비교하여 생성된 음악이 입력 텍스트 설명에 얼마나 부합하는지 평가합니다.

뮤란 사이클 일관성(MCC): 뮤란 임베딩을 기반으로 음악-텍스트 쌍 간의 유사성을 측정합니다.

정성적 평가: 평가자에게 텍스트 설명과 두 개의 음악 샘플을 제시하고 선호도를 표시하도록 요청하는 인적 평가 작업입니다.

마지막으로, 이 논문은 정확한 일치와 대략적인 일치를 감지하는 방법을 사용하여 MusicLM이 음악 세그먼트를 어느 정도 기억할 수 있는지 연구하기 위해 Carlini 외(2022)에서 사용된 방법론을 적용합니다.

5. Results

이 섹션에서는 저자들이 설명 텍스트로부터 음악을 생성하는 시스템인 MusicLM을 평가한 방법을 자세히 설명합니다. 저자들은 MusicLM을 기존의 두 가지 시스템인 Mubert와 Riffusion과 비교했습니다. 이를 위해 각 시스템으로 오디오를 생성하고 음악 클립과 해당 텍스트 설명으로 구성된 새로운 데이터 세트인 MusicCaps에서 이를 평가했습니다.

평가는 생성된 오디오의 품질과 생성된 음악이 입력된 텍스트 설명과 일치하는 정도라는 두 가지 측면에 중점을 두었습니다. 평가에 사용된 지표에는 오디오 품질을 측정하는 프리셋 오디오 거리(FAD), 제작된 음악과 텍스트 설명의 일치도를 측정하는 KL 다이버전스(KLD), 음악-텍스트 쌍 간의 유사성을 측정하는 뮤랜 사이클 일관성(MCC)이 포함되었습니다.

또한 저자들은 음악이 텍스트 설명에 얼마나 충실한지 평가하기 위해 사람이 직접 듣는 테스트를 실시했습니다. 평가자에게 텍스트 설명과 두 개의 음악 샘플을 제공하고 어떤 샘플이 설명에 더 잘 맞는지 결정하도록 요청했습니다.

그 결과 MusicLM은 모든 지표에서 다른 두 시스템을 능가하는 우수한 성능을 보였으며, 제공된 텍스트 설명에 밀접하게 부합하는 고품질 음악을 생성하는 능력을 보여주었습니다. 또한 저자들은 MusicLM에 시맨틱 모델링 단계를 포함시킴으로써 음악의 장기적인 구조를 보존하고 텍스트 설명과 생성된 음악 간의 정렬을 개선하는 데 도움이 된다는 사실을 발견했습니다.

또한 저자들은 MusicLM이 학습 데이터에서 시퀀스를 어느 정도까지 암기할 수 있는지도 조사했습니다. 비슷한 입력이 주어졌을 때에도 MusicLM이 다양한 출력을 생성하는 것을 발견했으며, 이는 단순히 암기된 시퀀스를 재현하는 것이 아님을 시사합니다.

요약하자면, 이번 평가를 통해 MusicLM은 주어진 텍스트 설명에 밀접하게 부합하는 고품질 음악을 생성할 수 있으며, 학습 데이터의 시퀀스를 암기하지 않고도 다양한 출력을 생성할 수 있음을 입증했습니다.

6. Extensions

이 섹션에서는 멜로디 컨디셔닝과 "스토리 모드"에서의 롱 제너레이션이라는 두 가지 향상된 MusicLM 기능에 대해 설명합니다.

멜로디 컨디셔닝
저자들은 텍스트 설명 외에 멜로디 입력을 수용하도록 MusicLM을 확장했습니다. 이 멜로디는 허밍, 노래, 휘파람 또는 악기 연주를 통해 제공될 수 있습니다. 이를 위해 저자는 멜로디는 같지만 음향 특성이 다른 오디오 쌍으로 구성된 합성 데이터 세트를 생성합니다. 그리고 동일한 멜로디를 포함하는 두 오디오 클립의 임베딩이 서로 가깝도록 공동 임베딩 모델을 훈련합니다. 추론하는 동안 작성자는 입력 오디오 클립에서 멜로디 토큰을 계산하고 이를 MuLan 텍스트 토큰으로 연결합니다. 이를 통해 MusicLM은 입력 멜로디와 제공된 텍스트 설명에 모두 부합하는 음악을 생성할 수 있습니다.

롱 제너레이션 및 스토리 모드
또한 저자들은 학습 중에 사용된 시퀀스보다 더 긴 시퀀스를 생성하도록 MusicLM을 조정하여 이를 "긴 생성"이라고 부릅니다. MusicLM의 시맨틱 모델링 단계는 30초 길이의 시퀀스로 학습됩니다. 더 긴 시퀀스를 생성하기 위해 작성자는 이전 15초를 접두사로 사용하여 15초를 추가로 생성하는 방식으로 15초 단위로 진행하며, 항상 동일한 텍스트 설명을 조건으로 합니다. 이 방법을 사용하면 길고 일관된 오디오 시퀀스를 생성할 수 있습니다.

또한 시간이 지남에 따라 텍스트 설명의 변화에 따라 변경되는 긴 오디오 시퀀스를 생성할 수 있는 '스토리 모드' 기능도 소개합니다. 저자들은 여러 텍스트 설명에서 뮤란 텍스트 토큰을 계산하고 15초마다 컨디셔닝 신호를 수정하여 이를 달성합니다. 그 결과 텍스트 설명에 따라 음악적 맥락을 변경하는 부드럽고 템포가 일관되며 의미적으로 그럴듯한 전환이 이루어집니다.

7. Conclusions

결론에서 저자들은 자신들의 공헌을 요약합니다: 24kHz의 주파수에서 고품질 음악을 생성할 수 있는 텍스트 컨디셔닝 생성 모델인 MusicLM. 이 음악은 몇 분 동안 일관되며 컨디셔닝 텍스트 신호와 밀접하게 일치합니다. 연구진은 전문 음악가가 만든 5.5만 개의 음악-텍스트 쌍으로 구성된 고품질 데이터 세트인 MusicCaps에서 기존 기준선보다 이 모델의 우수성을 입증했습니다.

이러한 성과에도 불구하고 저자들은 뮤란에서 물려받은 모델의 한계를 인정합니다. 여기에는 컨디셔닝 텍스트의 부정에 대한 오해와 텍스트에 설명된 정확한 시간 순서를 준수하지 못한다는 점이 포함됩니다. 또한 매튜스 상관관계 계수(MCC) 점수가 뮤란에 의존하기 때문에 편향적으로 왜곡될 수 있으므로 정량적 평가를 더욱 개선할 필요가 있음을 인정합니다.

저자들은 향후 연구를 위한 몇 가지 잠재적인 방향을 제시합니다. 여기에는 가사 생성, 텍스트 컨디셔닝 프로세스 개선 및 보컬 품질 향상, 도입부, 절, 코러스와 같은 높은 수준의 노래 구조 모델링, 더 높은 샘플 속도로 음악 작업 등이 포함됩니다.

Appendix

뮤직캡스 데이터 세트:
이 논문은 영어로 된 해당 텍스트 설명과 짝을 이루는 음악 클립으로 구성된 MusicCaps라는 데이터셋을 제시합니다. AudioSet에서 파생된 이 데이터 세트에는 다양한 장르의 음악이 포함된 총 5,521개의 예시가 포함되어 있습니다. AudioSet 평가 분할에서 추출한 1,000개의 예시 중 균형 잡힌 하위 집합이 추가로 강조 표시되어 있습니다.

텍스트 캡션 및 종횡비 목록:
데이터 세트에는 자유 텍스트 캡션과 종횡비 목록이 포함되어 있습니다. 자유 텍스트 캡션은 내러티브 스타일로 음악을 설명하는 반면, 종횡비 목록은 일련의 키워드 또는 짧은 문구를 사용합니다. 제공되는 예에는 다양한 음악 장르, 분위기, 사용된 악기, 녹음 품질 및 가상의 청취 상황에 대한 설명이 포함됩니다.

정성적 평가:
평가를 위해 듣기 테스트 참가자에게 10초짜리 클립 두 개와 텍스트 캡션을 제시하고 어떤 클립이 텍스트와 가장 잘 어울리는지 물었습니다. 오디오 품질은 무시하고 음악이 텍스트와 얼마나 잘 어울리는지에만 초점을 맞추었습니다. 이 테스트의 결과는 통계적으로 유의미했으며, 이는 모델에 의해 생성된 음악이 제공된 텍스트 설명과 일치한다는 것을 나타냅니다.

멜로디 컨디셔닝 구현 세부 사항:
저자들은 멜로디 컨디셔닝 모델의 기술적 세부 사항에 대해 설명합니다. 이 모델은 비전 트랜스포머(ViT)를 사용하여 구축되었으며, 4초의 오디오마다 192차원 임베딩을 생성하도록 학습되었습니다. 이러한 임베딩은 연주되는 악기의 특성에 변하지 않으면서 멜로디를 캡처합니다. 그런 다음 토큰으로 분리되어 뮤란 오디오 토큰과 함께 음악 생성 프로세스를 조절하는 데 사용됩니다.

- 요약
    1. 소개 및 동기:
    저자는 설명 텍스트를 기반으로 고품질의 음악을 생성할 수 있는 생성 모델인 MusicLM의 개념을 소개합니다. 이 아이디어는 텍스트에 묘사된 분위기나 주제에 맞는 음악을 만드는 것입니다.
    2. 모델 아키텍처:
    MusicLM은 텍스트의 의미론적 이해와 음악의 음향 모델링을 분리하는 2단계 아키텍처를 기반으로 합니다. 이 모델은 설명 텍스트를 의도한 음악을 나타내는 일련의 시맨틱 토큰으로 변환하는 기존 언어 모델인 MuLan을 사용합니다. 그런 다음 음향 모델링 단계에서는 이러한 시맨틱 토큰을 입력으로 받아 오디오로 합성할 수 있는 형태로 음악을 나타내는 음향 토큰 시퀀스를 생성합니다.
    3. 데이터 세트 및 기준선:
    저자들은 전문 음악가들이 만든 5.5만 개의 음악-텍스트 쌍으로 구성된 고품질 데이터 세트인 MusicCaps를 사용합니다. 그리고 MusicLM의 성능을 기존의 두 가지 기준선인 Mubert와 Riffusion과 비교합니다.
    4. 평가:
    MusicLM은 양적 및 질적 기준으로 평가됩니다. 평가 지표에는 FAD 지표로 측정되는 오디오 품질과 KLD 및 MCC 점수로 측정되는 입력 텍스트에 대한 생성된 음악의 충실도가 포함됩니다. 이 모델은 사람의 청취 테스트도 거칩니다. 이러한 평가에 따르면 MusicLM은 오디오 품질과 입력 텍스트에 대한 충실도 모두에서 기준선을 뛰어넘는 성능을 보였습니다.
    5. MusicLM의 확장:
    저자들은 텍스트뿐만 아니라 멜로디에 대해서도 MusicLM을 컨디셔닝하여 모델이 주어진 멜로디를 따르는 음악을 생성할 수 있는 가능성을 탐색합니다. 또한 시간이 지남에 따라 변화하는 텍스트 설명에 따라 변화하는 연속적인 음악을 생성할 수 있는 '스토리 모드'의 개념을 소개합니다.
    6. 결론 및 향후 작업:
    저자들은 부정 해석의 어려움과 텍스트에 설명된 시간 순서를 엄격하게 따르지 못하는 점 등 MusicLM의 몇 가지 한계를 인정하며 결론을 내립니다. 가사 생성, 음악의 컨디셔닝 및 보컬 품질 개선, 더 높은 수준의 노래 구조 모델링 등 향후 연구를 위한 몇 가지 영역을 제안합니다.
    7. 주요 내용:
    이 논문에서는 텍스트 설명을 기반으로 고품질의 음악을 생성하는 새로운 AI 모델인 MusicLM을 소개하고 기존 방법보다 우수성을 입증합니다. 또한 이 분야의 추가 개선 및 응용 가능성을 제시합니다.