# AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning

[https://arxiv.org/abs/2307.04725](https://arxiv.org/abs/2307.04725)

[https://animatediff.github.io/](https://animatediff.github.io/)

![저희는 AnimateDiff를 소개합니다, 이는 모델 특정 튜닝 없이 개인화된 텍스트-이미지 (T2I) 모델을 애니메이션 생성기로 확장하는 효과적인 프레임워크입니다. 대용량 비디오 데이터셋에서 학습된 동작에 대한 사전 정보를 바탕으로, AnimateDiff는 사용자가 훈련시키거나 CivitAI [4] 또는 Huggingface [8]와 같은 플랫폼에서 직접 다운로드한 개인화된 T2I 모델에 삽입되어 적절한 움직임의 애니메이션 클립을 생성할 수 있습니다.](AnimateDiff%20Animate%20Your%20Personalized%20Text-to-Imag%20b4f0410aa19d4a508cd13517c9b4b89c/Untitled.png)

저희는 AnimateDiff를 소개합니다, 이는 모델 특정 튜닝 없이 개인화된 텍스트-이미지 (T2I) 모델을 애니메이션 생성기로 확장하는 효과적인 프레임워크입니다. 대용량 비디오 데이터셋에서 학습된 동작에 대한 사전 정보를 바탕으로, AnimateDiff는 사용자가 훈련시키거나 CivitAI [4] 또는 Huggingface [8]와 같은 플랫폼에서 직접 다운로드한 개인화된 T2I 모델에 삽입되어 적절한 움직임의 애니메이션 클립을 생성할 수 있습니다.

### 1. Introduction

서면 설명을 해당 이미지로 변환하는 텍스트-이미지(T2I) 생성 모델은 최근 몇 년 동안 상당한 관심을 받고 있습니다. 고품질의 비주얼을 제공하고 텍스트로 제어할 수 있어 아티스트나 아마추어처럼 연구자가 아닌 사람도 쉽게 사용할 수 있기 때문에 인기가 높습니다. 드림부스나 LoRA와 같은 새로운 기술을 통해 사용자는 소규모 데이터 세트에서 이러한 모델을 미세 조정하여 개인화된 고품질 콘텐츠를 제작할 수 있습니다.

개인화된 T2I 모델이 만들어내는 인상적인 비주얼에도 불구하고 이러한 결과물은 정적인 이미지입니다. 다음 과제는 원래의 시각적 품질을 유지하면서 시간적 요소를 추가하고 애니메이션 이미지를 생성하는 것입니다. 그러나 애니메이션을 T2I 모델에 통합하려면 신중한 매개변수 조정, 개인화된 비디오 수집, 많은 컴퓨팅 성능이 필요하기 때문에 일반 사용자에게는 어려운 작업입니다.

이 연구에서 저자는 개인화된 T2I 모델에서 애니메이션 이미지를 생성할 수 있는 방법인 AnimateDiff를 제안합니다. 이 방법은 각 모델에 대한 특정 튜닝이 필요하지 않으며 시간이 지나도 매력적인 콘텐츠 일관성을 보장합니다. 모션 모델링 모듈을 기본 T2I 모델에 도입한 다음 대규모 비디오 클립에서 미세 조정을 통해 적절한 모션 패턴을 학습합니다. 중요한 점은 기본 모델의 파라미터가 동일하게 유지된다는 점입니다.

다양한 드림부스 및 LoRA 모델에 적용했을 때 AnimateDiff는 잘 훈련된 모션 모델링 모듈을 통합하여 대부분의 개인화된 T2I 모델에 성공적으로 애니메이션을 적용합니다. 이 방법은 3D 만화, 2D 애니메이션 등 다양한 시각적 영역에 적용할 수 있으며, 저렴한 비용으로 빠르게 개인화된 애니메이션을 얻을 수 있어 개인화된 애니메이션의 새로운 기준을 제시합니다.

### 2. Related Works

텍스트-이미지(T2I) 확산 모델은 대규모 텍스트-이미지 페어링 데이터에서 시각적으로 만족스러운 결과를 생성하는 능력과 확산 모델의 견고함으로 인해 인기를 얻고 있습니다. GLIDE, DALLE-2, Imagen, Stable Diffusion과 같은 모델은 텍스트 조건, 향상된 텍스트-이미지 정렬, 효율적인 계산 프로세스 등의 혁신을 도입했습니다. eDiff-I와 같은 모델은 여러 합성 단계에 여러 확산 모델을 활용합니다. 이 백서에서 제안하는 방법은 이러한 사전 학습된 T2I 모델을 기반으로 하며 모든 조정된 개인화 버전에 적용할 수 있습니다.

T2I 모델은 강력하지만 대규모 데이터와 상당한 컴퓨팅 리소스가 필요하기 때문에 개별 사용자가 쉽게 접근할 수 없습니다. 사용자가 사전 학습된 T2I 모델에 새로운 도메인을 추가할 수 있는 몇 가지 방법이 제안되었습니다. 텍스트 반전, 드림부스, 커스텀 확산, LoRA와 같은 기법을 통해 이러한 모델을 미세 조정할 수 있으며 시각적 품질이 좋은 결과를 얻을 수 있습니다. 이 백서에서는 기본 모델의 피처 공간을 변경하지 않고 유지하는 드림부스 및 LoRA와 같은 튜닝 기반 방법에 중점을 둡니다.

개인화된 T2I 애니메이션의 경우 아직 확립된 방법이 거의 없는 새로운 분야입니다. T2I 모델을 비디오 생성으로 확장하는 기존 작업은 전체 네트워크 파라미터를 업데이트하는 경우가 많으며, 이는 원래 T2I 모델의 도메인 지식에 부정적인 영향을 미칠 수 있습니다. 개인화된 T2I 모델을 애니메이션화하는 작업도 일부 수행되었습니다. 예를 들어, 튠어비디오(Tune-a-Video)와 텍스트2비디오제로(Text2Video-Zero)는 사전 학습된 T2I 모델을 약간의 아키텍처 수정이나 잠재적 래핑을 통해 애니메이션화하는 접근법을 제안합니다. 제안된 방법과 유사한 방법으로는 T2I 모델에서 별도의 시간적 레이어를 훈련하는 Align-Your-Latents가 있습니다. 저자의 방법은 네트워크 설계를 간소화하고 개인화된 T2I 모델을 애니메이션화하는 데 효과적입니다.

### 3. Method

먼저 일반적인 텍스트-이미지 변환(T2I) 모델, 특히 이 작업에 사용되는 안정적 확산(SD)의 기본 사항을 간략하게 설명합니다. SD는 자동 인코더의 잠재 공간에서 노이즈 제거 프로세스를 수행하여 계산 효율성과 고품질 시각적 결과물의 균형을 효과적으로 맞추는 방식으로 작동합니다.

![AnimateDiff의 파이프라인. 기본 T2I 모델(예: Stable Diffusion [22])이 주어지면, 우리의 방법은 먼저 비디오 데이터셋에서 동작 모델링 모듈을 훈련시켜 동작에 대한 사전 정보를 추출하도록 합니다. 이 단계에서는 동작 모듈의 매개변수만 업데이트되므로, 기본 T2I 모델의 피처 공간이 유지됩니다. 추론 시, 한 번 훈련된 동작 모듈은 기본 T2I 모델을 기반으로 튜닝된 모든 개인화 모델을 애니메이션 생성기로 변환한 후, 반복적인 노이즈 제거 과정을 통해 다양하고 개인화된 애니메이션 이미지를 생성할 수 있습니다.](AnimateDiff%20Animate%20Your%20Personalized%20Text-to-Imag%20b4f0410aa19d4a508cd13517c9b4b89c/Untitled%201.png)

AnimateDiff의 파이프라인. 기본 T2I 모델(예: Stable Diffusion [22])이 주어지면, 우리의 방법은 먼저 비디오 데이터셋에서 동작 모델링 모듈을 훈련시켜 동작에 대한 사전 정보를 추출하도록 합니다. 이 단계에서는 동작 모듈의 매개변수만 업데이트되므로, 기본 T2I 모델의 피처 공간이 유지됩니다. 추론 시, 한 번 훈련된 동작 모듈은 기본 T2I 모델을 기반으로 튜닝된 모든 개인화 모델을 애니메이션 생성기로 변환한 후, 반복적인 노이즈 제거 과정을 통해 다양하고 개인화된 애니메이션 이미지를 생성할 수 있습니다.

개인화된 이미지 생성 측면에서 이 연구는 널리 사용되는 두 가지 기술에 중점을 둡니다: 드림부스와 LoRA. 두 기법 모두 사용자가 사전 학습된 T2I 모델에 새로운 도메인(콘셉트, 스타일 등)을 도입할 수 있습니다. 드림부스는 고유한 문자열을 사용하여 대상 도메인을 표현하고 원래 T2I 모델에서 생성된 이미지로 데이터 세트를 보강합니다. 반면 LoRA는 모델의 잔여 가중치를 미세 조정하고 가중치 행렬을 두 개의 낮은 순위 행렬로 분해하여 효율성을 높이고 과적합을 방지합니다.

![동작 모듈의 세부 사항. 모듈 삽입(왼쪽): 우리의 동작 모듈들은 사전 훈련된 이미지 레이어 사이에 삽입됩니다. 데이터 배치가 이미지 레이어와 우리의 동작 모듈을 통과하면, 그것의 시간적 및 공간적 축은 별도로 배치 축으로 재구성됩니다. 모듈 설계(오른쪽): 우리의 모듈은 출력 프로젝트 레이어가 0으로 초기화된 평범한 시간 변환기입니다.](AnimateDiff%20Animate%20Your%20Personalized%20Text-to-Imag%20b4f0410aa19d4a508cd13517c9b4b89c/Untitled%202.png)

동작 모듈의 세부 사항. 모듈 삽입(왼쪽): 우리의 동작 모듈들은 사전 훈련된 이미지 레이어 사이에 삽입됩니다. 데이터 배치가 이미지 레이어와 우리의 동작 모듈을 통과하면, 그것의 시간적 및 공간적 축은 별도로 배치 축으로 재구성됩니다. 모듈 설계(오른쪽): 우리의 모듈은 출력 프로젝트 레이어가 0으로 초기화된 평범한 시간 변환기입니다.

이 연구의 목표는 원래의 도메인 지식과 품질을 유지하면서 개인화된 T2I 모델을 애니메이션화하는 것으로, 일반적으로 해당 비디오 컬렉션으로 추가 튜닝이 필요한 작업입니다. 이를 위해 저자는 일반화 가능한 모션 모델링 모듈을 별도로 학습하고 추론 시점에 개인화된 T2I에 연결할 것을 제안합니다. 이 접근 방식은 각 개인화된 모델에 대한 특정 튜닝이 필요하지 않으며 사전 학습된 가중치를 변경하지 않습니다.

실제 구현을 위해 원본 SD 모델을 인플레이션하여 5D 비디오 텐서 입력을 처리하고 모션 모델링 모듈은 각 배치의 프레임에 걸쳐 작동합니다. 모션 모델링 모듈은 템포럴 트랜스포머를 사용하여 프레임 간에 정보를 효율적으로 교환하도록 설계되었습니다. 이 작업은 시간 축을 가로질러 같은 위치에 있는 특징들 간의 시간적 종속성을 포착합니다.

모션 모델링 모듈의 훈련 과정은 잠재 확산 모델과 유사합니다. 샘플링된 비디오 데이터는 먼저 잠재 코드로 인코딩된 다음 정의된 순방향 확산 스케줄을 사용하여 노이즈에 영향을 받습니다. 모션 모듈로 부풀려진 확산 네트워크는 노이즈가 있는 잠재 코드와 해당 텍스트 프롬프트를 입력으로 받아 잠재 코드에 추가된 노이즈 강도를 예측합니다. 훈련 목표는 L2 손실 용어입니다. 중요한 점은 최적화 과정에서 기본 T2I 모델의 사전 훈련된 가중치가 동결되어 특징 공간을 유지한다는 것입니다.

### 4. Experiments

텍스트-비디오 애니메이션을 위한 안정적 확산 v1을 기반으로 개인화된 모델을 훈련하기 위해 설계된 모션 모델링 모듈의 방법을 소개하는 실험 섹션입니다. 이 백서의 주요 초점은 애니메이션에서 사실적인 사진에 이르기까지 다양한 스타일을 애니메이션화하는 동시에 각 스타일의 고유한 특성을 유지하는 모델의 일반화 능력에 있습니다.

이 논문은 실험 섹션에서 네 부분으로 구성된 구조를 제시합니다:

![요소 분석 연구. 우리는 세 가지 확산 일정을 실험하고, 각각이 Stable Diffusion이 사전 훈련된 일정에서 어느 정도로 다른지를 확인하고 결과를 질적으로 비교합니다.](AnimateDiff%20Animate%20Your%20Personalized%20Text-to-Imag%20b4f0410aa19d4a508cd13517c9b4b89c/Untitled%203.png)

요소 분석 연구. 우리는 세 가지 확산 일정을 실험하고, 각각이 Stable Diffusion이 사전 훈련된 일정에서 어느 정도로 다른지를 확인하고 결과를 질적으로 비교합니다.

4.1 구현 세부 사항: 텍스트-비디오 쌍 데이터 세트인 WebVid-10M을 사용하여 모션 모델링 모듈을 훈련한 방법을 설명합니다. 또한 더 나은 시각적 품질과 새로운 작업에 대한 적응성을 달성하기 위해 원래의 확산 일정을 수정한 내용도 언급합니다.

4.2 정성적 결과: 다양한 스타일에 걸쳐 이 방법으로 성공적으로 제작된 여러 애니메이션을 보여줍니다. 이 결과는 텍스트 프롬프트에서 모션을 성공적으로 캡처하고 장면의 여러 요소를 구분하여 다양한 영역에서 개인화된 텍스트-이미지(T2I) 모델을 애니메이션화하는 모션 모듈의 다용도성을 보여줍니다.

4.3 기준선과의 비교: 저자들은 자신들의 모델을 비디오 생성을 위해 T2I 모델을 확장하는 또 다른 프레임워크인 Text2Video-Zero와 비교했습니다. 두 방법 모두 개인화된 모델의 도메인 지식을 유지하지만, 이 접근 방식이 더 세분화된 프레임 간 일관성, 프레임 간 부드러운 전환, 기본 카메라 모션에 더 잘 부합하는 적절한 콘텐츠 변경을 제공한다는 사실을 발견했습니다.

4.4 절제 연구: 훈련 중 순방향 확산 프로세스에서 노이즈 스케줄 선택을 검증하는 실험을 실시했습니다. 연구진은 확산 스케줄을 약간 수정하면 시각적 품질과 모션 부드러움 사이의 균형이 유지되어 모델이 새로운 작업과 영역에 적응하는 데 도움이 된다는 사실을 발견했습니다. 결론은 확산 일정을 약간 수정하면 모델이 새로운 작업과 영역에 더 잘 적응하여 시각적 품질과 모션 부드러움 사이의 균형을 맞추는 데 도움이 된다는 것입니다.

![실패 사례. 우리의 방법은 개인화된 도메인이 현실에서 멀어질 경우 적절한 동작을 생성할 수 없습니다.](AnimateDiff%20Animate%20Your%20Personalized%20Text-to-Imag%20b4f0410aa19d4a508cd13517c9b4b89c/Untitled%204.png)

실패 사례. 우리의 방법은 개인화된 도메인이 현실에서 멀어질 경우 적절한 동작을 생성할 수 없습니다.

전반적으로 실험을 통해 모션 모델링 모듈이 다양한 스타일에 일반화할 수 있고 생성된 애니메이션에서 높은 수준의 품질과 모션 부드러움을 유지할 수 있음을 입증했습니다.

### 5. Limitations and Future Works

저자는 특히 2D 디즈니 만화와 같이 개인화된 T2I 모델의 영역이 현실과는 거리가 먼 경우의 연구에서 관찰된 한계에 대해 논의합니다. 이러한 경우 애니메이션 결과물에는 종종 눈에 띄는 아티팩트가 있으며 원하는 모션을 생성하지 못할 수 있습니다. 이 문제는 보다 사실적인 트레이닝 비디오 데이터와 개인화된 모델 간의 분포 차이가 크기 때문일 수 있습니다. 이 문제에 대한 한 가지 가능한 해결책은 대상 도메인에서 비디오를 수집하고 모션 모델링 모듈을 약간 미세 조정하는 것입니다. 그러나 이 접근 방식은 향후 연구에 맡겨져 있습니다.

결론적으로, 저자들은 AnimateDiff가 대부분의 기존 개인화된 T2I 모델을 애니메이션 제너레이터로 전환할 수 있는 실용적인 솔루션을 제공한다고 주장합니다. 이 솔루션은 대규모 비디오 데이터 세트에서 일반화 가능한 모션 프리오어를 추출하여 이를 수행합니다. 학습이 완료되면 모션 모듈을 다른 개인화된 모델에 추가하여 원하는 모션을 캡처하고 특정 도메인에 충실한 애니메이션 이미지를 생성할 수 있습니다. AnimateDiff는 다양한 개인화된 T2I 모델에 대해 광범위하게 평가되어 그 효과와 일반화 가능성을 확인했습니다. 따라서 AnimateDiff는 개인화된 애니메이션을 위한 간단하면서도 강력한 기준이 될 수 있으며, 잠재적으로 다양한 애플리케이션에 도움이 될 수 있습니다.

![모델의 다양성. 여기에서는 같은 프롬프트와 개인화된 모델로 생성된 두 그룹의 결과를 보여주며, 이를 통해 AnimateDiff로 인플레이션 된 후에도 개인화된 생성기가 그 다양성을 유지하는 것을 보여줍니다.](AnimateDiff%20Animate%20Your%20Personalized%20Text-to-Imag%20b4f0410aa19d4a508cd13517c9b4b89c/Untitled%205.png)

모델의 다양성. 여기에서는 같은 프롬프트와 개인화된 모델로 생성된 두 그룹의 결과를 보여주며, 이를 통해 AnimateDiff로 인플레이션 된 후에도 개인화된 생성기가 그 다양성을 유지하는 것을 보여줍니다.

![추가적인 질적 결과. 우리는 우리의 프레임워크에서 동작 모델링 모듈이 삽입된 모델로 생성된 여러 애니메이션 클립을 보여줍니다. 각 프롬프트에서 관련 없는 태그들, 예를 들면, “걸작”, “고품질”, 등은 명확성을 위해 생략되었습니다.](AnimateDiff%20Animate%20Your%20Personalized%20Text-to-Imag%20b4f0410aa19d4a508cd13517c9b4b89c/Untitled%206.png)

추가적인 질적 결과. 우리는 우리의 프레임워크에서 동작 모델링 모듈이 삽입된 모델로 생성된 여러 애니메이션 클립을 보여줍니다. 각 프롬프트에서 관련 없는 태그들, 예를 들면, “걸작”, “고품질”, 등은 명확성을 위해 생략되었습니다.

![추가적인 질적 결과. 우리는 우리의 프레임워크에서 동작 모델링 모듈이 삽입된 모델로 생성된 여러 애니메이션 클립을 보여줍니다. 각 프롬프트에서 관련 없는 태그들, 예를 들면, “걸작”, “고품질”, 등은 명확성을 위해 생략되었습니다.](AnimateDiff%20Animate%20Your%20Personalized%20Text-to-Imag%20b4f0410aa19d4a508cd13517c9b4b89c/Untitled%207.png)

추가적인 질적 결과. 우리는 우리의 프레임워크에서 동작 모델링 모듈이 삽입된 모델로 생성된 여러 애니메이션 클립을 보여줍니다. 각 프롬프트에서 관련 없는 태그들, 예를 들면, “걸작”, “고품질”, 등은 명확성을 위해 생략되었습니다.