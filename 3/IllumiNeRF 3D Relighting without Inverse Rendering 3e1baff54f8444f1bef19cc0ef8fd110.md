# IllumiNeRF: 3D Relighting without Inverse Rendering

[https://arxiv.org/abs/2406.06527](https://arxiv.org/abs/2406.06527)

- Jun 2026

![알려지지 않은 조명 하에서 촬영된 입력 이미지 세트(위쪽에 세트 중 네 개의 예제 이미지가 표시됨)가 주어지면, IllumiNeRF는 타겟 조명(크롬 볼로 나타낸) 하에서 재조명된 고품질의 새로운 뷰(아래쪽)를 생성한다. 입력은 Stanford-ORB 데이터셋에서 가져왔다.](IllumiNeRF%203D%20Relighting%20without%20Inverse%20Rendering%203e1baff54f8444f1bef19cc0ef8fd110/Untitled.png)

알려지지 않은 조명 하에서 촬영된 입력 이미지 세트(위쪽에 세트 중 네 개의 예제 이미지가 표시됨)가 주어지면, IllumiNeRF는 타겟 조명(크롬 볼로 나타낸) 하에서 재조명된 고품질의 새로운 뷰(아래쪽)를 생성한다. 입력은 Stanford-ORB 데이터셋에서 가져왔다.

## 1 Introduction

객체의 외형을 캡처하여 새로운 환경에서 정확하게 렌더링하는 것은 컴퓨터 비전의 중요한 문제로, 이는 증강 현실, 가상 현실, 사진 촬영, 영화 제작 및 게임 개발에서 3D 콘텐츠 생성을 보다 쉽게 할 수 있게 한다. 최근 뷰 합성 기술은 관찰된 이미지 세트만으로 새로운 시점에서 렌더링할 수 있는 3D 표현을 재구성하는 데 큰 진전을 이루었으나, 이는 캡처된 조명 하에서만 객체의 외형을 복원할 수 있으며, 임의의 조명 조건에서 재조명된 뷰를 렌더링하는 것은 여전히 어려운 과제이다.

전통적인 방법은 역 렌더링을 사용하여 입력 이미지의 기하학, 재료 및 조명을 추정하려고 하지만, 이는 계산 비용이 많이 들고 최적화 문제가 불안정하며 모호하다. 우리는 이러한 역 렌더링 접근법을 피하고, 대신 생성적 이미지 모델을 활용하여 재조명 작업을 수행하는 새로운 접근 방식을 제안한다. 이 모델은 객체의 이미지를 다양한 타겟 조명 조건 하에서 샘플링하여 그럴듯한 재조명된 이미지를 생성한다.

우리의 방법은 단일 설명을 복원하는 대신, 각 관찰된 시점에서 다수의 그럴듯한 재조명 이미지를 샘플링하고, 이를 하나의 일관된 3D 표현으로 통합하는 '잠재 NeRF'를 사용한다. 이 접근 방식은 기존의 3D 역 렌더링보다 경쟁력 있으며, 다양한 벤치마크에서 우수한 성능을 보인다.

## 2 Related Work

우리의 작업은 단일 이미지 재조명을 위한 생성적 사전으로 조명 조건부 확산 모델을 사용하여 재조명 가능한 3D 재구성을 다룬다. 이는 재조명 가능한 3D 재구성, 역 렌더링, 단일 이미지 재조명과 밀접한 관련이 있다. 아래에서는 이러한 작업을 검토하고 제안된 접근 방식과의 관련성을 논의한다.

### 재조명 가능한 3D 재구성

재조명 가능한 3D 재구성의 목표는 객체를 재조명 가능한 3D 표현으로 재구성하여 새로운 조명 조건과 새로운 카메라 위치에서 렌더링할 수 있게 하는 것이다. 객체를 여러 조명 조건 하에서 관찰하는 경우, 선형 조명 조건의 조합을 통해 객체의 외형을 렌더링하는 것이 간단하지만, 이는 실험실 캡처 시나리오에 국한된다. 일반적인 캡처 상황에서는 객체를 단일 또는 소수의 조명 조건 하에서만 관찰할 수 있으며, 이러한 설정에서는 역 렌더링 기반 방법을 사용하여 객체의 외형을 3D 기하학, 재료 특성 및 조명으로 분해한다. 최첨단 3D 역 렌더링 접근법은 입력 이미지에서 3D 기하학의 신경장 표현을 시작하여 물리 기반 렌더링 절차를 통해 이 요소들을 공동 최적화한다.

### 단일 이미지 재조명

객체의 재료 매개변수를 복원하여 물리 기반 렌더링 기술로 재조명하는 대신, 우리는 목표 조명 조건을 조건으로 하여 재조명된 이미지 분포에서 직접 샘플링할 수 있는 확산 모델을 훈련한다. 이 확산 모델은 본질적으로 생성적 단일 이미지 재조명 모델이다. 초기 단일 이미지 재조명 기술은 최적화 기반 역 렌더링을 사용했으나, 이후의 방법들은 심층 합성 신경망을 훈련하여 이미지의 기하학, 재료 및 조명을 출력하거나, 직접 재조명된 이미지를 출력했다.

가장 관련이 있는 최근의 연구로는 LightIt, DiffusionLight, DiLightNet이 있다. LightIt은 ControlNet과 유사한 모델을 훈련하여 임의의 태양 위치 하에서 야외 이미지를 재조명한다. DiffusionLight는 장면 중간에 크롬 볼의 색상 픽셀을 인페인트하여 이미지의 조명을 추정한다. DiLightNet은 단일 이미지 재조명을 목표로 하며, 목표 환경 맵을 조건으로 단일 이미지 재조명 확산 모델을 조건화하기 위해 ControlNet 기반 접근법을 사용한다.

우리의 방법은 다수의 이미지가 제공되는 3D 재조명에 초점을 맞추며, 유사한 단일 이미지 재조명 확산 모델을 사용하지만, 입력 뷰에서 기하학을 추정하여 방사선 큐를 생성한다. 이는 차폐된 기하학에 의해 발생하는 반사와 같은 복잡한 빛 전달 효과를 더 잘 모델링할 수 있게 한다.

## 3 Method

### 3.1 문제 정의

주어진 객체 이미지와 해당 카메라 자세의 데이터셋 D={(Ii,πi)}i=1ND = \{(I_i, \pi_i)\}_{i=1}^ND={(Ii,πi)}i=1N에서, 재조명 가능한 3D 재구성의 목표는 새로운 조명 조건 LTL_TLT 하에서 데이터셋의 재조명된 버전을 생성할 수 있는 모델 θ\thetaθ를 추정하는 것이다. 이는 다음과 같이 표현될 수 있다:

![Untitled](IllumiNeRF%203D%20Relighting%20without%20Inverse%20Rendering%203e1baff54f8444f1bef19cc0ef8fd110/Untitled%201.png)

여기서 DTθD_T^\thetaDTθ는 목표 조명 LTL_TLT 하에서 모델 θ\thetaθ를 사용하여 재조명된 원본 데이터셋의 버전이다.

![개요: (a)에서 주어진 이미지 세트 I와 카메라 자세 π를 기반으로, NeRF를 실행하여 (b)와 같이 3D 기하학을 추출한다. 이 기하학과 (c)에 표시된 타겟 조명을 기반으로, 각 입력 뷰에 대해 (d)와 같이 방사선 큐를 생성한다. 다음으로, 재조명 확산 모델을 사용하여 각 입력 이미지를 독립적으로 재조명하고, 각 주어진 이미지에 대해 (f)에 표시된 S개의 가능한 솔루션을 샘플링한다. 마지막으로, 재조명된 이미지 세트를 잠재 NeRF 최적화를 통해 3D 표현으로 증류한다.](IllumiNeRF%203D%20Relighting%20without%20Inverse%20Rendering%203e1baff54f8444f1bef19cc0ef8fd110/Untitled%202.png)

개요: (a)에서 주어진 이미지 세트 I와 카메라 자세 π를 기반으로, NeRF를 실행하여 (b)와 같이 3D 기하학을 추출한다. 이 기하학과 (c)에 표시된 타겟 조명을 기반으로, 각 입력 뷰에 대해 (d)와 같이 방사선 큐를 생성한다. 다음으로, 재조명 확산 모델을 사용하여 각 입력 이미지를 독립적으로 재조명하고, 각 주어진 이미지에 대해 (f)에 표시된 S개의 가능한 솔루션을 샘플링한다. 마지막으로, 재조명된 이미지 세트를 잠재 NeRF 최적화를 통해 3D 표현으로 증류한다.

### 3.2 모델 개요

우리는 객체의 조명이나 재료에 대한 명시적 물리 기반 모델을 사용하지 않고, 재조명된 이미지의 확률을 최대화하는 접근법을 제안한다. 먼저 입력 이미지의 조명과 객체의 재료 및 기하학 매개변수를 암묵적으로 나타내는 잠재 변수 ZZZ를 도입한다. 재조명된 데이터의 가능성은 다음과 같이 표현할 수 있다:

![Untitled](IllumiNeRF%203D%20Relighting%20without%20Inverse%20Rendering%203e1baff54f8444f1bef19cc0ef8fd110/Untitled%203.png)

우리는 이를 잠재 NeRF 모델로 모델링하여 목표 조명 하에서 객체의 재조명된 뷰를 렌더링할 수 있게 한다.

![‘핫도그’ 장면의 한 뷰에 대한 방사선 큐 예시.](IllumiNeRF%203D%20Relighting%20without%20Inverse%20Rendering%203e1baff54f8444f1bef19cc0ef8fd110/Untitled%204.png)

‘핫도그’ 장면의 한 뷰에 대한 방사선 큐 예시.

### 3.3 잠재 NeRF 모델

잠재 코드 NeRF 3D 표현을 사용하여 객체의 재조명된 뷰에 해당하는 이미지를 렌더링한다. 이는 NeRF를 잠재 코드에 조건화하여 목표 조명 하에서 객체의 재조명된 뷰를 렌더링할 수 있게 한다. 이 접근법에서는 NeRF의 기하학이 잠재 코드에 의존하지 않으므로, 잠재 코드는 객체의 재료 특성만을 나타내는 것으로 해석될 수 있다.

![(a) 동일한 타겟 환경 맵에 대한 재조명 확산 모델 샘플과 (b) 고정된 잠재값에 대한 최적화된 잠재 NeRF 렌더링. 확산 샘플은 장면의 다양한 잠재 설명에 해당하며, 우리의 잠재 NeRF 최적화는 이러한 잠재 변수를 NeRF 모델의 파라미터와 함께 효과적으로 최적화하여 각 잠재 설명에 대해 일관된 렌더링을 생성할 수 있다.](IllumiNeRF%203D%20Relighting%20without%20Inverse%20Rendering%203e1baff54f8444f1bef19cc0ef8fd110/Untitled%205.png)

(a) 동일한 타겟 환경 맵에 대한 재조명 확산 모델 샘플과 (b) 고정된 잠재값에 대한 최적화된 잠재 NeRF 렌더링. 확산 샘플은 장면의 다양한 잠재 설명에 해당하며, 우리의 잠재 NeRF 최적화는 이러한 잠재 변수를 NeRF 모델의 파라미터와 함께 효과적으로 최적화하여 각 잠재 설명에 대해 일관된 렌더링을 생성할 수 있다.

### 3.4 재조명 확산 모델

잠재 NeRF 모델을 훈련하기 위해, 우리는 재조명 확산 모델(RDM)을 사용하여 각 뷰포인트에서 여러 샘플을 생성한다. RDM은 입력 이미지와 목표 조명으로 조건화된 이미지 디노이징 확산 모델로 구현된다. 목표 조명을 인코딩하기 위해 이미지 공간 방사선 큐를 사용하며, 이는 객체의 추정된 기하학을 단순 음영 모델로 렌더링하여 생성한다. 이 절차는 확산 네트워크가 이러한 효과를 처음부터 학습하지 않고도 반사, 그림자 및 전역 조명 효과에 대한 정보를 제공하도록 설계되었다.

RDM 아키텍처는 사전 훈련된 잠재 이미지 확산 모델로 구현되며, radiance cues를 조건으로 사용하기 위해 ControlNet 기반 접근 방식을 사용한다. RDM을 사용하여 생성된 다수의 샘플을 사용하여 잠재 NeRF 모델을 훈련한다.

## 4 Experiments

### 4.1 실험 설정

먼저, 재조명 데이터셋을 설정하기 위해 Objaverse에서 다양한 포즈와 조명 조건 하에서 객체를 렌더링한 데이터를 사용했다. 각 객체는 4개의 포즈로 무작위로 선택되었고, 각 포즈는 4개의 다른 조명 조건에서 렌더링되었다. 조명은 HDR 환경 맵으로 표현되며, Polyhaven의 509개 환경 맵 데이터셋에서 무작위로 선택되었다. 평가 데이터셋으로는 두 가지를 사용하였다. 첫 번째는 TensoIR로, 이는 4개의 합성 객체를 6개의 조명 조건 하에서 렌더링한 합성 벤치마크이다. 두 번째는 Stanford-ORB로, 이는 야생에서 캡처된 데이터로 이루어진 실제 벤치마크로, 14개의 객체가 3개의 다른 조명 설정에서 캡처되어 총 42개의 (객체, 조명) 쌍을 포함한다.

비교 기준으로는 여러 기존 역 렌더링 접근법을 사용하였다. TensoIR 벤치마크에서는 NeRFactor와 InvRender와 비교하였으며, 추가로 TensoIR 벤치마크에서 최상위 성능을 보이는 TensoIR 방법과 비교하였다. Stanford-ORB 벤치마크에서는 PhySG, NVDiffRec, NeRD, NVDiffRecMC, Neural-PBIR과 추가로 비교하였다. 모델 추론 시에는 실제 재조명된 이미지에 접근할 수 없기 때문에, 테스트 이미지를 렌더링할 때 모든 뷰의 임베딩 벡터를 Z = 0으로 설정하였다. 이는 테스트 셋 이미지와 일치하도록 임베딩 벡터를 최적화하는 기존의 잠재 NeRF 모델과 대조된다. 평가 지표로는 렌더링된 이미지의 품질을 평가하기 위해 PSNR, SSIM, LPIPS-VGG를 보고하였다. 또한 Stanford-ORB 벤치마크 프로토콜에 따라 HDR 이미지에 대해 PSNR-H를 추가로 보고하였다. 기존 작업과 동일하게, 채널별 RGB 스케일 팩터를 적용하여 RGB 채널을 일치시킨 후 메트릭을 계산하였다.

![TensoIR의 정성적 결과. 모든 접근법의 렌더링은 Eq. (4.1)에서 언급한 대로 실제 값에 맞추어 재조정되었다. TensoIR과 달리, 우리의 방법은 빨간색으로 표시된 대로 스펙큘러 하이라이트와 색상을 충실히 복원한다.](IllumiNeRF%203D%20Relighting%20without%20Inverse%20Rendering%203e1baff54f8444f1bef19cc0ef8fd110/Untitled%206.png)

TensoIR의 정성적 결과. 모든 접근법의 렌더링은 Eq. (4.1)에서 언급한 대로 실제 값에 맞추어 재조정되었다. TensoIR과 달리, 우리의 방법은 빨간색으로 표시된 대로 스펙큘러 하이라이트와 색상을 충실히 복원한다.

### 4.2 벤치마킹

TensoIR 벤치마크에서의 정량적 결과는 Table 1에, 정성적 예시는 Figure 5에 나타나 있다. 우리는 모든 메트릭에서 기존 방법을 크게 능가하였다. 시각적으로도, 우리의 방법은 기존 방법들이 모델링하는 데 어려움을 겪는 스펙큘러 하이라이트를 복원할 수 있었다. Stanford-ORB에서도 우리의 접근법은 대부분의 기준 방법보다 우수한 성능을 보였다. 그러나 Neural-PBIR이 더 나은 지표를 보여주었지만, 이 방법은 대부분 확산된 외형을 복원하며, 높은 광택 객체의 강한 스펙큘러 하이라이트를 놓치는 경향이 있었다. 이러한 경향은 이들의 모델이 더 나은 지표를 보여주는 이유일 수 있다. 이는 Stanford-ORB의 조명 맵이 객체 위치의 입사 조명과 일치하지 않기 때문이다.

![Stanford-ORB의 정성적 결과. 모든 접근법의 렌더링은 섹션 4.1에서 언급한 대로 실제 값에 맞추어 재조정되었다. 우리의 접근법이 잘 작동하는 영역이 강조되어 있다. 우리의 접근법은 그럴듯한 스펙큘러 반사를 포함한 고품질의 렌더링을 생성한다.](IllumiNeRF%203D%20Relighting%20without%20Inverse%20Rendering%203e1baff54f8444f1bef19cc0ef8fd110/Untitled%207.png)

Stanford-ORB의 정성적 결과. 모든 접근법의 렌더링은 섹션 4.1에서 언급한 대로 실제 값에 맞추어 재조정되었다. 우리의 접근법이 잘 작동하는 영역이 강조되어 있다. 우리의 접근법은 그럴듯한 스펙큘러 반사를 포함한 고품질의 렌더링을 생성한다.

![표준 NeRF를 잠재 NeRF 모델 대신 사용하는 경우, 서로 다른 기저 잠재 설명이 있는 훈련 샘플을 조정할 수 없다. 잠재 NeRF 모델을 사용하면 렌더링된 스펙큘러 외형의 정확도가 크게 향상되며, 잠재 NeRF 모델을 훈련하는 데 사용되는 RDM 샘플의 수 S를 늘리면 출력 렌더링의 품질이 더욱 향상된다.](IllumiNeRF%203D%20Relighting%20without%20Inverse%20Rendering%203e1baff54f8444f1bef19cc0ef8fd110/Untitled%208.png)

표준 NeRF를 잠재 NeRF 모델 대신 사용하는 경우, 서로 다른 기저 잠재 설명이 있는 훈련 샘플을 조정할 수 없다. 잠재 NeRF 모델을 사용하면 렌더링된 스펙큘러 외형의 정확도가 크게 향상되며, 잠재 NeRF 모델을 훈련하는 데 사용되는 RDM 샘플의 수 S를 늘리면 출력 렌더링의 품질이 더욱 향상된다.

### 4.3 어블레이션

TensoIR의 hotdog 장면에서 모델의 구성 요소별 성능을 평가한 결과는 Table 3에, 시각화는 Figure 7에 나타나 있다. 주요 결론은 두 가지이다. 첫째, 잠재 NeRF 모델이 중요하다. 표준 NeRF를 최적화하는 것만으로는 뷰 간의 변화를 조정할 수 없다. 둘째, 더 많은 확산 샘플이 도움이 된다. RDM의 샘플 수를 증가시킴으로써 대부분의 메트릭에서 일관된 향상을 관찰할 수 있었다. 이는 증가된 샘플 수가 잠재 NeRF가 목표 분포를 효과적으로 맞추는 데 도움이 된다는 우리의 직관을 확인해준다.

### 4.4 제한 사항

우리 모델은 UniSDF로 추정된 고품질 기하학에 의존하므로, 구조가 누락되면 반사 효과를 놓칠 수 있다. 예를 들어, 특정 객체의 경우, 기하학적 오류로 인해 재조명 결과의 일부 세부사항이 누락될 수 있다. 또한, 우리의 접근법은 실시간 재조명에는 적합하지 않다. 이는 새로운 목표 조명 조건에 대해 새로운 샘플을 생성하고 NeRF를 최적화해야 하기 때문이다. 따라서, 실시간 애플리케이션에는 부적합할 수 있다.

## 5 Conclusion

우리는 역 렌더링을 대체하는 재조명 가능한 3D 재구성의 새로운 패러다임을 제안하였다. 우리의 방법은 다양한 재조명된 이미지 샘플을 생성하고 이를 단일 3D 잠재 NeRF 표현으로 증류하여 목표 조명 하에서 새로운 시점의 객체를 렌더링할 수 있다. 이 새로운 패러다임은 데이터 캡처, 재료 및 조명 추정의 향상을 기대할 수 있다.

## 부록

### A.1 잠재 NeRF 모델과 기하학 추정기

우리는 JAX [9]를 사용하여 기하학 추정기와 잠재 NeRF 모델을 UniSDF [52]로 구현합니다. UniSDF는 서명 거리 함수(SDF)를 기반으로 한 최첨단 볼륨 렌더링 접근법으로, SDF에서 메쉬를 쉽게 추출할 수 있어 표준 렌더링 엔진(예: Blender [46])에 가져와 방사선 큐를 계산할 수 있습니다. 또한 UniSDF는 기하학과 외형을 분리하여, 기하학 관련 가중치는 고정하고 외형을 모델링하는 가중치만 최적화할 수 있습니다.

우리의 UniSDF 모델 파라미터화는 원본 논문의 DTU 데이터셋 [1]에 사용된 것과 유사하지만, 네 가지 주요 변경 사항이 있습니다. 첫째, 제안 샘플링 라운드 수를 mip-NeRF 360 [5]에서 도입한 두 개에서 하나로 줄이고, 64개의 제안 샘플을 사용합니다. 둘째, NeRF-Casting [49]의 비대칭 예측 노멀 손실을 사용합니다. 셋째, NeRF-Casting [49]과 마찬가지로, 32에서 4096 해상도 사이에 15개의 스케일을 갖는 추가 해시 그리드 인코딩을 사용합니다. 넷째, 예측 노멀의 국부적 평활성을 장려하기 위해 유사한 평활성 손실을 사용합니다. 이러한 수정은 우리의 모델이 스펙큘러 하이라이트로 객체를 재조명할 수 있는 더 나은 기하학을 생성합니다.

GLO 임베딩을 통합하기 위해, 우리는 UniSDF의 '병목' 특징에 적용될 요소별 스케일 및 이동 값을 예측하기 위해 MLP를 사용합니다. 기하학 추정 및 잠재 NeRF 최적화를 위해, Adam [21] 최적화 도구를 사용하며, 학습률은 25,000번의 학습 반복 동안 5 × 10^-3에서 5 × 10^-4로 로그 스케줄로 감쇠됩니다. 최적화는 16개의 A100 40GB GPU로 약 45분이 소요됩니다.

### A.2 방사선 큐

기하학: 방사선 큐를 추출하기 위해 먼저 입력 이미지를 최적화하여 UniSDF [52]를 사용합니다. 최적화 후, 마칭 큐브 알고리즘을 사용하여 SDF 표현을 메쉬로 변환합니다.

렌더링: 방사선 큐를 렌더링하기 위해 물리 기반 경로 추적기인 Blender Cycles [46]를 사용합니다. 우리는 Kubric 파이썬 래퍼를 통해 Blender를 실행하며, 사전 정의된 재료(GGX 재료 모델 [51])를 사용하여 추정된 기하학을 사용합니다.

음영 노멀: 매끄러운 스펙큘러 하이라이트를 생성하기 위해, 음영에 사용되는 노멀을 매끄럽게 해야 합니다. 기본적으로 Blender는 입력 기하학을 기반으로 음영 노멀을 계산하는데, 이는 노이즈가 많을 수 있습니다. 이를 완화하기 위해 예측 노멀을 Blender에 제공하고 음영 노멀 평활화 기능을 활성화합니다. 최종 방사선 큐는 매끄럽지 않은 확산 재료와 매끄러운 스펙큘러 재료로 구성됩니다.

### A.3 재조명 확산 모델

우리는 JAX [9]에서 재조명 확산 모델을 구현합니다. 이 모델은 Rombach et al. [41]의 모델과 유사한 텍스트-이미지 잠재 확산 모델을 기반으로 하며, 64 × 64 × 8 크기의 가우시안 노이즈를 디노이징하여 512 × 512 × 3 크기의 재조명된 이미지를 생성합니다. 훈련 중 기본 모델은 고정됩니다. ControlNet [59]을 따르며, 우리는 기본 확산 모델의 UNet 인코더와 중간 블록을 복사하여 추가 블록과 결합합니다. 모델은 빈 문자열을 텍스트 입력으로 받아서, 텍스트 입력 대신 이미지를 기반으로 작동하게 됩니다.

훈련은 다양한 조명 조건 하에서 렌더링된 합성 객체로 구성된 큰 데이터셋을 사용합니다. 각 훈련 예시는 동일한 객체를 다른 조명 조건에서 본 두 이미지의 쌍으로 구성됩니다. 기본 모델을 150,000번의 스텝 동안 미세 조정하며, 학습률은 처음 1,000 스텝 동안 0에서 선형으로 증가합니다. 미세 조정은 32 TPUv5 칩에서 약 2일이 소요됩니다.

### A.4 훈련 데이터 처리

우리는 Objaverse [11]를 합성 데이터셋으로 사용합니다. 저품질 객체를 걸러내기 위해, 초기 156,330개의 객체에서 반투명한 객체를 제거하여 최종적으로 152,649개의 객체를 사용합니다. 객체에 재료 정보가 없는 경우, 균일한 텍스처와 색상을 수동으로 지정합니다. 확산 모델 훈련을 위해, 우리는 다양한 조명 조건에서의 이미지 쌍이 필요하며, 이를 위해 509개의 구형 환경 맵을 선택하고, 각 객체에 대해 네 개의 카메라 자세를 샘플링합니다. 각 카메라는 두 개의 환경 맵을 무작위로 샘플링하여 Blender의 경로 추적기를 사용하여 이미지를 렌더링합니다.