# OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on

[https://arxiv.org/abs/2403.01779](https://arxiv.org/abs/2403.01779)

- Mar 2024

### **1 Introduction**

![다양한 입력 인체 및 의상 이미지와 함께 VITON-HD [6](1열, 상체 의상 지원) 및 Dress Code [33](2열, 상체 의상, 하체 의상 및 드레스 지원) 데이터 세트에서 훈련된 OOTDiffusion으로 생성된 의상 이미지(1024 × 768).](OOTDiffusion%20Outfitting%20Fusion%20based%20Latent%20Diffus%20be848675f0b442c58fd0b86175d19705/Untitled.png)

다양한 입력 인체 및 의상 이미지와 함께 VITON-HD [6](1열, 상체 의상 지원) 및 Dress Code [33](2열, 상체 의상, 하체 의상 및 드레스 지원) 데이터 세트에서 훈련된 OOTDiffusion으로 생성된 의상 이미지(1024 × 768).

이미지 기반 가상 시착(VTON) 기술은 전자 상거래 산업에서 소비자의 쇼핑 경험을 개선하고 의류 상인의 광고 비용을 줄이는데 크게 기여하고 있습니다. 이 기술은 특정 의류를 착용한 사람의 이미지를 생성하는 것을 목표로 하며, 이를 위해 많은 연구자들이 자연스러운 가상 착용 효과를 달성하기 위해 노력해 왔습니다. 그러나 현재 VTON 기술은 생성된 이미지가 현실적이고 자연스러워야 하며, 의류의 세부 사항을 최대한 보존해야 한다는 두 가지 주요 도전에 직면해 있습니다. 대부분의 연구는 이미지 생성을 위해 생성적 적대 신경망(GANs)이나 잠재 확산 모델(LDMs)을 활용하지만, 올바른 의류 접힘, 자연스러운 빛과 그림자, 현실적인 인간 몸체를 생성하는 데 어려움을 겪습니다. 최근 연구는 LDM 기반 방법을 선호하여 이미지의 현실감을 개선하고 있지만, 의류의 복잡한 텍스트, 질감, 색상, 패턴, 선 등을 보존하는 데는 여전히 한계가 있습니다. 이러한 배경을 바탕으로, 저자들은 Outfitting over Try-on Diffusion(OOTDiffusion)이라는 새로운 LDM 기반 가상 시착 방법을 제안하여 이러한 도전을 해결하고자 합니다. OOTDiffusion은 사전 훈련된 LDMs의 이점을 활용하고, 의류의 세부 특성을 더 잘 학습하고 정확하게 정렬할 수 있는 착용 융합 과정을 도입함으로써, 다양한 목표 인간 몸체 유형과 자세에 맞게 의류 특성을 적응시키는 데 초점을 맞춥니다.

### **2 Related Work**

이미지 기반 가상 시착(VTON)은 다년간 많은 연구가 진행된 유망하고 도전적인 분야로, 자연스러우면서도 정확한 결과물을 목표로 합니다. 최근 연구들은 주로 생성적 적대 신경망(GANs)과 잠재 확산 모델(LDMs)을 활용한 이미지 생성 방법에 초점을 맞추고 있습니다. GAN 기반의 접근 방식은 고해상도 데이터셋 수집과 함께 와핑(warping)과 세그멘테이션(segmentation)을 동시에 수행하거나, 와핑 모듈을 개선하여 의류의 변형을 더 잘 처리하는 등 여러 전략을 제시합니다. 그러나 이러한 GAN 기반 방법들은 의류의 현실적인 접힘과 자연스러운 빛 및 그림자를 무시하고 훈련 데이터에 과적합될 위험이 있어, 와핑 과정에 의존하며 결과물의 충실도와 현실감을 저하시킵니다.

반면, LDM 기반 방법은 CLIP 텍스트 인베전을 수행하거나 옷과 사람 이미지를 결합해 초기 결과를 생성한 뒤, 이를 확산 모델로 세밀하게 다듬는 등의 과정을 포함합니다. 이 접근법은 의류 세부 사항의 전체적인 보존에는 더 나은 성능을 보이지만, CLIP 인코더로 인한 정보 손실로 인해 복잡한 패턴과 텍스트를 완전히 보존하는 데는 여전히 한계가 있습니다. 최근에는 독립적인 와핑 과정 없이 의류와 인간 몸체 간의 의미론적 상관 관계를 학습하는 새로운 방법들이 제안되었으나, 추가적인 블록이 훈련 및 추론 비용을 증가시키고 정보 손실 문제를 해결하지 못했습니다. 이러한 배경 속에서, OOTDiffusion은 이전 방법들의 한계를 극복하고자 사전 훈련된 LDM을 활용하여 의류 세부 사항을 한 단계에서 효율적으로 학습하고, 와핑 과정 없이도 의류 특성을 자연스럽게 목표 인간 몸체에 맞게 적응시키는 새로운 방식을 도입하였습니다.

### **3. Method**

OOTDiffusion 방법론은 기존의 잠재 확산 모델(LDM)을 활용하고 이를 가상 시착(VTON) 문제에 맞게 확장하여, 사람 이미지에 특정 의류를 착용시킨 현실적인 이미지를 생성하는 새로운 접근법을 제안합니다. 이 방법은 크게 세 가지 주요 구성 요소로 이루어져 있습니다: 사전 훈련된 잠재 확산 모델의 활용, 착용 유니텟(outfitting UNet)의 도입, 그리고 착용 융합(outfitting fusion) 과정의 설계입니다.

1. **사전 훈련된 잠재 확산 모델의 활용**: OOTDiffusion은 이미지를 잠재 공간에서 표현할 수 있는 변이형 오토인코더(VAE)를 포함하는 Stable Diffusion 모델을 기반으로 합니다. 이를 통해, 주어진 이미지와 텍스트 프롬프트를 사용하여 노이즈가 추가된 이미지를 점진적으로 정제하는 과정을 학습합니다.
    
    ![우리가 제안한 OOTDiffusion 모델의 개요. 왼쪽에서는 의복 이미지가 잠재 공간으로 인코딩되고 단일 단계 프로세스를 위해 의상 UNet에 공급됩니다. CLIP 인코더에서 생성된 보조 컨디셔닝 입력과 함께 의상의 특징은 아웃피팅 퓨전을 통해 노이즈 제거 UNet에 통합됩니다. 특히 훈련에서 의복 잠복에 대해 아웃피팅 드롭아웃을 수행하여 분류기 없이 안내할 수 있도록 합니다. 오른쪽에서는 입력된 사람 이미지가 목표 영역에 대해 마스킹되고 여러 샘플링 단계를 위해 노이즈 제거 UNet의 입력으로 가우시안 노이즈와 연결됩니다. 노이즈 제거 후, 특징 맵은 다시 이미지 공간으로 디코딩되어 트라이온 결과로 나타납니다.](OOTDiffusion%20Outfitting%20Fusion%20based%20Latent%20Diffus%20be848675f0b442c58fd0b86175d19705/Untitled%201.png)
    
    우리가 제안한 OOTDiffusion 모델의 개요. 왼쪽에서는 의복 이미지가 잠재 공간으로 인코딩되고 단일 단계 프로세스를 위해 의상 UNet에 공급됩니다. CLIP 인코더에서 생성된 보조 컨디셔닝 입력과 함께 의상의 특징은 아웃피팅 퓨전을 통해 노이즈 제거 UNet에 통합됩니다. 특히 훈련에서 의복 잠복에 대해 아웃피팅 드롭아웃을 수행하여 분류기 없이 안내할 수 있도록 합니다. 오른쪽에서는 입력된 사람 이미지가 목표 영역에 대해 마스킹되고 여러 샘플링 단계를 위해 노이즈 제거 UNet의 입력으로 가우시안 노이즈와 연결됩니다. 노이즈 제거 후, 특징 맵은 다시 이미지 공간으로 디코딩되어 트라이온 결과로 나타납니다.
    
2. **착용 유니텟의 도입**: 의류 이미지의 세부 특성을 한 단계에서 효율적으로 학습하기 위해, 착용 유니텟이라 불리는 특별한 구조의 네트워크가 설계되었습니다. 이는 의류의 잠재적 특성을 학습하고 이를 최종 이미지 생성 과정에 통합하기 위해 사용됩니다. 착용 유니텟은 Stable Diffusion의 덴노이징(denoising) UNet 구조와 유사하지만, 의류의 잠재 특성을 효과적으로 학습하고 이를 활용하여 더 자연스러운 착용 효과를 달성하는 것이 목적입니다.
3. **착용 융합 과정의 설계**: 의류 특성을 목표 인간 이미지와 정확하게 정렬하기 위한 착용 융합 과정이 제안되었습니다. 이 과정은 착용 유니텟에서 학습된 의류 특성을 덴노이징 UNet의 자기 주의(self-attention) 층에 통합하여, 다양한 인간 몸체 유형과 포즈에 맞게 의류 특성을 부드럽게 적응시키는 메커니즘을 제공합니다. 이를 통해 독립적인 와핑 과정 없이도 정보 손실이나 특성 왜곡 없이 의류의 세부 사항을 보존할 수 있습니다.

또한, 착용 드롭아웃(outfitting dropout)이라는 기법이 도입되어, 훈련 과정에서 의류 특성의 일부를 임의로 생략함으로써 최종 모델이 의류 특성에 대한 조절 가능성을 더욱 향상시킬 수 있도록 합니다. 이를 통해 생성된 이미지의 품질과 다양성 사이의 균형을 조정할 수 있는 분류기 없는 가이드라인(classifier-free guidance)을 제공합니다. OOTDiffusion 방법은 이러한 구성 요소들을 통합하여, 고해상도 가상 시착 데이터셋에서 현실적이고 제어 가능한 가상 시착 결과를 달성함으로써 기존 VTON 방법들을 뛰어넘는 성능을 보여줍니다.

![의상 융합을 통해 정렬된 인체(첫 번째 줄)와 의복 특징(두 번째 줄)에 대한 관심도 맵의 시각화.](OOTDiffusion%20Outfitting%20Fusion%20based%20Latent%20Diffus%20be848675f0b442c58fd0b86175d19705/Untitled%202.png)

의상 융합을 통해 정렬된 인체(첫 번째 줄)와 의복 특징(두 번째 줄)에 대한 관심도 맵의 시각화.

### **4. Experiments**

OOTDiffusion 방법론의 실험적 검증은 두 가지 고해상도 가상 시착 데이터셋, 즉 VITON-HD와 Dress Code를 활용하여 수행되었습니다. 이 데이터셋들은 다양한 전신 모델과 해당 의류 이미지를 포함하고 있으며, VITON-HD 데이터셋은 주로 상반신 의류에 초점을 맞춘 반면, Dress Code 데이터셋은 상반신 의류, 하반신 의류, 드레스를 포함한 다양한 의류 카테고리를 포함합니다. 실험은 OOTDiffusion 방법과 여러 최신의 상태 기술(VTON) 방법들과의 비교를 통해 진행되었습니다.

1. **데이터셋**: VITON-HD는 13,679개의 이미지 쌍으로 구성되며, 그 중 2032쌍이 테스트 세트로 사용되었습니다. Dress Code 데이터셋은 15,363개의 이미지 쌍으로 구성되며, 각 의류 카테고리별로 1,800쌍이 테스트 세트로 사용되었습니다. 두 데이터셋 모두 고해상도(1024x768) 이미지를 제공합니다.
2. **비교 방법론**: VITON-HD 데이터셋에서는 GAN 기반의 VITON-HD, HR-VITON, GP-VTON 및 LDM 기반의 LaDI-VTON, StableVITON과 같은 다양한 VTON 방법들과 OOTDiffusion을 비교하였습니다. Dress Code 데이터셋에서는 GP-VTON, LaDI-VTON 및 LDM 기반의 Paint-by-Example과 같은 방법들과 비교하였습니다.
3. **평가 지표**: 실험에서는 LPIPS와 SSIM을 사용하여 생성된 이미지의 품질을 원본 이미지와 비교하였고, FID와 KID 지표를 사용하여 실제성과 충실도를 평가하였습니다. 이러한 평가는 각각 쌍을 이룬 설정(원본 인간 및 해당 의류 이미지로 재구성)과 쌍을 이루지 않은 설정(다른 의류 이미지로 가상 시착)에서 수행되었습니다.
4. **실험 결과**: OOTDiffusion은 다양한 의류 이미지와 목표 인간 이미지에 대해 최상의 시착 효과를 달성함으로써, 현실성과 제어 가능성 모두에서 다른 VTON 방법들을 상당히 능가했습니다. 특히, GAN 기반 방법들이 인간 몸체 생성이나 의류 접힘을 현실적으로 재현하는 데 어려움을 겪는 반면, OOTDiffusion은 더 정확한 의류 세부 사항과 함께 더 자연스러운 결과를 제공했습니다. Dress Code 데이터셋에서도 OOTDiffusion은 다른 VTON 방법들에 비해 안정적인 성능을 보였으며, 다양한 의류 카테고리에 걸쳐 우수한 성능을 나타냈습니다.
    
    ![아웃핏 드롭아웃을 사용하지 않고/사용하여 훈련된 OOTDiffusion 모델과 다양한 가이드 스케일 sg 값을 사용하여 생성된 아웃핏 이미지의 정성적 비교](OOTDiffusion%20Outfitting%20Fusion%20based%20Latent%20Diffus%20be848675f0b442c58fd0b86175d19705/Untitled%203.png)
    
    아웃핏 드롭아웃을 사용하지 않고/사용하여 훈련된 OOTDiffusion 모델과 다양한 가이드 스케일 sg 값을 사용하여 생성된 아웃핏 이미지의 정성적 비교
    
    ![VITON-HD 데이터 세트 [6]에 대한 정성적 비교(상반신 의상을 착용한 반신 모델).](OOTDiffusion%20Outfitting%20Fusion%20based%20Latent%20Diffus%20be848675f0b442c58fd0b86175d19705/Untitled%204.png)
    
    VITON-HD 데이터 세트 [6]에 대한 정성적 비교(상반신 의상을 착용한 반신 모델).
    
    ![드레스 코드 데이터 세트 [33]에 대한 정성적 비교(상체 의상을 입은 전신 모델/하체 의상을 입은 하체 모델/드레스).](OOTDiffusion%20Outfitting%20Fusion%20based%20Latent%20Diffus%20be848675f0b442c58fd0b86175d19705/Untitled%205.png)
    
    드레스 코드 데이터 세트 [33]에 대한 정성적 비교(상체 의상을 입은 전신 모델/하체 의상을 입은 하체 모델/드레스).
    
    ![ 교차 데이터 세트 평가의 정성적 결과. 모델은 VITON-HD 데이터 세트 [6]로 학습하고 드레스 코드 데이터 세트 [33]로 테스트했습니다. ](OOTDiffusion%20Outfitting%20Fusion%20based%20Latent%20Diffus%20be848675f0b442c58fd0b86175d19705/Untitled%206.png)
    
     교차 데이터 세트 평가의 정성적 결과. 모델은 VITON-HD 데이터 세트 [6]로 학습하고 드레스 코드 데이터 세트 [33]로 테스트했습니다. 
    
5. **한계점**: OOTDiffusion은 이미지 기반 가상 시착 분야에서 뛰어난 성능을 보였지만, 몇 가지 한계도 있습니다. 예를 들어, 훈련 데이터에 없는 의류 카테고리로의 가상 시착이나, 원본 인간 이미지의 일부 세부 사항(근육, 시계, 문신 등)이 가상 시착 후 변경될 수 있다는 점입니다. 이러한 한계는 미래 연구에서 더 다양한 데이터셋 수집이나 효과적인 전처리 및 후처리 방법을 통해 해결될 수 있습니다.

### **5. Conclusion**

OOTDiffusion은 이미지 기반 가상 시착(VTON) 작업을 위한 새로운 잠재 확산 모델(LDM) 기반 네트워크 아키텍처를 소개합니다. 이 연구는 특히 의류의 세부 특성을 효과적으로 학습하고 이를 목표 인간 이미지에 자연스럽게 통합하는 데 중점을 둡니다. 제안된 방법은 사전 훈련된 잠재 확산 모델을 활용하여 생성된 이미지의 현실감을 높이고, 착용 유니텟을 통해 의류의 세부 특성을 한 단계에서 학습하여, 착용 융합 과정을 통해 목표 인간 몸체와의 정밀한 정렬을 달성합니다. 이 과정은 독립적인 와핑 과정 없이도 정보 손실이나 특성 왜곡 없이 의류 특성을 부드럽게 적응시키는 데 성공합니다.

OOTDiffusion의 실험 결과는 고해상도 가상 시착 데이터셋에서 진행된 광범위한 정성적 및 정량적 평가를 통해 입증되었습니다. 이 방법은 다양한 목표 인간 및 의류 이미지에 대해 현실성과 제어 가능성 모두에서 기존의 상태 기술 대비 우수한 성능을 보여주었습니다. 특히, GAN 기반 방법들이 겪는 인간 몸체 생성의 어려움이나 의류 접힘의 부자연스러움과 같은 문제를 효과적으로 극복하며, 복잡한 의류 텍스처와 패턴을 세밀하게 보존할 수 있음을 보여줍니다.

그러나 이 연구는 몇 가지 한계를 가지고 있으며, 예를 들어 훈련 데이터에 존재하지 않는 의류 카테고리에 대한 가상 시착의 어려움이나, 가상 시착 과정에서 원본 인간 이미지의 일부 세부 사항이 변경될 가능성이 있습니다. 이러한 문제는 향후 연구에서 더 다양한 데이터 수집 및 개선된 처리 방법을 통해 해결될 수 있습니다.

결론적으로, OOTDiffusion은 이미지 기반 가상 시착 분야에서 중요한 발전을 이루었으며, 이 분야의 연구 및 상용 애플리케이션에 있어서 새로운 가능성을 제시합니다. 이 방법은 더 현실적이고 다양한 가상 시착 경험을 제공하여, 소비자의 쇼핑 경험 개선과 의류 산업의 광고 비용 절감에 기여할 잠재력을 가지고 있습니다.