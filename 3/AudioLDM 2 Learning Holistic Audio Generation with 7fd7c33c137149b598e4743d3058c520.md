# AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining

[https://audioldm.github.io/audioldm2/](https://audioldm.github.io/audioldm2/)

[https://arxiv.org/abs/2308.05734](https://arxiv.org/abs/2308.05734)

- Aug 2023

### 1 INTRODUCTION

오디오 생성은 텍스트, 음소 또는 이미지와 같은 특정 조건을 기반으로 사운드를 생성하는 프로세스입니다. 음성, 음악, 음향 효과 등 특정 유형의 오디오에 특화된 모델을 만드는 데는 상당한 진전이 있었지만, 다양한 유형의 사운드를 생성하기 위한 보편적인 접근 방식을 만드는 데는 여전히 격차가 존재합니다.

![AudioLDM 2 아키텍처의 개요. AudioMAE 특징은 음성 의미 언어 모델 단계 (GPT-2)와 의미 복원 단계 (잠재 확산 모델) 사이의 다리 역할을 하는 프록시입니다. 확률 스위처는 기존의 AudioMAE (Pgt)와 GPT-2에서 생성된 AudioMAE 특징 (Ppred)을 조건으로 사용하는 잠재 확산 모델의 확률을 제어합니다. AudioMAE와 잠재 확산 모델은 모두 오디오 데이터로 자기 지도 학습으로 사전 훈련됩니다.](AudioLDM%202%20Learning%20Holistic%20Audio%20Generation%20with%207fd7c33c137149b598e4743d3058c520/Untitled.png)

AudioLDM 2 아키텍처의 개요. AudioMAE 특징은 음성 의미 언어 모델 단계 (GPT-2)와 의미 복원 단계 (잠재 확산 모델) 사이의 다리 역할을 하는 프록시입니다. 확률 스위처는 기존의 AudioMAE (Pgt)와 GPT-2에서 생성된 AudioMAE 특징 (Ppred)을 조건으로 사용하는 잠재 확산 모델의 확률을 제어합니다. AudioMAE와 잠재 확산 모델은 모두 오디오 데이터로 자기 지도 학습으로 사전 훈련됩니다.

특수 모델은 특히 영화와 같이 다양한 사운드가 공존하는 시나리오에서는 적용에 한계가 있습니다. 또한 한 도메인에 특화된 설계가 다른 도메인에서는 효과적으로 작동하지 않을 수도 있습니다.

이 논문에서는 다목적 오디오 생성 프레임워크인 AudioLDM 2를 소개합니다. 이 프레임워크의 주요 개념은 오디오 클립의 본질을 포착하는 일련의 벡터인 '오디오 언어'(LOA)입니다. 이를 통해 모델은 텍스트나 이미지와 같이 이해할 수 있는 정보를 이 LOA로 변환한 다음 이를 기반으로 사운드를 생성할 수 있습니다.

이 프레임워크는 오디오 마스크 자동 인코더(AudioMAE)에서 추출한 피처와 GPT 기반 언어 모델을 사용하여 컨디셔닝 데이터를 오디오MAE 피처로 변환합니다. 그런 다음 잠재 확산 모델을 사용하여 최종 오디오를 생성합니다. 이 접근 방식은 최신 언어 모델의 강점을 활용하면서 일부 문제점을 피할 수 있습니다.

테스트 결과에 따르면 AudioLDM 2는 텍스트를 오디오와 음악으로 변환하는 데 탁월하며, 텍스트를 음성으로 변환하는 데도 뛰어난 성능을 발휘하는 것으로 나타났습니다. 또한 이미지를 오디오 생성의 기초로 사용할 수 있는 고유한 기능도 제공합니다. 또한 AudioLDM 2는 품질과 다용도성 측면에서 이전 버전보다 개선되었습니다.

### 2 AUDIOLDM 2

오디오 생성 프로세스에는 종종 한 가지 형태의 데이터(컨디셔닝 정보)를 오디오 신호로 변환하는 작업이 포함됩니다. 그러나 이 둘 사이에는 큰 차이가 있어 직접 생성하기가 어렵습니다.

이 간극을 메우기 위해 오디오 언어(LOA)를 중개 기능으로 사용할 것을 제안합니다. 이 아이디어는 오디오를 이 LOA 형식으로 인코딩한 다음 다른 시스템을 사용하여 LOA에서 오디오로 다시 디코딩하는 것입니다. 이 프로세스는 직접 생성하는 것보다 더 강력하고 효율적일 것으로 예상됩니다.

오디오 마스크 자동 인코더(AudioMAE)라는 자체 감독 모델이 도입되었습니다. 이 모델은 의도적으로 마스킹 처리된 멜 스펙트로그램 패치를 재구성하여 오디오의 표현을 학습합니다. 이 모델의 설계는 시각 작업에 널리 사용되는 아키텍처인 비전 트랜스포머 모델(ViT)의 영향을 받았습니다. AudioMAE는 다양한 오디오 유형에 걸쳐 일반화할 수 있다는 점과 차별적 접근 방식이 아닌 재구성적 접근 방식이라는 점 등 여러 가지 장점을 제공합니다.

저자는 LOA를 다시 오디오로 변환하는 프로세스에 잠재 확산 모델(LDM)을 사용합니다. 이 모델은 가변 자동 인코더(VAE) 기반의 압축 잠재 공간 내에서 작동하므로 계산 비용이 절감됩니다.

![TSNE를 기반으로 한 잠재 공간의 시각화와 ESC50 (Piczak, 2015) 데이터셋의 무작위로 선택된 10개의 클래스. 그림의 각 산점도는 오디오 클립을 나타냅니다. AudioMAE 특징 공간은 비슷한 오디오를 함께 그룹화하는 경향이 있어, VAE 특징보다 더 의미론적 구조를 나타냅니다.](AudioLDM%202%20Learning%20Holistic%20Audio%20Generation%20with%207fd7c33c137149b598e4743d3058c520/Untitled%201.png)

TSNE를 기반으로 한 잠재 공간의 시각화와 ESC50 (Piczak, 2015) 데이터셋의 무작위로 선택된 10개의 클래스. 그림의 각 산점도는 오디오 클립을 나타냅니다. AudioMAE 특징 공간은 비슷한 오디오를 함께 그룹화하는 경향이 있어, VAE 특징보다 더 의미론적 구조를 나타냅니다.

AudioMAE와 VAE는 기능 및 결과에서 차이가 있습니다. AudioMAE는 오디오의 의미론적 측면에 더 중점을 두는 반면, VAE는 음향적 측면에 중점을 둡니다. 저자는 두 가지 모두의 필요성을 주장합니다: AudioMAE는 의미론적 뉘앙스를 포착하는 좋은 중간 표현을 제공하는 반면, VAE는 재구성에 적합한 보다 간결한 표현을 제공합니다.

아키텍처 세부 사항: 이 책은 멜 스펙트로그램 계산, T-UNet(특정 신경망 설계)의 아키텍처 등을 논의하면서 이러한 모델의 기술적 세부 사항에 대해 자세히 설명합니다. 또한 저자들은 이러한 모델을 효과적으로 훈련하는 데 사용되는 특정 손실과 목표에 대해서도 고려합니다.

핵심 사항

함수 M 설계: 다음을 생성하는 새로운 모델이 도입되었습니다. Yˆ 조건이 주어지면 C. 이 모델은 다음과 같이 표시됩니다. Mθ:C→Yˆ, 와 θ는 훈련 가능한 파라미터를 나타냅니다.

GPT-2를 사용한 언어 모델링 채택: 이 설계는 다음 세대의 Y의 생성을 언어 모델링 작업으로 처리합니다. GPT-2 모델은 AudioMAE 인코더의 출력에서 시간 및 주파수 순서를 모두 보존할 수 있는 기능으로 인해 주요 아키텍처로 사용됩니다.

훈련 접근 방식: GPT-2 모델은 교사 강제 방법을 사용하여 미세 조정되며, 다음 조건에 따라 시퀀스의 가능성을 최대화하도록 최적화됩니다. 이는 평균 제곱 오차 손실을 사용하여 생성된 출력을 실제 출력과 비교하여 수행됩니다. 이 모델은 AudioMAE 특징 공간을 이산화하지 않고 훈련됩니다.

유연한 컨디셔닝: 모델은 오디오 표현, 텍스트 임베딩, 음소 임베딩, 시각적 단서 등 다양한 데이터 표현을 나타낼 수 있습니다. "전문가 혼합" 접근 방식은 다음을 계산하는 데 사용됩니다. C를 계산하기 위해 여러 인코더가 특징 추출기 역할을 하는 "전문가 혼합" 접근 방식이 사용됩니다.

특징 추출 모듈:

- CLAP: 오디오-텍스트 삽입 공간을 학습합니다. 텍스트 캡션을 사용할 수 없는 시나리오에서는 오디오 인코더를 사용합니다.
- FLAN-T5: T5 모델의 개선된 버전입니다. 텍스트에서 시간 데이터를 캡처하는 데 있어 CLAP의 단점을 보완하는 데 사용됩니다.
- 음소 인코더: Espeak 음소 인코더를 사용하여 텍스트 입력을 음소로 변환합니다. 음소는 텍스트 음성 변환 연구에서 필수적인 요소입니다.
- 이미지 바인드: 이미지, 텍스트, 비디오, 오디오 등과 같은 여러 모달리티를 단일 임베딩 공간에 정렬합니다. 이 프레임워크는 여러 모달을 유연하게 사용할 수 있게 해줍니다. 예를 들어, 훈련 중에는 이미지를 조건으로 사용하고 추론 중에는 오디오를 임베딩할 수 있습니다.

사전 학습된 모델: 음소 인코더를 제외한 모든 특징 추출 모델은 사전 학습되며 실험 중에 가중치가 고정됩니다.

공동 미세 조정: 텍스트 음성 변환 작업을 제외하고는 GPT 및 잠재 확산 모델과의 공동 미세 조정이 수행됩니다. 이 결합된 접근 방식은 모델의 전반적인 성능을 크게 향상시키는 것으로 밝혀졌습니다.

### 3 EXPERIMENT SETUP

데이터 세트. 다음과 같이 다양한 오디오 유형과 크기를 아우르는 다양한 데이터 세트가 사용되었습니다:

- 오디오 세트: 527개 클래스에 걸쳐 약 200만 개의 10초 오디오 클립이 포함된 가장 큰 규모의 오디오 분류 데이터 세트입니다.
- WavCaps: 약 68초 분량의 403,050개의 오디오 클립이 포함되어 있으며, ChatGPT의 도움으로 약하게 레이블이 지정되어 있습니다.
- 오디오캡: AudioSet의 하위 집합이지만 약 46,000개의 10초 오디오 클립에 대한 수작업 캡션이 포함되어 있습니다.
- VGGSound: 20만 개 이상의 동영상이 포함된 대규모 데이터 세트이지만 오디오 데이터와 레이블만 사용되었습니다.
- FMA & MSD: 다양한 메타데이터가 포함된 음악 트랙 데이터 세트.
- LJSpeech & GigaSpeech: 다양한 크기와 세부 사항을 가진 영어 음성 데이터 세트.

평가 메트릭. 주로 텍스트-오디오 생성 작업에 중점을 두었습니다:

- 객관적인 지표: 여기에는 프리쳇 오디오 거리(FAD)와 쿨백-라이블러 다이버전스(KL)가 포함됩니다.
- 주관적 지표: 전반적인 인상(OVL), 오디오와 텍스트 간의 관계(REL), 평균 의견 점수(MOS) 등의 메트릭이 포함됩니다.
- 주관적 평가. 주관적 평가는 다양한 관점을 보장하기 위해 Amazon Mechanical Turk에서 수행되었습니다. 평가자를 선정하는 데 엄격한 기준을 사용하여 양질의 피드백을 보장했습니다.

모델 아키텍처 세부 사항.

- 잠재 확산 모델의 두 가지 버전인 AudioLDM 2와 AudioLDM 2-Large가 사용되었습니다.
- 10초 멜 스펙트로그램을 512개의 패치로 처리하여 768개의 임베딩 차원으로 처리하는 사전 학습된 AudioMAE가 사용되었습니다.
- 오디오 및 음악 생성을 위해 이 모델에서는 CLAP과 FLAN-T5를 인코더로 사용했습니다. 음성 생성에는 CLAP과 음소 인코더의 조합이 사용되었습니다.

훈련 및 추론 설정.

- 잠재 확산 모델과 GPT 모델은 별도로 훈련되었으며, 사전 훈련 중에 특정 파라미터를 무작위로 선택하여 견고성을 강화했습니다.
- 훈련은 8개의 NVIDIA A100 80GB GPU에서 수행되었습니다.
- 훈련 설정은 이전 작업에서 조정되었으며, AdamW 옵티마이저는 GPT-2 미세 조정과 잠복 확산 모델 모두에 사용되었습니다.

이 설정은 다양한 작업에서 모델을 검증하고, 강력한 메트릭을 기준으로 성능을 평가하며, 최상의 결과를 위해 최적으로 훈련되도록 하는 것을 목표로 합니다.

### 4 RESULT

또한 텍스트 음성 변환 작업에서 AudioLDM 2의 성능을 평가하고 DeepVoice3(Arik 외, 2017), Tacotron2(Shen 외, 2018), FastSpeech2(Ren 외, 2020), Parrotron(Pratap 외, 2020)과 같은 최신 모델과 성능을 대조합니다. 또한 비교를 위해 AudioLDM의 이전 버전인 AudioLDM(Liu et al., 2023a)의 결과도 포함했습니다.

![AudioLDM 2의 컨텍스트 내 학습 능력. 왼쪽 열은 실제 오디오를 보여주며, 앞의 2.5초는 오디오 생성을 위한 컨텍스트로 사용됩니다. 오디오 컨텍스트의 연속이 오른쪽 열에 표시됩니다. 더 나은 시연을 위해 연속 전에 0.15초의 비프 소리를 수동으로 삽입합니다. 더 많은 데모는 그림 4, 그림 5, 그림 9에서 확인할 수 있습니다.](AudioLDM%202%20Learning%20Holistic%20Audio%20Generation%20with%207fd7c33c137149b598e4743d3058c520/Untitled%202.png)

AudioLDM 2의 컨텍스트 내 학습 능력. 왼쪽 열은 실제 오디오를 보여주며, 앞의 2.5초는 오디오 생성을 위한 컨텍스트로 사용됩니다. 오디오 컨텍스트의 연속이 오른쪽 열에 표시됩니다. 더 나은 시연을 위해 연속 전에 0.15초의 비프 소리를 수동으로 삽입합니다. 더 많은 데모는 그림 4, 그림 5, 그림 9에서 확인할 수 있습니다.

표 4에서 알 수 있듯이, 제안된 AudioLDM 2-LJS 모델은 평균 의견 점수(MOS)가 4.27로, 앞서 언급한 모든 모델을 능가하는 우수한 성능을 보여줍니다. 두 번째로 우수한 성능을 보인 타코트론2는 4.10의 MOS를 기록했는데, 이는 사람과 유사한 음성을 생성하는 데 있어 이 모델이 인상적인 능력을 갖추고 있음을 의미합니다. 보다 일반화된 모델인 AudioLDM 2-Full은 4.15라는 높은 MOS를 달성했는데, 이는 다양한 데이터로 학습된 모델도 전문화된 작업에서 뛰어난 성능을 발휘할 수 있음을 시사하며 접근 방식의 다목적성을 더욱 강화합니다.

![표 4: LJSpeech 테스트 세트에서 평가된 텍스트-투-스피치 성능](AudioLDM%202%20Learning%20Holistic%20Audio%20Generation%20with%207fd7c33c137149b598e4743d3058c520/Untitled%203.png)

표 4: LJSpeech 테스트 세트에서 평가된 텍스트-투-스피치 성능

흥미롭게도 더 큰 모델 변형인 AudioLDM 2-LJS-Large는 MOS가 4.29로 미미한 개선에 그쳐 텍스트 음성 변환 작업에서 모델 크기만 확장할 경우 성능 향상의 잠재적 포화 상태를 암시합니다.

텍스트-오디오, 텍스트-음악, 텍스트-음성 작업에 걸쳐 제시된 결과는 AudioLDM 2 모델의 잠재력을 반영합니다. 특히 이 모델은 특수 훈련 데이터에서 보다 일반화된 데이터 세트로 전환할 때 강력한 성능을 보여줍니다.

한 가지 흥미로운 사실은 광범위한 오디오 데이터를 학습하면 특정 영역에서 성능이 향상될 수 있지만, 모든 지표에서 일관된 향상을 보장하지는 않는다는 것입니다. 텍스트-오디오 변환 작업의 결과에서 볼 수 있듯이, 광범위한 데이터 세트에 대한 훈련은 훈련 데이터의 방대한 가변성으로 인해 생성된 출력의 모호함을 초래했습니다. 이는 다양한 훈련 분포를 좁은 테스트 세트 분포에 맞추는 데 따르는 어려움을 잘 보여줍니다.

일반화된 데이터 세트(AudioLDM 2-Full)에 대한 훈련은 음악 데이터에 대해 특별히 훈련된 모델(AudioLDM 2-MSD)을 능가할 정도로 유익한 것으로 입증되었습니다. 이러한 결과는 다양한 데이터에서 지식을 습득하면 전문화된 작업을 처리할 수 있는 모델의 역량을 강화할 수 있다는 주장을 뒷받침합니다.

텍스트 음성 변환 작업에서 AudioLDM 2가 해당 도메인에 맞게 제작된 모델과 비교했을 때에도 우월한 성능을 보인다는 점은 그 효율성이 입증된 것입니다. 모델 확장을 통해 얻을 수 있는 이득이 제한적이라는 점은 이 분야의 발전을 위해 단순한 모델 크기 확장 이상의 방법을 모색해야 할 필요성을 강조합니다.

텍스트 음성 변환 생성에서 AudioLDM 2의 기능을 잘 알려진 FastSpeech 2 모델과 비교하면 방대한 기가스피치 데이터 세트에 대한 사전 학습 후 MOS(평균 의견 점수)가 현저하게 개선된 것은 도메인별 데이터 세트에 대한 미세 조정 전에 대규모 데이터 세트에 대한 모델 사전 학습의 가능성을 보여줍니다. 이 사전 훈련 없이도 AudioLDM 2는 FastSpeech 2와 비슷한 성능을 발휘한다는 점에 주목할 필요가 있습니다. 사전 훈련된 AudioLDM 2 모델의 향상된 운율은 보다 자연스러운 음성 출력을 생성하는 데 있어 대규모 사전 훈련의 유용성을 더욱 강조합니다.

AudioLDM 2의 흥미로운 기능인 인컨텍스트 학습을 소개합니다. 이는 모델이 제공된 컨텍스트에 따라 생성된 오디오를 조정하여 특정 시나리오에 맞게 조정할 수 있음을 시사합니다. 인컨텍스트 학습은 보다 정밀하고 미묘한 오디오 생성 작업에 가장 중요하며, AudioLDM 2가 이러한 기능을 갖추고 있다는 점을 고려하면 오디오 생성 영역에서 더욱 가치 있는 도구가 될 것입니다.

AudioLDM 2 설계에서 다양한 컴포넌트의 중요도와 영향을 파악하기 위한 중요한 평가 방법인 절제 연구에 대해 자세히 설명합니다. 이러한 연구는 전체 성능에 대한 다양한 컴포넌트와 방법론의 개별적인 기여도를 조명하는 데 매우 중요합니다. 몇 가지 중요한 시사점을 얻을 수 있습니다:

조인트 미세 조정: 조인트 미세 튜닝을 건너뛰었을 때 성능이 현저하게 떨어지는 것은 복잡한 디테일을 포착하고 전반적인 오디오 품질을 향상시키는 데 조인트 미세 튜닝이 중요하다는 것을 의미합니다.

컨디셔닝 모듈: CLAP 및 FLAN-T5 모듈은 모두 모델의 효율성에 크게 기여합니다. 절제 연구는 고성능을 달성하는 데 있어 이러한 모듈의 상호 의존성을 보여줍니다. 특히 CLAP의 직접 컨디셔닝은 평가 지표와 일치하여 단독으로 사용했을 때 개선 효과를 설명합니다.

교차 주의 메커니즘: 이 제거의 혼합된 결과는 절충점을 암시합니다. AudioMAE 컨디셔닝은 잠재적으로 오디오 품질을 향상시킬 수 있지만(FAD 점수 개선에서 볼 수 있듯이), FLAN-T5 컨디셔닝은 생성된 오디오가 텍스트와 더욱 일치하도록 보장합니다. 이 균형은 오디오가 텍스트 콘텐츠를 정확하게 미러링해야 하는 애플리케이션에서 매우 중요합니다.

후보 필터링 방법: 박수 점수를 기반으로 여러 후보 중에서 최상의 오디오를 선택하는 이 모델의 방식은 고품질 오디오 생성에 집중할 수 있는 기능을 강조합니다.

### 5 RELATED WORKS

일반 오디오 생성: 이 영역은 주로 음성부터 주변 소음까지 다양한 오디오 사운드를 생성하는 것을 다룹니다.

- AudioGen: 이 작업은 조건부 언어 모델링으로 접근하는 것 같습니다. 이 방법은 언어 모델의 렌즈를 통해 오디오 생성을 바라보며 시퀀스 예측 기능의 힘을 활용할 수 있습니다.
- AudioLDM 및 Make-an-Audio: 이러한 방법은 잠재 확산 접근법을 적용하여 초기 무작위 노이즈를 반복적으로 개선하여 원하는 사운드를 생성하는 전략을 의미합니다.
- 이미지-오디오 및 비디오-오디오 변환: 시각 정보가 오디오 합성을 유도하는 흥미로운 크로스 모달리티 접근 방식입니다. 앞서 언급한 모델인 SpecVQGAN은 일반적으로 이미지 생성에 사용되는 VQGAN의 기술을 오디오 작업에 통합했을 수 있습니다.
- AudioLM: 오디오 언어 모델링에 신경 코덱을 사용하는 무조건적인 접근 방식으로, 오디오 정보를 인코딩 및 디코딩하여 새로운 오디오 시퀀스를 생성할 수 있습니다.

텍스트 음성 변환(TTS): 이 분야는 텍스트 정보를 음성으로 변환하는 데 중점을 둡니다.

- FastSpeech 2, GradTTS, NaturalSpeech와 같은 모델은 매우 사실적인 인간과 같은 음성을 생성하는 TTS 연구의 정점에 있습니다.
- 모노토닉 정렬 알고리즘 및 운율 예측기와 같은 기술은 음소를 해당 스펙트럼 특징에 맞게 정렬하고 감정 표현력을 향상시키는 등 TTS의 주요 과제를 해결합니다.
- 텍스트-음악 변환(TTM): 설명적인 텍스트를 음악으로 만들어내는 흥미로운 영역입니다.

MusicLM: 음악과 언어 임베딩 사이의 격차를 해소하는 데 중점을 둔 AudioLM의 아이디어에 영향을 받은 것으로 보입니다.

- 노이즈2뮤직: 자세히 설명되어 있지는 않지만, 이름에서 알 수 있듯이 구조화되지 않은 노이즈를 의미 있는 음악으로 변환하는 잠재적인 노이즈 제거 기술을 암시합니다.
- MusicGen: 일반 오디오용 AudioGen과 마찬가지로 언어 모델링 접근 방식을 사용하지만 음악에 특화되어 있어 멜로디 기능을 통합하여 보다 미묘한 음악을 생성할 수 있습니다.
- MeLoDy: 컴퓨팅 리소스가 제한된 실시간 애플리케이션이나 시나리오에 매우 중요한 계산 오버헤드를 줄여주는 효율적인 LM 가이드 확산 모델입니다.

### 6 CONCLUSION

AudioLDM 2의 성과: 이 모델은 텍스트-오디오, 텍스트-음악, 텍스트-음성 생성 등 다양한 작업에서 최고 수준의 결과를 달성했습니다. 이러한 영역에서의 우수성은 적응성과 효율성을 강조합니다.

LOA(언어 오브 오디오): 이 혁신적인 개념은 연구의 중심이 되는 개념으로, 잠재 확산 모델에서 자체 감독 사전 학습을 위한 기반을 마련합니다. 기본적으로 오디오를 보다 구조화되고 해석 가능한 '언어'로 표현함으로써 이 모델은 오디오 패턴을 더 잘 이해하고 생성할 수 있습니다.

다양성 및 확장성: AudioLDM 2는 텍스트-오디오 영역에서 탁월한 성능을 발휘할 뿐만 아니라 이미지-오디오 생성 영역에서도 뛰어난 적응성을 보여줍니다. 이러한 적응성은 특히 AI에서 멀티 모달리티(텍스트, 이미지, 오디오 등 여러 유형의 입력 데이터 사용)가 점점 더 중요해지고 있는 시대에 매우 중요합니다.

향후 연구 방향은 오디오, 음악, 음성 생성 작업을 동시에 처리할 수 있는 보다 총체적인 GPT 모델을 개발하는 것으로 보입니다. 이는 진정으로 다재다능한 오디오 생성 AI를 구현하는 게임 체인저가 될 수 있습니다.