# SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling

[https://arxiv.org/abs/2312.15166](https://arxiv.org/abs/2312.15166)

- Dec 2023

### 1 Introduction

소개 요약

![32개의 레이어가 있는 기본 모델을 사용한 뎁스 업스케일링. 저희는 Llama2 아키텍처를 사용하지만, 다른 트랜스포머 아키텍처도 뎁스 업스케일링과 호환됩니다. 기본 모델의 32개 레이어 중 24개 레이어에 사전 학습된 가중치를 사용하여 따라서 뎁스 업스케일링 모델은 48개 레이어 모두에 대해 사전 학습된 가중치를 갖게 됩니다.](SOLAR%2010%207B%20Scaling%20Large%20Language%20Models%20with%20Sim%205b46306be3fa42b0a2e5a06d93dfa01b/Untitled.png)

32개의 레이어가 있는 기본 모델을 사용한 뎁스 업스케일링. 저희는 Llama2 아키텍처를 사용하지만, 다른 트랜스포머 아키텍처도 뎁스 업스케일링과 호환됩니다. 기본 모델의 32개 레이어 중 24개 레이어에 사전 학습된 가중치를 사용하여 따라서 뎁스 업스케일링 모델은 48개 레이어 모두에 대해 사전 학습된 가중치를 갖게 됩니다.

이 논문의 서론에서는 대규모 언어 모델(LLM)로 인한 자연어 처리(NLP) 분야의 변화를 강조합니다. 최근 NLP의 발전은 인간의 언어에 대한 이해와 상호작용을 크게 향상시켰습니다. 그러나 이러한 발전은 특히 성능 확장 법칙으로 인해 더 큰 모델을 학습해야 하는 과제를 안고 있습니다. 최근 연구에서는 전문가 혼합(MoE)과 같은 방법을 통해 언어 모델을 확장할 것을 제안했지만, 이는 훈련 및 추론 프레임워크에 상당한 변경이 필요하기 때문에 광범위한 적용이 제한되는 경우가 많습니다.

이 연구에서는 LLM을 효율적이고 효과적으로 확장하는 동시에 적용하기 쉽도록 설계된 깊이 업스케일링(DUS)이라는 새로운 방법을 소개합니다. DUS를 사용하여 107억 개의 파라미터를 가진 LLM인 SOLAR 10.7B를 개발하여 다양한 벤치마크에서 라마 2, 미스트랄 7B와 같은 기존 모델을 능가하는 결과를 얻었습니다. DUS는 허깅페이스와 같은 일반적인 LLM 프레임워크와 호환되며, 트레이닝이나 추론 프레임워크에 추가적인 변경이 필요하지 않습니다. 또한 모든 트랜스포머 아키텍처에 적용할 수 있어 LLM을 간단하게 확장할 수 있습니다.

또한 복잡한 지침을 엄격하게 준수해야 하는 작업을 위해 세밀하게 조정된 SOLAR 10.7B의 변형 버전인 SOLAR 10.7B-Instruct를 만들었습니다. 이 변형은 다양한 평가 지표에서 Mixtral-8x7B 모델을 능가하며 벤치마크 성능에서 더 큰 모델의 기능을 능가하는 고급 숙련도를 보여줍니다. Apache 2.0 라이선스에 따라 SOLAR 10.7B를 출시함으로써 전 세계 연구자와 개발자가 이러한 모델을 더 쉽게 이용할 수 있도록 하여 NLP 분야의 협업과 혁신을 촉진하는 것이 목표입니다.

### 2 SOLAR 10.7B Architectural Details

이 섹션에서는 크기 대비 성능 절충을 개선하기 위해 설계된 대규모 언어 모델(LLM)인 SOLAR 10.7B의 아키텍처 세부 사항을 자세히 살펴봅니다. 이를 달성하기 위한 기본 접근 방식은 파레토 최적 곡선 상에 있는 것으로 간주되는 7B 크기의 LLM을 확장하는 것으로, 이는 성능과 크기 간에 유리한 균형을 제공한다는 의미입니다. 확장 프로세스에는 기본 모델에서 미리 학습된 가중치를 사용하여 더 큰 LLM으로 효율적으로 전환하는 것이 포함됩니다.

이 프로세스의 핵심은 새로운 뎁스 업스케일링(DUS) 방식입니다. 효율적인 추론을 위해 별도의 훈련 프레임워크나 커스텀 CUDA 커널과 같은 복잡한 변경이 필요한 전문가 혼합(MoE)과 같은 다른 업스케일링 방법과 달리, DUS는 기본 LLM에서 사용하는 기존 훈련 및 추론 프레임워크와의 호환성을 유지합니다. 따라서 LLM 및 기타 트랜스포머 기반 아키텍처를 확장하는 데 더 간단하고 접근하기 쉬운 옵션이 됩니다.

이 프로젝트에서 선택한 기본 모델은 32층 라마 2 아키텍처로, 미스트랄 7B의 사전 학습된 가중치로 초기화됩니다. 이 선택은 라마 2의 견고하고 다재다능한 특성을 활용하는 동시에 미스트랄 7B의 고성능을 활용합니다.

DUS 방식 자체에는 업스케일링에 대한 독특한 접근 방식이 포함되어 있습니다. 예를 들어 기본 LLM의 레이어를 단순히 32개에서 64개로 두 배로 늘리면 레이어가 연결되는 이음새에 상당한 '레이어 거리' 또는 불일치가 발생할 수 있지만, DUS는 전략적으로 특정 레이어를 제거합니다. 구체적으로, 원본 기본 모델에서 마지막 8개 레이어를 제거하고 복제본에서 처음 8개 레이어를 제거합니다. 그런 다음 두 개의 24개 레이어 모델을 연결하여 107억 개의 파라미터가 포함된 48개 레이어 모델인 SOLAR 10.7B를 구성합니다. 이 접근 방식은 이음새에서 레이어 거리를 줄여 사전 학습된 가중치를 효과적으로 활용할 수 있는 모델의 능력을 향상시킵니다.

DUS 방식은 단순성과 효율성이 뛰어나다는 점에서 주목할 만합니다. 이 방법에는 게이팅 네트워크나 전문가 선택 과정과 같이 MoE에서 사용하는 추가 모듈이 필요하지 않습니다. 따라서 DUS로 확장된 LLM은 기존 학습 및 추론 프레임워크에 쉽게 통합할 수 있어 특별한 조정 없이도 높은 효율성을 유지할 수 있습니다. 이 백서에서는 다음 섹션에서 SOLAR 10.7B의 훈련 프로세스에 대해 설명합니다.

### 3 SOLAR 10.7B Training Details

이 섹션에서 자세히 설명하는 SOLAR 10.7B의 학습 프로세스는 모델의 성능과 인간과 유사한 이해력을 향상시키기 위해 고안된 다단계 절차입니다.

사전 훈련 및 빠른 복구
기본 모델에 뎁스 업스케일링(DUS)을 적용한 후 초기 성능 저하가 관찰됩니다. 그러나 섹션 2에서 언급한 가설에 따라 모델은 지속적인 사전 학습을 통해 빠르게 성능을 회복합니다. 이 단계는 업스케일링된 모델의 기능을 다시 설정하고 향상시키는 데 매우 중요합니다.

인스트럭션 튜닝
미세 조정의 첫 번째 단계는 인스트럭션 튜닝입니다. 이 단계에서는 질문과 답변(QA) 형식을 사용하여 모델이 지침을 따르도록 훈련합니다. 여기에는 대부분 오픈 소스 데이터 세트를 사용하며, 특별히 제작된 수학 QA 데이터 세트로 보완합니다. 이 데이터세트를 만들기 위해 수학 데이터세트의 시드 데이터는 일반적으로 사용되는 벤치마크 데이터세트와 겹치지 않도록 다시 표현됩니다. 메타매쓰에서 영감을 얻은 이 재구문 프로세스를 통해 'Synth'라는 고유한 QA 데이터 세트가 생성됩니다. Math-Instruct'라는 고유한 QA 데이터 세트를 생성하여 모델의 수학적 능력을 특별히 향상시킵니다.

얼라인먼트 튜닝
미세 조정의 두 번째 단계는 얼라인먼트 튜닝입니다. 이 단계에서는 인스트럭션 튜닝된 모델을 더욱 세분화하여 사람 또는 고급 AI의 선호도에 더 가깝게 조정합니다. 이는 직접 선호도 최적화(DPO)라는 방법을 사용하여 이루어집니다. 이 과정에는 수학에 초점을 맞춘 정렬 데이터 세트를 합성하는 작업이 포함되며, 이 때에도 'Synth. Math-Instruct' 데이터 세트를 활용합니다. 이 데이터 세트의 문구를 바꾼 질문-답변 쌍은 DPO 튜플을 생성하는 데 사용되며, 여기서 문구를 바꾼 답변이 선호 답변으로 간주됩니다. 이 방법은 특히 수학적 맥락에서 인간의 추론과 모델의 일치도를 향상시키는 것을 목표로 합니다.

모델 병합

SOLAR 10.7B의 훈련에서 주목할 만한 측면은 모델 병합입니다. 이 기술은 학습 및 정렬 튜닝 단계에서 훈련된 모델을 결합하여 성능을 더욱 향상시키는 기술입니다. 모델 병합에는 가중치의 단순 평균과 구형 선형 보간(SLERP)의 두 가지 방법이 실험됩니다. 이러한 병합 기법은 서로 다른 훈련 단계의 강점을 보다 강력한 단일 모델로 통합하는 것을 목표로 합니다.

요약하면, SOLAR 10.7B의 훈련에는 사전 훈련, 명령어 튜닝, 정렬 튜닝, 모델 병합의 세심하게 구조화된 프로세스가 포함됩니다. 각 단계는 모델의 성능, 명령어 추종 능력, 사람 선호도와의 일치도, 전반적인 수학적 능력을 점진적으로 향상시키도록 설계되었습니다.

### 4 Experimental Results

이 섹션에서는 SOLAR 10.7B와 그 변형 버전인 SOLAR 10.7B-Instruct의 성능을 평가하기 위해 수행된 광범위한 실험 및 평가에 대해 간략하게 설명하며, 훈련 및 미세 조정 프로세스에 대한 통찰력을 제공합니다.

4.1 훈련 데이터 세트 및 평가

트레이닝 데이터 세트: SOLAR 10.7B의 트레이닝에는 대부분 오픈 소스 데이터 세트를 활용한 인스트럭션 및 정렬 튜닝 단계가 모두 포함되었습니다. 일부 데이터 세트는 알파카 스타일의 채팅 템플릿을 사용하여 포맷을 변경하고 벤치마크 데이터 세트와 겹치는 데이터를 필터링했습니다. 정렬 데이터 세트는 {프롬프트, 선택, 거부} 세 가지 형식으로 포맷되었으며 Zephyr 가이드라인에 따라 사전 처리되었습니다.

평가: ARC, HellaSWAG, MMLU, TruthfulQA, Winogrande, GSM8K를 포함한 HuggingFace 오픈 LLM 리더보드의 평가 방법이 벤치마크로 사용되었습니다. 이 여섯 가지 작업의 평균 점수는 복합 측정값(H6)으로 보고되었습니다.

4.2 주요 결과

SOLAR 10.7B는 크기가 더 작음에도 불구하고 비슷한 크기의 사전 훈련된 모델인 Qwen 14B 및 Mistral 7B보다 우수한 성능을 보여 DUS의 효과를 입증했습니다. 특히 SOLAR 10.7B-Instruct는 믹스트랄 8x7B-Instruct-0.1과 큐웬 72B와 같은 대형 모델을 제치고 H6에서 가장 높은 점수를 획득했습니다.

4.3 어블레이션 연구

이러한 연구는 다양한 훈련 데이터 세트와 전략이 모델 성능에 미치는 영향에 대한 인사이트를 제공했습니다.

4.3.1 명령어 튜닝

훈련 데이터 세트 제거: 훈련 데이터 세트의 다양한 조합을 테스트했습니다. 예를 들어, Synth. 수학-인스트럭트 데이터 세트로 훈련한 모델은 성능이 향상되어 유익한 영향을 미쳤습니다. 또한 OpenOrca 데이터 세트로 훈련된 모델과 그렇지 않은 모델을 병합한 결과, 다양한 작업에서 우수한 성능을 발휘하는 보다 균형 잡힌 모델이 탄생했습니다.

4.3.2 정렬 튜닝

훈련 데이터 세트 제거: 울트라피드백 클린 및 신디사이저 사용. 수학-정렬 데이터 세트의 사용은 직접 선호도 최적화(DPO) 중에 탐색되었습니다. Synth. 수학 정렬을 포함하면 일반적으로 점수가 향상되었지만, 서로 다른 데이터 세트로 학습된 모델을 병합하는 것이 항상 더 나은 결과를 가져오는 것은 아니었습니다.

SFT 기본 모델: DPO에 대한 다양한 기본 모델에서 특정 작업의 성능 격차가 정렬 조정된 모델에 항상 이어지지는 않는 것으로 나타났습니다.

병합 방법: 두 가지 병합 방법, 즉 가중치 평균과 SLERP를 테스트했습니다. 실험 결과, 병합 후보에 뚜렷한 강점이 있는 한 특정 병합 방법이 전체 성능에 큰 영향을 미치지 않을 수 있음을 시사했습니다.

요약하면, 이러한 실험 결과는 SOLAR 10.7B와 그 인스트럭터 튜닝 변형의 견고성과 효율성을 입증합니다. 이 연구는 다양한 작업에서 모델의 성능을 향상시키는 데 있어 전략적 데이터 세트 선택과 모델 병합 기술의 중요성을 강조합니다.

### 5 Conclusion

이 논문에서는 대규모 언어 모델(LLM)을 효율적으로 확장하기 위한 깊이 업스케일링(DUS)이라는 새로운 접근 방식을 소개합니다. DUS는 최고의 효율성을 위해 전문 교육이나 추론 프레임워크가 필요하지 않다는 점에서 전문가 혼합(MoE)과 같은 다른 업스케일링 기법과 차별화됩니다. 이 백서에서는 SOLAR 10.7B의 개발과 미세 조정된 변형 버전인 SOLAR 10.7B-Instruct를 통해 DUS의 효과를 보여줍니다. Apache 2.0 라이선스에 따라 SOLAR 10.7B를 출시하는 것은 공동 연구를 장려하고 NLP 분야의 접근성과 지속 가능성을 높이는 것을 목표로 합니다.

제한 사항

이 백서에서는 SOLAR 10.7B의 몇 가지 한계를 인정합니다:

- 계산 요구 사항: 훈련과 추론에 필요한 상당한 계산 요구 사항으로 인해 리소스가 제한된 사람들의 접근성이 제한될 수 있습니다.
- 데이터 편향성: 이를 최소화하려는 노력에도 불구하고 SOLAR 10.7B는 여전히 훈련 데이터의 편향성을 내포하고 있어 특정 시나리오에서 왜곡된 결과를 초래할 수 있습니다.
- 복잡성 및 해석 가능성: 모델의 복잡성은 특히 모델의 의사 결정 프로세스에 대한 이해가 필요한 애플리케이션에서 모델의 해석 가능성과 설명 가능성에 문제를 야기합니다.
- 언어 및 도메인 특이성: SOLAR 10.7B의 효과는 다양한 언어, 특히 리소스가 부족한 언어와 특수한 도메인에 따라 달라집니다.
- 환경 영향: 모델을 훈련하고 실행하는 데 상당한 에너지가 소비되기 때문에 환경적 지속 가능성에 대한 우려가 제기됩니다.
- 작업별 미세 조정: SOLAR 10.7B-Instruct 버전은 리소스 집약적일 수 있는 특수 애플리케이션에서 최적의 성능을 발휘하기 위해 작업별 미세 조정이 필요합니다.

윤리 선언문

SOLAR 10.7B의 개발은 최고의 윤리 기준을 준수합니다:

- 데이터 오염: 이 모델은 엄격한 데이터 처리 및 처리를 반영하여 낮은 수준의 데이터 오염을 보여줍니다.
- 윤리적 연구 관행: 윤리적으로 의심스러운 관행을 피하는 데 중점을 두고 연구를 수행하여 책임감 있는 혁신에 대한 의지를 보여줍니다.
- 윤리 규범 준수: SOLAR는 개인정보 보호 규범을 준수하고 지적 재산을 존중하며 알고리즘의 편견을 없애기 위해 노력하여 신뢰성과 사회적 수용성을 강화합니다.

결론적으로, 이 백서에서는 SOLAR 10.7B를 자연어 처리 분야에서 과학적으로 진보하고 윤리적으로 책임감 있는 개발로 소개하지만, 모델의 기능을 포괄적으로 이해하고 향후 LLM 연구 및 개발을 안내하는 데 중요한 몇 가지 한계가 있습니다.

### Appendix

A. 기여도

이 연구는 대규모 언어 모델(LLM) 분야에 몇 가지 중요한 공헌을 했습니다:

- 혁신적인 업스케일링 방법: LLM을 업스케일링하는 효과적이고 효율적이며 간단한 방법인 깊이 업스케일링(DUS)을 소개하여 전문가 혼합(MoE)과 같은 기존 방법에 대한 보다 쉬운 대안을 제공합니다.
- 세계 최초의 107억 개 파라미터 모델: SOLAR 10.7B는 전례 없는 규모로 언어 모델 개발의 새로운 기준을 제시합니다.
- 뛰어난 성능: 추론, 수학 등 다양한 벤치마크에서 기존 모델을 뛰어넘는 탁월한 성능을 보여줍니다.
- 고급 명령어 추종 능력: 향상된 명령어 추종 능력을 위해 미세 조정된 SOLAR 10.7B-Instruct를 도입하여 복잡한 명령어를 실행하는 모델의 능력이 크게 향상되었습니다.
- 상업적 실행 가능성: Apache 2.0 라이선스에 따라 SOLAR 10.7B를 출시하여 상용 제품 및 서비스와의 통합을 용이하게 하고 학술 연구와 실제 애플리케이션 간의 격차를 해소합니다.

B. 관련 작업 및 배경

이 섹션에서는 LLM 분야의 발전과 과제에 대해 논의합니다:

- LLM과 스케일링 법칙: 모델 및 데이터 크기와 성능 간의 양의 상관관계를 강조하여 상황 내 학습이 가능한 LLM의 개발을 이끌어 냅니다.
- 전문가 혼합(MoE): 복잡성, 사후 붕괴 및 구현 어려움과 같은 문제를 포함하여 MoE 모델의 장점과 과제를 살펴봅니다.
- 프롬프트 엔지니어링: 생각의 사슬 접근 방식과 프롬프트 엔지니어링을 자동화하기 위한 노력을 포함하여 LLM의 작업 성과를 향상시키기 위한 프롬프트 설계에 중점을 둡니다.
- 명령어 튜닝: 명령어 입출력 형식의 데이터로 LLM을 미세 조정하여 보다 제어되고 작업 지향적으로 개선하는 기술을 설명합니다.
- 정렬 튜닝: 사람의 의도와 선호도에 맞게 LLM을 조정하기 위한 강화 학습(RLHF) 및 직접 정책 최적화(DPO)와 같은 기법에 대해 설명합니다.
- 데이터 오염: 벤치마크 데이터 세트와의 잠재적인 학습 데이터 중복 및 다양한 유형의 데이터 오염을 측정하는 것의 중요성을 강조합니다.

C. 추가 정보

방법론 및 결과에 대한 추가 세부 정보를 제공합니다:

- 필터링된 작업 이름: OpenOrca와 같이 FLAN에서 파생된 데이터 세트를 필터링하는 데 사용된 특정 작업 이름을 나열합니다.
- 데이터 오염 결과: SOLAR 10.7B-Instruct의 데이터 오염 테스트에 대한 보고서로, 모델의 무결성을 확인하고 수학 관련 데이터 세트의 유사성이 높은 잠재적 이유를 강조합니다.

요약하면, 이 논문은 LLM을 위한 획기적인 업스케일링 방법을 제시하고, SOLAR 10.7B의 고성능 모델을 소개하며, 모델 훈련 및 데이터 처리의 복잡성을 탐색하는 동시에 윤리적 기준과 상업적 적용 가능성에 대한 강력한 약속을 유지합니다.