# DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing

[https://arxiv.org/abs/2310.10624](https://arxiv.org/abs/2310.10624)

- Oct 2023

### 1 INTRODUCTION

최근 이미지 확산 모델의 발전으로 비디오 편집에 적용하는 것에 대한 관심이 급증하고 있습니다. 그러나 동영상에서 시간적 일관성을 유지하는 것은 여전히 어려운 과제입니다. 이러한 시간적 일관성을 개선하기 위해 주의 지도, 공간 지도, 광학적 흐름 등을 사용하는 여러 가지 확산 기반 방법이 연구되어 왔습니다. 이러한 접근 방식은 움직임이 제한적인 짧은 동영상에는 효과적이지만, 움직임과 시점이 많이 바뀌는 긴 동영상에서는 어려움을 겪습니다.

![소니의 DynVideo-E는 레퍼런스 피사체 이미지와 배경 스타일 이미지가 주어지면 대규모 모션 및 뷰 변경이 필요한 사람 중심의 비디오를 일관성 있게 편집할 수 있습니다. 아이덴티티 및 옷 질감과 같은 디테일이 매우 높은 시간적 일관성(a-c)으로 잘 보존됩니다.](DynVideo-E%20Harnessing%20Dynamic%20NeRF%20for%20Large-Scale%203988caa627aa4931b8e956fcca62b460/Untitled.png)

소니의 DynVideo-E는 레퍼런스 피사체 이미지와 배경 스타일 이미지가 주어지면 대규모 모션 및 뷰 변경이 필요한 사람 중심의 비디오를 일관성 있게 편집할 수 있습니다. 아이덴티티 및 옷 질감과 같은 디테일이 매우 높은 시간적 일관성(a-c)으로 잘 보존됩니다.

연구자들은 다양한 2D 표현을 사용하여 비디오 편집을 이미지 편집으로 전환하는 방법도 시도했습니다. 이를 통해 개별 프레임 또는 전체 프레임 세트를 편집할 수 있으므로 동영상 전체에서 일관된 편집이 가능합니다. 이러한 2D 방식은 잠재력에도 불구하고 대규모 모션과 시점 변화가 있는 동영상에서는 어려움을 겪습니다.

이 연구에서는 비디오 편집을 위한 새로운 3D 표현 방식을 소개하며, 특히 대규모 모션과 시점 변화가 있는 비디오에 중점을 둡니다. DynVideo-E라고 불리는 이 새로운 접근 방식은 동적 NeRF(신경 방사 필드)를 활용하여 까다로운 비디오 편집 작업을 보다 쉬운 3D 공간 편집 문제로 전환합니다. 동적 NeRF를 사용하여 비디오 데이터를 3D 공간으로 통합하면 전체 비디오에서 편집이 더욱 간소화되고 일관성이 유지됩니다.

DynVideo-E 방식은 3D 공간에서 비디오 편집을 개선하기 위한 몇 가지 전략을 통합합니다:

- 재구성 손실: 이는 사람의 포즈와 카메라 시점을 고려하여 참조 이미지에 적용되어 이미지의 콘텐츠를 편집된 3D 인간 공간에 통합합니다.
- 스코어 증류 샘플링(SDS): 편집된 공간의 3D 일관성을 향상시키기 위해 고안된 SDS는 2D 및 3D 확산 선행과 다양한 훈련 전략에서 사람 및 카메라 포즈를 고려한 결과를 도출합니다.
- 해상도 개선: 이 전략은 텍스트 가이드 로컬 파트 초해상도를 사용하여 7개의 특정 신체 부위에 초점을 맞춰 3D 인체 공간의 해상도와 기하학적 디테일을 향상시킵니다.
- 스타일 전송: 이 모듈은 참조 이미지에서 3D 배경 모델로 스타일을 전송하여 미적 일관성을 향상시킵니다.

훈련 후 DynVideo-E는 변형 필드를 효율적으로 사용하여 전체적으로 일관된 편집으로 비디오를 제작할 수 있습니다.

DynVideo-E는 동적인 사람 중심의 동영상이 포함된 두 개의 데이터 세트에 대해 엄격한 테스트를 거쳤습니다. 그 결과 이 접근 방식이 매우 일관된 고품질 비디오 편집을 생성하는 것으로 나타났습니다. 놀랍게도 DynVideo-E는 인간 평가자들 사이에서 50~95% 더 높은 선호도를 보이며 현재의 최신 방법보다 뛰어난 성능을 보였습니다.

### 2 RELATED WORK

확산 기반 비디오 편집:

확산 모델의 강력한 성능 덕분에 비디오 편집 및 생성에 확산 모델을 적용할 수 있게 되었습니다. 초기 노력 중 하나인 튠어비디오(Tune-A-Video)는 이미지 확산과 프레임 간 주의력을 결합하여 소스에서 대상 비디오로 모션을 전송했습니다. 그러나 이 방법은 비디오 전체에서 시간적 일관성을 유지하는 데 문제가 있었습니다. 이후 FateZero 및 Video-P2P와 같은 작업에서는 소스 비디오에서 주목도 특징을 추출하여 사용함으로써 시간적 일관성을 향상시켰습니다. Rerender-A-Video, ControlVideo, TokenFlow와 같은 다른 방법도 비슷한 목표를 달성하기 위해 광학 흐름, 공간 맵, nn-필드에서 아이디어를 얻었습니다. 이러한 방법은 유망하지만 주로 모션 변화가 제한적인 짧은 동영상에 효과적입니다. 또한, 레이어드 뉴럴 아틀라스를 중간 단계의 비디오 편집 표현으로 사용하려는 노력도 있었습니다. 이 접근 방식은 비디오를 레이어로 분할하고 콘텐츠를 2D UV 맵에 매핑하여 효율적인 편집을 용이하게 합니다. 또 다른 방법인 CoDeF는 3D 변형과 2D 표현을 혼합하여 비디오 표현을 향상시키지만, 광범위한 3D 뷰 변경에 어려움을 겪습니다.

![DynVideo-E 개요. (1) 입력 비디오를 변형 필드 및 3D 배경 정적 공간과 결합된 3D 전경 표준 인간 공간으로 표현하는 비디오-NeRF 모델입니다. (2) 주황색 순서도: 레퍼런스 피사체 이미지가 주어지면 재구성 손실, 2D 개인화된 확산 전구, 3D 확산 전구, 로컬 파트 초해상도를 활용하여 멀티뷰 멀티포즈 구성에서 애니메이션 가능한 표준 인체 공간을 편집합니다. (3) 녹색 순서도: 피처 공간의 스타일 전송 손실은 참조 스타일을 3D 배경 모델로 전송하는 데 활용됩니다. (4) 편집된 비디오는 소스 비디오 카메라 포즈에 따라 편집된 비디오-NeRF 모델에서 볼륨 렌더링을 통해 적절하게 렌더링할 수 있습니다.](DynVideo-E%20Harnessing%20Dynamic%20NeRF%20for%20Large-Scale%203988caa627aa4931b8e956fcca62b460/Untitled%201.png)

DynVideo-E 개요. (1) 입력 비디오를 변형 필드 및 3D 배경 정적 공간과 결합된 3D 전경 표준 인간 공간으로 표현하는 비디오-NeRF 모델입니다. (2) 주황색 순서도: 레퍼런스 피사체 이미지가 주어지면 재구성 손실, 2D 개인화된 확산 전구, 3D 확산 전구, 로컬 파트 초해상도를 활용하여 멀티뷰 멀티포즈 구성에서 애니메이션 가능한 표준 인체 공간을 편집합니다. (3) 녹색 순서도: 피처 공간의 스타일 전송 손실은 참조 스타일을 3D 배경 모델로 전송하는 데 활용됩니다. (4) 편집된 비디오는 소스 비디오 카메라 포즈에 따라 편집된 비디오-NeRF 모델에서 볼륨 렌더링을 통해 적절하게 렌더링할 수 있습니다.

다이내믹 뉴럴 래디언스 필드(NeRF):

신경 방사 필드(NeRF)가 도입된 이후 이 분야는 특히 새로운 뷰를 합성하는 데 있어 급속도로 발전했습니다. 동적 공간을 표준 필드로 변환하거나 4D 방사 필드를 형성하여 단일 뷰 비디오에서 동적 NeRF를 생성하려는 노력이 있었습니다. 다른 연구에서는 복셀 그리드 또는 평면 표현을 통합하여 이러한 동적 NeRF의 훈련 효율을 높였습니다. 기본적인 변형이 있는 짧은 동영상에 적용하는 것이 가장 큰 한계였습니다. 인간 중심 모델링은 인간의 포즈를 활용하여 복잡한 움직임을 가진 역동적인 인간을 재현하는 하위 분야로 부상했습니다. 예를 들어 노이만은 인간 중심의 장면에 초점을 맞추는 반면, HOSNeRF는 복잡한 인간-사물-장면 상호작용을 캡슐화합니다. 이 리뷰의 목적은 사람 피사체에 초점을 맞춘 비디오 편집에 동적 NeRF를 통합할 수 있는 잠재력을 탐구하는 것입니다.

NeRF 기반 편집 및 생성:

확산 모델의 등장으로 텍스트 가이드 3D NeRF 편집은 클립 기반 방식에서 2D 확산 기반 기법으로 발전했습니다. SINE은 단일 뷰에서 정적 NeRF를 편집한 후 이 편집된 콘텐츠를 여러 뷰에 배포할 수 있는 고유한 접근 방식을 제공합니다. 최근 주목할 만한 두 가지 작품인 Control4D와 Dyn-E가 동적 NeRF 편집에 뛰어들었습니다. Control4D는 최소한의 움직임이 있는 사람이 등장하는 동영상에만 적합하며, Dyn-E는 짧은 동영상에서 특정 영역으로 편집을 제한하는 제약이 있습니다.

### 3 METHOD

3.1 비디오-NERF 모델

소개: Video-NeRF 모델의 목표는 비디오 편집 프로세스를 정적 3D 편집 작업으로 변환하여 단순화하는 것입니다. 이는 특히 방대한 시점 변화, 복잡한 콘텐츠, 정교한 사람의 움직임을 보여주는 동영상과 관련이 있습니다.

![DynVideo-E와 SOTA 접근 방식의 정성적 비교.](DynVideo-E%20Harnessing%20Dynamic%20NeRF%20for%20Large-Scale%203988caa627aa4931b8e956fcca62b460/Untitled%202.png)

DynVideo-E와 SOTA 접근 방식의 정성적 비교.

모델 컴포넌트:

HSNeRF(동적 인간-신 신경 방사 필드): 특히 동적인 사람 중심의 동영상에 초점을 맞춘 HOSNeRF 모델을 수정한 것입니다.
동적 인간 모델: 동적 비디오 데이터를 3D 공간으로 통합하여 사람의 움직임과 포즈 변화를 캡처합니다.
정적 장면 모델: 장면의 정적 구성 요소에 대한 정보를 정의된 3D 공간으로 수집합니다.
최적화: 이 모델은 일련의 확립된 알고리즘과 프레임워크를 사용하여 최적화됩니다. 주요 목표는 인위적으로 렌더링된 이미지와 비디오의 실제 이미지 간의 차이를 최소화하는 것입니다. 품질과 정확성을 보장하기 위해 여러 성능 지표와 손실 함수가 사용됩니다.

3.2 이미지 기반 3D 표준 공간 편집

소개: 과거의 비디오 편집 방법은 텍스트 지침에 크게 의존했지만, 정밀도와 제어력을 높이기 위해 이미지 기반 편집으로 초점이 이동하고 있습니다.

구성 요소:

- 포 그라운드 편집: 이미지와 텍스트 설명을 모두 사용하여 동적인 사람 모델을 편집하는 데 중점을 둡니다.
- 배경 편집: 레퍼런스 스타일 이미지를 사용하여 비디오의 정적인 부분을 대상으로 합니다.

과제 및 솔루션:

고품질 편집 유지: 편집된 동영상이 다양한 뷰와 모션에서 품질과 일관성을 유지하도록 하기 위해 일련의 전략이 사용됩니다:

- 레퍼런스 이미지 재구성 손실: 편집 프로세스를 안내하기 위해 레퍼런스 이미지를 사용하여 최종 결과물이 원본과 유사하도록 합니다.
- 3D 확산 이전의 SDS: 참조 이미지에서 3D 기하학적 정보를 추출하여 추가 편집 지침을 제공합니다.
- 2D 퍼스널라이즈드 디퓨전 프리로부터의 SDS: 이 기술은 소스 비디오에서 사람의 포즈를 사용하여 3D 모델에 애니메이션을 적용합니다. 그런 다음 2D 확산 모델을 사용하여 레퍼런스 이미지의 고유한 특성을 유지하면서 편집 프로세스를 안내합니다.
- 텍스트 가이드 로컬 파트 초해상도: 이 모델은 GPU 메모리의 한계를 해결하기 위해 텍스트 안내를 사용하여 비디오의 특정 부분의 해상도를 향상시켜 더 선명한 텍스처와 세밀한 디테일을 보장합니다.

기본적으로 이 방법론은 3D 모델과 레퍼런스 이미지를 활용하고 정교한 최적화 기술을 사용하여 비디오 편집을 보다 직관적이고 정밀하게 만드는 것을 목표로 합니다.

### 4 EXPERIMENTS

데이터 세트 및 최신 접근 방식과의 비교:

긴 동영상과 짧은 동영상을 편집할 때 DynVideo-E의 효율성을 평가하기 위해 두 가지 데이터 세트를 사용했습니다: 300~400프레임의 비디오가 포함된 HOSNeRF와 NeuMan입니다. 텍스트2비디오제로, 리렌더-A-비디오, 텍스트2라이브, 스테이블비디오, CoDeF 등 5가지 주요 비디오 편집 방법과 비교했습니다. 이러한 방법의 학습에는 미드저니에서 생성한 텍스트 설명이 사용되었습니다.

정성적 결과:

![(a) 배낭 장면과 (b) 실험실 장면에 대해 제안된 각 컴포넌트에 대한 제거 연구.](DynVideo-E%20Harnessing%20Dynamic%20NeRF%20for%20Large-Scale%203988caa627aa4931b8e956fcca62b460/Untitled%203.png)

(a) 배낭 장면과 (b) 실험실 장면에 대해 제안된 각 컴포넌트에 대한 제거 연구.

긴 동영상과 짧은 동영상을 모두 사용하여 다른 기법과 DynVideo-E를 시각적으로 비교했을 때(그림 3 참조), 기본 방법은 움직임과 시점 변화가 심한 동영상에서 어려움을 겪는다는 것이 분명했습니다. 일관된 구조를 유지하면서 전경이나 배경을 제대로 편집하지 못하는 경우가 많았습니다. 그러나 DynVideo-E는 이러한 조건에서 탁월한 성능을 발휘하여 두 비디오 길이 모두에서 다른 방법을 능가하는 고품질 비디오 결과를 제공했습니다. 더 까다로운 비디오 시나리오에서는 CoDeF, Text2LIVE, StableVideo와 같은 특정 방법이 입력에 너무 좁게 적용되어 의미 있는 편집을 생성하지 못했습니다.

정량적 결과와 인간의 선호도:

DynVideo-E의 효율성은 지표와 사람의 피드백을 통해 더욱 입증되었습니다. CLIPScore를 사용하여 제작된 비디오와 해당 텍스트 설명 간의 텍스트 정확도를 측정했습니다. 평가된 모든 접근 방식 중 텍스트 충실도 측면에서 DynVideo-E가 가장 높은 순위를 차지했습니다. 인간 선호도 연구에서는 평가자들에게 동영상과 설명을 보여주어 텍스트 충실도, 시간적 일관성, 전반적인 품질을 기준으로 평가했습니다. 총 32명의 평가자의 의견을 바탕으로 1140건의 비교가 이루어졌습니다. 그 결과, DynVideo-E가 경쟁사보다 50%에서 95%까지 큰 차이로 선호도가 높은 것으로 나타났습니다.

절제 연구:

DynVideo-E에서 각 요소의 기여도를 파악하기 위해 HOSNeRF 및 NeuMan 데이터 세트의 비디오에 대한 제거 연구를 수행했습니다. 로컬 부품 초해상도, 재구성 손실, 2D 개인화 SDS, 3D SDS, 2D 개인화 로라 등 다양한 구성 요소를 개별적으로 제거했습니다. 그림 4에 표시된 결과는 각 구성 요소를 제거함에 따라 DynVideo-E의 성능이 점차 저하되는 것을 보여 주며, 각 기능이 중요한 역할을 한다는 점을 강조합니다. 모든 구성 요소를 그대로 유지한 모델 전체가 가장 우수한 결과를 제공했습니다.

### 5 CONCLUSION

우리는 모션과 시점이 크게 변화하는 사람 중심의 비디오를 편집할 수 있도록 설계된 혁신적인 프레임워크인 DynVideo-E를 선보였습니다. 이 작업의 복잡성을 해결하기 위해 먼저 핵심 비디오 표현으로 동적 NeRF를 사용했습니다. 이를 통해 비디오 편집 과제를 변형 필드를 사용하여 편집 요소가 비디오 전체에 균일하게 분산되는 보다 관리하기 쉬운 3D 정적 편집 작업으로 전환했습니다. 이러한 3D 공간 내에서 편집을 더욱 용이하게 하기 위해 몇 가지 기술을 통합했습니다. 여기에는 개인화된 확산 사전에서 가져온 2D 및 3D 스코어 증류 샘플링(SDS)의 조합, 레퍼런스 이미지의 재구성 손실 메커니즘, 로컬 섹션의 텍스트 기반 향상, 3D 배경의 스타일 전환 등이 포함되었습니다. 종합적인 테스트 결과, DynVideo-E는 성능 면에서 다른 주요 비디오 편집 방법을 능가하는 것으로 나타났습니다.

DynVideo-E는 비디오 편집 시나리오에서 탁월한 결과를 보여줬지만, 제약 조건이 있습니다. 비디오-NeRF 모델을 구축하는 초기 단계는 시간 측면에서 특히 리소스 집약적입니다. 이 프로세스를 가속화할 수 있는 잠재적인 해결책은 비디오-NeRF 모델 내에 복셀 표현 또는 해시 그리드를 통합하는 것입니다. 이는 향후 비디오 편집 분야의 연구 개발을 위한 유망한 길을 제시합니다.