# SoundStorm: Efficient Parallel Audio Generation

[https://arxiv.org/abs/2305.09636](https://arxiv.org/abs/2305.09636)

[https://google-research.github.io/seanet/soundstorm/examples/](https://google-research.github.io/seanet/soundstorm/examples/)

1

이 논문에서는 효율적인 고품질 오디오 생성을 위한 새로운 방법인 SoundStorm에 대해 설명합니다.

오디오 생성의 과제는 지각 품질과 런타임 사이의 균형을 맞추는 것입니다. 고품질 오디오를 생성하려면 오디오의 대규모 개별 표현 또는 긴 토큰 시퀀스를 모델링해야 합니다. 이로 인해 코드북 크기가 커지거나(메모리가 많이 필요함), 특히 주의 기반 모델의 경우 긴 시퀀스로 인해 계산 문제가 발생합니다.

긴 오디오 토큰 시퀀스 생성 문제를 해결하기 위해 저자는 효율적인 주의 메커니즘, 비회귀적(병렬) 디코딩 방식, 신경 코덱에서 생성되는 오디오 토큰의 구조에 적합한 맞춤형 아키텍처 등 세 가지 접근 방식을 제안합니다.

SoundStorm은 오디오 토큰 시퀀스의 특수한 구조에 초점을 맞춰 이러한 문제를 해결합니다. 사운드스트림과 엔코덱은 모두 오디오 압축을 위해 잔여 벡터 양자화(RVQ)라는 프로세스를 사용합니다. 이 방식은 계층적 토큰 구조를 유도하며, 더 미세한 RVQ 레벨의 토큰은 지각 품질에 덜 기여하므로 보다 효율적인 모델링 및 디코딩 체계를 구축할 수 있습니다.

사운드스톰 방식은 이러한 오디오 토큰의 계층적 구조에 적합한 아키텍처와 비회귀적 병렬 신뢰도 기반 디코딩 체계를 사용합니다. 오디오의 계층적 토큰 구조를 고려하여 마스킹된 오디오 토큰을 예측하도록 훈련된 양방향 주의 기반 컨포머를 사용합니다.

추론하는 동안 SoundStorm은 모든 오디오 토큰을 마스킹한 상태로 시작하여 여러 반복에 걸쳐 RVQ를 레벨별로 채우고, 한 레벨 내에서 단일 반복 동안 여러 토큰을 병렬로 예측합니다. 추론 절차를 모방한 훈련용 마스킹 체계를 지원합니다.

SoundStorm은 거친 음향 모델과 미세한 음향 모델을 대체하여 AudioLM의 음향 생성기 역할을 할 수 있습니다. 이 기능은 AudioLM의 계층적 자동 회귀 음향 제너레이터보다 훨씬 빠르며, 일관성이 개선된 동일한 품질을 제공합니다.

또한 SPEAR-TTS의 텍스트-시맨틱 모델링 단계와 결합하면 사운드스톰은 고품질의 자연스러운 대화를 합성할 수 있어 음성 콘텐츠, 화자 목소리, 화자 전환을 제어할 수 있습니다. 이 방법은 단일 TPU-v4에서 단 2초 만에 30초 분량의 대화를 합성할 수 있습니다.

2

이 발췌문에서 저자는 신경 오디오 코덱의 토큰 모델링, RVQ 인식 아키텍처, 병렬 디코딩 등 오디오 생성에 사용되는 여러 기술과 도구에 대해 설명합니다.

비지도 음성 임베딩은 이산화 후에도 풍부한 오디오 신호를 저프레임률로 표현할 수 있습니다. SoundStream과 같은 신경 오디오 코덱은 이러한 이산 모델링을 멀티 스피커 음성, 피아노, 음악, 음향 효과 등 다양한 오디오 신호로 확장했습니다. AudioLM 도구는 계층적 시퀀스 간 접근 방식을 적용하여 높은 수준의 시맨틱 토큰을 생성한 다음 이를 사용하여 SoundStream 코덱 토큰을 예측합니다. 이 접근 방식은 음성 및 음악 모델링에 대한 가능성을 보여 주었지만 계산 비용이 높아 생성된 오디오의 길이와 품질이 제한됩니다. SoundStorm은 신경 코덱의 다단계 토큰을 병렬로 모델링하여 이 문제를 해결함으로써 속도를 크게 개선하고 더 길고 고품질의 오디오를 생성할 수 있습니다.

아키텍처 측면에서 RVQ 토큰 시퀀스는 일반적으로 동일한 RVQ 입력 프레임에 해당하는 임베딩을 합산하여 모델링합니다. AudioGen과 같은 툴은 트랜스포머와 다양한 RVQ 레벨을 위한 별도의 헤드에서 이 접근 방식을 사용했습니다. 이렇게 하면 추론 속도가 크게 빨라지지만 텍스트-오디오 생성 성능은 비슷한 비트 전송률과 재구성 품질을 가진 다른 모델에 비해 떨어집니다. VALL-E 모델은 첫 번째 RVQ 레벨 토큰을 자동 회귀적으로 예측하고 후속 레벨은 비자동 회귀적으로 예측하는 하이브리드 접근 방식을 사용합니다. 이 모델은 또한 레벨별 그리디 디코딩을 사용합니다.

![훈련을 위한 SoundStorm 아키텍처 및 마스킹 스킴 (프롬프트 없음). 이 모델은 동일한 SoundStream 프레임에 대응하는 토큰의 임베딩을 합산하여 입력 시퀀스 길이를 줄입니다. 훈련 중에는 RVQ 레벨 q가 샘플링되며 (그림에서는 Q = 3 레벨 중 q = 2), 레벨 q에서 무작위로 샘플링된 토큰의 부분 집합과 모든 RVQ 레벨 q + 1, . . . , Q의 토큰이 함께 마스킹됩니다. 손실은 레벨 q에서 마스킹된 토큰에 대해서만 계산됩니다.](SoundStorm%20Efficient%20Parallel%20Audio%20Generation%200c96d4170fff4b4d913eae8d6f5bb6f3/Untitled.png)

훈련을 위한 SoundStorm 아키텍처 및 마스킹 스킴 (프롬프트 없음). 이 모델은 동일한 SoundStream 프레임에 대응하는 토큰의 임베딩을 합산하여 입력 시퀀스 길이를 줄입니다. 훈련 중에는 RVQ 레벨 q가 샘플링되며 (그림에서는 Q = 3 레벨 중 q = 2), 레벨 q에서 무작위로 샘플링된 토큰의 부분 집합과 모든 RVQ 레벨 q + 1, . . . , Q의 토큰이 함께 마스킹됩니다. 손실은 레벨 q에서 마스킹된 토큰에 대해서만 계산됩니다.

추론 시간을 개선하기 위한 방법으로 병렬 디코딩도 제안되었습니다. 이 접근 방식은 입력 시퀀스에 대한 비인과적 주의를 허용하며 텍스트, 이미지 및 비디오 생성에 사용되었습니다. 이 접근법의 한 예로, 저자들은 제안한 디코딩 체계를 잔여 양자화에 의해 생성된 시퀀스에 대한 MaskGIT의 디코딩을 확장한 것으로 볼 수 있다고 제안합니다.

3

이 발췌문에서는 오디오 시퀀스를 생성하는 접근 방식인 SoundStorm에 대해 설명합니다. SoundStorm은 이산 토큰의 시퀀스인 컨디셔닝 신호를 수신하고 오디오 파형으로 다시 변환할 수 있는 SoundStream 토큰을 출력합니다. 컨디셔닝 신호는 사운드스트림 프레임과 시간적으로 일치하거나 동일한 속도로 업샘플링할 수 있어야 합니다.

SoundStorm의 아키텍처는 다음과 같이 작동합니다:

시간 정렬된 컨디셔닝 토큰은 프레임 수준에서 사운드스트림 토큰과 인터리빙됩니다.
그 결과 시퀀스가 임베딩되고 컨디셔닝 토큰을 포함하여 동일한 프레임에 해당하는 임베딩이 합산됩니다.
그 결과 연속 임베딩은 컨포머로 전달됩니다.
컨포머에서 양방향 셀프 어텐션의 시퀀스 길이는 사운드스트림 프레임 수에 따라 결정되며 RVQ 레벨 수와는 무관하므로 오디오 길이가 길어질 수 있습니다.
출력 측에서는 Q 고밀도 레이어가 헤드로 사용되어 타겟 SoundStream 토큰을 생성합니다.
또한 사운드스톰은 마스크GIT의 병렬 디코딩 체계를 확장하는 마스킹 체계를 트레이닝에 사용합니다. 마스킹 체계는 다음과 같이 작동합니다:

타임스텝이 무작위로 샘플링됩니다.
이 타임스텝 이전에는 어떤 토큰도 마스킹되지 않으며, 조건부 토큰은 마스킹되지 않습니다.
프롬프트 구분 기호 시간 간격, 현재 RVQ 레벨 및 마스크가 샘플링됩니다.
현재 RVQ 수준에서 선택된 비프롬프트 토큰과 더 미세한 RVQ 수준에서 선택된 모든 비프롬프트 토큰이 마스킹됩니다.
모델은 실측값 토큰을 대상으로 교차 엔트로피 손실을 사용하여 훈련되며, 손실은 q번째 RVQ 레벨 내의 마스킹된 토큰에 대해서만 계산됩니다.
사운드스톰의 반복 병렬 디코딩 방식은 프롬프트(제공된 경우)의 토큰을 제외한 모든 사운드스트림 토큰을 마스킹한 상태에서 시작합니다. 그런 다음 토큰을 거친 순서에서 미세한 순서로 RVQ 레벨별로 샘플링합니다. RVQ 레벨 내에서 여러 번의 포워드 패스가 수행되고 코사인 스케줄에 따라 신뢰도 점수에 따라 마스킹된 위치의 후보가 샘플링되고 유지됩니다. 각 RVQ 레벨 내의 마지막 반복에는 그레디 디코딩이 사용되며, 이는 인지된 오디오 품질을 개선하는 것으로 밝혀졌습니다. RVQ 레벨 단위로 디코딩을 수행하면 더 세밀한 레벨에서 조건부 독립성 가정을 활용할 수 있으므로 디코딩이 더 세밀한 RVQ 레벨로 진행됨에 따라 필요한 포워드 패스가 줄어듭니다.

4

제공하신 텍스트는 개별 토큰에서 오디오 신호를 생성하는 새로운 모델인 SoundStorm에 대해 논의하고 있는 것으로 보입니다. SoundStorm은 AudioLM, SPEAR-TTS 또는 MusicLM과 같은 모델의 음향 발생기를 대체할 수 있습니다. 이 모델의 아키텍처는 컨디셔닝 토큰(시맨틱 토큰 시퀀스)이 인터리빙된 SoundStream 토큰의 컨포머 처리 시퀀스를 포함합니다.

SoundStorm을 훈련하기 위해 MaskGIT에서 영감을 얻은 마스킹 체계가 활용됩니다. 여기서는 타임스텝이 샘플링되고 그 이전의 토큰은 마스킹되지 않으며, 그 후 코사인 스케줄에 따라 특정 잔여 벡터 양자화(RVQ) 레벨과 마스크가 샘플링됩니다. 트레이닝은 실측 데이터 토큰을 대상으로 교차 엔트로피 손실을 통해 진행됩니다. 예측 중에 반복 병렬 디코딩 전략이 적용되며, 프롬프트 토큰(제공된 경우)을 제외한 모든 SoundStream 토큰을 마스킹한 상태에서 시작합니다.

SoundStorm 실험에서는 초당 50프레임을 생성하는 SoundStream 코덱과 12레벨의 RVQ를 사용하여 6000bps의 비트레이트를 생성합니다. SoundStorm은 LibriLight 데이터 세트에서 학습되며 디코딩은 반복 샘플링과 그리디 디코딩을 혼합하여 사용합니다.

SoundStorm과 AudioLM의 음향 생성기 단계를 비교한 결과, 프롬프트 시나리오와 프롬프트가 없는 시나리오 모두에서 단어 오류율(WER)과 문자 오류율(CER) 측면에서 개선된 성능을 보여줍니다. 또한 사운드스톰은 음성 보존 기능도 크게 개선되었습니다. 또한 SoundStorm은 시간이 지남에 따라 특히 긴 오디오 샘플에 대해 음향 일관성을 유지합니다.

!['long' 스플릿의 LibriSpeech 테스트-클린 샘플에서 프롬프트와 생성된 오디오 사이의 음향 일관성. 그림자 영역은 사분위수 범위를 나타냅니다.](SoundStorm%20Efficient%20Parallel%20Audio%20Generation%200c96d4170fff4b4d913eae8d6f5bb6f3/Untitled%201.png)

'long' 스플릿의 LibriSpeech 테스트-클린 샘플에서 프롬프트와 생성된 오디오 사이의 음향 일관성. 그림자 영역은 사분위수 범위를 나타냅니다.

SoundStorm 모델의 런타임은 AudioLM의 음향 제너레이터보다 훨씬 짧습니다. 또한 실험에서는 다양한 RVQ 레벨에서 디코딩 반복 횟수가 오디오 품질에 미치는 영향을 살펴본 결과, 첫 번째 RVQ 레벨에서는 16회 반복하고 이후 레벨에서는 욕심 많은 디코딩을 수행하는 것이 최적의 결과를 제공한다는 결론을 내렸습니다.

5

이 섹션에서는 여러 화자 교체와 긴 시간 동안 화자의 동일성을 유지해야 하는 음성 대화 합성에 SoundStorm을 적용하는 방법에 대해 설명합니다. 이 과정은 SPEAR-TTS에서 사용하는 접근 방식과 유사합니다.

30초 단위로 분할된 약 10만 시간의 대화 코퍼스가 수집되었습니다. 각 세그먼트에 대한 트랜스크립트를 생성하기 위해 ASR 시스템을 사용했으며, 여기에는 화자 턴 주석이 포함되었습니다. 시맨틱 토큰은 데이터 세트에 대해 0.6억 개의 파라미터 BEST-RQ 모델을 훈련하고 이 모델의 레이어 13의 활성화에 대해 K-평균 클러스터링을 사용하여 추출했습니다.

24kHz에서 작동하는 SoundStream 코덱은 12 RVQ로 초당 50프레임을 생성하도록 훈련되었습니다. 텍스트-시맨틱 토큰 매핑에는 텍스트의 바이트 수준 표현을 취하고 중복되지 않은 시맨틱 토큰을 예측하는 ByT5 대형 트랜스포머 모델이 사용되었습니다. 디코딩은 온도 샘플링을 통해 수행되었습니다.

텍스트-시맨틱 모델과 SoundStorm은 대화 말뭉치에 대해 10회에 걸쳐 훈련되었습니다. 추론을 위해 훈련 중에 보이지 않는 화자의 짧은 대화가 모델의 프롬프트로 사용되었고, 이러한 프롬프트의 연속으로 텍스트 트랜스크립트가 생성되었습니다. 텍스트 대본은 텍스트-시맨틱 모델에 입력되었고, 그 출력은 두 단계 모두에 사용된 화자 프롬프트와 함께 사운드스톰에 입력되었습니다.

이 접근 방식은 대본에서 필러 단어가 발생할 때 불일치를 생성하고 턴 마커 기호를 삽입하여 화자 전환을 제어할 수 있는 기능을 통해 고품질의 자연스러운 대화 시퀀스를 생성하는 것으로 밝혀졌습니다. 30초 분량의 세그먼트를 합성하는 데 걸린 총 런타임은 약 2초였으며, 독자들은 함께 제공되는 웹페이지에서 생성된 샘플을 들어볼 수 있었습니다.

6

이 백서에서는 개별 컨디셔닝 토큰에서 고품질 오디오를 효율적으로 합성하도록 설계된 모델인 SoundStorm을 소개합니다. SoundStorm의 성능은 특히 긴 오디오 샘플에서 속도와 시간적 일관성 측면에서 AudioLM의 음향 제너레이터를 훨씬 능가합니다.

SPEAR-TTS와 유사한 텍스트-시맨틱 토큰 모델을 SoundStorm과 통합하여 텍스트-음성 합성을 더 긴 컨텍스트로 확장함으로써 화자가 여러 번 바뀌는 자연스러운 대화를 생성하는 데 성공했습니다. 이를 통해 화자의 목소리와 생성된 콘텐츠를 모두 효과적으로 제어할 수 있습니다.

저자들은 AudioLM과 SPEAR-TTS의 음향 생성 파이프라인을 대체하기 위해 SoundStorm을 사용할 것을 제안합니다. 그러나 저자들은 생성된 오디오 샘플에 잠재적인 편향이 있을 수 있으며, 이는 표현된 악센트와 음성 특성 등 훈련 데이터에 존재하는 편향으로 인해 발생할 수 있다고 인정합니다. 이 연구에서 생성된 샘플은 프롬프트를 통해 화자 특성을 안정적으로 제어할 수 있음을 보여 주지만, 향후 연구에서는 훈련 데이터와 그 한계에 대한 보다 포괄적인 분석이 필요합니다.

생체 인식 우회나 사칭과 같은 SoundStorm의 음성 모방 기능의 잠재적 오용 가능성도 다루고 있습니다. 저자는 이러한 오용에 대한 안전장치가 필요하다고 강조합니다. 저자들은 전용 분류기가 98.5%의 성공률로 사운드스톰에서 생성된 오디오를 탐지할 수 있었다고 보고하며, 이는 사운드스톰이 이전 연구자들이 이미 논의한 것 외에 추가적인 위험을 초래하지 않을 가능성이 높다는 것을 시사합니다.

저자들은 더 많은 커뮤니티가 오디오 생성 연구에 더 쉽게 접근할 수 있도록 하는 것이 중요하다는 점을 강조하며 결론을 내립니다. 저자들은 오디오 워터마킹과 같이 합성 음성을 탐지하는 다른 방법을 연구하여 SoundStorm의 잠재적 적용이 책임 있는 AI 원칙을 엄격하게 준수할 수 있도록 하겠다는 의사를 표명했습니다.

- 요약
    
    이 백서에서는 개별 컨디셔닝 토큰에서 고품질 오디오를 효율적으로 합성하는 모델인 SoundStorm을 소개하는 데 중점을 둡니다. 다음은 연구 과정과 연구 결과를 단계별로 분석한 내용입니다:
    
    개념 소개: 이 논문에서는 개별 컨디셔닝 토큰을 해석하여 고품질 오디오를 생성하도록 설계된 모델인 SoundStorm에 대해 소개합니다. 저자들은 SoundStorm의 속도와 긴 오디오 샘플에서 시간적 일관성을 유지하는 능력이 뛰어나 AudioLM의 음향 제너레이터보다 우수하다고 설명합니다.
    
    SPEAR-TTS와의 통합: 연구진은 SoundStorm을 SPEAR-TTS와 유사한 텍스트-시맨틱 토큰 모델과 결합하여 텍스트 음성 합성을 더 긴 대화 및 컨텍스트에 맞게 확장할 수 있도록 했습니다. 이 조합을 통해 다중 턴 대화 내에서 화자 특성과 콘텐츠를 제어할 수 있습니다.
    
    훈련 및 데이터 수집: 저자들은 약 10만 시간 분량의 대화 코퍼스를 30초 분량으로 나누어 생성했다고 설명합니다. 자동 음성 인식(ASR) 기술을 사용하여 각 세그먼트에 대한 트랜스크립트를 생성하고 화자 전환을 적절히 표시했습니다.
    
    BEST-RQ 및 ByT5 구현: 팀은 대화 데이터 세트에 대해 BEST-RQ 모델을 학습시키고 이를 사용하여 의미 토큰을 추출합니다. 텍스트와 시맨틱 토큰을 매핑하기 위해 ByT5 대형 트랜스포머를 사용합니다. 게시된 ByT5 체크포인트에서 사전 학습된 인코더가 사용되며, 모델은 텍스트의 바이트 수준 표현을 기반으로 중복되지 않은 시맨틱 토큰을 예측합니다.
    
    추론 및 출력: 연구원들은 보이지 않는 화자와의 짧은 대화를 모델의 프롬프트로 기록하고 이 프롬프트에 따라 텍스트 트랜스크립트를 생성합니다. 이러한 텍스트 트랜스크립트는 텍스트-시맨틱 모델에 입력되며, 출력은 두 단계 모두에서 화자 프롬프트를 활용하여 SoundStorm에 전달됩니다.
    
    평가 및 분석: 생성된 대화가 평가되어 적절한 순간에 발생하는 불협화음과 함께 고품질의 자연스러운 사운드를 보여줍니다. 시스템은 트랜스크립트에 턴 마커 기호를 삽입하여 화자 전환을 제어할 수 있습니다.
    
    편견 및 잠재적 오용에 대한 인식: 저자는 훈련 데이터의 특성으로 인해 모델에서 생성된 오디오 샘플에 잠재적인 편향이 있을 수 있음을 인식합니다. 또한 음성 모방이 오용될 수 있음을 인정하고 안전장치의 필요성을 강조합니다.
    
    합성 음성 탐지: 전용 분류기를 사용하여 합성 음성을 탐지하고 있으며, 98.5%의 탐지율을 달성하여 SoundStorm이 이전에 논의된 것 외에 추가적인 위험을 초래할 가능성은 낮다고 합니다.
    
    향후 계획: 이 논문은 오디오 워터마킹과 같은 합성 음성을 탐지하는 다른 방법을 모색하여 책임 있는 AI 원칙을 준수하고 해당 분야의 연구에 더 쉽게 접근할 수 있도록 하겠다는 연구진의 계획으로 마무리됩니다.
    
    이 논문에서 SoundStorm을 발표함으로써 저자들은 효율적이면서도 고품질 오디오를 생성할 수 있는 모델을 제공함으로써 오디오 합성 분야에 중요한 공헌을 했습니다. 이 접근 방식은 음성 합성의 복잡성을 해결하는 데 딥러닝을 효과적으로 활용할 수 있음을 보여주며 향후 이 분야의 발전을 위한 기반을 마련했습니다.