# A Shading-Guided Generative Implicit Model for Shape-Accurate 3D-Aware Image Synthesis

[https://xingangpan.github.io/projects/ShadeGAN.html](https://xingangpan.github.io/projects/ShadeGAN.html)

[https://arxiv.org/abs/2110.15678](https://arxiv.org/abs/2110.15678)

[https://github.com/XingangPan/ShadeGAN](https://github.com/XingangPan/ShadeGAN)

- Oct 2021

이 논문에서는 3D 인식 이미지 합성 분야, 특히 2D 이미지에서 사실적인 3D 형상을 생성하는 분야의 개선 사항에 대해 설명합니다. 기존 방식은 모양-색상 모호함이라는 문제로 인해 정확한 3D 모양을 캡처하는 데 어려움을 겪었습니다.

저자들은 이 모호성을 해결하는 새로운 음영 유도 생성 모델을 제안합니다. 이 모델의 기본 개념은 정확한 3D 셰이프는 다양한 조명 조건에서도 사실적인 렌더링을 생성해야 한다는 것입니다. 이를 위해 조명을 직접 모델링하고 다양한 조명 조건에서 셰이딩을 적용하여 다중 조명 제약 조건을 통합합니다.

그런 다음 정제된 이미지는 실제 이미지와 합성 이미지를 구분하는 데 도움을 주는 AI 시스템인 판별기로 평가됩니다.

이 프로세스의 계산 부하, 특히 표면 법선(음영 및 빛 반사의 필수 요소) 계산을 고려하여 저자는 학습 및 추론 시간을 각각 24%와 48%까지 크게 단축하는 효율적인 볼륨 렌더링 전략도 개발했습니다.

실험 결과, 이 새로운 접근 방식은 정확한 기본 3D 형상을 캡처하면서 고품질의 3D 인식 이미지 합성을 달성하는 것으로 나타났습니다. 이 방법은 3D 형상 재구성에서 기존 방법보다 성능이 뛰어나며 이미지 재조명에도 사용할 수 있습니다.

저자들은 다른 사람들이 이 코드를 사용하고 구축할 수 있도록 Github에 코드를 공개할 예정입니다.

### 1 Introduction

이 백서에서는 심층 생성 모델을 사용한 3D 인식 이미지 합성, 특히 2D 이미지에서 정확한 3D 형상을 생성하는 데 중점을 둔 개선 사항을 소개합니다.

기존에는 StyleGAN, BigGAN과 같은 심층 생성 모델이 2D 자연 이미지 합성에는 뛰어났지만 3D 물체의 새로운 시각을 일관된 방식으로 생성하는 데는 어려움을 겪었습니다. 또한 3D 형상을 명시적으로 표현하는 기능이 부족했습니다. 최근 일부 모델은 3D 장면을 신경 방사장으로 표현하여 진전을 이루었습니다. 그러나 이러한 모델은 모양-색상 모호성으로 알려진 훈련 전략 모호성으로 인해 거칠고 부정확한 3D 모양을 생성하는 경우가 많습니다.

![(a) pi-GAN[4]과 같은 이전 방법은 3D 표현이 판별자에게 가짜 이미지로 다른 시점에 투영되는 "다중 뷰 제약"에 의존합니다. 추출된 3D 메쉬는 모양과 색상의 모호함으로 인해 부정확한 경우가 많습니다. (b) 제안된 접근 방식 ShadeGAN은 다양한 조명 조건에서 3D 표현이 사실적으로 보이도록 동기를 부여하는 "다중 조명 제약 조건"을 추가로 채택합니다. 이 제약 조건은 모호함을 효과적으로 해결하여 보다 자연스럽고 정밀한 3D 모양을 생성합니다.](A%20Shading-Guided%20Generative%20Implicit%20Model%20for%20Sha%207abaf5e407444234ae9fe3a7aef68abe/Untitled.png)

(a) pi-GAN[4]과 같은 이전 방법은 3D 표현이 판별자에게 가짜 이미지로 다른 시점에 투영되는 "다중 뷰 제약"에 의존합니다. 추출된 3D 메쉬는 모양과 색상의 모호함으로 인해 부정확한 경우가 많습니다. (b) 제안된 접근 방식 ShadeGAN은 다양한 조명 조건에서 3D 표현이 사실적으로 보이도록 동기를 부여하는 "다중 조명 제약 조건"을 추가로 채택합니다. 이 제약 조건은 모호함을 효과적으로 해결하여 보다 자연스럽고 정밀한 3D 모양을 생성합니다.

이 문제를 해결하기 위해 저자는 음영 유도 생성 모델인 ShadeGAN을 제안합니다. 이 모델은 빛과 모양의 상호 작용, 즉 음영을 명시적으로 모델링하여 보다 정확한 3D 모양을 학습합니다. 이 새로운 접근 방식은 정확한 3D 도형이 다양한 시점뿐만 아니라 다양한 조명 조건에서도 사실적으로 보여야 한다는 '다중 조명 제약 조건'이라는 개념입니다. 이 제약 조건을 충족시킴으로써 셰이드간은 보다 정확한 3D 형태를 추론할 수 있습니다.

이 셰이딩 프로세스를 구현하려면 집약적인 계산이 필요합니다. 이 문제를 해결하기 위해 저자는 표면 추적이라는 효율적인 렌더링 기법을 고안하여 렌더링된 이미지의 품질을 저하시키지 않으면서도 학습 및 추론의 계산 오버헤드를 크게 줄였습니다.

여러 데이터 세트에 대한 실험 결과, ShadeGAN은 이전 방법보다 더 정확한 기본 3D 모양을 캡처하면서 사실적인 이미지를 합성할 수 있음을 확인했습니다. 3D 형상 재구성 작업에서 다른 기법보다 성능이 뛰어나며 이미지 재조명에도 사용할 수 있어 알베도에 근사한 음영과 색상의 엉킴을 해소할 수 있습니다.

이 백서에서는 1) 3D 인식 이미지 합성에서 형태-색상 모호성을 해결하는 모델인 ShadeGAN, 2) 학습 및 추론 시간을 절약하는 효율적인 렌더링 기법, 3) 이미지 합성에서 자연스러운 재조명 효과를 위해 셰이딩과 색상을 분리하는 ShadeGAN의 기능을 소개합니다.

### 2 Related Work

이 백서에서는 신경 볼륨 렌더링과 생성 모델에 초점을 맞춘 3D 인식 이미지 합성의 발전에 대해 설명합니다.

신경 방사 필드(NeRF) 개념에서 시작된 신경 볼륨 렌더링은 3D 장면을 표현하고 새로운 뷰를 생성하는 데 널리 사용되고 있습니다. 신경망과 볼륨 렌더링을 결합하여 고품질 이미지 합성을 생성합니다. 조명 모델링, 음영으로 반사율을 분리하고 정적 장면의 렌더링을 가속하는 등 여러 가지 개선 사항이 NeRF에 제안되었습니다. 그러나 이러한 기술은 일반적으로 비정형 이미지에서 학습하고 동적 장면을 표현하는 제너레이티브 모델에 적용할 때 문제가 있습니다. 저자들의 연구는 볼륨 렌더링 기반 제너레이티브 모델에서 조명을 모델링하여 정확한 3D 형상의 학습을 향상시키는 최초의 시도를 제시합니다.

생성적 적대 신경망(GAN)은 고해상도의 사실적인 이미지를 생성하는 데는 성공했지만 카메라 시점에 대한 제어가 부족했습니다. 최근 연구에서는 3D 표현을 GAN에 통합하여 3D 인식 합성을 생성하려는 시도가 있었습니다. 그러나 일부 접근 방식은 해석할 수 없는 3D 복셀(3D 모양으로 변환할 수 없음)을 생성하고, 해석 가능한 3D 복셀과 메시를 학습하는 다른 접근 방식은 종종 시각적 품질이 떨어지거나 눈에 띄는 왜곡을 초래합니다. 이와는 대조적으로 저자들은 렌더링 과정에서 조명을 모델링하여 3D 형상 정확도를 개선하고 3D 인식 이미지 합성을 개선하는 것을 목표로 합니다.

이 논문에서는 2D 이미지에서 3D 물체 모양을 비지도 학습하는 방법도 논의합니다. 일부 접근 방식은 3D 모양 템플릿이나 2D 키포인트를 사용하여 학습을 용이하게 하지만, 저자는 2D 이미지만 사용할 수 있는 더 까다로운 시나리오에 초점을 맞춥니다. 이들은 강력한 정규화나 각 이미지의 시점을 추론해야 하는 GAN 기반 접근 방식에 의존하지 않습니다. 실험을 통해 최근의 최신 방법보다 우수한 성능이 입증되었습니다.

### 3 Methodology

이 연구에서는 2D 이미지 모음을 사용한 3D 인식 이미지 합성에 대해 설명합니다. 주요 초점은 음영 모델링, 즉 조명과 모양 간의 상호 작용을 통해 3D 물체 모양을 더 잘 이해할 수 있도록 하는 것입니다.

첫 번째 파트에서는 다층 퍼셉트론(MLP) 네트워크를 사용하여 3D 장면을 빛의 필드(래디언스)로 표현하는 방법인 NeRF(신경 래디언스 필드)의 기본 사항을 설명합니다. 이 방법은 다양한 카메라 각도에서 촬영한 이미지를 렌더링하고 고품질의 새로운 뷰를 생성하는 데 사용됩니다.

다음 섹션에서는 셰이딩 가이드 생성 모델을 소개합니다. 이 모델은 잠재 코드(무작위 입력)를 추가하여 NeRF를 확장하고 색상을 직접 출력하는 대신 다양한 조명 조건에서 음영 처리할 수 있는 사전 코사인 색상 용어를 출력합니다. 이를 통해 모델은 더 나은 음영과 더 정확한 오브젝트 모양으로 더욱 사실적인 3D 이미지를 생성할 수 있습니다.

또한 이 모델은 조명 조건을 무작위로 샘플링하여 다중 조명 제약 조건을 도입하여 3D 오브젝트 모양의 정확도를 향상시킵니다. 훈련 과정에는 생성적 적대 신경망(GAN)과 유사하게 이미지를 생성하는 제너레이터와 진위 여부를 판단하는 판별기가 포함됩니다.

![Untitled](A%20Shading-Guided%20Generative%20Implicit%20Model%20for%20Sha%207abaf5e407444234ae9fe3a7aef68abe/Untitled%201.png)

핵심 논의 사항은 볼륨 렌더링을 얻은 후 수행되는 셰이딩에 관한 것입니다. 연구진은 각 로컬 포인트에서 셰이딩을 수행하는 실험을 했지만 각 포인트에서 볼륨 밀도 기울기의 크기를 무시하는 등 특정 문제로 인해 결과가 최선이 아니라는 사실을 발견했습니다. 이들이 사용한 램버시안 셰이딩 모델은 근사치이므로 생성된 이미지와 실제 이미지의 분포 사이에 차이가 발생할 수 있습니다. 이를 완화하기 위해 조명 조건에 따라 출력 색상 용어를 조절하는 방법을 제안하여 결과를 개선했습니다.

마지막으로, 보다 효율적인 볼륨 렌더링을 위해 잠재 코드를 기반으로 물체 표면의 위치를 추정하는 방법을 학습하는 표면 추적 네트워크를 제안합니다. 이 기법은 물체 표면에 가까운 영역에 연산을 집중함으로써 계산 비용을 절감할 수 있습니다. 표면 추적 네트워크는 생성기 및 판별기와 함께 최적화되어 전체 프로세스를 더욱 효율적으로 만듭니다.

### 4 Experiments

저자들은 3D 인식 이미지 합성을 위한 새로운 방법인 ShadeGAN을 소개합니다. 이전 방법과 비교했을 때 ShadeGAN은 조명 조건을 명시적으로 제어하면서 정확한 3D 모양을 학습하는 능력이 뛰어납니다. 실험에 사용된 데이터 세트에는 2D RGB 이미지가 포함된 CelebA, BFM, Cats가 포함됩니다.

아키텍처 측면에서는 SIREN 기반 MLP를 생성기로, CNN을 판별기로 사용합니다. ShadeGAN의 구현에는 실제 데이터의 조명 조건을 추정하고 이러한 조건에 대한 다변량 가우스 분포를 맞추는 작업이 포함됩니다.

GRAF 및 pi-GAN과 비교할 때 ShadeGAN은 사실적인 3D 일관된 이미지를 합성할 뿐만 아니라 더 정확한 3D 모양과 표면 법선을 학습합니다. 이는 제안된 다중 조명 제약 조건의 효과를 나타냅니다. 또한 ShadeGAN이 알베도 및 디퓨즈 셰이딩 구성 요소를 학습할 수 있음을 보여줍니다.

이미지와 그에 상응하는 깊이 맵을 생성하여 ShadeGAN의 성능을 정량적으로 평가합니다. 저자는 스케일 불변 깊이 오차(SIDE)와 평균 각도 편차(MAD) 메트릭을 사용하여 학습된 3D 형상의 품질을 측정합니다. 그 결과 ShadeGAN이 GRAF와 pi-GAN보다 훨씬 뛰어난 성능을 보였습니다.

또한 저자는 조명 사전 설정과 제안된 효율적인 볼륨 렌더링 기법과 같은 측면을 연구하면서 ShadeGAN에서 디자인 선택의 효과를 조사합니다. 또한 ShadeGAN은 조명을 인식하는 이미지 합성 기능을 제공하여 조명 조건을 명시적으로 제어할 수 있습니다.

마지막으로, 공간적으로 변화하는 오브젝트의 머티리얼 속성을 고려하고 더 정교한 셰이딩 모델을 통합하여 더 잘 풀린 생성 반사율 필드를 학습하는 등 ShadeGAN의 잠재적인 개선 사항을 강조합니다.

### 5 Conclusion

저자들은 2D 데이터에서 정확한 3D 이미지를 생성하기 위한 새로운 생성 모델인 ShadeGAN을 소개합니다. 이 모델은 다중 조명 제약 조건을 통합하여 2D 이미지에서 3D 형상을 보다 정확하게 캡처할 수 있도록 함으로써 이전 작업을 개선합니다. 또한 이미지 생성 과정에서 조명 조건을 조정할 수 있어 생성된 이미지에 더욱 사실적인 조명 효과를 구현할 수 있습니다.

ShadeGAN의 효율성을 높이기 위해 개발자들은 경량 표면 추적 네트워크를 설계에 포함시켰으며, 이를 통해 학습 및 추론 프로세스의 속도를 높일 수 있습니다.

ShadeGAN을 통해 보다 정확한 3D 표현이 가능한 생성 모델을 생성하면 컴퓨터 비전 및 그래픽 분야에서 폭넓게 응용할 수 있습니다. 이 연구는 이러한 목표를 추구하는 데 있어 상당한 진전을 이루었습니다.

- 학습
    
    제너레이터와 판별자: pi-GAN의 설계에 따라 ShadeGAN의 제너레이터는 256개의 유닛으로 구성된 완전 연결 및 FiLM-SIREN 레이어로 구성된 MLP입니다. 체적 밀도와 알베도 예측을 위한 두 가지 분기가 있습니다. 잠재 코드 입력인 z는 생성기로 전송되기 전에 매핑 네트워크를 통해 처리됩니다. 판별기는 pi-GAN과 유사한 컨볼루션 신경망(CNN) 아키텍처를 사용하며, 여기에는 CoordConv 레이어와 잔여 연결이 포함됩니다.
    
    표면 추적 네트워크: 저자들은 매핑 네트워크의 출력과 카메라 포즈를 입력으로 받아 깊이 맵을 생성하는 디코더 스타일의 CNN인 표면 추적 네트워크에 대해서도 자세히 설명합니다. 컨볼루션 및 디컨볼루션 레이어, 그룹 정규화, 배치 정규화, ReLU 레이어, 업샘플링 레이어, 잔여 블록 등 네트워크에 사용되는 다양한 레이어에 대해서도 설명합니다.
    
    깊이 예측 CNN: 저자들은 BFM 데이터세트에서 모델을 평가하기 위해 입력 이미지의 깊이 맵을 예측하는 추가 CNN도 훈련했습니다. 이 네트워크는 1e-4의 학습률로 30회에 걸쳐 Adam을 사용하여 훈련됩니다.