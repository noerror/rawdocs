# Adding Conditional Control to Text-to-Image Diffusion Models

[https://arxiv.org/abs/2302.05543](https://arxiv.org/abs/2302.05543)

이 논문에서는 추가 입력 조건을 통합하여 사전 학습된 대규모 확산 모델을 제어하도록 설계된 신경망 구조인 ControlNet을 소개합니다. 컨트롤넷은 작업별 조건을 엔드투엔드 방식으로 학습할 수 있어 5만 개 미만의 작은 데이터셋으로 학습할 때도 견고함을 보여줍니다. 훈련 프로세스는 확산 모델을 미세 조정하는 것만큼이나 빠르기 때문에 개인용 디바이스에 적합하며, 강력한 계산 클러스터를 사용할 수 있는 경우 대규모 데이터 세트를 처리할 수 있도록 확장할 수 있습니다.

컨트롤넷을 사용하여 안정적 확산과 같은 대규모 확산 모델을 보강하면 에지 맵, 세분화 맵, 키포인트와 같은 조건부 입력을 포함할 수 있습니다. 이러한 발전으로 대규모 확산 모델을 제어하는 데 사용할 수 있는 방법이 더욱 풍부해져 다양한 분야의 애플리케이션이 더욱 용이해졌습니다.

### 1 Introduction

이 글에서는 텍스트 프롬프트를 기반으로 이미지를 생성하기 위해 대규모 AI 모델을 사용할 때의 문제점과 해결책을 간단히 설명합니다. 세 가지 주요 발견 사항이 있습니다:

특정 작업에 사용할 수 있는 데이터는 일반적으로 대규모 모델을 학습시키는 데 사용되는 데이터보다 작기 때문에 과적합을 방지하고 우수한 성능을 유지하기 위해 특별한 학습 기술이 필요합니다.
이러한 모델을 훈련하는 데 항상 대규모 컴퓨팅 리소스를 사용할 수 있는 것은 아니므로 합리적인 시간과 공간 내에서 특정 작업에 맞게 모델을 최적화하려면 빠른 훈련 방법과 전이 학습과 같은 기법을 사용하는 것이 필수적입니다.
이미지 처리 작업마다 요구 사항과 제약 조건이 다르기 때문에 모든 작업에 단일 방법을 사용하기는 어렵습니다. 다양한 작업에서 좋은 결과를 얻으려면 엔드투엔드 학습이 필요합니다.
이러한 문제를 해결하기 위해 저자는 대규모 이미지 생성 모델을 조정하여 작업별 입력 조건을 학습하는 신경망 아키텍처인 컨트롤넷(ControlNet)을 제안합니다. 컨트롤넷은 '제로 컨볼루션'이라는 고유한 기술을 사용하여 원래의 대규모 모델을 작업별 모델과 연결함으로써 훈련 과정을 더 빠르고 효율적으로 만듭니다. 실험 결과, 컨트롤넷은 개인용 컴퓨터에서 학습한 경우에도 다양한 작업을 처리하고 크고 작은 데이터 세트 모두에서 우수한 성능을 발휘할 수 있는 것으로 나타났습니다.

### 2 Related Work

2.1 하이퍼네트워크와 신경망 구조

하이퍼네트워크는 자연어 처리에서 유래한 개념으로, 작은 신경망이 더 큰 신경망의 가중치에 영향을 미치도록 훈련됩니다. 이 접근 방식은 생성적 적대 신경망 및 기타 머신 러닝 작업을 사용한 이미지 생성에서 성공적이었습니다. 이러한 아이디어에서 영감을 얻은 연구자들은 안정 확산과 같이 큰 모델에 작은 신경망을 연결하여 출력 이미지의 예술적 스타일을 변경하는 방법을 개발했습니다.

컨트롤넷과 하이퍼네트워크는 신경망의 동작에 영향을 미치는 방식이 비슷합니다. 컨트롤넷은 "제로 컨볼루션"이라는 특수한 유형의 컨볼루션 레이어를 사용합니다. 초기 신경망 연구에서는 가우스 분포로 가중치를 초기화할 때의 합리성과 0으로 초기화할 때의 위험성을 포함하여 네트워크 가중치 초기화에 대해 논의했습니다. 최근에는 확산 모델에서 컨볼루션 레이어의 초기 가중치를 조정하여 학습을 개선하는 방법을 연구하고 있는데, 이는 제로 컨볼루션의 개념과 유사합니다.

초기 컨볼루션 가중치 조작은 ProGAN, StyleGAN, 노이즈2노이즈, Stability와 같은 다른 모델에서도 논의되고 있습니다.

2.2 확산 확률 모델

확산 확률 모델은 이미지 생성을 위해 제안되었으며 소규모 및 대규모 모두에서 성공적으로 적용되었습니다. 이 아키텍처는 노이즈 제거 확산 확률론적 모델(DDPM), 노이즈 제거 확산 암시적 모델(DDIM), 점수 기반 확산과 같은 중요한 훈련 및 샘플링 방법으로 개선되었습니다.

이미지 확산 방법은 픽셀 색상을 훈련 데이터로 직접 사용할 수 있습니다. 연구자들은 고해상도 이미지를 처리할 때 계산 능력을 절약하거나 피라미드 기반 또는 다단계 방법을 사용하는 전략을 고려하는 경우가 많습니다. 이러한 방법은 일반적으로 U-net을 신경망 아키텍처로 사용합니다.

확산 모델 훈련에 필요한 계산 능력을 줄이기 위해 잠재 이미지에 대한 아이디어를 바탕으로 잠재 확산 모델(LDM)이 제안되었습니다. 이 접근 방식은 안정 확산으로 더욱 확장되었습니다.

2.3 텍스트-이미지 확산

확산 모델을 텍스트-이미지 생성 작업에 적용하여 고품질 이미지 생성 결과를 얻을 수 있습니다. 이 작업은 CLIP과 같이 사전 학습된 언어 모델을 사용하여 텍스트 입력을 잠재 벡터로 인코딩하는 방식으로 수행됩니다. 텍스트 유도 확산 모델의 몇 가지 예는 다음과 같습니다:

- 글라이드 Glide: 텍스트 프롬프트를 기반으로 이미지 생성 및 편집을 모두 지원하는 모델입니다.
- 디스코 확산 Disco Diffusion: 이전 확산 모델을 기반으로 텍스트 프롬프트를 처리하는 CLIP 안내 구현입니다.
- 안정적인 확산 Stable Diffusion: 텍스트-이미지 생성을 달성하는 잠재 확산의 대규모 구현입니다.
- 이미지 Imagen: 잠재 이미지를 사용하지 않고 피라미드 구조를 사용하여 픽셀을 직접 확산하는 텍스트-이미지 모델입니다.

이 모델은 텍스트 설명을 기반으로 이미지를 생성하는 데 확산 기법을 사용할 수 있는 잠재력을 보여줍니다.

2.4 사전 학습된 확산 모델의 개인화, 커스터마이징 및 제어

최신 이미지 확산 모델은 대부분 텍스트 대 이미지 방식이며, 이러한 모델에 대한 제어를 강화하는 가장 간단한 방법은 텍스트 안내인 경우가 많습니다. 이러한 모델을 제어하는 몇 가지 방법은 다음과 같습니다:

- 텍스트 안내 방식: 텍스트 프롬프트를 사용하여 이미지 생성 프로세스를 제어합니다.
- 클립 기능 조작: 특정 결과를 얻기 위해 사전 학습된 언어 모델의 기능을 조정합니다.
- 색상 레벨 디테일 변형: 이미지 확산 프로세스 자체를 사용하여 색상 수준 세부 사항을 조정합니다.
- 채색: 이미지 확산 알고리즘을 사용하여 이미지의 누락된 부분을 완성하여 결과를 제어할 수 있는 또 다른 방법을 제공합니다.
- 텍스트 반전 및 드림부스: 관련 이미지의 작은 집합을 참조로 사용하여 생성된 이미지를 사용자 지정하거나 개인화합니다.

이러한 기술은 특정 요구 사항이나 선호도에 맞게 사전 학습된 확산 모델의 출력을 제어하고 개인화할 수 있는 다양한 방법을 보여줍니다.

2.5 이미지 간 번역

컨트롤넷과 이미지 간 번역은 일부 중복되는 애플리케이션이 있을 수 있지만, 그 동기는 서로 다릅니다. 이미지 간 번역은 서로 다른 도메인의 이미지 간 매핑을 학습하는 것을 목표로 하는 반면, ControlNet은 작업별 조건으로 확산 모델을 제어하는 데 중점을 둡니다.

Pix2Pix는 이미지 간 번역이라는 개념을 도입했으며, 초기에는 조건부 생성 신경망이 주류를 이루었습니다. 트랜스포머와 비전 트랜스포머(ViT)가 인기를 끌면서 자동 회귀 방법을 사용한 성공적인 결과들이 보고되었습니다. 일부 연구에 따르면 다중 모델 방법은 다양한 번역 작업에서 강력한 생성기를 학습할 수 있습니다.

이미지 간 번역에서 가장 강력한 방법 중 일부는 다음과 같습니다:

- 길들이기 트랜스포머: 이미지 생성 및 이미지 간 번역을 모두 수행할 수 있는 비전 트랜스포머입니다.
- 팔레트: 통합된 확산 기반 이미지 간 번역 프레임워크.
- PITI: 대규모 사전 학습을 활용하여 생성된 결과의 품질을 개선하는 확산 기반 이미지 간 번역 방법입니다.
- 스케치 가이드 확산: 스케치 가이드 확산과 같이 특정 작업을 위해 확산 프로세스를 조작하는 최적화 기반 방법.

이 방법은 다양한 실험에서 테스트되어 이미지 간 번역 작업에서 그 잠재력을 입증했습니다.

### 3 방법

컨트롤넷은 사전 학습된 이미지 확산 모델을 작업별 조건으로 강화하는 신경망 아키텍처입니다. 신경망 블록의 입력 조건을 조작하여 전체 신경망의 전반적인 동작을 제어합니다. 네트워크 블록은 신경망을 구축하는 단위로 자주 사용되는 신경 레이어로 구성됩니다(예: resnet 블록, conv-bn-relu 블록 등).

3.1 ControlNet

![Untitled](Adding%20Conditional%20Control%20to%20Text-to-Image%20Diffus%20b1645e59912b4e29b30c7cf9e2f9dc46/Untitled.png)

컨트롤넷은 원본 파라미터를 잠그고 외부 조건 벡터로 훈련된 훈련 가능한 복사본을 생성합니다. 이렇게 하면 데이터 세트가 작을 때 과적합을 방지하고 대규모 모델의 품질을 유지할 수 있습니다. 신경망 블록은 가중치와 바이어스가 모두 0으로 초기화되는 '제로 컨볼루션'이라는 고유한 유형의 컨볼루션 레이어를 사용하여 연결됩니다.

![Untitled](Adding%20Conditional%20Control%20to%20Text-to-Image%20Diffus%20b1645e59912b4e29b30c7cf9e2f9dc46/Untitled%201.png)

첫 번째 훈련 단계에서 제로 컨볼루션 레이어는 심층 신경 특징에 영향을 미치지 않으므로 신경망 블록의 기능과 품질이 유지됩니다. 추가 최적화는 미세 조정만큼 빠르게 이루어집니다. 경사도 계산이 수행되고 입력 피처가 0이 아닌 한 첫 번째 경사도 하강 반복에서 가중치가 최적화됩니다. 0 컨볼루션은 학습된 방식으로 0에서 최적화된 파라미터까지 점진적으로 증가합니다.

3.2 이미지 확산 모델의 컨트롤넷

대규모 텍스트-이미지 확산 모델인 안정적 확산에 제어망을 적용하여 작업별 조건으로 제어합니다. 안정적 확산은 VQ-GAN과 유사한 전처리 방법을 사용하여 큰 이미지를 작은 잠재 이미지로 변환하여 안정적으로 학습합니다. 컨트롤넷은 컨볼루션 크기에 맞게 이미지 기반 조건을 특징 맵으로 변환합니다. 컨트롤넷은 계산적으로 효율적인 방식으로 U-net의 각 레벨을 제어하여 GPU 메모리를 절약하고 훈련 속도를 높이는 데 사용됩니다.

3.3 훈련

이미지 확산 모델은 샘플을 생성하기 위해 이미지의 노이즈를 점진적으로 제거하는 방법을 학습합니다. 안정적 확산은 잠재 이미지를 훈련 도메인으로 사용합니다. 모델은 시간 단계, 텍스트 프롬프트 및 작업별 조건을 포함한 일련의 조건이 주어지면 노이즈가 있는 이미지에 추가되는 노이즈를 예측하는 네트워크를 학습합니다. 훈련 중에 텍스트 프롬프트의 50%가 빈 문자열로 대체되어 입력 조건 맵에서 의미론적 콘텐츠를 인식하는 ControlNet의 기능이 향상됩니다.

3.4 개선된 훈련

제한된 계산 장치 또는 방대한 데이터 세트가 있는 대규모 GPU와 같은 극단적인 경우 ControlNet 훈련을 개선하기 위한 몇 가지 전략이 논의되었습니다. 소규모 훈련의 경우, 컨트롤넷과 스테이블 디퓨전 간의 연결을 부분적으로 끊으면 컨버전스를 가속화할 수 있습니다. 대규모 훈련에는 강력한 연산 클러스터와 대규모 데이터 세트가 필요합니다. 이 경우 컨트롤넷을 많은 수의 반복으로 훈련한 다음 안정 확산의 모든 가중치를 해제하고 전체 모델을 공동으로 훈련하여 보다 문제에 특화된 모델을 만들 수 있습니다.

3.5 구현

대규모 확산 모델을 다양한 방식으로 제어하기 위해 서로 다른 이미지 기반 조건을 가진 여러 가지 제어망 구현이 제시됩니다:

![Untitled](Adding%20Conditional%20Control%20to%20Text-to-Image%20Diffus%20b1645e59912b4e29b30c7cf9e2f9dc46/Untitled%202.png)

- 캐니 에지: 캐니 에지 감지기를 사용하여 얻은 3M 에지-이미지-캡션 쌍. Nvidia A100 80G에서 600 GPU 시간으로 훈련되었습니다.
- 캐니 에지 (변경): 크기가 다른 하위 집합을 샘플링하여 데이터 세트 스케일의 효과를 테스트합니다.
- Hough Line: 학습 기반 딥 Hough 변환을 사용하여 600만 개의 엣지-이미지-캡션 쌍을 학습합니다. Nvidia A100 80G에서 150 GPU 시간으로 훈련되었습니다.
- HED 경계: HED 경계 감지를 사용하여 3억 개의 에지-이미지-캡션 쌍을 학습했습니다. Nvidia A100 80G에서 300 GPU-시간으로 훈련.
- 사용자 스케치: HED 경계 감지 및 데이터 증강을 사용하여 이미지에서 합성한 500만 개의 낙서-이미지-캡션 쌍입니다. Nvidia A100 80G에서 150 GPU 시간으로 훈련되었습니다.
- 사람 포즈(Openpifpaf): 학습 기반 포즈 추정 방법 Openpifpaf를 사용하여 80만 개의 포즈-이미지-캡션 쌍을 학습합니다. Nvidia RTX 3090TI에서 400 GPU 시간으로 훈련.
- 사람 포즈(Openpose): 학습 기반 포즈 추정 방법 Openpose를 사용하여 200만 개의 포즈-이미지-캡션 쌍을 학습했습니다. Nvidia A100 80G에서 300 GPU 시간으로 훈련.
- 시맨틱 세그멘테이션(COCO): COCO-Stuff 데이터 세트에서 164k 세그먼트-이미지-캡션 쌍을 학습했습니다. Nvidia RTX 3090TI에서 400 GPU 시간으로 훈련.
- 시맨틱 세분화(ADE20K): ADE20K 데이터 세트에서 164만 개의 세분화-이미지-캡션 쌍을 학습했습니다. Nvidia A100 80G에서 200 GPU 시간으로 훈련.
- 깊이(대규모): Midas를 사용한 3M 깊이 이미지-캡션 쌍. Nvidia A100 80G에서 500 GPU-시간으로 훈련.
- 뎁스(소규모): 최소한의 필요한 데이터 세트 크기로 실험하는 데 사용되는 위의 심도 데이터 세트에서 200만 쌍의 하위 집합입니다.
- 노멀 맵: DIODE 데이터 세트의 25,452개의 노멀 이미지-캡션 쌍. 엔비디아 A100 80G에서 100 GPU 시간으로 훈련.
- 노멀 맵(확장): 마이다스를 사용하여 계산된 거친 노멀 맵입니다. Nvidia A100 80G에서 200 GPU-시간으로 훈련.
- 카툰 라인 드로잉: 카툰 라인 드로잉 추출 방법을 사용하여 1백만 개의 선화-카툰-캡션 쌍을 추출합니다. Nvidia A100 80G에서 300 GPU 시간으로 훈련되었습니다.

### 4 실험

4.1 실험 설정

실험은 기본적으로 CFG 스케일 9.0, DDIM 샘플러 및 20단계를 사용합니다. 네 가지 유형의 프롬프트가 테스트됩니다:

프롬프트 없음: 빈 문자열 프롬프트.
기본 프롬프트: "전문적이고 섬세한 고품질 이미지"와 같은 의미 없는 프롬프트입니다.
자동 프롬프트: BLIP과 같은 방법으로 생성된 이미지 캡션입니다.
사용자 프롬프트: 사용자가 제공하는 프롬프트.

4.2 정성적 결과

정성적 결과는 그림 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15에 제시되어 있습니다.

4.3 절제 연구

그림 20: 컨트롤넷을 사용하지 않고 훈련한 모델과 비교.
그림 21: "급격한 수렴 현상"을 강조하는 훈련 과정.
그림 22: 다양한 데이터 세트 규모로 훈련된 캐니 에지 기반 컨트롤넷.

4.4 이전 방법과의 비교

그림 14: Stability의 심도-이미지 모델과 비교.
그림 17: PITI와 비교.
그림 18: 스케치 가이드 확산과 비교.
그림 19: 길들이기 트랜스포머와 비교.

4.5 사전 훈련된 모델 비교

그림 23, 24, 25에는 사전 훈련된 여러 모델의 비교가 나와 있습니다.

4.6 기타 애플리케이션

그림 16: 마스크된 확산 프로세스를 사용한 펜 기반 이미지 편집 데모.
그림 26: 물체가 비교적 단순할 때 디테일을 정확하게 제어하는 모습을 보여줍니다.
그림 27: 컨트롤넷을 확산 반복의 50%에만 적용했을 때의 결과, 입력 모양을 따르지 않는 결과.