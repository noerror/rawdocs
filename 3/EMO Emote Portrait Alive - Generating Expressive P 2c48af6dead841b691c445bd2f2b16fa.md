# EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions

[https://arxiv.org/abs/2402.17485](https://arxiv.org/abs/2402.17485)

[https://humanaigc.github.io/emote-portrait-alive/](https://humanaigc.github.io/emote-portrait-alive/)

- Feb 2024

![우리는 표현력 있는 오디오 기반 초상화 비디오 생성 프레임워크인 EMO를 제안했습니다. 단일 참조 이미지와 보컬 오디오(예: 말하기 및 노래하기)를 입력으로, 우리의 방법은 표현력 있는 얼굴 표정과 다양한 머리 포즈를 가진 보컬 아바타 비디오를 생성할 수 있으며, 입력 오디오의 길이에 따라 어떤 길이의 비디오도 생성할 수 있습니다.](EMO%20Emote%20Portrait%20Alive%20-%20Generating%20Expressive%20P%202c48af6dead841b691c445bd2f2b16fa/Untitled.png)

우리는 표현력 있는 오디오 기반 초상화 비디오 생성 프레임워크인 EMO를 제안했습니다. 단일 참조 이미지와 보컬 오디오(예: 말하기 및 노래하기)를 입력으로, 우리의 방법은 표현력 있는 얼굴 표정과 다양한 머리 포즈를 가진 보컬 아바타 비디오를 생성할 수 있으며, 입력 오디오의 길이에 따라 어떤 길이의 비디오도 생성할 수 있습니다.

### 1 Introduction

### 1.1 이미지 생성 분야의 발전

최근 이미지 생성 분야는 확산 모델의 등장으로 인해 큰 발전을 이루었습니다. 이 모델들은 대규모 이미지 데이터셋을 이용한 훈련과 점진적인 생성 방법 덕분에 고품질의 이미지를 만들어내는 능력으로 유명해졌습니다. 이러한 혁신적인 접근 방식은 이미지 생성의 디테일과 리얼리즘을 전례 없는 수준으로 향상시켜, 생성 모델 분야에서 새로운 기준을 설정했습니다.

### 1.2 확산 모델의 다양한 적용

확산 모델은 정지 이미지 생성뿐만 아니라, 동적이고 매력적인 비주얼 내러티브를 만들어내는 비디오 생성 분야에서도 그 잠재력을 탐색하기 시작했습니다. 이러한 모델을 이용한 연구는 비디오 생성 능력뿐만 아니라, 특히 인간 중심의 비디오 생성에 초점을 맞추고 있습니다. 예를 들어, '토킹 헤드' 연구는 사용자가 제공한 오디오 클립에서 얼굴 표정을 생성하는 것을 목표로 합니다.

### 1.3 토킹 헤드 생성의 도전

토킹 헤드 비디오를 생성하는 것은 다양한 인간의 얼굴 움직임을 포착하는 것에서 큰 도전을 안고 있습니다. 기존 방법들은 이 작업을 단순화하기 위해 최종 비디오 출력에 제약을 가하기도 합니다. 예를 들어, 일부 방법은 3D 모델을 사용하여 얼굴의 키포인트를 제한하거나, 기본 비디오에서 머리 움직임의 순서를 추출하여 전체 움직임을 안내합니다. 이러한 제약은 비디오 생성의 복잡성을 줄이지만, 결과적으로 생성된 얼굴 표정의 풍부함과 자연스러움을 제한할 수 있습니다.

### 1.4 본 논문의 목표

이 논문에서는 현실적인 얼굴 표정의 광범위한 스펙트럼을 포착하고 자연스러운 머리 움직임을 촉진하여 생성된 헤드 비디오에 표현력을 부여하는 새로운 토킹 헤드 프레임워크를 제안합니다. 이를 위해, 주어진 이미지와 오디오 클립에서 직접 캐릭터 헤드 비디오를 합성할 수 있는 확산 모델의 생성 능력을 활용하는 방법을 제안합니다. 이 접근 방식은 중간 표현이나 복잡한 전처리 없이 높은 시각적 및 감정적 충실도를 가진 토킹 헤드 비디오를 생성하는 과정을 간소화합니다.

### 2 Related Work

### 2.1 확산 모델의 발전과 적용

- **확산 모델**은 이미지 생성, 이미지 편집, 비디오 생성, 3D 콘텐츠 생성 등 다양한 분야에서 뛰어난 성능을 보여주고 있습니다. 특히 **안정적인 확산(Stable Diffusion)** 모델은 UNet 아키텍처를 사용하여 주목할 만한 텍스트-이미지 변환 기능을 제공합니다. 이 모델들은 대규모 텍스트-이미지 데이터셋에서 광범위한 훈련을 거쳐 다양한 이미지 및 비디오 생성 작업에 널리 적용됩니다.
- 최근 연구 중 일부는 **DiT(Diffusion-in-Transformer)** 모델을 채택하여, 시간 모듈과 3D 컨볼루션을 포함한 변형된 UNet 구조를 사용합니다. 이를 통해 더 큰 규모의 데이터와 모델 파라미터를 지원하며, 전체 텍스트-비디오 모델을 처음부터 훈련시켜 우수한 비디오 생성 결과를 달성합니다.

### 2.2 오디오 기반 토킹 헤드 생성

- 오디오 기반 토킹 헤드 생성은 주로 **비디오 기반 방법**과 **단일 이미지 기반 방법**으로 분류됩니다.
    - **비디오 기반 방법**: 입력 비디오 세그먼트에 직접 편집을 적용합니다. 예를 들어, **Wav2Lip**은 오디오에 기반하여 비디오 내 입 움직임을 재생성하는 방법으로, 오디오-입술 동기화를 위한 판별자를 사용합니다. 이 방법의 한계는 고정된 머리 움직임과 입 움직임만 생성할 수 있어 리얼리즘을 제한한다는 점입니다.
    - **단일 이미지 기반 방법**: 참조 사진을 사용하여 사진의 외모를 모방하는 비디오를 생성합니다. 이 접근 방식은 헤드 모션과 얼굴 표정을 독립적으로 생성한 후, 이를 결합하여 최종 비디오 프레임을 생성하는 중간 3D 얼굴 메쉬를 사용합니다.

### 2.3 확산 모델을 이용한 토킹 헤드 생성

- 확산 모델을 이용한 토킹 헤드 생성에 대한 연구도 진행되고 있습니다. 이 방법들은 직접 이미지 프레임에 적용하는 대신, 3D 모폴로지컬 모델(3DMM)에 대한 계수를 생성하여 향상된 결과를 제공합니다. 하지만, 이러한 방법들은 여전히 매우 자연스러운 얼굴 비디오 생성에 있어 일부 한계를 가지고 있습니다.

이 요약은 확산 모델의 발전과 다양한 분야, 특히 오디오 기반 토킹 헤드 생성 분야에서의 적용과 관련된 주요 내용을 간략하게 설명합니다. 확산 모델의 기술적 진보와 다양한 접근 방식을 통해 토킹 헤드 비디오의 자연스러움과 리얼리즘을 향상시키려는 연구의 방향성을 보여줍니다.

### 3 Method

### 3.1 기본 개념

![제안된 방법의 개요입니다. 우리의 프레임워크는 주로 두 단계로 구성됩니다. 초기 단계인 프레임 인코딩에서는 ReferenceNet이 참조 이미지와 모션 프레임에서 특징을 추출하는 데 사용됩니다. 이어지는 확산 과정 단계에서는 사전 훈련된 오디오 인코더가 오디오 임베딩을 처리합니다. 얼굴 영역 마스크는 멀티 프레임 노이즈와 통합되어 얼굴 이미지 생성을 지배합니다. 이후 Backbone Network가 노이즈 제거 작업을 촉진하기 위해 사용됩니다. Backbone Network 내에서 두 가지 형태의 주의 메커니즘이 적용됩니다: 참조-주의와 오디오-주의. 이 메커니즘들은 각각 캐릭터의 정체성을 보존하고 캐릭터의 움직임을 조절하는 데 필수적입니다. 또한, 시간 모듈이 시간 차원을 조작하고 모션의 속도를 조정하는 데 사용됩니다.](EMO%20Emote%20Portrait%20Alive%20-%20Generating%20Expressive%20P%202c48af6dead841b691c445bd2f2b16fa/Untitled%201.png)

제안된 방법의 개요입니다. 우리의 프레임워크는 주로 두 단계로 구성됩니다. 초기 단계인 프레임 인코딩에서는 ReferenceNet이 참조 이미지와 모션 프레임에서 특징을 추출하는 데 사용됩니다. 이어지는 확산 과정 단계에서는 사전 훈련된 오디오 인코더가 오디오 임베딩을 처리합니다. 얼굴 영역 마스크는 멀티 프레임 노이즈와 통합되어 얼굴 이미지 생성을 지배합니다. 이후 Backbone Network가 노이즈 제거 작업을 촉진하기 위해 사용됩니다. Backbone Network 내에서 두 가지 형태의 주의 메커니즘이 적용됩니다: 참조-주의와 오디오-주의. 이 메커니즘들은 각각 캐릭터의 정체성을 보존하고 캐릭터의 움직임을 조절하는 데 필수적입니다. 또한, 시간 모듈이 시간 차원을 조작하고 모션의 속도를 조정하는 데 사용됩니다.

- 본 연구는 **안정적인 확산(Stable Diffusion, SD)** 모델을 기반으로 합니다. SD는 원본 이미지 기능 분포를 잠재 공간으로 매핑하고, 가우시안 노이즈를 추가하여 시간에 따라 노이즈가 있는 잠재 상태를 생성한 후, 이를 제거하여 원하는 이미지를 생성합니다.
- 이 과정에서 텍스트 기능을 통합하여 목표 이미지를 더 정확하게 생성할 수 있도록 합니다. SD 모델은 텍스트 제어를 통해 이미지 생성 과정에서 원하는 결과를 달성하기 위한 기능을 포함하고 있습니다.

### 3.2 네트워크 파이프라인

- **Backbone Network**: 다중 프레임 노이즈 잠재 입력을 받아, 각 시간 단계에서 연속적인 비디오 프레임으로 변환하는 역할을 합니다. 이 네트워크는 UNet 구조를 바탕으로 하며, 시간적 연속성을 보장하기 위해 시간 모듈을 포함하고, 생성된 프레임의 ID 일관성을 유지하기 위해 참조 이미지의 특징을 추출하는 ReferenceNet과 병렬로 구성됩니다.
- **Audio Layers**: 오디오 특징은 캐릭터의 말하기 모션을 주도합니다. 프레임별 오디오 표현을 추출하고, 이를 통해 생성 과정에 주입하여 캐릭터의 입 모양과 표정을 조절합니다.
- **Temporal Modules**: 시간적 연속성을 유지하고, 비디오 스트림의 일관성을 보장하기 위해 설계되었습니다. 이 모듈은 비디오 프레임 간의 동적 내용을 효과적으로 캡처하여, 부드럽고 연속적인 비디오 시퀀스를 생성합니다.

### 3.3 참조 이미지와 오디오 클립의 통합

- 본 연구의 목표는 단일 참조 이미지와 입력된 음성 오디오 클립에 동기화된 비디오를 생성하는 것입니다. 이를 위해 참조 이미지에서 추출한 특징과 오디오 클립에서 추출한 오디오 특징을 모델에 통합하여, 자연스러운 머리 움직임과 생생한 표정이 조화를 이루는 비디오를 생성합니다.

### 3.4 훈련 전략

- 훈련 과정은 이미지 사전 훈련, 비디오 훈련, 속도 층 통합의 세 단계로 구성됩니다. 이 단계별 접근 방식은 모델이 오디오 신호에 따라 캐릭터의 표정과 입 모양, 머리 움직임의 주파수를 정확하게 조절할 수 있도록 합니다.

### 3.5 안정성과 제어 메커니즘

- 모델은 안정적인 제어 메커니즘을 통해 비디오 생성 과정에서 안정성을 강화합니다. 이는 얼굴 위치와 속도 조절을 통해 이루어지며, 생성된 캐릭터의 얼굴 표정과 머리 움직임을 더 자연스럽고 안정적으로 만듭니다.

이 요약은 본 논문에서 제안하는 방법의 기본 개념, 네트워크 구조, 통합 접근 방식, 훈련 전략 및 안정성 제어 메커니즘에 대해 간략하게 설명합니다. 이러한 접근 방식은 오디오 클립과 동기화된, 자연스러운 머리 움직임과 표정을 가진 토킹 헤드 비디오를 생성하는 데 중점을 둡니다.

### 4 Experiments

### 4.1 구현 세부사항

- 약 250시간 분량의 토킹 헤드 비디오를 인터넷에서 수집하여 모델을 훈련시켰습니다. 추가로 HDTF와 VFHQ 데이터셋을 이용해 훈련을 보완했습니다. VFHQ 데이터셋은 오디오가 없기 때문에, 첫 번째 훈련 단계에서만 사용되었습니다.
- 비디오 클립은 512x512 크기로 재조정되고, 얼굴 검출 프레임워크(MediaPipe)를 이용해 얼굴 영역을 탐지했습니다. 머리 회전 속도는 각 프레임에서 얼굴 랜드마크를 이용해 계산되었습니다.

### 4.2 실험 설정

- HDTF 데이터셋을 분할하여, 10%를 테스트 세트로 사용하고 나머지 90%는 훈련에 사용했습니다. 이때, 훈련 세트와 테스트 세트 간 캐릭터 ID가 중복되지 않도록 주의했습니다.
- 비교 대상으로는 Wav2Lip, SadTalker, DreamTalk 등의 기존 방법들이 사용되었습니다. 또한, Diffused Heads 방법의 코드를 이용해 결과를 생성했으나, 이 모델은 오직 녹색 배경이 있는 CREMA 데이터셋에서 훈련되어 최적의 결과를 얻지 못했습니다.

### 4.3 질적 비교

- Wav2Lip은 흐릿한 입술 영역과 거의 움직이지 않는 머리 포즈를 생성하는 경향이 있었습니다. 반면, DreamTalk은 원본 얼굴을 왜곡시키고 표정과 머리 움직임의 다이나믹함을 제한했습니다. 제안된 방법은 더 넓은 범위의 머리 움직임과 더 동적인 얼굴 표정을 생성할 수 있었습니다.
- 다양한 스타일의 캐릭터를 대상으로 한 실험에서, 제안된 모델은 실제적, 애니메이션, 3D 스타일의 캐릭터에 대해 일관된 입술 동기화를 보여주며, 다양한 포트레잇 유형에 대한 토킹 헤드 비디오를 효과적으로 생성할 수 있음을 보여주었습니다.

![여러 토킹 헤드 생성 작업과의 질적 비교입니다.](EMO%20Emote%20Portrait%20Alive%20-%20Generating%20Expressive%20P%202c48af6dead841b691c445bd2f2b16fa/Untitled%202.png)

여러 토킹 헤드 생성 작업과의 질적 비교입니다.

![다양한 초상화 스타일을 기반으로 한 우리 방법의 질적 결과입니다. 여기에서는 동일한 보컬 오디오 클립으로 구동되는 14개의 생성된 비디오 클립을 보여줍니다. 각 생성된 클립의 지속 시간은 약 8초입니다. 공간 제한으로 인해 각 클립에서 네 프레임만 샘플링합니다.](EMO%20Emote%20Portrait%20Alive%20-%20Generating%20Expressive%20P%202c48af6dead841b691c445bd2f2b16fa/Untitled%203.png)

다양한 초상화 스타일을 기반으로 한 우리 방법의 질적 결과입니다. 여기에서는 동일한 보컬 오디오 클립으로 구동되는 14개의 생성된 비디오 클립을 보여줍니다. 각 생성된 클립의 지속 시간은 약 8초입니다. 공간 제한으로 인해 각 클립에서 네 프레임만 샘플링합니다.

### 4.4 양적 비교

- 제안된 방법은 비디오 품질 평가에서 FVD 점수가 낮게 나와 타 방법들에 비해 우수한 성능을 보였습니다. 개별 프레임 품질 평가에서도 FID 점수가 개선되어 높은 품질의 이미지를 생성하는 것으로 나타났습니다. 입술 동기화(SyncNet 점수)에서는 최고 점수를 달성하지 못했지만, 생동감 있는 얼굴 표정 생성에서는 E-FID 점수를 통해 뛰어난 성능을 보였습니다.

### **4.5 한계점**

- 제안된 방법은 확산 모델에 의존하기 때문에 다른 방법에 비해 시간이 더 소요될 수 있습니다. 또한, 캐릭터의 움직임을 제어하기 위한 명시적인 제어 신호를 사용하지 않아, 때때로 비디오에서 원치 않는 신체 부위(예: 손)의 생성으로 이어질 수 있습니다.

이 요약은 제안된 모델의 구현 세부사항, 실험 설정, 질적 및 양적 비교 결과, 그리고 방법의 한계점에 대해 간략하게 설명합니다. 실험 결과는 제안된 방법이 기존 방법들에 비해 더 높은 품질의 토킹 헤드 비디오를 생성할 수 있음을 보여줍니다.

### 5 Limitation

본 연구와 제안된 모델은 여러 면에서 혁신적이고 뛰어난 성과를 보였지만, 몇 가지 한계점도 존재합니다.

![강한 음조 품질을 가진 보컬 오디오에 대해 우리 방법으로 생성된 결과입니다. 각 클립에서 캐릭터는 강한 음조 품질의 오디오(예: 노래하기)에 의해 구동되며, 각 클립의 지속 시간은 약 1분입니다.](EMO%20Emote%20Portrait%20Alive%20-%20Generating%20Expressive%20P%202c48af6dead841b691c445bd2f2b16fa/Untitled%204.png)

강한 음조 품질을 가진 보컬 오디오에 대해 우리 방법으로 생성된 결과입니다. 각 클립에서 캐릭터는 강한 음조 품질의 오디오(예: 노래하기)에 의해 구동되며, 각 클립의 지속 시간은 약 1분입니다.

![Diffused Heads [27]와의 비교, 생성된 클립의 지속 시간은 6초이며, Diffused Heads의 결과는 낮은 해상도를 가지고 생성된 프레임들 사이의 오류 누적으로 인해 저하됩니다.](EMO%20Emote%20Portrait%20Alive%20-%20Generating%20Expressive%20P%202c48af6dead841b691c445bd2f2b16fa/Untitled%205.png)

Diffused Heads [27]와의 비교, 생성된 클립의 지속 시간은 6초이며, Diffused Heads의 결과는 낮은 해상도를 가지고 생성된 프레임들 사이의 오류 누적으로 인해 저하됩니다.

### 5.1 시간 소모성

- 제안된 모델은 확산 모델에 기반하기 때문에, 계산 과정이 복잡하고 시간이 많이 소요됩니다. 이는 실시간 응용 프로그램이나 빠른 처리가 필요한 상황에는 제약이 될 수 있습니다.

### 5.2 명시적인 제어 신호 부재

- 캐릭터의 움직임을 제어하기 위해 명시적인 제어 신호를 사용하지 않습니다. 이로 인해 때때로 원치 않는 신체 부위의 생성(예: 손)으로 이어질 수 있으며, 비디오에서 아티팩트가 발생할 가능성이 있습니다.

### 5.3 개선 가능한 동기화

- 비록 제안된 모델이 다양한 얼굴 표정과 머리 움직임을 생성할 수 있으나, 오디오와 완벽하게 동기화된 입 모양을 생성하는 데 있어서는 여전히 개선의 여지가 있습니다. 특히, 복잡한 언어적 내용이나 빠른 대화 속도를 가진 오디오에 대한 처리에서 더 정교한 동기화가 필요할 수 있습니다.

### 5.4 특정 신체 부위에 대한 제어 신호 적용

- 한계점에 대한 해결책으로, 특정 신체 부위(예: 손)에 대한 명시적인 제어 신호를 도입하는 것이 제안되었습니다. 이는 비디오 생성 과정에서 원하지 않는 신체 부위의 생성을 방지하고, 비디오의 전반적인 품질을 향상시킬 수 있는 방법입니다.

이 요약은 제안된 모델과 연구의 주요 한계점을 설명하며, 이러한 한계점들이 향후 연구에서 어떻게 개선될 수 있는지에 대한 가능성을 제시합니다. 연구의 진전에 따라 이러한 한계점을 극복하고 더 나은 성능과 효율성을 달성할 수 있는 방법을 모색하는 것이 중요합니다.