# CharacterGen: Efficient 3D Character Generation from Single Imageswith Multi-View Pose Calibration

[https://arxiv.org/abs/2402.17214](https://arxiv.org/abs/2402.17214)

[https://github.com/zjp-shadow/CharacterGen](https://github.com/zjp-shadow/CharacterGen)

[https://charactergen.github.io/](https://charactergen.github.io/)

- Feb 2024

![CharacterGen은 효율적인 3D 캐릭터 생성 프레임워크입니다. 단일 입력 이미지를 받아 일관된 외형을 가진 고품질의 정규화된 포즈의 3D 캐릭터 메쉬를 생성하며, 이후 리깅 및 애니메이션 워크플로우에 적합합니다.](CharacterGen%20Efficient%203D%20Character%20Generation%20fro%20247bace7d5094381b2fccce08388b2c5/Untitled.png)

CharacterGen은 효율적인 3D 캐릭터 생성 프레임워크입니다. 단일 입력 이미지를 받아 일관된 외형을 가진 고품질의 정규화된 포즈의 3D 캐릭터 메쉬를 생성하며, 이후 리깅 및 애니메이션 워크플로우에 적합합니다.

## 1 INTRODUCTION

디지털 콘텐츠 산업의 급속한 발전으로 인해 영화, 비디오 게임, 온라인 스트리밍, 가상 현실(VR) 등 다양한 분야에서 고품질 3D 콘텐츠의 생성이 중요한 요소로 떠오르고 있습니다. 수작업으로 모델링된 3D 콘텐츠는 높은 품질을 자랑하지만, 막대한 시간과 노동이 필요하여 큰 걸림돌이 됩니다. 이를 해결하기 위해 단일 이미지에서 3D 모델을 생성하는 연구가 활발히 진행되고 있으며, 이는 초보자들도 쉽게 3D 콘텐츠를 생성할 수 있게 하여 3D 콘텐츠 제작의 문턱을 낮추고 있습니다.

그러나 3D 캐릭터 모델은 복잡한 관절과 자주 발생하는 2D 이미지에서의 자가 가림 현상 때문에 재구성, 생성, 애니메이션화하는 데 큰 어려움을 겪습니다. 이러한 캐릭터들은 다양한 신체 자세를 취할 수 있으며, 드문 자세는 해석하기 어려워 데이터 도메인이 다양하지만 불균형하게 됩니다. 이러한 복잡성은 효과적인 모델 생성, 리깅, 애니메이션화를 방해합니다. 기존 연구들은 3D 인간 신체의 파라메트릭 모델을 3D 프라이어로 사용했지만, 이는 주로 현실적인 인간 비율과 밀착된 의류에 맞추어져 있어 적용 범위가 제한적입니다. 특히, 과장된 신체 비율과 복잡한 의상 디자인을 가진 스타일화된 캐릭터의 경우 이러한 접근 방식의 한계가 두드러집니다.

이에 본 논문에서는 단일 이미지에서 3D 캐릭터를 생성하는 새로운 접근법인 CharacterGen을 소개합니다. 이 방법은 입력 이미지의 어떤 신체 자세도 허용하고, 깨끗한 3D 캐릭터 모델을 출력하는 점에서 이전 방법들과 차별화됩니다. CharacterGen의 기본 원리는 신체 자세를 동시에 정규화하고 생성 과정에서 일관된 다중 뷰 이미지를 생성하는 것입니다. 이를 통해 각 자세를 3D 캐릭터 모델링에서 널리 사용되는 정규화된 "A-포즈"로 변환하고, 여러 시점에서 이미지 일관성을 보장합니다. 이 접근 방식은 자가 가림 현상과 모호한 신체 자세의 문제를 효과적으로 해결하여 이후의 재구성, 리깅, 애니메이션 단계를 크게 간소화합니다.

CharacterGen의 3D 캐릭터 생성 방법은 두 단계로 구성됩니다. 첫 번째 단계에서는 단일 이미지를 다중 시점으로 변환하면서 입력 자세를 정규화하고, 두 번째 단계에서는 이 정규화된 자세를 사용하여 3D 캐릭터를 재구성합니다. 이 방법은 제어 가능한 이미지 생성의 최근 발전된 기술을 활용하고, 3D 캐릭터의 희소 뷰 재구성과 관련된 문제를 극복합니다. 첫 번째 단계에서는 확산 기반의 이미지 조건부 다중 뷰 생성 모델을 사용하여 입력 이미지의 전역 및 국부 특성을 정규화된 자세로 변환하고, 두 번째 단계에서는 트랜스포머 기반의 일반화 가능한 희소 뷰 재구성 모델을 사용하여 생성된 이미지를 바탕으로 3D 캐릭터 모델을 생성합니다.

CharacterGen의 전체 생성 과정은 1분 이내에 완료되며, 이를 위해 애니메이션 캐릭터의 다중 자세, 다중 뷰 데이터를 포함하는 Anime3D 데이터셋을 구축했습니다. 13,746개의 캐릭터를 수집하고, 다양한 뷰포인트와 여러 신체 자세로 렌더링하여 데이터셋을 구성했습니다.

결론적으로, 본 논문에서는 이미지 조건부 확산 모델, 효율적인 파이프라인, 그리고 다양한 학습 및 평가 리소스를 제공하는 Anime3D 데이터셋을 포함하여 3D 캐릭터 생성에 기여하는 주요 성과를 제시합니다.

## 2 RELATED WORKS

이 장에서는 3D 객체 및 아바타 생성과 관련된 기존 연구들을 다룹니다. CharacterGen이 채택한 트랜스포머 기반 재구성 모델과 관련된 연구들도 포함됩니다. 공간의 제약으로 3D 인간 재구성과 관련된 연구는 논의에서 제외합니다.

### 2.1 Diffusion-Based 3D Object Generation

최근 확산 방법은 3D 객체 생성 작업을 효과적으로 수행하는 능력을 보여주었습니다. DreamFusion과 SJC는 사전 학습된 2D 확산 모델로부터 텍스트 기반 3D 생성 작업을 위한 그라디언트 지침을 제공하는 SDS(score distillation sampling)를 활용합니다. Magic3D와 Fantasia3D는 고해상도 렌더링을 지원하는 암묵적 사면체 필드를 사용하여 세부 정제 단계에서 높은 해상도를 제공합니다. ProlificDreamer는 LoRA 네트워크에서 그라디언트 점수를 증류하여 3D 객체의 분포를 더 잘 학습합니다. Zero123은 주어진 카메라 포즈와 입력 이미지에 일치하는 다중 뷰 이미지를 생성하는 새로운 확산 모델을 제시합니다. Magic123은 SDS와 Zero123 지침을 결합하여 이미지 프롬프트에서 3D 객체를 생성하고, 전면 텍스처 품질을 향상시키기 위해 재구성 손실을 채택합니다. MVDream과 ImageDream은 다중 뷰 확산 모델을 활용하여 3D 객체 생성 과정에서 높은 일관성을 유지합니다. SyncDreamer는 3D 인식 주의 모듈을 사용하여 동기화된 다중 뷰 이미지 생성을 달성합니다. 여러 다른 연구들도 3D 데이터를 사용하여 확산 모델을 훈련하여 직접 3D 객체를 생성하는 방법을 제안하지만, 이러한 방법들은 다양성에서 어려움을 겪습니다.

### 2.2 3D Avatar Generation

강력한 인간 신체 프라이어인 SMPL과 SMPL-X를 사용하여 일반적인 3D 생성 방법을 기반으로 고품질의 인간 아바타를 생성할 수 있습니다. EVA3D는 GAN 백본과 포즈 안내 샘플링 방법을 결합하여 고품질의 3D 인간 아바타를 생성합니다. AvatarCLIP은 사전 학습된 CLIP 모델을 활용하여 지오메트리와 색상 네트워크의 최적화를 안내하여 텍스트 기반 인간 생성을 처음으로 해결합니다. Dreamavatar와 AvatarCraft는 SMPL을 사용하여 확산 기반 생성 과정에서 암묵적 인간 지오메트리를 초기화합니다. DreamHuman은 ImGHum을 신체 프라이어로 채택하고, 포커스 렌더링 메커니즘을 제안하여 아바타의 세부 지오메트리를 더 잘 재구성합니다. DreamWaltz는 ControlNet을 사용하여 애니메이션 표현을 미세 조정하기 위한 포즈 안내를 제공합니다. AvatarVerse와 AvatarStudio는 다중 얼굴 문제를 해결하고 부분 지오메트리 최적화를 지원하기 위해 DensePose 안내 ControlNet을 사용합니다. TeCH는 추가 DreamBooth 모델을 훈련하여 SDS 지침 모델을 위한 이미지 프롬프트 아바타 생성을 지원합니다. TADA는 2D 확산 모델을 직접 증류하여 SMPL 바디 메쉬의 노멀과 변위를 최적화합니다.

![네 개의 다른 카메라 뷰에서 본 Anime3D 데이터셋의 예시 캐릭터로, UNet의 정규화된 포즈 결정 능력을 확장하기 위해 훈련 중 이미지 쌍을 어떻게 구성하는지 보여줍니다.](CharacterGen%20Efficient%203D%20Character%20Generation%20fro%20247bace7d5094381b2fccce08388b2c5/Untitled%201.png)

네 개의 다른 카메라 뷰에서 본 Anime3D 데이터셋의 예시 캐릭터로, UNet의 정규화된 포즈 결정 능력을 확장하기 위해 훈련 중 이미지 쌍을 어떻게 구성하는지 보여줍니다.

이들 대부분의 방법은 주로 텍스트 기반 3D 캐릭터 생성에 중점을 두고 있으며, 이미지 프롬프트를 사용할 수 없어 제어 가능한 캐릭터 생성을 필요로 합니다. DreamBooth 기반 방법들은 단일 입력 이미지에 대한 과적합으로 인해 강한 전면 뷰 편향이 발생하여 다중 얼굴 문제에 심각하게 제약을 받습니다.

## 3 METHOD

### 3.1 Anime3D Dataset

CharacterGen의 성능을 향상시키기 위해 우리는 Anime3D라는 데이터셋을 구축했습니다. 이 데이터셋은 13,746개의 스타일화된 캐릭터를 포함하며, 각 캐릭터는 다양한 포즈와 뷰포인트에서 렌더링되었습니다.

![일관된 네 개의 뷰 이미지를 생성하는 파이프라인으로, IDUNet이 로컬 픽셀 수준의 특징을 추출하여 멀티뷰 UNet을 강화하는 방법을 보여줍니다. 여기서 "Q", "K", "V"는 주의 메커니즘의 쿼리, 키, 값 매트릭스를 나타냅니다.](CharacterGen%20Efficient%203D%20Character%20Generation%20fro%20247bace7d5094381b2fccce08388b2c5/Untitled%202.png)

일관된 네 개의 뷰 이미지를 생성하는 파이프라인으로, IDUNet이 로컬 픽셀 수준의 특징을 추출하여 멀티뷰 UNet을 강화하는 방법을 보여줍니다. 여기서 "Q", "K", "V"는 주의 메커니즘의 쿼리, 키, 값 매트릭스를 나타냅니다.

### 3.1.1 Data Acquisition

기존의 대형 3D 객체 데이터셋인 Objaverse나 OmniObject3D는 우리의 학습 목적에 충분한 3D 스타일화된 캐릭터를 포함하고 있지 않습니다. 따라서 우리는 VRoidHub에서 약 14,500개의 애니메이션 캐릭터를 수집하고, 비인간형 데이터를 제거하여 최종적으로 13,746개의 캐릭터 모델을 확보했습니다.

### 3.1.2 Data Processing

이들 캐릭터 모델을 2D 이미지 형식으로 렌더링하여 2D 확산 모델을 미세 조정합니다. 이를 위해 우리는 threevrm 프레임워크를 사용했습니다. 각 캐릭터는 "A-포즈"와 다양한 포즈로 렌더링되며, 다양한 뷰포인트에서의 이미지 쌍을 생성합니다. 이러한 이미지들은 학습 과정에서 3D 공간 이해와 캐릭터 포즈 정규화를 돕습니다.

### 3.2 Multi-view Image Generation and Pose Canonicalization

CharacterGen의 핵심은 단일 입력 이미지에서 여러 시점의 일관된 A-포즈 캐릭터 이미지를 생성하는 것입니다. 이를 위해 우리는 IDUNet과 Multi-view UNet을 사용합니다.

### 3.2.1 IDUNet

IDUNet은 원래의 포즈 이미지로부터 충분한 특징을 유지하고, 생성된 네 개의 뷰 간의 높은 일관성을 보장합니다. 이를 위해 픽셀 수준의 지침을 제공하여 자세한 텍스처를 포착합니다.

### 3.2.2 Multi-view UNet

Multi-view UNet은 단일 포즈 입력 이미지로부터 다중 시점의 A-포즈 이미지를 생성합니다. 이 과정에서 네 개의 시점에 대한 카메라 매트릭스를 공간 지침으로 사용하여, 각 뷰 간의 전역 관계를 포착하고 일관성 있는 이미지 생성을 보장합니다.

### 3.2.3 Pose Canonicalization

포즈 임베딩 네트워크를 도입하여 캐릭터 레이아웃을 유지하고, OpenPose를 사용해 추가적인 포즈 임베딩을 제공합니다. 이를 통해 다양한 신체 형태의 캐릭터에 대해 높은 일관성을 유지할 수 있습니다.

### 3.3 3D Character Generation

이 섹션에서는 생성된 네 개의 시점 이미지를 사용하여 3D 캐릭터를 생성하는 방법을 설명합니다. 두 단계로 구성된 트랜스포머 네트워크를 활용해 캐릭터의 기하학과 텍스처를 재구성하고 정제합니다.

![생성된 다중 뷰 이미지로부터 최종 정제된 캐릭터 메쉬를 생성하는 파이프라인입니다. 첫 번째 단계에서는 깊은 트랜스포머 기반 네트워크를 사용하여 거친 텍스처를 가진 캐릭터를 생성하고, 그런 다음 텍스처 백프로젝션 전략을 사용하여 생성된 메쉬의 외형을 향상시킵니다.](CharacterGen%20Efficient%203D%20Character%20Generation%20fro%20247bace7d5094381b2fccce08388b2c5/Untitled%203.png)

생성된 다중 뷰 이미지로부터 최종 정제된 캐릭터 메쉬를 생성하는 파이프라인입니다. 첫 번째 단계에서는 깊은 트랜스포머 기반 네트워크를 사용하여 거친 텍스처를 가진 캐릭터를 생성하고, 그런 다음 텍스처 백프로젝션 전략을 사용하여 생성된 메쉬의 외형을 향상시킵니다.

### 3.3.1 Character Reconstruction with Coarse Texture

우리는 LRM의 디자인을 따라 네 개의 시점 이미지로부터 캐릭터의 기하학과 거친 외형을 재구성합니다. 이를 위해 네트워크를 Objaverse 데이터셋으로 사전 훈련한 후, Anime3D 데이터셋으로 미세 조정하여 인간 신체 구조에 대한 프라이어를 학습시킵니다.

### 3.3.2 3D Character Refinement

재구성된 3D 캐릭터 메쉬는 거친 텍스처를 가지므로, 생성된 네 개의 시점 이미지를 사용해 텍스처 품질을 개선합니다. 이를 위해 우리는 NvDiffRast를 사용하여 효과적으로 텍스처를 다시 투영하고, Poisson Blending을 활용해 최종 텍스처 맵의 이음새를 줄입니다.

## 4 EXPERIMENTS

### 4.1 Implementation Details

우리는 Anime3D 데이터셋을 학습 및 테스트 세트로 나누어 CharacterGen의 학습을 진행했습니다. Stable Diffusion 2.1 모델을 기반으로 IDUNet과 Multi-view UNet을 훈련했으며, 8개의 NVIDIA A800 GPU를 사용하여 512×512 해상도 이미지로 3일 동안, 768×512 해상도 이미지로 추가 2일 동안 훈련을 진행했습니다. 트랜스포머 기반 재구성 모델은 Objaverse 데이터셋으로 사전 훈련한 후 Anime3D 데이터셋으로 미세 조정했습니다. 전체 생성 파이프라인은 단일 GPU에서 1분 이내에 실행됩니다.

### 4.2 Results and Comparison

CharacterGen의 성능을 평가하기 위해 2D 멀티뷰 캐릭터 이미지 생성 및 3D 캐릭터 메쉬 생성 실험을 수행했습니다. Zero123, SyncDreamer와 같은 기존 방법들과 비교하여 CharacterGen이 더 일관성 있는 멀티뷰 이미지를 생성하며, 기하학적 및 텍스처 품질 면에서 우수한 결과를 보여줍니다. 정량적 평가에서는 Chamfer Distance와 텍스처 품질 지표를 사용하여 다른 방법들과의 성능 차이를 확인했습니다.

![ 다른 방법과 비교한 네 개의 A-포즈 캐릭터 이미지입니다. 모든 예제의 방위각은 {0도, 90도, 180도, 270도}로 설정되어 있습니다.](CharacterGen%20Efficient%203D%20Character%20Generation%20fro%20247bace7d5094381b2fccce08388b2c5/Untitled%204.png)

 다른 방법과 비교한 네 개의 A-포즈 캐릭터 이미지입니다. 모든 예제의 방위각은 {0도, 90도, 180도, 270도}로 설정되어 있습니다.

### 4.3 User Study

우리는 21명의 자원봉사자를 대상으로 사용자 연구를 수행하여 CharacterGen의 생성 결과에 대한 주관적인 평가를 수집했습니다. 참가자들은 스타일 일관성, 공간적 일관성, 기하학적 품질, 텍스처 품질을 기준으로 각 예제를 평가했습니다. 연구 결과, CharacterGen이 다른 방법들보다 높은 선호도를 받았으며, 2D 및 3D 생성 작업 모두에서 우수한 성과를 보였습니다.

![다른 방법들과 비교한 생성된 3D 캐릭터의 외형과 기하학입니다.](CharacterGen%20Efficient%203D%20Character%20Generation%20fro%20247bace7d5094381b2fccce08388b2c5/Untitled%205.png)

다른 방법들과 비교한 생성된 3D 캐릭터의 외형과 기하학입니다.

![CharacterGen과 IP-Adapter-SDXL의 결과 비교입니다.](CharacterGen%20Efficient%203D%20Character%20Generation%20fro%20247bace7d5094381b2fccce08388b2c5/Untitled%206.png)

CharacterGen과 IP-Adapter-SDXL의 결과 비교입니다.

![동결된 IDUNet은 프롬프트 이미지에서 충분한 외형 정보를 추출하지 못하여 비슷하지 않은 이미지를 생성합니다.](CharacterGen%20Efficient%203D%20Character%20Generation%20fro%20247bace7d5094381b2fccce08388b2c5/Untitled%207.png)

동결된 IDUNet은 프롬프트 이미지에서 충분한 외형 정보를 추출하지 못하여 비슷하지 않은 이미지를 생성합니다.

### 4.4 Ablation Study

CharacterGen의 주요 구성 요소들의 중요성을 평가하기 위해 제거 실험을 수행했습니다. IDUNet을 동결한 상태에서 네트워크를 훈련했을 때, 입력 이미지로부터 충분한 특징을 유지하지 못하고 유사성이 감소하는 결과를 보였습니다. 또한, 포즈 임베딩 네트워크를 제거한 경우, 생성된 캐릭터 이미지가 중간에 배치되지 않거나 일관성 없는 옷 부분이 생성되는 문제가 발생했습니다.

![ 포즈 임베딩 네트워크 없이 생성된 캐릭터는 위치가 잘못될 수 있습니다.](CharacterGen%20Efficient%203D%20Character%20Generation%20fro%20247bace7d5094381b2fccce08388b2c5/Untitled%208.png)

 포즈 임베딩 네트워크 없이 생성된 캐릭터는 위치가 잘못될 수 있습니다.

### 4.5 Applications

CharacterGen으로 생성된 A-포즈 3D 캐릭터는 자동 리깅을 통해 다양한 애니메이션 3D 자산으로 활용될 수 있습니다. AccuRig를 사용해 생성된 캐릭터 메쉬를 자동으로 리깅하고, Warudo에서 다양한 애니메이션된 모델을 렌더링하여 결과를 시각화했습니다. 또한, A-포즈 캐릭터가 리깅 및 애니메이션 과정에서 구조적 일관성을 유지하는 데 도움이 된다는 것을 확인했습니다.

![생성된 캐릭터를 리깅하고 이를 다운스트림 애플리케이션에서 3D 자산으로 활용합니다.](CharacterGen%20Efficient%203D%20Character%20Generation%20fro%20247bace7d5094381b2fccce08388b2c5/Untitled%209.png)

생성된 캐릭터를 리깅하고 이를 다운스트림 애플리케이션에서 3D 자산으로 활용합니다.

![CharacterGen의 애니메이션 3D 캐릭터와 ImageDream [Wang and Shi 2023]의 비교입니다.](CharacterGen%20Efficient%203D%20Character%20Generation%20fro%20247bace7d5094381b2fccce08388b2c5/Untitled%2010.png)

CharacterGen의 애니메이션 3D 캐릭터와 ImageDream [Wang and Shi 2023]의 비교입니다.

이와 같이, CharacterGen은 단일 이미지에서 고품질의 3D 캐릭터를 효율적으로 생성하여 다양한 다운스트림 애플리케이션에 적용할 수 있습니다.

## 5 LIMITATIONS AND DISCUSSION

CharacterGen은 단일 입력 이미지에서 3D 캐릭터를 생성하는데 매우 효율적이지만, 몇 가지 한계도 존재합니다. 첫째, 네 개의 시점 A-포즈 이미지 생성 단계에서 캐릭터가 극단적인 포즈를 취하거나 흔하지 않은 시점에서 렌더링된 경우 충분한 정보를 유지하지 못할 수 있습니다. 둘째, 생성된 3D 캐릭터의 텍스처 품질이 일부 경우에 만족스럽지 않을 수 있으며, 특히 캐릭터의 복잡한 의상이나 액세서리의 세부 사항을 재현하는 데 어려움이 있을 수 있습니다.

향후 연구에서는 비사실적 렌더링(NPR) 기술을 텍스처 정제 단계에 통합하여 생성된 캐릭터의 텍스처 품질을 더욱 향상시킬 수 있을 것입니다. 또한, 우리의 다중 뷰 UNet 구조를 활용하여 SDS 최적화 방법을 통합하면 지오메트리 품질이 더욱 향상된 3D 캐릭터 생성을 달성할 수 있을 것입니다.

## 6 CONCLUSIONS

이 논문에서는 CharacterGen이라는 새로운 효율적인 이미지 기반 3D 캐릭터 생성 프레임워크를 제안했습니다. 우리는 다중 포즈, 스타일화된 캐릭터 데이터셋인 Anime3D를 구축하여 우리의 파이프라인을 훈련했습니다. CharacterGen의 설계는 다음과 같은 주요 성과를 포함합니다:

1. 입력 이미지에서 패치 수준의 특징을 추출하여 다중 뷰 A-포즈 캐릭터 이미지를 생성하는 IDUNet을 도입했습니다.
2. 생성된 네 개의 시점 이미지를 사용하여 3D 캐릭터 메쉬를 재구성하는 트랜스포머 기반 네트워크를 활용했으며, 텍스처 백프로젝션 정제 전략을 제안하여 재구성된 캐릭터 메쉬의 외형을 개선했습니다.
3. 실험 결과, CharacterGen은 효율적으로 고품질의 3D 캐릭터를 생성할 수 있으며, 다양한 다운스트림 애플리케이션에 적합한 결과를 보여주었습니다.

CharacterGen은 단일 이미지에서 시작해 빠르고 효율적으로 고품질의 3D 캐릭터를 생성할 수 있는 새로운 가능성을 제시하며, 다양한 분야에서의 활용을 기대할 수 있습니다.