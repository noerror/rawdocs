# DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaption by Combining 3D GANs and Diffusion Priors

[https://arxiv.org/abs/2312.16837](https://arxiv.org/abs/2312.16837)

[https://younglbw.github.io/DiffusionGAN3D-homepage/](https://younglbw.github.io/DiffusionGAN3D-homepage/)

- Dec 2023

![다양한 작업에서 제안된 DiffusionGAN3D의 일부 결과.](DiffusionGAN3D%20Boosting%20Text-guided%203D%20Generation%20%20feef96545ceb46ee862235b05bbcac9c/Untitled.png)

다양한 작업에서 제안된 DiffusionGAN3D의 일부 결과.

### 1. Introduction

이 글에서는 게임, 광고, 영화 제작 등 다양한 산업 분야에서 주목받고 있는 3D 인물 생성 및 스타일라이제이션의 발전과 도전 과제에 초점을 맞춥니다. 사실적인 인물 사진 생성에 상당한 진전이 있었음에도 불구하고, 3D 훈련 데이터의 부족과 다양한 지오메트리 및 텍스처 모델링의 복잡성으로 인해 양식화되고 예술적이며 텍스트가 안내하는 3D 아바타를 만드는 것은 여전히 어려운 과제입니다.

이전 접근 방식과 한계

- 3D GAN을 사용한 전이 학습: 기존 연구에서는 사전 학습된 3D 생성적 적대 신경망(GAN)에 대한 전이 학습을 통해 3D 스타일화를 달성하려고 시도했습니다. 이를 위해서는 상당한 양의 스타일라이즈드 이미지와 학습을 위한 특정 카메라 정렬이 필요합니다.
- 적대적 미세 조정 및 확산 모델: 일부 연구에서는 훈련 데이터를 합성하기 위해 기존의 2D-GAN을 사용한 후 적대적 손실로 미세 조정을 수행했습니다. 또 다른 연구에서는 텍스트-이미지 확산 모델을 사용하여 목표 스타일의 데이터 세트를 생성했습니다. 하지만 이러한 방법은 포즈 편향, 데이터 처리의 번거로움, 높은 연산 요구량으로 인해 어려움을 겪는 경우가 많습니다.
- StyleGAN-Fusion 및 SDS 손실: StyleGAN-Fusion이라는 접근 방식은 2D 및 3D 제너레이터의 텍스트 가이드 적응을 위해 점수 증류 샘플링(SDS) 손실을 사용하여 도메인 적응을 위한 더 간단한 방법을 제공합니다. 하지만 다양성이 제한적이고 텍스트와 이미지의 대응이 불완전하다는 단점이 있습니다.

제안된 프레임워크: DiffusionGAN3D

- 목표: 3D 생성 모델을 확산 사전과 통합하여 3D 도메인 적응 및 텍스트-아바타 변환 작업을 향상시킵니다.
- 2단계 프레임워크:
    - 텍스트 가이드 3D 도메인 적응: 확산 모델과 SDS 손실을 활용하여 사전 학습된 EG3D 기반 모델을 미세 조정합니다. 상대 거리 손실을 통합하여 SDS 기법의 다양성 손실을 상쇄하고 로컬 편집 시나리오를 위한 확산 유도 재구성 손실을 도입합니다.
    - 텍스트-아바타: CLIP 모델에 의해 안내되는 고정 잠상 코드로 3D GAN을 미세 조정하는 데 중점을 두며, 생성 기능 향상을 위해 사례별 학습 가능한 트라이플레인을 도입합니다.
- 프로그레시브 텍스처 개선 모듈: 두 번째 단계로 확산 모델의 2D 합성 기능을 활용하여 텍스처 품질을 크게 향상시킵니다.

기여도

- 고품질 텍스트 가이드 3D 도메인 적응: 3D GAN과 디퓨전 프리어 및 새로운 상대 거리 손실을 결합하여 달성합니다.
- 로컬 편집 시나리오에 적응: 고유한 확산 유도 재구성 손실을 통해 구현합니다.
- 향상된 텍스트-아바타 성능: 사례별 학습 가능한 트라이플레인을 사용하여 안정성과 품질을 개선합니다.
- 프로그레시브 텍스처 개선: 디퓨전 모델의 이미지 생성 기능을 활용하여 텍스처 품질을 향상시키는 새로운 단계.

### 2. Related Work

3D GAN의 도메인 적응

- 발전: 최근 3D 생성 모델의 발전으로 지오메트리 인식 및 포즈 제어 이미지 생성이 크게 개선되었습니다.
- 핵심 기술: EG3D는 삼면체를 3D 표현으로 사용하고, 고품질 3D 형상 생성을 위해 StyleGAN2를 통합하며, 보기 일관적인 이미지 합성을 위해 신경 렌더링을 활용하는 것으로 유명합니다. 이러한 발전은 3D 스타일라이제이션 및 GAN 반전과 같은 애플리케이션을 용이하게 합니다.
- 접근 방식과 도전 과제: 여러 연구에서 3D 도메인 적응을 위해 양식화된 2D 생성기를 사용하거나 이로부터 지식을 추출했습니다. 또 다른 연구에서는 훈련 데이터 세트를 생성하기 위해 확산 모델을 사용했습니다. 하지만 이러한 방법은 포즈 편향, 복잡한 데이터 처리, 높은 컴퓨팅 비용과 같은 문제에 직면하는 경우가 많습니다. StyleGAN-Fusion과 같은 비적대적 미세 조정 방법은 가능성을 보이지만 제한된 다양성과 텍스트-이미지 대응의 차선책으로 어려움을 겪습니다.

텍스트-3D 생성

- 배경: 텍스트 기반 2D 이미지 합성의 성공을 바탕으로 최근의 연구들은 텍스트-3D 생성을 탐구했습니다.
- 선구적인 작품: CLIP-forge, CLIP-Mesh, 드림필드는 3D 표현을 최적화하기 위해 CLIP 안내를 사용하는 것으로 유명합니다. 드림퓨전은 텍스트에서 3D로 생성할 때 SDS 손실을 도입했지만 과포화 및 다중면 문제와 같은 문제가 있었습니다.
- 개선 사항과 한계: ProlificDreamer 및 Magic3D와 같은 후속 작업은 텍스처 충실도와 멀티뷰 일관성을 개선했습니다. 하지만 여전히 노이즈, 디테일 부족, 견고성, 특히 아바타 생성과 관련된 문제에 직면해 있습니다.

텍스트-아바타 생성

- 최근의 발전: 텍스트로 3D 아바타를 생성하는 데 상당한 발전이 있었습니다.
- 혁신적인 방법: Avatar-CLIP과 DreamAvatar는 인간 파라메트릭 모델과 텍스트-이미지 확산 모델을 통합한 것으로 주목할 만합니다. 드림휴먼은 신체 부위를 세밀하게 표현하기 위해 카메라 줌인 전략을 도입했습니다.
- 현황과 과제: 아바타버스와 같은 최근의 방법은 안정적인 아바타 생성 및 포즈 제어에 있어 진전을 이루었습니다. 하지만 취약한 SDS 가이드로 인해 멀티뷰 일관성 및 텍스처 충실도 측면에서 여전히 어려움을 겪고 있습니다.

요약하면, 도메인 적응과 텍스트 가이드 3D 및 아바타 생성을 위한 3D GAN은 상당한 발전을 이루었지만, 이 분야는 특히 생성된 이미지의 일관성, 디테일, 견고성을 유지하는 데 있어 여전히 상당한 과제에 직면해 있습니다.

### 3. Methods

DiffusionGAN3D는 3D GAN을 디퓨전 프리어와 통합하여 3D 도메인 적응과 텍스트-아바타 생성을 향상시키는 것을 목표로 합니다. 이 프레임워크에는 3D 이미지 생성 영역의 특정 문제를 해결하는 몇 가지 주요 구성 요소가 포함되어 있습니다.

![제안된 2단계 프레임워크 DiffusionGAN3D의 개요.](DiffusionGAN3D%20Boosting%20Text-guided%203D%20Generation%20%20feef96545ceb46ee862235b05bbcac9c/Untitled%201.png)

제안된 2단계 프레임워크 DiffusionGAN3D의 개요.

3.1 사전 준비

- 기본 모델로서의 EG3D: 최첨단 3D 생성 모델인 EG3D는 DiffusionGAN3D의 기초를 형성합니다. 이 모델은 3D 표현을 위해 삼면체를 사용하고 고품질 3D 모양과 이미지를 생성하기 위해 신경 렌더링과 StyleGAN2를 통합합니다.
- 스코어 증류 샘플링(SDS): 드림퓨전에서 시작된 이 기법은 3D 표현을 최적화하기 위해 사전 학습된 확산 모델을 사용합니다. SDS 손실은 예측된 노이즈와 추가된 노이즈 간의 차이를 최소화하여 효과적인 텍스트 안내 3D 생성을 지원합니다.

3.2 확산 유도 3D 도메인 적응

- 확산 모델을 통한 효율적인 적응: 지루한 데이터 처리와 같은 3D 도메인 적응의 과제를 극복하기 위해 DiffusionGAN3D는 EG3D 기반 3D GAN에서 전이 학습을 위해 확산 모델과 SDS 손실을 사용합니다.
- 상대 거리 손실: SDS로 인한 다양성 손실 문제를 해결하기 위해 도입된 이 손실은 삼면체 공간의 샘플이 상대적 거리를 유지하도록 하여 스타일 전송을 손상시키지 않으면서 다양성을 향상시킵니다.
    
    ![상대적 거리 손실에 대한 그림.](DiffusionGAN3D%20Boosting%20Text-guided%203D%20Generation%20%20feef96545ceb46ee862235b05bbcac9c/Untitled%202.png)
    
    상대적 거리 손실에 대한 그림.
    
- 확산 유도 재구성 손실: 이 손실은 로컬 편집 시나리오를 위해 특별히 설계된 것으로, 타겟 영역을 편집하는 동안 비타겟 영역을 보존하여 도메인 적응에 유연성을 제공합니다.

!["녹색 머리를 가진 남자"라는 텍스트가 주어졌을 때 다양한 노이즈 레벨에서 SDS 손실의 기울기 응답을 시각화한 것입니다.](DiffusionGAN3D%20Boosting%20Text-guided%203D%20Generation%20%20feef96545ceb46ee862235b05bbcac9c/Untitled%203.png)

"녹색 머리를 가진 남자"라는 텍스트가 주어졌을 때 다양한 노이즈 레벨에서 SDS 손실의 기울기 응답을 시각화한 것입니다.

3.3 3D-GAN 기반 텍스트-아바타 변환

- 강력한 아바타 생성: 텍스트-3D 생성 시 야누스 문제와 같은 문제를 해결하기 위해 이 프레임워크는 사전 학습된 3D GAN을 사용하도록 아키텍처를 확장하여 강력한 아바타 생성을 보장합니다.
- 사례별 학습 가능한 트라이플레인: 이 구성 요소는 가변 지오메트리와 텍스처를 모델링하기 위해 도입되어 네트워크의 생성 기능을 향상시킵니다. 멀티스케일 총 변동 손실과 결합하여 더욱 부드러운 결과를 얻을 수 있습니다.

3.4 프로그레시브 텍스처 개선

- 텍스처 품질 향상: 텍스처 생성에 있어 SDS의 한계를 인식하고 점진적 텍스처 개선 단계를 제안합니다. 이 단계에서는 명시적 텍스처 모델링을 사용하여 텍스처 품질을 개선합니다.
- 적응형 블렌드 모듈 및 프로그레시브 리파이닝: 이 모듈은 차별적인 렌더링을 통해 텍스처 맵을 최적화합니다. 그런 다음 점진적 인페인팅 전략을 사용하여 여러 뷰에서 일관성을 유지하는 방식으로 텍스처를 다듬습니다.
    
    ![제안된 적응형 블렌드 모듈의 세부 사항.](DiffusionGAN3D%20Boosting%20Text-guided%203D%20Generation%20%20feef96545ceb46ee862235b05bbcac9c/Untitled%204.png)
    
    제안된 적응형 블렌드 모듈의 세부 사항.
    

DiffusionGAN3D는 3D 도메인 적응 및 텍스트-아바타 생성을 위한 포괄적인 프레임워크를 제공하여 다양성, 로컬 편집 및 텍스처 품질의 주요 과제를 해결합니다. 상대적 거리 손실, 사례별 학습 가능한 트라이플레인, 점진적 텍스처 개선 단계와 같은 혁신적인 구성 요소를 통해 3D 이미지 생성의 성능과 품질을 크게 향상시킵니다.

### 4. Experiments

4.1 구현 세부 사항

- 프레임워크 기초: DiffusionGAN3D 프레임워크는 EG3D 기반 모델을 기반으로 합니다.
- 응용 분야: 머리, 얼굴, 고양이 영역 적응(PanoHead, EG3D-FFHQ, EG3D-AFHQ)과 텍스트-아바타 작업에서 머리 및 신체 생성(PanoHead, AG3D)을 위해 다양한 모델에 적용됩니다.
- 사용된 디퓨전 모델: 텍스트-이미지 작업에는 StableDiffusion v2.1을, 이미지-이미지 및 텍스처 다듬기 단계의 인페인팅에는 v1.5와 컨트롤넷을 함께 사용합니다.

4.2 질적 비교

- 3D 도메인 적응: StyleGAN-NADA* 및 StyleGAN-Fusion과 비교했을 때, 다양성, 이미지 품질, 텍스트-이미지 대응에서 DiffusionGAN3D가 우수한 성능을 보여줍니다.

![3D 도메인 적응에 대한 질적 비교(EG3D-FFHQ [7]에 적용).](DiffusionGAN3D%20Boosting%20Text-guided%203D%20Generation%20%20feef96545ceb46ee862235b05bbcac9c/Untitled%205.png)

3D 도메인 적응에 대한 질적 비교(EG3D-FFHQ [7]에 적용).

- 텍스트-아바타 생성: 이 방법은 안정성과 충실도 측면에서 일반적인 텍스트-3D 방식(예: DreamFusion, ProlificDreamer, Magic-3D)과 아바타 생성 방식(DreamAvatar, DreamHuman, AvatarVerse)보다 성능이 뛰어납니다.

![텍스트-아바타 변환 작업에 대한 시각적 비교. 처음 두 줄은 '머리'의 결과이고 나머지는 '몸통'의 결과입니다.](DiffusionGAN3D%20Boosting%20Text-guided%203D%20Generation%20%20feef96545ceb46ee862235b05bbcac9c/Untitled%206.png)

텍스트-아바타 변환 작업에 대한 시각적 비교. 처음 두 줄은 '머리'의 결과이고 나머지는 '몸통'의 결과입니다.

4.3 정량적 비교

- FID 점수 및 사용자 연구: DiffusionGAN3D는 더 낮은 FID 점수를 획득하여 더 나은 이미지 충실도를 나타냅니다. 사용자 연구에서도 이미지 품질과 다양성 측면에서 우수한 것으로 나타났습니다.
텍스트-아바타 사용자 연구: 특히 텍스처와 지오메트리 품질에서 드림휴먼과 같은 방법과 비교하여 우수한 결과를 보였습니다.

4.4 제거 연구

- 점진적 텍스처 개선: 제안된 전략이 UV 텍스처, 이미지 간 변환 및 기타 개선 전략에 대한 직접적인 이미지 간 전략보다 효과적임을 보여줍니다.

![텍스처 개선에 대한 제거 연구](DiffusionGAN3D%20Boosting%20Text-guided%203D%20Generation%20%20feef96545ceb46ee862235b05bbcac9c/Untitled%207.png)

텍스처 개선에 대한 제거 연구

- 상대적 거리 손실: 제너레이터의 다양성을 유지하여 고정된 출력 패턴으로 축소되는 것을 방지하는 데 필수적입니다.
    
    ![상대적 거리 손실에 대한 제거 연구.](DiffusionGAN3D%20Boosting%20Text-guided%203D%20Generation%20%20feef96545ceb46ee862235b05bbcac9c/Untitled%208.png)
    
    상대적 거리 손실에 대한 제거 연구.
    
- 확산 유도 재구성 손실: 적절한 편집, 구조 유지 및 대상 영역 변환의 균형을 유지합니다.
    
    ![확산 유도 재구성 손실에 대한 제거 연구. EG3D의 ToRGB 모듈은 Gtrain과 함께 훈련됩니다. 입력 텍스트는 "녹색 머리카락을 가진 여성의 클로즈업"입니다.](DiffusionGAN3D%20Boosting%20Text-guided%203D%20Generation%20%20feef96545ceb46ee862235b05bbcac9c/Untitled%209.png)
    
    확산 유도 재구성 손실에 대한 제거 연구. EG3D의 ToRGB 모듈은 Gtrain과 함께 훈련됩니다. 입력 텍스트는 "녹색 머리카락을 가진 여성의 클로즈업"입니다.
    
- 학습 가능한 추가 트라이플레인: 특히 까다로운 시나리오에서 정확한 생성을 위해 매우 중요합니다.

![사례별 학습 가능한 트라이플레인 및 멀티스케일 총 변동 손실에 대한 제거 연구.](DiffusionGAN3D%20Boosting%20Text-guided%203D%20Generation%20%20feef96545ceb46ee862235b05bbcac9c/Untitled%2010.png)

사례별 학습 가능한 트라이플레인 및 멀티스케일 총 변동 손실에 대한 제거 연구.

4.5 애플리케이션 및 한계

- 이 백서에서는 실제 이미지에 대한 DiffusionGAN3D의 적용과 그 한계에 대해서도 부록 자료에 자세히 설명합니다.

실험을 통해 DiffusionGAN3D가 3D 도메인 적응과 텍스트-아바타 생성을 크게 개선하여 다양한 측면에서 기존 방법보다 성능이 뛰어나다는 것을 확인할 수 있었습니다. 상대 거리 손실, 확산 유도 재구성 손실, 학습 가능한 삼면과 같은 프레임워크의 구성 요소는 이러한 발전을 달성하는 데 결정적인 역할을 합니다. 자세한 제거 연구는 이러한 구성 요소의 효과를 더욱 검증합니다.

### 5 Conclusion

DiffusionGAN3D는 3D 이미지 생성 기술에서 중요한 진전을 이루었습니다. 3D GAN을 디퓨전 프리어와 효과적으로 통합하고 점진적인 텍스처 개선 단계를 도입함으로써 텍스트 가이드 3D 도메인 적응 및 아바타 생성의 품질과 효율성에 대한 새로운 표준을 제시합니다.