# SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement

[https://arxiv.org/abs/2408.00653](https://arxiv.org/abs/2408.00653)

[https://stable-fast-3d.github.io/](https://stable-fast-3d.github.io/)

[https://huggingface.co/spaces/stabilityai/stable-fast-3d](https://huggingface.co/spaces/stabilityai/stable-fast-3d)

- Aug 2024

![단일 이미지에서 고품질 객체 메쉬를 생성하는 SF3D는 재료, 디라이트(조명 효과 제거), UV 언래핑된 텍스처 메쉬를 0.5초 내에 만듭니다. 여기서는 다양한 입력 이미지에서의 SF3D 결과를 보여줍니다. SF3D는 현실적 스타일과 비현실적 스타일 모두 잘 처리합니다.](SF3D%20Stable%20Fast%203D%20Mesh%20Reconstruction%20with%20UV-un%20977078bf645d4d0fa06e1e5fc5cdfb2f/Untitled.png)

단일 이미지에서 고품질 객체 메쉬를 생성하는 SF3D는 재료, 디라이트(조명 효과 제거), UV 언래핑된 텍스처 메쉬를 0.5초 내에 만듭니다. 여기서는 다양한 입력 이미지에서의 SF3D 결과를 보여줍니다. SF3D는 현실적 스타일과 비현실적 스타일 모두 잘 처리합니다.

## **1 Introduction**

이 논문에서는 단일 이미지로부터 고품질의 3D 객체 메쉬를 생성하는 문제를 다룹니다. 고품질 객체 메쉬는 영화, 게임, 전자상거래, AR/VR 등 다양한 분야에서 필수적입니다. 단일 이미지로부터 3D 객체를 생성하는 것은 매우 까다롭고 도전적인 문제로, 이는 단 하나의 2D 투영 이미지를 통해 객체의 3D 형상과 텍스처를 추론해야 하기 때문입니다. 단일 이미지 객체 생성은 복잡하고 수작업이 많이 필요한 객체 생성 과정을 단순화할 수 있습니다.

지난 몇 년간 트랜스포머 모델, 대규모 합성 데이터셋, 3D 인식 이미지/비디오 생성 모델 등의 발전으로 단일 이미지에서 3D 객체 메쉬를 생성하는 기술의 품질이 크게 향상되었습니다. 특히, 트랜스포머 기반의 재구성 모델은 합성 데이터셋에서만 훈련되었음에도 불구하고 실세계 이미지에서 뛰어난 일반화 능력을 보여주었고, 단일 이미지에서 1초 이내에 3D 자산을 생성할 수 있었습니다.

그러나 이러한 빠른 3D 재구성 모델에도 여전히 몇 가지 문제가 존재합니다. 이 기술들은 종종 실제 응용 프로그램에서 사용하기에 적합하지 않은 3D 자산을 생성하거나, 많은 수작업 후처리가 필요합니다. 본 연구에서는 이러한 문제들을 해결하고, 단일 이미지로부터 더 높은 품질의 사용 가능한 3D 자산을 빠르게 생성하는 'Stable Fast 3D' (SF3D) 기법을 제안합니다. 이 기법은 H100 GPU에서 0.5초 이내에 고품질의 3D 자산을 생성할 수 있습니다.

SF3D는 그림자나 조명 효과가 포함된 입력 이미지를 처리하여 일관된 조명을 유지하며, 이를 통해 그래픽 파이프라인에 쉽게 통합될 수 있도록 합니다. 또한, 고정된 버텍스 컬러링 대신 고도로 병렬화된 박스 프로젝션 기반의 UV 언래핑 기법을 사용하여 0.5초의 생성 시간을 달성합니다. SF3D는 또한 기존의 메쉬 생성 방식에서 발생하는 계단 현상을 줄이고, 더 매끄러운 메쉬 표면을 생성하기 위해 DMTet을 사용합니다. 마지막으로, 생성된 3D 객체의 재료 속성을 예측하여 다양한 조명 조건에서도 더욱 현실적인 렌더링이 가능하게 합니다.

결론적으로, SF3D는 단일 이미지로부터 고속으로 고품질의 3D 객체 메쉬를 생성하는 포괄적인 기법을 제공하며, 이는 실용적인 응용 프로그램에서의 속도와 사용성을 모두 향상시킵니다.

## **2 Related Work**

이 장에서는 3D 재구성 분야에서의 기존 연구들을 검토하고, 특히 이미지 생성 모델과 피드포워드 3D 재구성 모델을 중심으로 논의합니다.

### 3D Reconstruction using Image Generative Priors

이미지 생성 모델에서 시작된 3D 재구성 연구는 강력한 생성 모델인 확산 모델(Diffusion Models)을 활용합니다. 이 모델들은 다양한 작업에서 우수한 성능을 보이며, Zero123과 같은 몇몇 연구에서는 이러한 확산 모델을 3D 생성에 적응시키고 있습니다. Score Distillation Sampling(SDS) 같은 기법을 사용하여 2D 확산 모델을 통해 3D 표현을 최적화하지만, 단일 이미지 기반 접근법에서는 일관성 있는 다중 뷰 결과를 얻는 것이 어려웠습니다. 이 문제는 후속 연구들에서 다중 뷰 이미지를 동시에 생성하거나 3D 인식을 명시적으로 도입하는 방식으로 개선되었습니다. 그러나 이러한 모델들은 여전히 3D 메쉬로 변환하는 단계가 필요하고, 이는 몇 분이 소요될 수 있습니다. 본 연구에서는 0.5초 내에 이미지를 3D로 빠르게 생성하는 데 중점을 둡니다.

![다양한 일반적인 문제에 대한 SF3D의 개선 사항. 여기서 우리는 TripoSR [54]와 우리의 결과를 비교합니다. 상단은 자산에 재조명을 적용할 때의 빛 베이크인 효과를 보여줍니다. SF3D는 더 그럴듯한 재조명을 생성합니다. 버텍스 컬러를 사용하지 않음으로써, 우리의 방법은 더 낮은 폴리곤 수로도 더 세밀한 디테일을 인코딩할 수 있습니다. 버텍스 변위를 통해 매끄러운 형상을 추정할 수 있으며, 이는 큐브 행진에서 발생하는 계단 현상을 도입하지 않습니다. 마지막으로, 재료 속성 예측을 통해 다양한 표면 유형을 표현할 수 있습니다.](SF3D%20Stable%20Fast%203D%20Mesh%20Reconstruction%20with%20UV-un%20977078bf645d4d0fa06e1e5fc5cdfb2f/Untitled%201.png)

다양한 일반적인 문제에 대한 SF3D의 개선 사항. 여기서 우리는 TripoSR [54]와 우리의 결과를 비교합니다. 상단은 자산에 재조명을 적용할 때의 빛 베이크인 효과를 보여줍니다. SF3D는 더 그럴듯한 재조명을 생성합니다. 버텍스 컬러를 사용하지 않음으로써, 우리의 방법은 더 낮은 폴리곤 수로도 더 세밀한 디테일을 인코딩할 수 있습니다. 버텍스 변위를 통해 매끄러운 형상을 추정할 수 있으며, 이는 큐브 행진에서 발생하는 계단 현상을 도입하지 않습니다. 마지막으로, 재료 속성 예측을 통해 다양한 표면 유형을 표현할 수 있습니다.

### Feed-Forward 3D Reconstruction

최근 연구에서는 대규모 합성 데이터셋을 이용해 피드포워드 네트워크를 훈련시켜 빠르고 신뢰성 있는 3D 생성을 가능하게 했습니다. 이러한 연구들은 큰 트랜스포머 모델을 사용하여 트리플레인(triplane) 형태의 볼륨 표현을 직접 예측합니다. 이 모델들은 다중 뷰 데이터셋을 사용하여 렌더링 손실만으로 훈련되며, 몇 초 내에 이미지를 3D 메쉬로 변환할 수 있습니다. 후속 연구에서는 Gaussian Splatting과 같은 표현 방식을 사용하거나 메쉬 예측을 직접 통합하는 방법들도 등장했습니다. 또한, 확산 모델과 피드포워드 모델을 통합하여 트리플레인을 직접 생성하거나, 피드포워드 모델을 조건으로 사용하는 방식도 시도되었습니다. 그러나 이러한 접근법들은 여전히 조명 정보가 객체에 포함되어 있어, 조명 학습을 추가로 시도하는 연구도 있습니다.

### Material Decomposition

객체의 방사율만 예측하는 방법은 재조명 시 설득력 있는 결과를 생성하지 못하는 단점이 있습니다. 단일 장면 최적화는 여러 입력 이미지를 조명과 재료로 분해하는 방식으로 이루어지며, 이는 물리기반 렌더링(PBR) 모델을 사용하는 경우가 많습니다. 최근 연구에서는 3D 형상과 재료를 동시에 생성하거나, 확산 모델을 사용하여 기존 객체에 텍스처를 추가하는 방식이 시도되고 있습니다. 그러나 이러한 최적화 과정은 수 시간이 소요될 수 있습니다. 본 연구에서는 단일 이미지에서 자연 조명하에 균일한 재료 속성을 가진 텍스처 객체를 빠르게 생성하는 방법을 제안합니다.

종합적으로, 이 장에서는 3D 재구성의 기존 접근법들을 살펴보고, 본 연구가 어떻게 이러한 문제들을 해결하며 빠르고 고품질의 3D 객체 생성을 달성하는지 설명합니다.

## **3 Method**

![SF3D 개요. SF3D는 Fig. 2의 문제를 해결하기 위해 5가지 새로운 모듈을 도입하여 TripoSR을 개선합니다: 1. 고해상도 트리플레인을 위한 향상된 트랜스포머 (왼쪽 상단); 2. Material Net을 통한 재료 추정 (왼쪽 하단); 3. Light Net을 사용한 명시적인 조명 추정 (오른쪽 하단); 4. 버텍스 오프셋과 노멀을 추정하여 매끄러운 메쉬 추출 (오른쪽 상단); 5. 빠른 UV 언래핑을 포함한 내보내기 파이프라인 (오른쪽).](SF3D%20Stable%20Fast%203D%20Mesh%20Reconstruction%20with%20UV-un%20977078bf645d4d0fa06e1e5fc5cdfb2f/Untitled%202.png)

SF3D 개요. SF3D는 Fig. 2의 문제를 해결하기 위해 5가지 새로운 모듈을 도입하여 TripoSR을 개선합니다: 1. 고해상도 트리플레인을 위한 향상된 트랜스포머 (왼쪽 상단); 2. Material Net을 통한 재료 추정 (왼쪽 하단); 3. Light Net을 사용한 명시적인 조명 추정 (오른쪽 하단); 4. 버텍스 오프셋과 노멀을 추정하여 매끄러운 메쉬 추출 (오른쪽 상단); 5. 빠른 UV 언래핑을 포함한 내보내기 파이프라인 (오른쪽).

![트리플레인 해상도 앨리어싱. 저해상도 트리플레인은 고주파 및 고대비 텍스처에서 격자 모양의 앨리어싱 인공물을 생성합니다. 우리의 방법은 트리플레인 크기를 64²에서 384²로 증가시켜 이러한 텍스처를 더 적은 인공물로 재현할 수 있습니다.](SF3D%20Stable%20Fast%203D%20Mesh%20Reconstruction%20with%20UV-un%20977078bf645d4d0fa06e1e5fc5cdfb2f/Untitled%203.png)

트리플레인 해상도 앨리어싱. 저해상도 트리플레인은 고주파 및 고대비 텍스처에서 격자 모양의 앨리어싱 인공물을 생성합니다. 우리의 방법은 트리플레인 크기를 64²에서 384²로 증가시켜 이러한 텍스처를 더 적은 인공물로 재현할 수 있습니다.

### 3.1 Enhanced Transformer

SF3D는 TripoSR에서 사용된 DINO 대신 DINOv2 네트워크를 사용하여 이미지 토큰을 얻습니다. TripoSR에서 사용된 저해상도(64x64) 트리플레인은 높은 주파수와 대비가 있는 텍스처 패턴에서 눈에 띄는 인공물을 생성하므로, SF3D는 트리플레인 해상도를 증가시켜 이러한 문제를 해결합니다. 그러나 해상도를 단순히 증가시키면 트랜스포머의 복잡도가 증가하기 때문에 PointInfinity에서 영감을 받아 고해상도 트리플레인을 출력하는 향상된 트랜스포머 네트워크를 제안합니다. 이를 통해 96x96 해상도의 트리플레인을 생성하며, 채널 수를 증가시켜 최종적으로 384x384 해상도의 트리플레인을 얻습니다.

### 3.2 Material Estimation

SF3D는 반사 특성을 가진 객체의 시각적 품질을 향상시키기 위해 재료 속성을 예측합니다. 공간적으로 변화하는 재료 속성을 예측하는 것은 매우 도전적인 문제이므로, SF3D는 전체 객체에 대해 단일 금속성과 거칠기 값을 예측합니다. 이를 위해 'Material Net'을 도입하여 입력 이미지로부터 재료 값을 예측합니다. Material Net은 이미지 인코더와 MLP 네트워크로 구성되어 있으며, 베타 분포의 매개변수를 예측하여 불확실성을 허용하는 방식으로 훈련을 안정화합니다.

### 3.3 Illumination Modeling

입력 이미지의 조명을 명시적으로 추정하여 다양한 그림자 효과를 처리합니다. 이를 위해 'Light Net'을 도입하여 예측된 트리플레인에서 구형 가우시안(SG) 조명 지도를 예측합니다. Light Net은 트리플레인을 CNN 레이어를 통해 처리하고, SG 조명 지도의 진폭 값을 출력합니다. 훈련 과정에서 조명 비조정 손실(LDemod)을 도입하여, 완전히 흰색 알베도의 객체에서 조명이 입력 이미지와 일치하도록 보장합니다.

### 3.4 Mesh Extraction and Refinement

SF3D는 예측된 트리플레인을 차별화된 테트라헤드론 행진법(DMTet)을 사용하여 메쉬로 변환합니다. DMTet은 기존의 Marching Cubes(MC)에서 발생하는 계단 현상을 줄입니다. 두 개의 MLP 헤드를 도입하여 메쉬를 정제하며, 버텍스 오프셋과 표면 노멀을 예측합니다. 버텍스 오프셋은 테트라헤드론 격자의 인공물을 줄이고, 표면 노멀은 평면 메쉬 삼각형에 세부 사항을 추가합니다. 훈련 초기 단계에서는 구면 선형 보간법(slerp)을 사용하여 안정성을 확보합니다.

![내보내기 파이프라인. 우리의 내보내기 과정은 메쉬로 시작하여 UV 언래핑, 점유 및 월드 위치 베이킹, 재료 쿼리, UV 섬 여백으로 이어집니다.](SF3D%20Stable%20Fast%203D%20Mesh%20Reconstruction%20with%20UV-un%20977078bf645d4d0fa06e1e5fc5cdfb2f/Untitled%204.png)

내보내기 파이프라인. 우리의 내보내기 과정은 메쉬로 시작하여 UV 언래핑, 점유 및 월드 위치 베이킹, 재료 쿼리, UV 섬 여백으로 이어집니다.

![UV 언래핑. 우리의 UV 언래핑 기술은 투영 매핑을 사용하여 각 면이 독립적으로 투영을 선택할 수 있도록 하여 병렬 처리를 용이하게 합니다. 단순한 접근법은 차폐로 인해 다른 버텍스에 동일한 UV 좌표를 할당하는 문제를 초래할 수 있습니다. 우리는 2D 매핑된 표면에서의 차폐로 인한 잠재적 중복을 식별하고 이를 UV 아틀라스 내의 다른 영역으로 재배치합니다. 나머지 영역은 UV 아틀라스의 오른쪽 하단에 배치됩니다. 이 방법은 왜곡을 최소화하고 대부분의 표면을 연결된 영역에 보존합니다.](SF3D%20Stable%20Fast%203D%20Mesh%20Reconstruction%20with%20UV-un%20977078bf645d4d0fa06e1e5fc5cdfb2f/Untitled%205.png)

UV 언래핑. 우리의 UV 언래핑 기술은 투영 매핑을 사용하여 각 면이 독립적으로 투영을 선택할 수 있도록 하여 병렬 처리를 용이하게 합니다. 단순한 접근법은 차폐로 인해 다른 버텍스에 동일한 UV 좌표를 할당하는 문제를 초래할 수 있습니다. 우리는 2D 매핑된 표면에서의 차폐로 인한 잠재적 중복을 식별하고 이를 UV 아틀라스 내의 다른 영역으로 재배치합니다. 나머지 영역은 UV 아틀라스의 오른쪽 하단에 배치됩니다. 이 방법은 왜곡을 최소화하고 대부분의 표면을 연결된 영역에 보존합니다.

### 3.5 Fast UV-Unwrapping and Export

최종 3D 메쉬와 UV 아틀라스를 출력하기 위해 빠른 UV 언래핑 및 내보내기 파이프라인을 도입합니다. 큐브 프로젝션 기반의 언래핑 방법을 사용하여 병렬 처리를 통해 UV 언래핑을 가속화합니다. 메쉬 얼굴을 큐브의 각 면에 투영하고, UV 아틀라스에서의 충돌을 감지하여 효율적으로 처리합니다. 또한, 텍스처의 매끄러운 블렌딩을 위해 UV 아틀라스의 경계에 여백을 추가합니다. 최종적으로 GLB 파일로 패킹하여 다양한 응용 프로그램에서 효율적으로 사용할 수 있도록 합니다.

### 3.6 Overall Training and Loss Functions

SF3D의 훈련은 NeRF 태스크로부터 사전 훈련된 모델을 사용하며, 메쉬 렌더링과 SG 기반의 음영 처리를 도입합니다. 큰 배치 크기를 사용하여 수렴을 돕고, 단계별로 해상도를 증가시키며 훈련합니다. 손실 함수는 렌더링, 메쉬 정규화, 음영 처리를 포함하며, 각각의 손실을 결합하여 최종 손실로 정의합니다.

이 장에서는 SF3D의 구성 요소와 각 모듈의 세부 사항을 설명하며, 제안된 방법이 기존의 문제를 어떻게 해결하고 3D 객체 생성의 품질과 속도를 향상시키는지 구체적으로 서술합니다.

## **4 Results**

![GSO 및 OmniObject3D 비교. 우리의 재구성이 세부적인 텍스처와 매끄러운 음영으로 일관된 결과를 생성하는 것을 확인할 수 있습니다.](SF3D%20Stable%20Fast%203D%20Mesh%20Reconstruction%20with%20UV-un%20977078bf645d4d0fa06e1e5fc5cdfb2f/Untitled%206.png)

GSO 및 OmniObject3D 비교. 우리의 재구성이 세부적인 텍스처와 매끄러운 음영으로 일관된 결과를 생성하는 것을 확인할 수 있습니다.

### Datasets

비교를 위해 GSO와 OmniObject3D 데이터셋을 주로 사용하였으며, 각각 278개와 308개의 장면을 선택하여 실험을 진행하였습니다. 각 객체에 대해 16개의 뷰를 렌더링하고, 정면 뷰를 기준으로 조건부 뷰를 선택하였습니다.

### Baselines

단일 이미지로부터 빠른 3D 객체 재구성을 수행하는 최근의 방법들과 비교하였습니다. 비교 대상에는 OpenLRM, TripoSR, LGM, CRM, InstantMesh, ZeroShape가 포함됩니다. 모든 실험은 동일한 프로토콜 하에 진행되었으며, H100 GPU를 사용하여 평가하였습니다.

### Evaluations

런타임 비교를 위해 입력 이미지에서 최종 메쉬까지의 전체 실행 시간을 계산하였습니다. 형상 품질 평가를 위해 Chamfer Distance(CD)와 F-score(FS)를 사용하였으며, 이를 위해 정렬 단계를 수행하였습니다. 정렬 단계에서는 메쉬를 정규화하고, 회전 정렬 테스트와 ICP를 통해 최적의 회전과 변환을 선택하였습니다. 이러한 정렬 후 CD와 FS를 계산하였고, 일부 대칭 객체의 경우 재렌더링 시 여전히 오정렬될 수 있음을 언급합니다.

### Results

Table 1에서는 SF3D가 모든 기준선 모델들보다 CD와 F-score에서 우수한 성능을 보임을 확인할 수 있습니다. 이는 SF3D가 더 적은 폴리곤 수로도 정확한 형상을 재구성할 수 있음을 나타냅니다. Fig. 7의 시각적 비교에서는 SF3D가 더 정확한 형상 재구성을 통해 3D 자산의 시각적 품질도 높음을 보여줍니다. 또한, SF3D는 정밀한 기하학적 세부 사항과 더 세부적인 텍스처를 생성하며, Fig. 8에서는 자연 조명 하에서도 설득력 있는 재료 속성과 알베도를 보여줍니다.

![분해 결과. 여기서 우리는 Polyhaven [74]의 고품질 객체를 사용하여 자연 조명 하에서 렌더링합니다. 이러한 조명은 재료 추정에 매우 도전적이지만, 우리의 모델은 설득력 있는 재조명을 가능하게 하는 합리적인 재료 속성을 추정합니다.](SF3D%20Stable%20Fast%203D%20Mesh%20Reconstruction%20with%20UV-un%20977078bf645d4d0fa06e1e5fc5cdfb2f/Untitled%207.png)

분해 결과. 여기서 우리는 Polyhaven [74]의 고품질 객체를 사용하여 자연 조명 하에서 렌더링합니다. 이러한 조명은 재료 추정에 매우 도전적이지만, 우리의 모델은 설득력 있는 재조명을 가능하게 하는 합리적인 재료 속성을 추정합니다.

![이미지-메쉬 변환 시간 대 재구성 품질. 우리의 방법은 가장 빠른 재구성 방법 중 하나일 뿐만 아니라 매우 정확한 기하학을 생성할 수 있습니다.](SF3D%20Stable%20Fast%203D%20Mesh%20Reconstruction%20with%20UV-un%20977078bf645d4d0fa06e1e5fc5cdfb2f/Untitled%208.png)

이미지-메쉬 변환 시간 대 재구성 품질. 우리의 방법은 가장 빠른 재구성 방법 중 하나일 뿐만 아니라 매우 정확한 기하학을 생성할 수 있습니다.

### Inference Speed vs. Reconstruction Quality

Fig. 9에서는 다양한 기술의 추론 속도와 재구성 품질을 비교합니다. 최상의 성능은 왼쪽 상단 코너에 위치해야 하며, SF3D는 TripoSR보다 약간 느리지만 재구성 정확도는 훨씬 뛰어납니다. 또한, SF3D는 Marching Cubes 인공물이 덜 두드러지며 더 매끄러운 음영을 제공합니다.

### Ablations

Table 2에서는 다양한 베이스라인 모델에 대한 추가 평가를 수행하였습니다. SF3D는 TripoSR을 기반으로 구축되었으며, 메쉬 훈련과 재조명 세부 조정을 추가함으로써 'SF3D w/o Enhanced Transformer'가 TripoSR보다 성능이 향상됨을 보여줍니다. 또한, 고해상도 트리플레인을 사용하여 SF3D는 베이스라인보다 상당히 우수한 성능을 보입니다.

### Limitations and Outlook

Fig. 7(상단 행)에서는 컵의 알베도가 완벽하게 일치하지 않음을 보여줍니다. 이는 LDR 입력에서 어두운 부분이 유용한 정보를 포함하지 않기 때문입니다. 또한, SF3D는 공간적으로 변화하는 재료 속성을 예측하지 못하여 여러 다양한 재료를 포함하는 객체에 대한 유용성이 제한됩니다. SF3D는 명시적인 감독 없이 재료 예측과 조명 제거를 도입하였으며, 이를 실세계 데이터셋에서 훈련할 수 있는 가능성을 남겨두었습니다. 또한, UV 언래핑은 기존 데이터셋을 활용하여 추가 개선이 가능할 것입니다.

이 장에서는 SF3D의 성능을 기존의 모델들과 비교하고, 실험 결과를 통해 SF3D가 속도와 품질 면에서 우수함을 입증합니다.

## **5 Conclusion**

이 논문에서는 단일 이미지로부터 고속으로 텍스처가 적용된 UV 언래핑된 3D 객체를 재구성하는 'Stable Fast 3D' (SF3D) 기법을 제안합니다. SF3D는 기존의 피드포워드 기반 3D 재구성 방법에 여러 가지 건축적 개선을 도입하여, 출력의 기하학적 세부 사항과 텍스처 품질을 크게 향상시킵니다. 다음은 SF3D의 주요 기여와 결과를 요약한 것입니다:

1. **고속 3D 객체 생성**: SF3D는 단일 이미지로부터 0.5초 이내에 고품질의 3D 메쉬를 생성합니다. 이는 기존의 방법들이 수 분이 소요되는 것에 비해 매우 빠른 속도입니다.
2. **고품질 텍스처와 기하학적 세부 사항**: SF3D는 고해상도 트리플레인과 향상된 트랜스포머 네트워크를 통해 더 정밀한 텍스처와 매끄러운 기하학적 세부 사항을 생성합니다.
3. **재료 속성 예측**: SF3D는 객체의 금속성과 거칠기 값을 예측하여 다양한 조명 조건에서도 현실적인 렌더링이 가능하게 합니다.
4. **효율적인 UV 언래핑**: 병렬화 가능한 큐브 프로젝션 기반의 UV 언래핑 기법을 도입하여 빠르고 효율적인 UV 언래핑을 실현합니다.
5. **광원 모델링**: 입력 이미지의 조명을 명시적으로 추정하여 그림자 효과를 제거하고 일관된 조명을 유지합니다.

SF3D는 다양한 응용 프로그램에서 사용할 수 있는 고품질의 3D 객체 메쉬를 빠르게 생성할 수 있는 포괄적인 기법을 제공합니다. 실험 결과, SF3D는 기존의 최첨단 기법들보다 뛰어난 성능을 보였으며, 특히 속도와 품질 면에서 우수한 결과를 입증하였습니다. 앞으로의 연구에서는 실세계 데이터셋을 활용한 추가적인 훈련과 UV 언래핑의 개선을 통해 더욱 향상된 3D 객체 생성 기술을 개발할 수 있을 것입니다.

## 부록

### A1. Enhanced Transformer

SF3D는 고해상도(384×384) 트리플레인을 생성하여 앨리어싱 인공물을 줄이기 위해 트랜스포머 백본을 업그레이드합니다. 그러나 TripoSR에서 트리플레인 토큰의 해상도를 단순히 증가시키는 것은 셀프 어텐션의 이차 복잡성으로 인해 계산적으로 부담이 됩니다. PointInfinity에서 영감을 받아, SF3D는 두 개의 스트림을 사용하는 트랜스포머를 활용하여 토큰 수에 대한 선형 복잡성을 달성합니다. Fig. A1에 나타난 바와 같이, SF3D의 아키텍처는 트리플레인 스트림과 잠재 스트림 두 가지 처리 스트림으로 구성됩니다. 각 두 스트림 유닛에서 잠재 스트림은 크로스 어텐션을 통해 트리플레인 스트림에서 정보를 가져와 일정 크기의 잠재 토큰 집합에 주요 계산을 수행합니다. 처리된 잠재 토큰은 다시 트리플레인 스트림을 업데이트합니다. 전체 아키텍처는 이러한 두 스트림 유닛 네 개로 구성됩니다. 이와 같은 계산적으로 분리된 설계를 통해 트랜스포머는 96×96 해상도와 1024 채널의 트리플레인을 생성할 수 있습니다. 해상도를 더욱 증가시키고 앨리어싱을 줄이기 위해 픽셀 셔플링 연산을 통합하여 트리플레인 해상도를 384×384, 특징 차원을 40으로 향상시킵니다.

![향상된 트랜스포머 아키텍처. 우리의 새로운 백본은 더 높은 해상도의 출력 트리플레인을 생성합니다. 우리는 픽셀 셔플링 [48]을 사용하여 이를 더욱 업스케일합니다. 이는 Fig. 4와 같이 고주파, 고대비 텍스처를 앨리어싱을 줄여 캡처하는 데 도움을 줍니다.](SF3D%20Stable%20Fast%203D%20Mesh%20Reconstruction%20with%20UV-un%20977078bf645d4d0fa06e1e5fc5cdfb2f/Untitled%209.png)

향상된 트랜스포머 아키텍처. 우리의 새로운 백본은 더 높은 해상도의 출력 트리플레인을 생성합니다. 우리는 픽셀 셔플링 [48]을 사용하여 이를 더욱 업스케일합니다. 이는 Fig. 4와 같이 고주파, 고대비 텍스처를 앨리어싱을 줄여 캡처하는 데 도움을 줍니다.

### A2. Image Metrics

이미지 메트릭 계산을 위해 형상 메트릭 계산 파이프라인을 따릅니다. 정규화된 객체를 실제 GT 규모로 다시 스케일링하고, 세밀한 ICP 최적화를 실행하여 미세한 스케일을 조정합니다. 이 변환을 사용하여 메쉬를 렌더링합니다. 고대칭 객체의 경우 텍스처 정렬 불일치가 여전히 발생할 수 있으므로, 최종 자산 재구성의 품질을 평가하기 위한 보조 메트릭으로 이미지 메트릭을 처리합니다. 따라서 이 메트릭은 보충 자료에서만 보고합니다. Table A1은 메인 논문에서 볼 수 있는 렌더링 중 개선된 시각적 품질을 뒷받침합니다.